[
    {
        "id": "2307.02046",
        "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
        "abstract": "With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.",
        "pdfLink": "https://arxiv.org/pdf/2307.02046.pdf",
        "semantic_relevancy_score": 0.3792254328727722
    },
    {
        "id": "2307.02295",
        "title": "Meta-Learning Adversarial Bandit Algorithms",
        "abstract": "We study online meta-learning with bandit feedback, with the goal of\nimproving performance across multiple tasks if they are similar according to\nsome natural similarity measure. As the first to target the adversarial\nonline-within-online partial-information setting, we design meta-algorithms\nthat combine outer learners to simultaneously tune the initialization and other\nhyperparameters of an inner learner for two important cases: multi-armed\nbandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners\ninitialize and set hyperparameters of the Tsallis-entropy generalization of\nExp3, with the task-averaged regret improving if the entropy of the\noptima-in-hindsight is small. For BLO, we learn to initialize and tune online\nmirror descent (OMD) with self-concordant barrier regularizers, showing that\ntask-averaged regret varies directly with an action space-dependent measure\nthey induce. Our guarantees rely on proving that unregularized\nfollow-the-leader combined with two levels of low-dimensional hyperparameter\ntuning is enough to learn a sequence of affine functions of non-Lipschitz and\nsometimes non-convex Bregman divergences bounding the regret of OMD.",
        "pdfLink": "https://arxiv.org/pdf/2307.02295.pdf",
        "semantic_relevancy_score": 0.3526840806007385
    },
    {
        "id": "2307.01204",
        "title": "Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach",
        "abstract": "Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict\nmissing links for unseen entities with few-shot links observed. Previous\nmethods are limited to transductive scenarios, where entities exist in the\nknowledge graphs, so they are unable to handle unseen entities. Therefore,\nrecent inductive methods utilize the sub-graphs around unseen entities to\nobtain the semantics and predict links inductively. However, in the few-shot\nsetting, the sub-graphs are often sparse and cannot provide meaningful\ninductive patterns. In this paper, we propose a novel relational anonymous\nwalk-guided neural process for few-shot inductive link prediction on knowledge\ngraphs, denoted as RawNP. Specifically, we develop a neural process-based\nmethod to model a flexible distribution over link prediction functions. This\nenables the model to quickly adapt to new entities and estimate the uncertainty\nwhen making predictions. To capture general inductive patterns, we present a\nrelational anonymous walk to extract a series of relational motifs from\nfew-shot observations. These motifs reveal the distinctive semantic patterns on\nKGs that support inductive predictions. Extensive experiments on typical\nbenchmark datasets demonstrate that our model derives new state-of-the-art\nperformance.",
        "pdfLink": "https://arxiv.org/pdf/2307.01204.pdf",
        "semantic_relevancy_score": 0.332105815410614
    },
    {
        "id": "2307.07255",
        "title": "Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems",
        "abstract": "Sharing ideas through communication with peers is the primary mode of human\ninteraction. Consequently, extensive research has been conducted in the area of\nconversational AI, leading to an increase in the availability and diversity of\nconversational tasks, datasets, and methods. However, with numerous tasks being\nexplored simultaneously, the current landscape of conversational AI becomes\nfragmented. Therefore, initiating a well-thought-out model for a dialogue agent\ncan pose significant challenges for a practitioner. Towards highlighting the\ncritical ingredients needed for a practitioner to design a dialogue agent from\nscratch, the current study provides a comprehensive overview of the primary\ncharacteristics of a dialogue agent, the supporting tasks, their corresponding\nopen-domain datasets, and the methods used to benchmark these datasets. We\nobserve that different methods have been used to tackle distinct dialogue\ntasks. However, building separate models for each task is costly and does not\nleverage the correlation among the several tasks of a dialogue agent. As a\nresult, recent trends suggest a shift towards building unified foundation\nmodels. To this end, we propose UNIT, a UNified dIalogue dataseT constructed\nfrom conversations of existing datasets for different dialogue tasks capturing\nthe nuances for each of them. We also examine the evaluation strategies used to\nmeasure the performance of dialogue agents and highlight the scope for future\nresearch in the area of conversational AI.",
        "pdfLink": "https://arxiv.org/pdf/2307.07255.pdf",
        "semantic_relevancy_score": 0.3253949284553528
    },
    {
        "id": "2206.08853",
        "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
        "abstract": "Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (this https URL) to promote\nresearch towards the goal of generally capable embodied agents.",
        "pdfLink": "https://arxiv.org/pdf/2206.08853.pdf",
        "semantic_relevancy_score": 0.31665802001953125
    },
    {
        "id": "2307.09721",
        "title": "Multi-Grained Multimodal Interaction Network for Entity Linking",
        "abstract": "Multimodal entity linking (MEL) task, which aims at resolving ambiguous\nmentions to a multimodal knowledge graph, has attracted wide attention in\nrecent years. Though large efforts have been made to explore the complementary\neffect among multiple modalities, however, they may fail to fully absorb the\ncomprehensive expression of abbreviated textual context and implicit visual\nindication. Even worse, the inevitable noisy data may cause inconsistency of\ndifferent modalities during the learning process, which severely degenerates\nthe performance. To address the above issues, in this paper, we propose a novel\nMulti-GraIned Multimodal InteraCtion Network (MIMIC)\\textbf{(MIMIC)} framework for\nsolving the MEL task. Specifically, the unified inputs of mentions and entities\nare first encoded by textual/visual encoders separately, to extract global\ndescriptive features and local detailed features. Then, to derive the\nsimilarity matching score for each mention-entity pair, we device three\ninteraction units to comprehensively explore the intra-modal interaction and\ninter-modal fusion among features of entities and mentions. In particular,\nthree modules, namely the Text-based Global-Local interaction Unit (TGLU),\nVision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based\ninteraction Unit (CMFU) are designed to capture and integrate the fine-grained\nrepresentation lying in abbreviated text and implicit visual cues. Afterwards,\nwe introduce a unit-consistency objective function via contrastive learning to\navoid inconsistency and model degradation. Experimental results on three public\nbenchmark datasets demonstrate that our solution outperforms various\nstate-of-the-art baselines, and ablation studies verify the effectiveness of\ndesigned modules.",
        "pdfLink": "https://arxiv.org/pdf/2307.09721.pdf",
        "semantic_relevancy_score": 0.30923402309417725
    }
]