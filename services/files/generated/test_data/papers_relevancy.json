[
    {
        "id": "2308.11592",
        "title": "UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding",
        "abstract": "In the era of Large Language Models (LLMs), tremendous strides have been made\nin the field of multimodal understanding. However, existing advanced algorithms\nare limited to effectively utilizing the immense representation capabilities\nand rich world knowledge inherent to these large pre-trained models, and the\nbeneficial connections among tasks within the context of text-rich scenarios\nhave not been sufficiently explored. In this work, we introduce UniDoc, a novel\nmultimodal model equipped with text detection and recognition capabilities,\nwhich are deficient in existing approaches. Moreover, UniDoc capitalizes on the\nbeneficial interactions among tasks to enhance the performance of each\nindividual task. To implement UniDoc, we perform unified multimodal instruct\ntuning on the contributed large-scale instruction following datasets.\nQuantitative and qualitative experimental results show that UniDoc sets\nstate-of-the-art scores across multiple challenging benchmarks. To the best of\nour knowledge, this is the first large multimodal model capable of simultaneous\ntext detection, recognition, spotting, and understanding.",
        "pdfLink": "https://arxiv.org/pdf/2308.11592.pdf",
        "metaData": {
            "relevancy": 0.520956951379776
        }
    },
    {
        "id": "2308.11585",
        "title": "Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes",
        "abstract": "In the wake of the explosive growth of machine learning (ML) usage,\nparticularly within the context of emerging Large Language Models (LLMs),\ncomprehending the semantic significance rooted in their internal workings is\ncrucial. While causal analyses focus on defining semantics and its\nquantification, the gradient-based approach is central to explainable AI (XAI),\ntackling the interpretation of the black box. By synergizing these approaches,\nthe exploration of how a model's internal mechanisms illuminate its causal\neffect has become integral for evidence-based decision-making. A parallel line\nof research has revealed that intersectionality - the combinatory impact of\nmultiple demographics of an individual - can be structured in the form of an\nAveraged Treatment Effect (ATE). Initially, this study illustrates that the\nhateful memes detection problem can be formulated as an ATE, assisted by the\nprinciples of intersectionality, and that a modality-wise summarization of\ngradient-based attention attribution scores can delineate the distinct\nbehaviors of three Transformerbased models concerning ATE. Subsequently, we\nshow that the latest LLM LLaMA2 has the ability to disentangle the\nintersectional nature of memes detection in an in-context learning setting,\nwith their mechanistic properties elucidated via meta-gradient, a secondary\nform of gradient. In conclusion, this research contributes to the ongoing\ndialogue surrounding XAI and the multifaceted nature of ML models.",
        "pdfLink": "https://arxiv.org/pdf/2308.11585.pdf",
        "metaData": {
            "relevancy": 0.45635077357292175
        }
    },
    {
        "id": "2308.11432",
        "title": "A Survey on Large Language Model based Autonomous Agents",
        "abstract": "Autonomous agents have long been a prominent research topic in the academic\ncommunity. Previous research in this field often focuses on training agents\nwith limited knowledge within isolated environments, which diverges\nsignificantly from the human learning processes, and thus makes the agents hard\nto achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating autonomous agents based on LLMs. To harness\nthe full potential of LLMs, researchers have devised diverse agent\narchitectures tailored to different applications. In this paper, we present a\ncomprehensive survey of these studies, delivering a systematic review of the\nfield of autonomous agents from a holistic perspective. More specifically, our\nfocus lies in the construction of LLM-based agents, for which we propose a\nunified framework that encompasses a majority of the previous work.\nAdditionally, we provide a summary of the various applications of LLM-based AI\nagents in the domains of social science, natural science, and engineering.\nLastly, we discuss the commonly employed evaluation strategies for LLM-based AI\nagents. Based on the previous studies, we also present several challenges and\nfuture directions in this field. To keep track of this field and continuously\nupdate our survey, we maintain a repository for the related references at\nthis https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11432.pdf",
        "metaData": {
            "relevancy": 0.6144073963165283
        }
    },
    {
        "id": "2308.11339",
        "title": "ProAgent: Building Proactive Cooperative AI with Large Language Models",
        "abstract": "Building AIs with adaptive behaviors in human-AI cooperation stands as a\npivotal focus in AGI research. Current methods for developing cooperative\nagents predominantly rely on learning-based methods, where policy\ngeneralization heavily hinges on past interactions with specific teammates.\nThese approaches constrain the agent's capacity to recalibrate its strategy\nwhen confronted with novel teammates. We propose \\textbf{ProAgent}, a novel\nframework that harnesses large language models (LLMs) to fashion a\n\\textit{pro}active \\textit{agent} empowered with the ability to anticipate\nteammates' forthcoming decisions and formulate enhanced plans for itself.\nProAgent excels at cooperative reasoning with the capacity to dynamically adapt\nits behavior to enhance collaborative efforts with teammates. Moreover, the\nProAgent framework exhibits a high degree of modularity and interpretability,\nfacilitating seamless integration to address a wide array of coordination\nscenarios. Experimental evaluations conducted within the framework of\n\\textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent,\noutperforming five methods based on self-play and population-based training in\ncooperation with AI agents. Further, when cooperating with human proxy models,\nits performance exhibits an average improvement exceeding 10\\% compared to the\ncurrent state-of-the-art, COLE. The advancement was consistently observed\nacross diverse scenarios involving interactions with both AI agents of varying\ncharacteristics and human counterparts. These findings inspire future research\nfor human-robot collaborations. For a hands-on demonstration, please visit\n\\url{this https URL}.",
        "pdfLink": "https://arxiv.org/pdf/2308.11339.pdf",
        "metaData": {
            "relevancy": 0.6004541754722595
        }
    },
    {
        "id": "2308.11234",
        "title": "Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding",
        "abstract": "Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that\nasks us to compute collision-free paths for a team of agents, all moving across\na shared map. Although many works appear on this topic, all current algorithms\nstruggle as the number of agents grows. The principal reason is that existing\napproaches typically plan free-flow optimal paths, which creates congestion. To\ntackle this issue we propose a new approach for MAPF where agents are guided to\ntheir destination by following congestion-avoiding paths. We evaluate the idea\nin two large-scale settings: one-shot MAPF, where each agent has a single\ndestination, and lifelong MAPF, where agents are continuously assigned new\ntasks. For one-shot MAPF we show that our approach substantially improves\nsolution quality. For Lifelong MAPF we report large improvements in overall\nthroughput.",
        "pdfLink": "https://arxiv.org/pdf/2308.11234.pdf",
        "metaData": {
            "relevancy": 0.42457369565963743
        }
    },
    {
        "id": "2308.11224",
        "title": "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis",
        "abstract": "Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nthis https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11224.pdf",
        "metaData": {
            "relevancy": 0.5712207198143006
        }
    },
    {
        "id": "2308.11088",
        "title": "Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response",
        "abstract": "Efficiently obtaining the up-to-date information in the disaster-stricken\narea is the key to successful disaster response. Unmanned aerial vehicles\n(UAVs), workers and cars can collaborate to accomplish sensing tasks, such as\ndata collection, in disaster-stricken areas. In this paper, we explicitly\naddress the route planning for a group of agents, including UAVs, workers, and\ncars, with the goal of maximizing the task completion rate. We propose\nMANF-RL-RP, a heterogeneous multi-agent route planning algorithm that\nincorporates several efficient designs, including global-local dual information\nprocessing and a tailored model structure for heterogeneous multi-agent\nsystems. Global-local dual information processing encompasses the extraction\nand dissemination of spatial features from global information, as well as the\npartitioning and filtering of local information from individual agents.\nRegarding the construction of the model structure for heterogeneous\nmulti-agent, we perform the following work. We design the same data structure\nto represent the states of different agents, prove the Markovian property of\nthe decision-making process of agents to simplify the model structure, and also\ndesign a reasonable reward function to train the model. Finally, we conducted\ndetailed experiments based on the rich simulation data. In comparison to the\nbaseline algorithms, namely Greedy-SC-RP and MANF-DNN-RP, MANF-RL-RP has\nexhibited a significant improvement in terms of task completion rate.",
        "pdfLink": "https://arxiv.org/pdf/2308.11088.pdf",
        "metaData": {
            "relevancy": 0.4220488131046295
        }
    },
    {
        "id": "2308.11071",
        "title": "Neural Amortized Inference for Nested Multi-agent Reasoning",
        "abstract": "Multi-agent interactions, such as communication, teaching, and bluffing,\noften rely on higher-order social inference, i.e., understanding how others\ninfer oneself. Such intricate reasoning can be effectively modeled through\nnested multi-agent reasoning. Nonetheless, the computational complexity\nescalates exponentially with each level of reasoning, posing a significant\nchallenge. However, humans effortlessly perform complex social inferences as\npart of their daily lives. To bridge the gap between human-like inference\ncapabilities and computational limitations, we propose a novel approach:\nleveraging neural networks to amortize high-order social inference, thereby\nexpediting nested multi-agent reasoning. We evaluate our method in two\nchallenging multi-agent interaction domains. The experimental results\ndemonstrate that our method is computationally efficient while exhibiting\nminimal degradation in accuracy.",
        "pdfLink": "https://arxiv.org/pdf/2308.11071.pdf",
        "metaData": {
            "relevancy": 0.5019629836082459
        }
    },
    {
        "id": "2308.11066",
        "title": "CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection",
        "abstract": "Automation of High-Level Context (HLC) reasoning for intelligent systems at\nscale is imperative due to the unceasing accumulation of contextual data in the\nIoT era, the trend of the fusion of data from multi-sources, and the intrinsic\ncomplexity and dynamism of the context-based decision-making process. To\nmitigate this issue, we propose an automatic context reasoning framework\nCSM-H-R, which programmatically combines ontologies and states at runtime and\nthe model-storage phase for attaining the ability to recognize meaningful HLC,\nand the resulting data representation can be applied to different reasoning\ntechniques. Case studies are developed based on an intelligent elevator system\nin a smart campus setting. An implementation of the framework - a CSM Engine,\nand the experiments of translating the HLC reasoning into vector and matrix\ncomputing especially take care of the dynamic aspects of context and present\nthe potentiality of using advanced mathematical and probabilistic models to\nachieve the next level of automation in integrating intelligent systems;\nmeanwhile, privacy protection support is achieved by anonymization through\nlabel embedding and reducing information correlation. The code of this study is\navailable at: this https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11066.pdf",
        "metaData": {
            "relevancy": 0.35072928071022036
        }
    },
    {
        "id": "2308.11032",
        "title": "AI For Fraud Awareness",
        "abstract": "In today's world, with the rise of numerous social platforms, it has become\nrelatively easy for anyone to spread false information and lure people into\ntraps. Fraudulent schemes and traps are growing rapidly in the investment\nworld. Due to this, countries and individuals face huge financial risks. We\npresent an awareness system with the use of machine learning and gamification\ntechniques to educate the people about investment scams and traps. Our system\napplies machine learning techniques to provide a personalized learning\nexperience to the user. The system chooses distinct game-design elements and\nscams from the knowledge pool crafted by domain experts for each individual.\nThe objective of the research project is to reduce inequalities in all\ncountries by educating investors via Active Learning. Our goal is to assist the\nregulators in assuring a conducive environment for a fair, efficient, and\ninclusive capital market. In the paper, we discuss the impact of the problem,\nprovide implementation details, and showcase the potentiality of the system\nthrough preliminary experiments and results.",
        "pdfLink": "https://arxiv.org/pdf/2308.11032.pdf",
        "metaData": {
            "relevancy": 0.33836494088172914
        }
    },
    {
        "id": "2308.11029",
        "title": "RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition",
        "abstract": "Emotion recognition in conversation (ERC) has received increasing attention\nfrom researchers due to its wide range of applications. As conversation has a\nnatural graph structure, numerous approaches used to model ERC based on graph\nconvolutional networks (GCNs) have yielded significant results. However, the\naggregation approach of traditional GCNs suffers from the node information\nredundancy problem, leading to node discriminant information loss.\nAdditionally, single-layer GCNs lack the capacity to capture long-range\ncontextual information from the graph. Furthermore, the majority of approaches\nare based on textual modality or stitching together different modalities,\nresulting in a weak ability to capture interactions between modalities. To\naddress these problems, we present the relational bilevel aggregation graph\nconvolutional network (RBA-GCN), which consists of three modules: the graph\ngeneration module (GGM), similarity-based cluster building module (SCBM) and\nbilevel aggregation module (BiAM). First, GGM constructs a novel graph to\nreduce the redundancy of target node information. Then, SCBM calculates the\nnode similarity in the target node and its structural neighborhood, where noisy\ninformation with low similarity is filtered out to preserve the discriminant\ninformation of the node. Meanwhile, BiAM is a novel aggregation method that can\npreserve the information of nodes during the aggregation process. This module\ncan construct the interaction between different modalities and capture\nlong-range contextual information based on similarity clusters. On both the\nIEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a\n2.17\u223c\\sim5.21\\% improvement over that of the most advanced method.",
        "pdfLink": "https://arxiv.org/pdf/2308.11029.pdf",
        "metaData": {
            "relevancy": 0.4953014194965363
        }
    },
    {
        "id": "2308.10988",
        "title": "ERA*: Enhanced Relaxed A* algorithm for Solving the Shortest Path Problem in Regular Grid Maps",
        "abstract": "This paper introduces a novel algorithm for solving the point-to-point\nshortest path problem in a static regular 8-neighbor connectivity (G8) grid.\nThis algorithm can be seen as a generalization of Hadlock algorithm to G8\ngrids, and is shown to be theoretically equivalent to the relaxed A\u2217A^*\n(RA\u2217RA^*) algorithm in terms of the provided solution's path length, but with\nsubstantial time and memory savings, due to a completely different computation\nstrategy, based on defining a set of lookup matrices. Through an experimental\nstudy on grid maps of various types and sizes (1290 runs on 43 maps), it is\nproven to be 2.25 times faster than RA\u2217RA^* and 17 times faster than the\noriginal A\u2217A^*, in average. Moreover, it is more memory-efficient, since it\ndoes not need to store a G score matrix.",
        "pdfLink": "https://arxiv.org/pdf/2308.10988.pdf",
        "metaData": {
            "relevancy": 0.1865723729133606
        }
    },
    {
        "id": "2308.10974",
        "title": "\"Guinea Pig Trials\" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion",
        "abstract": "Firm competition and collusion involve complex dynamics, particularly when\nconsidering communication among firms. Such issues can be modeled as problems\nof complex systems, traditionally approached through experiments involving\nhuman subjects or agent-based modeling methods. We propose an innovative\nframework called Smart Agent-Based Modeling (SABM), wherein smart agents,\nsupported by GPT-4 technologies, represent firms, and interact with one\nanother. We conducted a controlled experiment to study firm price competition\nand collusion behaviors under various conditions. SABM is more cost-effective\nand flexible compared to conducting experiments with human subjects. Smart\nagents possess an extensive knowledge base for decision-making and exhibit\nhuman-like strategic abilities, surpassing traditional ABM agents. Furthermore,\nsmart agents can simulate human conversation and be personalized, making them\nideal for studying complex situations involving communication. Our results\ndemonstrate that, in the absence of communication, smart agents consistently\nreach tacit collusion, leading to prices converging at levels higher than the\nBertrand equilibrium price but lower than monopoly or cartel prices. When\ncommunication is allowed, smart agents achieve a higher-level collusion with\nprices close to cartel prices. Collusion forms more quickly with communication,\nwhile price convergence is smoother without it. These results indicate that\ncommunication enhances trust between firms, encouraging frequent small price\ndeviations to explore opportunities for a higher-level win-win situation and\nreducing the likelihood of triggering a price war. We also assigned different\npersonas to firms to analyze behavioral differences and tested variant models\nunder diverse market structures. The findings showcase the effectiveness and\nrobustness of SABM and provide intriguing insights into competition and\ncollusion.",
        "pdfLink": "https://arxiv.org/pdf/2308.10974.pdf",
        "metaData": {
            "relevancy": 0.3717810451984406
        }
    },
    {
        "id": "2308.11601",
        "title": "Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models",
        "abstract": "The introduction of the transformer architecture and the self-attention\nmechanism has led to an explosive production of language models trained on\nspecific downstream tasks and data domains. With over 200, 000 models in the\nHugging Face ecosystem, users grapple with selecting and optimizing models to\nsuit multifaceted workflows and data domains while addressing computational,\nsecurity, and recency concerns. There is an urgent need for machine learning\nframeworks that can eliminate the burden of model selection and customization\nand unleash the incredible power of the vast emerging model library for end\nusers. Here, we propose a context-aware routing system, Tryage, that leverages\na language model router for optimal selection of expert models from a model\nlibrary based on analysis of individual input prompts. Inspired by the thalamic\nrouter in the brain, Tryage employs a perceptive router to predict down-stream\nmodel performance on prompts and, then, makes a routing decision using an\nobjective function that integrates performance predictions with user goals and\nconstraints that are incorporated through flags (e.g., model size, model\nrecency). Tryage allows users to explore a Pareto front and automatically\ntrade-off between task accuracy and secondary goals including minimization of\nmodel size, recency, security, verbosity, and readability. Across heterogeneous\ndata sets that include code, text, clinical data, and patents, the Tryage\nframework surpasses Gorilla and GPT3.5 turbo in dynamic model selection\nidentifying the optimal model with an accuracy of 50.9% , compared to 23.6% by\nGPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how\nrouting models can be applied to program and control the behavior of\nmulti-model LLM systems to maximize efficient use of the expanding and evolving\nlanguage model ecosystem.",
        "pdfLink": "https://arxiv.org/pdf/2308.11601.pdf",
        "metaData": {
            "relevancy": 0.5266191005706787
        }
    },
    {
        "id": "2308.11584",
        "title": "Building Emotional Support Chatbots in the Era of LLMs",
        "abstract": "The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.",
        "pdfLink": "https://arxiv.org/pdf/2308.11584.pdf",
        "metaData": {
            "relevancy": 0.6178502202033996
        }
    },
    {
        "id": "2308.11578",
        "title": "Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models",
        "abstract": "After the inception of emotion recognition or affective computing, it has\nincreasingly become an active research topic due to its broad applications.\nOver the past couple of decades, emotion recognition models have gradually\nmigrated from statistically shallow models to neural network-based deep models,\nwhich can significantly boost the performance of emotion recognition models and\nconsistently achieve the best results on different benchmarks. Therefore, in\nrecent years, deep models have always been considered the first option for\nemotion recognition. However, the debut of large language models (LLMs), such\nas ChatGPT, has remarkably astonished the world due to their emerged\ncapabilities of zero/few-shot learning, in-context learning, chain-of-thought,\nand others that are never shown in previous deep models. In the present paper,\nwe comprehensively investigate how the LLMs perform in emotion recognition in\nterms of diverse aspects, including in-context learning, few-short learning,\naccuracy, generalisation, and explanation. Moreover, we offer some insights and\npose other potential challenges, hoping to ignite broader discussions about\nenhancing emotion recognition in the new era of advanced and generalised large\nmodels.",
        "pdfLink": "https://arxiv.org/pdf/2308.11578.pdf",
        "metaData": {
            "relevancy": 0.512370252609253
        }
    },
    {
        "id": "2308.11534",
        "title": "Large Language Model as a User Simulator",
        "abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT conversations, as evidenced by Vicuna. However, while current\nendeavors like Baize and UltraChat aim to auto-generate conversational data due\nto challenges in gathering human participation, they primarily rely on ChatGPT\nto simulate human behaviors based on directives rather than genuine human\nlearning. This results in a limited scope, diminished diversity, and an absence\nof genuine multi-round conversational dynamics. To address the above issues, we\ninnovatively target human questions extracted from genuine human-machine\nconversations as a learning goal and train a user simulator, UserGPT, to\nproduce a high-quality human-centric synthetic conversation dataset, RealChat.\nSubsequently, this dataset trains our assistant model, ReaLM. Experimentally,\nReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise\ncomparison when considering equivalent training set sizes, and manual\nevaluation also shows that our model is highly competitive. Impressively, when\nfine-tuned with the latest LLaMA 2 model, ReaLM secured a leading score of 6.33\nin the MT-Bench, outshining the contemporary same-scale models, including the\nLLaMA-2-7B-chat model. Further in-depth analysis demonstrates the scalability\nand transferability of our approach. A preliminary exploration into the\ninterplay between training set data quality and resultant model performance is\nalso undertaken, laying a robust groundwork for future investigations. The code\nis available at this https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11534.pdf",
        "metaData": {
            "relevancy": 0.566998815536499
        }
    },
    {
        "id": "2308.11530",
        "title": "Furnishing Sound Event Detection with Language Model Abilities",
        "abstract": "Recently, the ability of language models (LMs) has attracted increasing\nattention in visual cross-modality. In this paper, we further explore the\ngeneration capacity of LMs for sound event detection (SED), beyond the visual\ndomain. Specifically, we propose an elegant method that aligns audio features\nand text features to accomplish sound event classification and temporal\nlocation. The framework consists of an acoustic encoder, a contrastive module\nthat align the corresponding representations of the text and audio, and a\ndecoupled language decoder that generates temporal and event sequences from the\naudio characteristic. Compared with conventional works that require complicated\nprocessing and barely utilize limited audio features, our model is more concise\nand comprehensive since language model directly leverage its semantic\ncapabilities to generate the sequences. We investigate different decoupling\nmodules to demonstrate the effectiveness for timestamps capture and event\nclassification. Evaluation results show that the proposed method achieves\naccurate sequences of sound event detection.",
        "pdfLink": "https://arxiv.org/pdf/2308.11530.pdf",
        "metaData": {
            "relevancy": 0.3380977690219879
        }
    },
    {
        "id": "2308.11527",
        "title": "BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction",
        "abstract": "Although deep pre-trained language models have shown promising benefit in a\nlarge set of industrial scenarios, including Click-Through-Rate (CTR)\nprediction, how to integrate pre-trained language models that handle only\ntextual signals into a prediction pipeline with non-textual features is\nchallenging.\nUp to now two directions have been explored to integrate multi-modal inputs\nin fine-tuning of pre-trained language models. One consists of fusing the\noutcome of language models and non-textual features through an aggregation\nlayer, resulting into ensemble framework, where the cross-information between\ntextual and non-textual inputs are only learned in the aggregation layer. The\nsecond one consists of splitting non-textual features into fine-grained\nfragments and transforming the fragments to new tokens combined with textual\nones, so that they can be fed directly to transformer layers in language\nmodels. However, this approach increases the complexity of the learning and\ninference because of the numerous additional tokens.\nTo address these limitations, we propose in this work a novel framework\nBERT4CTR, with the Uni-Attention mechanism that can benefit from the\ninteractions between non-textual and textual features while maintaining low\ntime-costs in training and inference through a dimensionality reduction.\nComprehensive experiments on both public and commercial data demonstrate that\nBERT4CTR can outperform significantly the state-of-the-art frameworks to handle\nmulti-modal inputs and be applicable to CTR prediction.",
        "pdfLink": "https://arxiv.org/pdf/2308.11527.pdf",
        "metaData": {
            "relevancy": 0.49445768594741824
        }
    },
    {
        "id": "2308.11526",
        "title": "Learning Representations on Logs for AIOps",
        "abstract": "AI for IT Operations (AIOps) is a powerful platform that Site Reliability\nEngineers (SREs) use to automate and streamline operational workflows with\nminimal human intervention. Automated log analysis is a critical task in AIOps\nas it provides key insights for SREs to identify and address ongoing faults.\nTasks such as log format detection, log classification, and log parsing are key\ncomponents of automated log analysis. Most of these tasks require supervised\nlearning; however, there are multiple challenges due to limited labelled log\ndata and the diverse nature of log data. Large Language Models (LLMs) such as\nBERT and GPT3 are trained using self-supervision on a vast amount of unlabeled\ndata. These models provide generalized representations that can be effectively\nused for various downstream tasks with limited labelled data. Motivated by the\nsuccess of LLMs in specific domains like science and biology, this paper\nintroduces a LLM for log data which is trained on public and proprietary log\ndata. The results of our experiments demonstrate that the proposed LLM\noutperforms existing models on multiple downstream tasks. In summary, AIOps\npowered by LLMs offers an efficient and effective solution for automating log\nanalysis tasks and enabling SREs to focus on higher-level tasks. Our proposed\nLLM, trained on public and proprietary log data, offers superior performance on\nmultiple downstream tasks, making it a valuable addition to the AIOps platform.",
        "pdfLink": "https://arxiv.org/pdf/2308.11526.pdf",
        "metaData": {
            "relevancy": 0.49967692494392396
        }
    },
    {
        "id": "2308.11521",
        "title": "Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models",
        "abstract": "Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\nThis paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.",
        "pdfLink": "https://arxiv.org/pdf/2308.11521.pdf",
        "metaData": {
            "relevancy": 0.3837821364402771
        }
    },
    {
        "id": "2308.11520",
        "title": "Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis",
        "abstract": "The exponential growth of online social network platforms and applications\nhas led to a staggering volume of user-generated textual content, including\ncomments and reviews. Consequently, users often face difficulties in extracting\nvaluable insights or relevant information from such content. To address this\nchallenge, machine learning and natural language processing algorithms have\nbeen deployed to analyze the vast amount of textual data available online. In\nrecent years, topic modeling techniques have gained significant popularity in\nthis domain. In this study, we comprehensively examine and compare five\nfrequently used topic modeling methods specifically applied to customer\nreviews. The methods under investigation are latent semantic analysis (LSA),\nlatent Dirichlet allocation (LDA), non-negative matrix factorization (NMF),\npachinko allocation model (PAM), Top2Vec, and BERTopic. By practically\ndemonstrating their benefits in detecting important topics, we aim to highlight\ntheir efficacy in real-world scenarios. To evaluate the performance of these\ntopic modeling methods, we carefully select two textual datasets. The\nevaluation is based on standard statistical evaluation metrics such as topic\ncoherence score. Our findings reveal that BERTopic consistently yield more\nmeaningful extracted topics and achieve favorable results.",
        "pdfLink": "https://arxiv.org/pdf/2308.11520.pdf",
        "metaData": {
            "relevancy": 0.41431881189346315
        }
    },
    {
        "id": "2308.11519",
        "title": "Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers",
        "abstract": "Customer reviews play a crucial role in assessing customer satisfaction,\ngathering feedback, and driving improvements for businesses. Analyzing these\nreviews provides valuable insights into customer sentiments, including\ncompliments, comments, and suggestions. Text classification techniques enable\nbusinesses to categorize customer reviews into distinct categories,\nfacilitating a better understanding of customer feedback. However, challenges\nsuch as overfitting and bias limit the effectiveness of a single classifier in\nensuring optimal prediction. This study proposes a novel approach to address\nthese challenges by introducing a stacking ensemble-based multi-text\nclassification method that leverages transformer models. By combining multiple\nsingle transformers, including BERT, ELECTRA, and DistilBERT, as base-level\nclassifiers, and a meta-level classifier based on RoBERTa, an optimal\npredictive model is generated. The proposed stacking ensemble-based multi-text\nclassification method aims to enhance the accuracy and robustness of customer\nreview analysis. Experimental evaluations conducted on a real-world customer\nreview dataset demonstrate the effectiveness and superiority of the proposed\napproach over traditional single classifier models. The stacking ensemble-based\nmulti-text classification method using transformers proves to be a promising\nsolution for businesses seeking to extract valuable insights from customer\nreviews and make data-driven decisions to enhance customer satisfaction and\ndrive continuous improvement.",
        "pdfLink": "https://arxiv.org/pdf/2308.11519.pdf",
        "metaData": {
            "relevancy": 0.3734878182411194
        }
    },
    {
        "id": "2308.11513",
        "title": "TrackFlow: Multi-Object Tracking with Normalizing Flows",
        "abstract": "The field of multi-object tracking has recently seen a renewed interest in\nthe good old schema of tracking-by-detection, as its simplicity and strong\npriors spare it from the complex design and painful babysitting of\ntracking-by-attention approaches. In view of this, we aim at extending\ntracking-by-detection to multi-modal settings, where a comprehensive cost has\nto be computed from heterogeneous information e.g., 2D motion cues, visual\nappearance, and pose estimates. More precisely, we follow a case study where a\nrough estimate of 3D information is also available and must be merged with\nother traditional metrics (e.g., the IoU). To achieve that, recent approaches\nresort to either simple rules or complex heuristics to balance the contribution\nof each cost. However, i) they require careful tuning of tailored\nhyperparameters on a hold-out set, and ii) they imply these costs to be\nindependent, which does not hold in reality. We address these issues by\nbuilding upon an elegant probabilistic formulation, which considers the cost of\na candidate association as the negative log-likelihood yielded by a deep\ndensity estimator, trained to model the conditional joint probability\ndistribution of correct associations. Our experiments, conducted on both\nsimulated and real benchmarks, show that our approach consistently enhances the\nperformance of several tracking-by-detection algorithms.",
        "pdfLink": "https://arxiv.org/pdf/2308.11513.pdf",
        "metaData": {
            "relevancy": 0.3749553918838501
        }
    },
    {
        "id": "2308.11483",
        "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious NLP tasks. However, previous works have shown these models are\nsensitive towards prompt wording, and few-shot demonstrations and their order,\nposing challenges to fair assessment of these models. As these models become\nmore powerful, it becomes imperative to understand and address these\nlimitations. In this paper, we focus on LLMs robustness on the task of\nmultiple-choice questions -- commonly adopted task to study reasoning and\nfact-retrieving capability of LLMs. Investigating the sensitivity of LLMs\ntowards the order of options in multiple-choice questions, we demonstrate a\nconsiderable performance gap of approximately 13% to 75% in LLMs on different\nbenchmarks, when answer options are reordered, even when using demonstrations\nin a few-shot setting. Through a detailed analysis, we conjecture that this\nsensitivity arises when LLMs are uncertain about the prediction between the\ntop-2/3 choices, and specific options placements may favor certain prediction\nbetween those top choices depending on the question caused by positional bias.\nWe also identify patterns in top-2 choices that amplify or mitigate the model's\nbias toward option placement. We found that for amplifying bias, the optimal\nstrategy involves positioning the top two choices as the first and last\noptions. Conversely, to mitigate bias, we recommend placing these choices among\nthe adjacent options. To validate our conjecture, we conduct various\nexperiments and adopt two approaches to calibrate LLMs' predictions, leading to\nup to 8 percentage points improvement across different models and benchmarks.",
        "pdfLink": "https://arxiv.org/pdf/2308.11483.pdf",
        "metaData": {
            "relevancy": 0.5810283780097961
        }
    },
    {
        "id": "2308.11480",
        "title": "Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection",
        "abstract": "Improving the reliability of deployed machine learning systems often involves\ndeveloping methods to detect out-of-distribution (OOD) inputs. However,\nexisting research often narrowly focuses on samples from classes that are\nabsent from the training set, neglecting other types of plausible distribution\nshifts. This limitation reduces the applicability of these methods in\nreal-world scenarios, where systems encounter a wide variety of anomalous\ninputs. In this study, we categorize five distinct types of distribution shifts\nand critically evaluate the performance of recent OOD detection methods on each\nof them. We publicly release our benchmark under the name BROAD (Benchmarking\nResilience Over Anomaly Diversity). Our findings reveal that while these\nmethods excel in detecting unknown classes, their performance is inconsistent\nwhen encountering other types of distribution shifts. In other words, they only\nreliably detect unexpected inputs that they have been specifically designed to\nexpect. As a first step toward broad OOD detection, we learn a generative model\nof existing detection scores with a Gaussian mixture. By doing so, we present\nan ensemble approach that offers a more consistent and comprehensive solution\nfor broad OOD detection, demonstrating superior performance compared to\nexisting methods. Our code to download BROAD and reproduce our experiments is\npublicly available.",
        "pdfLink": "https://arxiv.org/pdf/2308.11480.pdf",
        "metaData": {
            "relevancy": 0.3037732481956482
        }
    },
    {
        "id": "2308.11477",
        "title": "Revisiting column-generation-based matheuristic for learning classification trees",
        "abstract": "Decision trees are highly interpretable models for solving classification\nproblems in machine learning (ML). The standard ML algorithms for training\ndecision trees are fast but generate suboptimal trees in terms of accuracy.\nOther discrete optimization models in the literature address the optimality\nproblem but only work well on relatively small datasets. \\cite{firat2020column}\nproposed a column-generation-based heuristic approach for learning decision\ntrees. This approach improves scalability and can work with large datasets. In\nthis paper, we describe improvements to this column generation approach. First,\nwe modify the subproblem model to significantly reduce the number of\nsubproblems in multiclass classification instances. Next, we show that the\ndata-dependent constraints in the master problem are implied, and use them as\ncutting planes. Furthermore, we describe a separation model to generate data\npoints for which the linear programming relaxation solution violates their\ncorresponding constraints. We conclude by presenting computational results that\nshow that these modifications result in better scalability.",
        "pdfLink": "https://arxiv.org/pdf/2308.11477.pdf",
        "metaData": {
            "relevancy": 0.3233340382575989
        }
    },
    {
        "id": "2308.11473",
        "title": "IT3D: Improved Text-to-3D Generation with Explicit View Synthesis",
        "abstract": "Recent strides in Text-to-3D techniques have been propelled by distilling\nknowledge from powerful large text-to-image diffusion models (LDMs).\nNonetheless, existing Text-to-3D approaches often grapple with challenges such\nas over-saturation, inadequate detailing, and unrealistic outputs. This study\npresents a novel strategy that leverages explicitly synthesized multi-view\nimages to address these issues. Our approach involves the utilization of\nimage-to-image pipelines, empowered by LDMs, to generate posed high-quality\nimages based on the renderings of coarse 3D models. Although the generated\nimages mostly alleviate the aforementioned issues, challenges such as view\ninconsistency and significant content variance persist due to the inherent\ngenerative nature of large diffusion models, posing extensive difficulties in\nleveraging these images effectively. To overcome this hurdle, we advocate\nintegrating a discriminator alongside a novel Diffusion-GAN dual training\nstrategy to guide the training of 3D models. For the incorporated\ndiscriminator, the synthesized multi-view images are considered real data,\nwhile the renderings of the optimized 3D models function as fake data. We\nconduct a comprehensive set of experiments that demonstrate the effectiveness\nof our method over baseline approaches.",
        "pdfLink": "https://arxiv.org/pdf/2308.11473.pdf",
        "metaData": {
            "relevancy": 0.35571265816688535
        }
    },
    {
        "id": "2308.11471",
        "title": "Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)",
        "abstract": "This work targets what we consider to be the foundational step for urban\nairborne robots, a safe landing. Our attention is directed toward what we deem\nthe most crucial aspect of the safe landing perception stack: segmentation. We\npresent a streamlined reactive UAV system that employs visual servoing by\nharnessing the capabilities of open vocabulary image segmentation. This\napproach can adapt to various scenarios with minimal adjustments, bypassing the\nnecessity for extensive data accumulation for refining internal models, thanks\nto its open vocabulary methodology. Given the limitations imposed by local\nauthorities, our primary focus centers on operations originating from altitudes\nof 100 meters. This choice is deliberate, as numerous preceding works have\ndealt with altitudes up to 30 meters, aligning with the capabilities of small\nstereo cameras. Consequently, we leave the remaining 20m to be navigated using\nconventional 3D path planning methods. Utilizing monocular cameras and image\nsegmentation, our findings demonstrate the system's capability to successfully\nexecute landing maneuvers at altitudes as low as 20 meters. However, this\napproach is vulnerable to intermittent and occasionally abrupt fluctuations in\nthe segmentation between frames in a video stream. To address this challenge,\nwe enhance the image segmentation output by introducing what we call a dynamic\nfocus: a masking mechanism that self adjusts according to the current landing\nstage. This dynamic focus guides the control system to avoid regions beyond the\ndrone's safety radius projected onto the ground, thus mitigating the problems\nwith fluctuations. Through the implementation of this supplementary layer, our\nexperiments have reached improvements in the landing success rate of almost\ntenfold when compared to global segmentation. All the source code is open\nsource and available online (this http URL).",
        "pdfLink": "https://arxiv.org/pdf/2308.11471.pdf",
        "metaData": {
            "relevancy": 0.3626211881637573
        }
    },
    {
        "id": "2308.11464",
        "title": "Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning",
        "abstract": "Federated learning (FL) inevitably confronts the challenge of system\nheterogeneity in practical scenarios. To enhance the capabilities of most\nmodel-homogeneous FL methods in handling system heterogeneity, we propose a\ntraining scheme that can extend their capabilities to cope with this challenge.\nIn this paper, we commence our study with a detailed exploration of homogeneous\nand heterogeneous FL settings and discover three key observations: (1) a\npositive correlation between client performance and layer similarities, (2)\nhigher similarities in the shallow layers in contrast to the deep layers, and\n(3) the smoother gradients distributions indicate the higher layer\nsimilarities. Building upon these observations, we propose InCo Aggregation\nthat leverags internal cross-layer gradients, a mixture of gradients from\nshallow and deep layers within a server model, to augment the similarity in the\ndeep layers without requiring additional communication between clients.\nFurthermore, our methods can be tailored to accommodate model-homogeneous FL\nmethods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their\ncapabilities to handle the system heterogeneity. Copious experimental results\nvalidate the effectiveness of InCo Aggregation, spotlighting internal\ncross-layer gradients as a promising avenue to enhance the performance in\nheterogenous FL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11464.pdf",
        "metaData": {
            "relevancy": 0.41288807392120364
        }
    },
    {
        "id": "2308.11462",
        "title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
        "abstract": "The advent of large language models (LLMs) and their adoption by the legal\ncommunity has given rise to the question: what types of legal reasoning can\nLLMs perform? To enable greater study of this question, we present LegalBench:\na collaboratively constructed legal reasoning benchmark consisting of 162 tasks\ncovering six different types of legal reasoning. LegalBench was built through\nan interdisciplinary process, in which we collected tasks designed and\nhand-crafted by legal professionals. Because these subject matter experts took\na leading role in construction, tasks either measure legal reasoning\ncapabilities that are practically useful, or measure reasoning skills that\nlawyers find interesting. To enable cross-disciplinary conversations about LLMs\nin the law, we additionally show how popular legal frameworks for describing\nlegal reasoning -- which distinguish between its many forms -- correspond to\nLegalBench tasks, thus giving lawyers and LLM developers a common vocabulary.\nThis paper describes LegalBench, presents an empirical evaluation of 20\nopen-source and commercial LLMs, and illustrates the types of research\nexplorations LegalBench enables.",
        "pdfLink": "https://arxiv.org/pdf/2308.11462.pdf",
        "metaData": {
            "relevancy": 0.4309988796710968
        }
    },
    {
        "id": "2308.11455",
        "title": "A Survey on Self-Supervised Representation Learning",
        "abstract": "Learning meaningful representations is at the heart of many tasks in the\nfield of modern machine learning. Recently, a lot of methods were introduced\nthat allow learning of image representations without supervision. These\nrepresentations can then be used in downstream tasks like classification or\nobject detection. The quality of these representations is close to supervised\nlearning, while no labeled images are needed. This survey paper provides a\ncomprehensive review of these methods in a unified notation, points out\nsimilarities and differences of these methods, and proposes a taxonomy which\nsets these methods in relation to each other. Furthermore, our survey\nsummarizes the most-recent experimental results reported in the literature in\nform of a meta-study. Our survey is intended as a starting point for\nresearchers and practitioners who want to dive into the field of representation\nlearning.",
        "pdfLink": "https://arxiv.org/pdf/2308.11455.pdf",
        "metaData": {
            "relevancy": 0.3836950123310089
        }
    },
    {
        "id": "2308.11449",
        "title": "Convergence guarantee for consistency models",
        "abstract": "We provide the first convergence guarantees for the Consistency Models (CMs),\na newly emerging type of one-step generative models that can generate\ncomparable samples to those generated by Diffusion Models. Our main result is\nthat, under the basic assumptions on score-matching errors, consistency errors\nand smoothness of the data distribution, CMs can efficiently sample from any\nrealistic data distribution in one step with small W2W_2 error. Our results (1)\nhold for L2L^2-accurate score and consistency assumption (rather than\nL\u221eL^\\infty-accurate); (2) do note require strong assumptions on the data\ndistribution such as log-Sobelev inequality; (3) scale polynomially in all\nparameters; and (4) match the state-of-the-art convergence guarantee for\nscore-based generative models (SGMs). We also provide the result that the\nMultistep Consistency Sampling procedure can further reduce the error comparing\nto one step sampling, which support the original statement of \"Consistency\nModels, Yang Song 2023\". Our result further imply a TV error guarantee when\ntake some Langevin-based modifications to the output distributions.",
        "pdfLink": "https://arxiv.org/pdf/2308.11449.pdf",
        "metaData": {
            "relevancy": 0.30983192324638364
        }
    },
    {
        "id": "2308.11447",
        "title": "Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification",
        "abstract": "Aspect-based sentiment classification is a crucial problem in fine-grained\nsentiment analysis, which aims to predict the sentiment polarity of the given\naspect according to its context. Previous works have made remarkable progress\nin leveraging attention mechanism to extract opinion words for different\naspects. However, a persistent challenge is the effective management of\nsemantic mismatches, which stem from attention mechanisms that fall short in\nadequately aligning opinions words with their corresponding aspect in\nmulti-aspect sentences. To address this issue, we propose a novel\nAspect-oriented Opinion Alignment Network (AOAN) to capture the contextual\nassociation between opinion words and the corresponding aspect. Specifically,\nwe first introduce a neighboring span enhanced module which highlights various\ncompositions of neighboring words and given aspects. In addition, we design a\nmulti-perspective attention mechanism that align relevant opinion information\nwith respect to the given aspect. Extensive experiments on three benchmark\ndatasets demonstrate that our model achieves state-of-the-art results. The\nsource code is available at this https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11447.pdf",
        "metaData": {
            "relevancy": 0.42375812530517576
        }
    },
    {
        "id": "2308.11446",
        "title": "Exploration of Rashomon Set Assists Explanations for Medical Data",
        "abstract": "The machine learning modeling process conventionally culminates in selecting\na single model that maximizes a selected performance metric. However, this\napproach leads to abandoning a more profound analysis of slightly inferior\nmodels. Particularly in medical and healthcare studies, where the objective\nextends beyond predictions to valuable insight generation, relying solely on\nperformance metrics can result in misleading or incomplete conclusions. This\nproblem is particularly pertinent when dealing with a set of models with\nperformance close to maximum one, known as Rashomon set\\textit{Rashomon set}. Such a set\ncan be numerous and may contain models describing the data in a different way,\nwhich calls for comprehensive analysis. This paper introduces a novel process\nto explore Rashomon set models, extending the conventional modeling approach.\nThe cornerstone is the identification of the most different models within the\nRashomon set, facilitated by the introduced \ud835\ude81\ud835\ude8a\ud835\ude9c\ud835\ude91\ud835\ude98\ud835\ude96\ud835\ude98\ud835\ude97_\ud835\ude73\ud835\ude74\ud835\ude83\ud835\ude74\ud835\ude72\ud835\ude83\\texttt{Rashomon_DETECT}\nalgorithm. This algorithm compares profiles illustrating prediction\ndependencies on variable values generated by eXplainable Artificial\nIntelligence (XAI) techniques. To quantify differences in variable effects\namong models, we introduce the Profile Disparity Index (PDI) based on measures\nfrom functional data analysis. To illustrate the effectiveness of our approach,\nwe showcase its application in predicting survival among hemophagocytic\nlymphohistiocytosis (HLH) patients - a foundational case study. Additionally,\nwe benchmark our approach on other medical data sets, demonstrating its\nversatility and utility in various contexts.",
        "pdfLink": "https://arxiv.org/pdf/2308.11446.pdf",
        "metaData": {
            "relevancy": 0.37797945737838745
        }
    },
    {
        "id": "2308.11424",
        "title": "AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block",
        "abstract": "The future of the arts and artificial intelligence (AI) is promising as\ntechnology advances. As the use of AI in design becomes more widespread, art\npractice may not be a human-only art form and could instead become a digitally\nintegrated experience. With enhanced creativity and collaboration, arts and AI\ncould work together towards creating artistic outputs that are visually\nappealing and meet the needs of the artist and viewer. While it is uncertain\nhow far the integration will go, arts and AI will likely influence one another.\nThis workshop pictorial puts forward first-person research that shares\ninteractions between an HCI researcher and AI as they try to escape the\ncreative block. The pictorial paper explores two questions: How can AI support\nartists' creativity, and what does it mean to be explainable in this context?\nHIs, ChatGPT and Midjourney were engaged; the result was a series of\nreflections that require further discussion and explorations in the XAIxArts\ncommunity: Transparency of attribution, the creation process, ethics of asking,\nand inspiration vs copying.",
        "pdfLink": "https://arxiv.org/pdf/2308.11424.pdf",
        "metaData": {
            "relevancy": 0.3429624855518341
        }
    },
    {
        "id": "2308.11421",
        "title": "TurboViT: Generating Fast Vision Transformers via Generative Architecture Search",
        "abstract": "Vision transformers have shown unprecedented levels of performance in\ntackling various visual perception tasks in recent years. However, the\narchitectural and computational complexity of such network architectures have\nmade them challenging to deploy in real-world applications with\nhigh-throughput, low-memory requirements. As such, there has been significant\nresearch recently on the design of efficient vision transformer architectures.\nIn this study, we explore the generation of fast vision transformer\narchitecture designs via generative architecture search (GAS) to achieve a\nstrong balance between accuracy and architectural and computational efficiency.\nThrough this generative architecture search process, we create TurboViT, a\nhighly efficient hierarchical vision transformer architecture design that is\ngenerated around mask unit attention and Q-pooling design patterns. The\nresulting TurboViT architecture design achieves significantly lower\narchitectural computational complexity (>2.47\u00d7\\times smaller than FasterViT-0\nwhile achieving same accuracy) and computational complexity (>3.4\u00d7\\times fewer\nFLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other\nstate-of-the-art efficient vision transformer network architecture designs\nwithin a similar range of accuracy on the ImageNet-1K dataset. Furthermore,\nTurboViT demonstrated strong inference latency and throughput in both\nlow-latency and batch processing scenarios (>3.21\u00d7\\times lower latency and\n>3.18\u00d7\\times higher throughput compared to FasterViT-0 for low-latency\nscenario). These promising results demonstrate the efficacy of leveraging\ngenerative architecture search for generating efficient transformer\narchitecture designs for high-throughput scenarios.",
        "pdfLink": "https://arxiv.org/pdf/2308.11421.pdf",
        "metaData": {
            "relevancy": 0.2366506576538086
        }
    },
    {
        "id": "2308.11419",
        "title": "Tensor Regression",
        "abstract": "Regression analysis is a key area of interest in the field of data analysis\nand machine learning which is devoted to exploring the dependencies between\nvariables, often using vectors. The emergence of high dimensional data in\ntechnologies such as neuroimaging, computer vision, climatology and social\nnetworks, has brought challenges to traditional data representation methods.\nTensors, as high dimensional extensions of vectors, are considered as natural\nrepresentations of high dimensional data. In this book, the authors provide a\nsystematic study and analysis of tensor-based regression models and their\napplications in recent years. It groups and illustrates the existing\ntensor-based regression methods and covers the basics, core ideas, and\ntheoretical characteristics of most tensor-based regression methods. In\naddition, readers can learn how to use existing tensor-based regression methods\nto solve specific regression tasks with multiway data, what datasets can be\nselected, and what software packages are available to start related work as\nsoon as possible. Tensor Regression is the first thorough overview of the\nfundamentals, motivations, popular algorithms, strategies for efficient\nimplementation, related applications, available datasets, and software\nresources for tensor-based regression analysis. It is essential reading for all\nstudents, researchers and practitioners of working on high dimensional data.",
        "pdfLink": "https://arxiv.org/pdf/2308.11419.pdf",
        "metaData": {
            "relevancy": 0.3056244790554047
        }
    },
    {
        "id": "2308.11375",
        "title": "Interpretable Distribution-Invariant Fairness Measures for Continuous Scores",
        "abstract": "Measures of algorithmic fairness are usually discussed in the context of\nbinary decisions. We extend the approach to continuous scores. So far,\nROC-based measures have mainly been suggested for this purpose. Other existing\nmethods depend heavily on the distribution of scores, are unsuitable for\nranking tasks, or their effect sizes are not interpretable. Here, we propose a\ndistributionally invariant version of fairness measures for continuous scores\nwith a reasonable interpretation based on the Wasserstein distance. Our\nmeasures are easily computable and well suited for quantifying and interpreting\nthe strength of group disparities as well as for comparing biases across\ndifferent models, datasets, or time points. We derive a link between the\ndifferent families of existing fairness measures for scores and show that the\nproposed distributionally invariant fairness measures outperform ROC-based\nfairness measures because they are more explicit and can quantify significant\nbiases that ROC-based fairness measures miss. Finally, we demonstrate their\neffectiveness through experiments on the most commonly used fairness benchmark\ndatasets.",
        "pdfLink": "https://arxiv.org/pdf/2308.11375.pdf",
        "metaData": {
            "relevancy": 0.23149891793727875
        }
    },
    {
        "id": "2308.11358",
        "title": "How Much Temporal Long-Term Context is Needed for Action Segmentation?",
        "abstract": "Modeling long-term context in videos is crucial for many fine-grained tasks\nincluding temporal action segmentation. An interesting question that is still\nopen is how much long-term temporal context is needed for optimal performance.\nWhile transformers can model the long-term context of a video, this becomes\ncomputationally prohibitive for long videos. Recent works on temporal action\nsegmentation thus combine temporal convolutional networks with self-attentions\nthat are computed only for a local temporal window. While these approaches show\ngood results, their performance is limited by their inability to capture the\nfull context of a video. In this work, we try to answer how much long-term\ntemporal context is required for temporal action segmentation by introducing a\ntransformer-based model that leverages sparse attention to capture the full\ncontext of a video. We compare our model with the current state of the art on\nthree datasets for temporal action segmentation, namely 50Salads, Breakfast,\nand Assembly101. Our experiments show that modeling the full context of a video\nis necessary to obtain the best performance for temporal action segmentation.",
        "pdfLink": "https://arxiv.org/pdf/2308.11358.pdf",
        "metaData": {
            "relevancy": 0.3906866192817688
        }
    },
    {
        "id": "2308.11356",
        "title": "Semantic RGB-D Image Synthesis",
        "abstract": "Collecting diverse sets of training images for RGB-D semantic image\nsegmentation is not always possible. In particular, when robots need to operate\nin privacy-sensitive areas like homes, the collection is often limited to a\nsmall set of locations. As a consequence, the annotated images lack diversity\nin appearance and approaches for RGB-D semantic image segmentation tend to\noverfit the training data. In this paper, we thus introduce semantic RGB-D\nimage synthesis to address this problem. It requires synthesising a\nrealistic-looking RGB-D image for a given semantic label map. Current\napproaches, however, are uni-modal and cannot cope with multi-modal data.\nIndeed, we show that extending uni-modal approaches to multi-modal data does\nnot perform well. In this paper, we therefore propose a generator for\nmulti-modal data that separates modal-independent information of the semantic\nlayout from the modal-dependent information that is needed to generate an RGB\nand a depth image, respectively. Furthermore, we propose a discriminator that\nensures semantic consistency between the label maps and the generated images\nand perceptual similarity between the real and generated images. Our\ncomprehensive experiments demonstrate that the proposed method outperforms\nprevious uni-modal methods by a large margin and that the accuracy of an\napproach for RGB-D semantic segmentation can be significantly improved by\nmixing real and generated images during training.",
        "pdfLink": "https://arxiv.org/pdf/2308.11356.pdf",
        "metaData": {
            "relevancy": 0.3957142949104309
        }
    },
    {
        "id": "2308.11336",
        "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
        "abstract": "Reinforcement learning serves as a potent tool for modeling dynamic user\ninterests within recommender systems, garnering increasing research attention\nof late. However, a significant drawback persists: its poor data efficiency,\nstemming from its interactive nature. The training of reinforcement\nlearning-based recommender systems demands expensive online interactions to\namass adequate trajectories, essential for agents to learn user preferences.\nThis inefficiency renders reinforcement learning-based recommender systems a\nformidable undertaking, necessitating the exploration of potential solutions.\nRecent strides in offline reinforcement learning present a new perspective.\nOffline reinforcement learning empowers agents to glean insights from offline\ndatasets and deploy learned policies in online settings. Given that recommender\nsystems possess extensive offline datasets, the framework of offline\nreinforcement learning aligns seamlessly. Despite being a burgeoning field,\nworks centered on recommender systems utilizing offline reinforcement learning\nremain limited. This survey aims to introduce and delve into offline\nreinforcement learning within recommender systems, offering an inclusive review\nof existing literature in this domain. Furthermore, we strive to underscore\nprevalent challenges, opportunities, and future pathways, poised to propel\nresearch in this evolving field.",
        "pdfLink": "https://arxiv.org/pdf/2308.11336.pdf",
        "metaData": {
            "relevancy": 0.4990370810031891
        }
    },
    {
        "id": "2308.11331",
        "title": "GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training",
        "abstract": "Cross-modal pre-training has shown impressive performance on a wide range of\ndownstream tasks, benefiting from massive image-text pairs collected from the\nInternet. In practice, online data are growing constantly, highlighting the\nimportance of the ability of pre-trained model to learn from data that is\ncontinuously growing. Existing works on cross-modal pre-training mainly focus\non training a network with fixed architecture. However, it is impractical to\nlimit the model capacity when considering the continuously growing nature of\npre-training data in real-world applications. On the other hand, it is\nimportant to utilize the knowledge in the current model to obtain efficient\ntraining and better performance. To address the above issues, in this paper, we\npropose GrowCLIP, a data-driven automatic model growing algorithm for\ncontrastive language-image pre-training with continuous image-text pairs as\ninput. Specially, we adopt a dynamic growth space and seek out the optimal\narchitecture at each growth step to adapt to online learning scenarios. And the\nshared encoder is proposed in our growth space to enhance the degree of\ncross-modal fusion. Besides, we explore the effect of growth in different\ndimensions, which could provide future references for the design of cross-modal\nmodel architecture. Finally, we employ parameter inheriting with momentum (PIM)\nto maintain the previous knowledge and address the issue of the local minimum\ndilemma. Compared with the existing methods, GrowCLIP improves 2.3% average\ntop-1 accuracy on zero-shot image classification of 9 downstream tasks. As for\nzero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text\nrecall on Flickr30K dataset.",
        "pdfLink": "https://arxiv.org/pdf/2308.11331.pdf",
        "metaData": {
            "relevancy": 0.4877007484436035
        }
    },
    {
        "id": "2308.11302",
        "title": "From Mundane to Meaningful: AI's Influence on Work Dynamics -- evidence from ChatGPT and Stack Overflow",
        "abstract": "This paper illustrates how generative AI could give opportunities for big\nproductivity gains but also opens up questions about the impact of these new\npowerful technologies on the way we work and share knowledge. More\nspecifically, we explore how ChatGPT changed a fundamental aspect of coding:\nproblem-solving. To do so, we exploit the effect of the sudden release of\nChatGPT on the 30th of November 2022 on the usage of the largest online\ncommunity for coders: Stack Overflow. Using quasi-experimental methods\n(Difference-in-Difference), we find a significant drop in the number of\nquestions. In addition, the questions are better documented after the release\nof ChatGPT. Finally, we find evidence that the remaining questions are more\ncomplex. These findings suggest not only productivity gains but also a\nfundamental change in the way we work where routine inquiries are solved by AI\nallowing humans to focus on more complex tasks.",
        "pdfLink": "https://arxiv.org/pdf/2308.11302.pdf",
        "metaData": {
            "relevancy": 0.4626080334186554
        }
    },
    {
        "id": "2308.11291",
        "title": "Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation",
        "abstract": "The quality of a wood log in the wood industry depends heavily on the\npresence of both outer and inner defects, including inner knots that are a\nresult of the growth of tree branches. Today, locating the inner knots require\nthe use of expensive equipment such as X-ray scanners. In this paper, we\naddress the task of predicting the location of inner defects from the outer\nshape of the logs. The dataset is built by extracting both the contours and the\nknots with X-ray measurements. We propose to solve this binary segmentation\ntask by leveraging convolutional recurrent neural networks. Once the neural\nnetwork is trained, inference can be performed from the outer shape measured\nwith cheap devices such as laser profilers. We demonstrate the effectiveness of\nour approach on fir and spruce tree species and perform ablation on the\nrecurrence to demonstrate its importance.",
        "pdfLink": "https://arxiv.org/pdf/2308.11291.pdf",
        "metaData": {
            "relevancy": 0.24607438147068023
        }
    },
    {
        "id": "2308.11290",
        "title": "ShadowNet for Data-Centric Quantum System Learning",
        "abstract": "Understanding the dynamics of large quantum systems is hindered by the curse\nof dimensionality. Statistical learning offers new possibilities in this regime\nby neural-network protocols and classical shadows, while both methods have\nlimitations: the former is plagued by the predictive uncertainty and the latter\nlacks the generalization ability. Here we propose a data-centric learning\nparadigm combining the strength of these two approaches to facilitate diverse\nquantum system learning (QSL) tasks. Particularly, our paradigm utilizes\nclassical shadows along with other easily obtainable information of quantum\nsystems to create the training dataset, which is then learnt by neural networks\nto unveil the underlying mapping rule of the explored QSL problem. Capitalizing\non the generalization power of neural networks, this paradigm can be trained\noffline and excel at predicting previously unseen systems at the inference\nstage, even with few state copies. Besides, it inherits the characteristic of\nclassical shadows, enabling memory-efficient storage and faithful prediction.\nThese features underscore the immense potential of the proposed data-centric\napproach in discovering novel and large-scale quantum systems. For\nconcreteness, we present the instantiation of our paradigm in quantum state\ntomography and direct fidelity estimation tasks and conduct numerical analysis\nup to 60 qubits. Our work showcases the profound prospects of data-centric\nartificial intelligence to advance QSL in a faithful and generalizable manner.",
        "pdfLink": "https://arxiv.org/pdf/2308.11290.pdf",
        "metaData": {
            "relevancy": 0.3406871557235718
        }
    },
    {
        "id": "2308.11277",
        "title": "CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation",
        "abstract": "Motivated by the challenges of the Digital Ancient Near Eastern Studies\n(DANES) community, we develop digital tools for processing cuneiform script\nbeing a 3D script imprinted into clay tablets used for more than three\nmillennia and at least eight major languages. It consists of thousands of\ncharacters that have changed over time and space. Photographs are the most\ncommon representations usable for machine learning, while ink drawings are\nprone to interpretation. Best suited 3D datasets that are becoming available.\nWe created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of\naround 500 annotated tablets. For our novel OCR-like approach to mixed image\ndata, we provide an additional mapping tool for transferring annotations\nbetween 3D renderings and photographs. Our sign localization uses a RepPoints\ndetector to predict the locations of characters as bounding boxes. We use image\ndata from GigaMesh's MSII (curvature, see this https URL) based rendering,\nPhong-shaded 3D models, and photographs as well as illumination augmentation.\nThe results show that using rendered 3D images for sign detection performs\nbetter than other work on photographs. In addition, our approach gives\nreasonably good results for photographs only, while it is best used for mixed\ndatasets. More importantly, the Phong renderings, and especially the MSII\nrenderings, improve the results on photographs, which is the largest dataset on\na global scale.",
        "pdfLink": "https://arxiv.org/pdf/2308.11277.pdf",
        "metaData": {
            "relevancy": 0.28184368908405305
        }
    },
    {
        "id": "2308.11276",
        "title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning",
        "abstract": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity\nof large-scale publicly available music datasets with natural language\ncaptions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),\ncapable of answering music-related questions and generating captions for music\nfiles. Our model utilizes audio representations from a pretrained MERT model to\nextract music features. However, obtaining a suitable dataset for training the\nMU-LLaMA model remains challenging, as existing publicly accessible audio\nquestion answering datasets lack the necessary depth for open-ended music\nquestion answering. To fill this gap, we present a methodology for generating\nquestion-answer pairs from existing audio captioning datasets and introduce the\nMusicQA Dataset designed for answering open-ended music-related questions. The\nexperiments demonstrate that the proposed MU-LLaMA model, trained on our\ndesigned MusicQA dataset, achieves outstanding performance in both music\nquestion answering and music caption generation across various metrics,\noutperforming current state-of-the-art (SOTA) models in both fields and\noffering a promising advancement in the T2M-Gen research field.",
        "pdfLink": "https://arxiv.org/pdf/2308.11276.pdf",
        "metaData": {
            "relevancy": 0.49313631653785706
        }
    },
    {
        "id": "2308.11267",
        "title": "Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes",
        "abstract": "The robust constrained Markov decision process (RCMDP) is a recent\ntask-modelling framework for reinforcement learning that incorporates\nbehavioural constraints and that provides robustness to errors in the\ntransition dynamics model through the use of an uncertainty set. Simulating\nRCMDPs requires computing the worst-case dynamics based on value estimates for\neach state, an approach which has previously been used in the Robust\nConstrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG\nsuch as not robustifying the full constrained objective and the lack of\nincremental learning, this paper introduces two algorithms, called RCPG with\nRobust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies\nRCPG by taking the worst-case dynamics based on the Lagrangian rather than\neither the value or the constraint. Adversarial RCPG also formulates the\nworst-case dynamics based on the Lagrangian but learns this directly and\nincrementally as an adversarial policy through gradient descent rather than\nindirectly and abruptly through constrained optimisation on a sorted value\nlist. A theoretical analysis first derives the Lagrangian policy gradient for\nthe policy optimisation of both proposed algorithms and then the adversarial\npolicy gradient to learn the adversary for Adversarial RCPG. Empirical\nexperiments injecting perturbations in inventory management and safe navigation\ntasks demonstrate the competitive performance of both algorithms compared to\ntraditional RCPG variants as well as non-robust and non-constrained ablations.\nIn particular, Adversarial RCPG ranks among the top two performing algorithms\non all tests.",
        "pdfLink": "https://arxiv.org/pdf/2308.11267.pdf",
        "metaData": {
            "relevancy": 0.4642310976982117
        }
    },
    {
        "id": "2308.11256",
        "title": "Efficient Last-iterate Convergence Algorithms in Solving Games",
        "abstract": "No-regret algorithms are popular for learning Nash equilibrium (NE) in\ntwo-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs).\nMany recent works consider the last-iterate convergence no-regret algorithms.\nAmong them, the two most famous algorithms are Optimistic Gradient Descent\nAscent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA\nhas high per-iteration complexity. OMWU exhibits a lower per-iteration\ncomplexity but poorer empirical performance, and its convergence holds only\nwhen NE is unique. Recent works propose a Reward Transformation (RT) framework\nfor MWU, which removes the uniqueness condition and achieves competitive\nperformance with OMWU. Unfortunately, RT-based algorithms perform worse than\nOGDA under the same number of iterations, and their convergence guarantee is\nbased on the continuous-time feedback assumption, which does not hold in most\nscenarios. To address these issues, we provide a closer analysis of the RT\nframework, which holds for both continuous and discrete-time feedback. We\ndemonstrate that the essence of the RT framework is to transform the problem of\nlearning NE in the original game into a series of strongly convex-concave\noptimization problems (SCCPs). We show that the bottleneck of RT-based\nalgorithms is the speed of solving SCCPs. To improve the their empirical\nperformance, we design a novel transformation method to enable the SCCPs can be\nsolved by Regret Matching+ (RM+), a no-regret algorithm with better empirical\nperformance, resulting in Reward Transformation RM+ (RTRM+). RTRM+ enjoys\nlast-iterate convergence under the discrete-time feedback setting. Using the\ncounterfactual regret decomposition framework, we propose Reward Transformation\nCFR+ (RTCFR+) to extend RTRM+ to EFGs. Experimental results show that our\nalgorithms significantly outperform existing last-iterate convergence\nalgorithms and RM+ (CFR+).",
        "pdfLink": "https://arxiv.org/pdf/2308.11256.pdf",
        "metaData": {
            "relevancy": 0.4675479054450989
        }
    },
    {
        "id": "2308.11254",
        "title": "A survey on bias in machine learning research",
        "abstract": "Current research on bias in machine learning often focuses on fairness, while\noverlooking the roots or causes of bias. However, bias was originally defined\nas a \"systematic error,\" often caused by humans at different stages of the\nresearch process. This article aims to bridge the gap between past literature\non bias in research by providing taxonomy for potential sources of bias and\nerrors in data and models. The paper focus on bias in machine learning\npipelines. Survey analyses over forty potential sources of bias in the machine\nlearning (ML) pipeline, providing clear examples for each. By understanding the\nsources and consequences of bias in machine learning, better methods can be\ndeveloped for its detecting and mitigating, leading to fairer, more\ntransparent, and more accurate ML models.",
        "pdfLink": "https://arxiv.org/pdf/2308.11254.pdf",
        "metaData": {
            "relevancy": 0.26511492133140563
        }
    },
    {
        "id": "2308.11247",
        "title": "Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes",
        "abstract": "Fault diagnosis is an essential component in process supervision. Indeed, it\ndetermines which kind of fault has occurred, given that it has been previously\ndetected, allowing for appropriate intervention. Automatic fault diagnosis\nsystems use machine learning for predicting the fault type from sensor\nreadings. Nonetheless, these models are sensible to changes in the data\ndistributions, which may be caused by changes in the monitored process, such as\nchanges in the mode of operation. This scenario is known as Cross-Domain Fault\nDiagnosis (CDFD). We provide an extensive comparison of single and multi-source\nunsupervised domain adaptation (SSDA and MSDA respectively) algorithms for\nCDFD. We study these methods in the context of the Tennessee-Eastmann Process,\na widely used benchmark in the chemical industry. We show that using multiple\ndomains during training has a positive effect, even when no adaptation is\nemployed. As such, the MSDA baseline improves over the SSDA baseline\nclassification accuracy by 23% on average. In addition, under the\nmultiple-sources scenario, we improve classification accuracy of the no\nadaptation setting by 8.4% on average.",
        "pdfLink": "https://arxiv.org/pdf/2308.11247.pdf",
        "metaData": {
            "relevancy": 0.286491596698761
        }
    },
    {
        "id": "2308.11242",
        "title": "Faster Optimization in S-Graphs Exploiting Hierarchy",
        "abstract": "3D scene graphs hierarchically represent the environment appropriately\norganizing different environmental entities in various layers. Our previous\nwork on situational graphs extends the concept of 3D scene graph to SLAM by\ntightly coupling the robot poses with the scene graph entities, achieving\nstate-of-the-art results. Though, one of the limitations of S-Graphs is\nscalability in really large environments due to the increased graph size over\ntime, increasing the computational complexity.\nTo overcome this limitation in this work we present an initial research of an\nimproved version of S-Graphs exploiting the hierarchy to reduce the graph size\nby marginalizing redundant robot poses and their connections to the\nobservations of the same structural entities. Firstly, we propose the\ngeneration and optimization of room-local graphs encompassing all graph\nentities within a room-like structure. These room-local graphs are used to\ncompress the S-Graphs marginalizing the redundant robot keyframes within the\ngiven room. We then perform windowed local optimization of the compressed graph\nat regular time-distance intervals. A global optimization of the compressed\ngraph is performed every time a loop closure is detected. We show similar\naccuracy compared to the baseline while showing a 39.81% reduction in the\ncomputation time with respect to the baseline.",
        "pdfLink": "https://arxiv.org/pdf/2308.11242.pdf",
        "metaData": {
            "relevancy": 0.3974141776561737
        }
    },
    {
        "id": "2308.11236",
        "title": "ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts",
        "abstract": "In this paper, we argue that the next generation of robots can be commanded\nusing only Language Models' prompts. Every prompt interrogates separately a\nspecific Robotic Modality via its Modality Language Model (MLM). A central Task\nModality mediates the whole communication to execute the robotic mission via a\nLarge Language Model (LLM). This paper gives this new robotic design pattern\nthe name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies\nthis PRM design pattern in building a new robotic framework named\nROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only\ntwo prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural\nlanguage, the visual semantic features related to the task under consideration\n(Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic\nreaction to the visual description (Task Modality). The framework automates all\nthe mechanisms behind these two prompts. The framework enables the robot to\naddress complex real-world scenarios by processing visual data, making informed\ndecisions, and carrying out actions automatically. The framework comprises one\ngeneric vision module and two independent ROS nodes. As a test application, we\nused ROSGPT_Vision to develop CarMate, which monitors the driver's distraction\non the roads and makes real-time vocal notifications to the driver. We showed\nhow ROSGPT_Vision significantly reduced the development cost compared to\ntraditional methods. We demonstrated how to improve the quality of the\napplication by optimizing the prompting strategies, without delving into\ntechnical details. ROSGPT_Vision is shared with the community (link:\nthis https URL) to advance robotic research in this\ndirection and to build more robotic frameworks that implement the PRM design\npattern and enables controlling robots using only prompts.",
        "pdfLink": "https://arxiv.org/pdf/2308.11236.pdf",
        "metaData": {
            "relevancy": 0.5334818124771118
        }
    },
    {
        "id": "2308.11235",
        "title": "Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks",
        "abstract": "Artificial Intelligence (AI) has found wide application, but also poses risks\ndue to unintentional or malicious tampering during deployment. Regular checks\nare therefore necessary to detect and prevent such risks. Fragile watermarking\nis a technique used to identify tampering in AI models. However, previous\nmethods have faced challenges including risks of omission, additional\ninformation transmission, and inability to locate tampering precisely. In this\npaper, we propose a method for detecting tampered parameters and bits, which\ncan be used to detect, locate, and restore parameters that have been tampered\nwith. We also propose an adaptive embedding method that maximizes information\ncapacity while maintaining model accuracy. Our approach was tested on multiple\nneural networks subjected to attacks that modified weight parameters, and our\nresults demonstrate that our method achieved great recovery performance when\nthe modification rate was below 20%. Furthermore, for models where watermarking\nsignificantly affected accuracy, we utilized an adaptive bit technique to\nrecover more than 15% of the accuracy loss of the model.",
        "pdfLink": "https://arxiv.org/pdf/2308.11235.pdf",
        "metaData": {
            "relevancy": 0.27429423332214353
        }
    },
    {
        "id": "2308.11225",
        "title": "On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report",
        "abstract": "Information Technology has become a critical component in various industries,\nleading to an increased focus on software maintenance and monitoring. With the\ncomplexities of modern software systems, traditional maintenance approaches\nhave become insufficient. The concept of AIOps has emerged to enhance\npredictive maintenance using Big Data and Machine Learning capabilities.\nHowever, exploiting AIOps requires addressing several challenges related to the\ncomplexity of data and incident management. Commercial solutions exist, but\nthey may not be suitable for certain companies due to high costs, data\ngovernance issues, and limitations in covering private software. This paper\ninvestigates the feasibility of implementing on-premise AIOps solutions by\nleveraging open-source tools. We introduce a comprehensive AIOps infrastructure\nthat we have successfully deployed in our company, and we provide the rationale\nbehind different choices that we made to build its various components.\nParticularly, we provide insights into our approach and criteria for selecting\na data management system and we explain its integration. Our experience can be\nbeneficial for companies seeking to internally manage their software\nmaintenance processes with a modern AIOps approach.",
        "pdfLink": "https://arxiv.org/pdf/2308.11225.pdf",
        "metaData": {
            "relevancy": 0.39535790085792544
        }
    },
    {
        "id": "2308.11220",
        "title": "Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment",
        "abstract": "The field of women's endocrinology has trailed behind data-driven medical\nsolutions, largely due to concerns over the privacy of patient data. Valuable\ndatapoints about hormone levels or menstrual cycling could expose patients who\nsuffer from comorbidities or terminate a pregnancy, violating their privacy. We\nexplore the application of Federated Learning (FL) to predict the optimal drug\nfor patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal\ndisorder impacting millions of women worldwide, yet it's poorly understood and\nits research is stunted by a lack of patient data. We demonstrate that a\nvariety of FL approaches succeed on a synthetic PCOS patient dataset. Our\nproposed FL models are a tool to access massive quantities of diverse data and\nidentify the most effective treatment option while providing PCOS patients with\nprivacy guarantees.",
        "pdfLink": "https://arxiv.org/pdf/2308.11220.pdf",
        "metaData": {
            "relevancy": 0.23595098555088043
        }
    },
    {
        "id": "2308.11217",
        "title": "Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models",
        "abstract": "Multimodal data, which can comprehensively perceive and recognize the\nphysical world, has become an essential path towards general artificial\nintelligence. However, multimodal large models trained on public datasets often\nunderperform in specific industrial domains. This paper proposes a multimodal\nfederated learning framework that enables multiple enterprises to utilize\nprivate domain data to collaboratively train large models for vertical domains,\nachieving intelligent services across scenarios. The authors discuss in-depth\nthe strategic transformation of federated learning in terms of intelligence\nfoundation and objectives in the era of big model, as well as the new\nchallenges faced in heterogeneous data, model aggregation, performance and cost\ntrade-off, data privacy, and incentive mechanism. The paper elaborates a case\nstudy of leading enterprises contributing multimodal data and expert knowledge\nto city safety operation management , including distributed deployment and\nefficient coordination of the federated learning platform, technical\ninnovations on data quality improvement based on large model capabilities and\nefficient joint fine-tuning approaches. Preliminary experiments show that\nenterprises can enhance and accumulate intelligent capabilities through\nmultimodal model federated learning, thereby jointly creating an smart city\nmodel that provides high-quality intelligent services covering energy\ninfrastructure safety, residential community security, and urban operation\nmanagement. The established federated learning cooperation ecosystem is\nexpected to further aggregate industry, academia, and research resources,\nrealize large models in multiple vertical domains, and promote the large-scale\nindustrial application of artificial intelligence and cutting-edge research on\nmultimodal federated learning.",
        "pdfLink": "https://arxiv.org/pdf/2308.11217.pdf",
        "metaData": {
            "relevancy": 0.41642972230911257
        }
    },
    {
        "id": "2308.11199",
        "title": "ConcatPlexer: Additional Dim1 Batching for Faster ViTs",
        "abstract": "Transformers have demonstrated tremendous success not only in the natural\nlanguage processing (NLP) domain but also the field of computer vision,\nigniting various creative approaches and applications. Yet, the superior\nperformance and modeling flexibility of transformers came with a severe\nincrease in computation costs, and hence several works have proposed methods to\nreduce this burden. Inspired by a cost-cutting method originally proposed for\nlanguage models, Data Multiplexing (DataMUX), we propose a novel approach for\nefficient visual recognition that employs additional dim1 batching (i.e.,\nconcatenation) that greatly improves the throughput with little compromise in\nthe accuracy. We first introduce a naive adaptation of DataMux for vision\nmodels, Image Multiplexer, and devise novel components to overcome its\nweaknesses, rendering our final model, ConcatPlexer, at the sweet spot between\ninference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and\nCIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B/16 with 69.5% and\n83.4% validation accuracy, respectively.",
        "pdfLink": "https://arxiv.org/pdf/2308.11199.pdf",
        "metaData": {
            "relevancy": 0.3982756197452545
        }
    },
    {
        "id": "2308.11194",
        "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data",
        "abstract": "Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained\non datasets consisting of image-caption pairs obtained from the web. However,\nreal-world multimodal datasets, such as healthcare data, are significantly more\ncomplex: each image (e.g. X-ray) is often paired with text (e.g. physician\nreport) that describes many distinct attributes occurring in fine-grained\nregions of the image. We refer to these samples as exhibiting high pairwise\ncomplexity, since each image-text pair can be decomposed into a large number of\nregion-attribute pairings. The extent to which VLMs can capture fine-grained\nrelationships between image regions and textual attributes when trained on such\ndata has not been previously evaluated. The first key contribution of this work\nis to demonstrate through systematic evaluations that as the pairwise\ncomplexity of the training dataset increases, standard VLMs struggle to learn\nregion-attribute relationships, exhibiting performance degradations of up to\n37% on retrieval tasks. In order to address this issue, we introduce ViLLA as\nour second key contribution. ViLLA, which is trained to capture fine-grained\nregion-attribute relationships from complex datasets, involves two components:\n(a) a lightweight, self-supervised mapping model to decompose image-text\nsamples into region-attribute pairs, and (b) a contrastive VLM to learn\nrepresentations from generated region-attribute pairs. We demonstrate with\nexperiments across four domains (synthetic, product, medical, and natural\nimages) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks,\nsuch as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP\npoints on LVIS) and retrieval (up to 14.2 R-Precision points).",
        "pdfLink": "https://arxiv.org/pdf/2308.11194.pdf",
        "metaData": {
            "relevancy": 0.4739574074745178
        }
    },
    {
        "id": "2308.11189",
        "title": "Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries",
        "abstract": "Error prediction in large language models often relies on domain-specific\ninformation. In this paper, we present measures for quantification of error in\nthe response of a large language model based on the diversity of responses to a\ngiven prompt - hence independent of the underlying application. We describe how\nthree such measures - based on entropy, Gini impurity, and centroid distance -\ncan be employed. We perform a suite of experiments on multiple datasets and\ntemperature settings to demonstrate that these measures strongly correlate with\nthe probability of failure. Additionally, we present empirical results\ndemonstrating how these measures can be applied to few-shot prompting,\nchain-of-thought reasoning, and error detection.",
        "pdfLink": "https://arxiv.org/pdf/2308.11189.pdf",
        "metaData": {
            "relevancy": 0.4737818956375122
        }
    },
    {
        "id": "2308.11175",
        "title": "MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation",
        "abstract": "The goal of sequential recommendation (SR) is to predict a user's potential\ninterested items based on her/his historical interaction sequences. Most\nexisting sequential recommenders are developed based on ID features, which,\ndespite their widespread use, often underperform with sparse IDs and struggle\nwith the cold-start problem. Besides, inconsistent ID mappings hinder the\nmodel's transferability, isolating similar recommendation domains that could\nhave been co-optimized. This paper aims to address these issues by exploring\nthe potential of multi-modal information in learning robust and generalizable\nsequence representations. We propose MISSRec, a multi-modal pre-training and\ntransfer learning framework for SR. On the user side, we design a\nTransformer-based encoder-decoder model, where the contextual encoder learns to\ncapture the sequence-level multi-modal synergy while a novel interest-aware\ndecoder is developed to grasp item-modality-interest relations for better\nsequence representation. On the candidate item side, we adopt a dynamic fusion\nmodule to produce user-adaptive item representation, providing more precise\nmatching between users and items. We pre-train the model with contrastive\nlearning objectives and fine-tune it in an efficient manner. Extensive\nexperiments demonstrate the effectiveness and flexibility of MISSRec, promising\nan practical solution for real-world recommendation scenarios.",
        "pdfLink": "https://arxiv.org/pdf/2308.11175.pdf",
        "metaData": {
            "relevancy": 0.5250923275947571
        }
    },
    {
        "id": "2308.11166",
        "title": "Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation",
        "abstract": "Impressive performance on point cloud semantic segmentation has been achieved\nby fully-supervised methods with large amounts of labelled data. As it is\nlabour-intensive to acquire large-scale point cloud data with point-wise\nlabels, many attempts have been made to explore learning 3D point cloud\nsegmentation with limited annotations. Active learning is one of the effective\nstrategies to achieve this purpose but is still under-explored. The most recent\nmethods of this kind measure the uncertainty of each pre-divided region for\nmanual labelling but they suffer from redundant information and require\nadditional efforts for region division. This paper aims at addressing this\nissue by developing a hierarchical point-based active learning strategy.\nSpecifically, we measure the uncertainty for each point by a hierarchical\nminimum margin uncertainty module which considers the contextual information at\nmultiple levels. Then, a feature-distance suppression strategy is designed to\nselect important and representative points for manual labelling. Besides, to\nbetter exploit the unlabelled data, we build a semi-supervised segmentation\nframework based on our active strategy. Extensive experiments on the S3DIS and\nScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and\n100% performance of fully-supervised baseline with only 0.07% and 0.1% training\ndata, respectively, outperforming the state-of-the-art weakly-supervised and\nactive learning methods. The code will be available at\nthis https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.11166.pdf",
        "metaData": {
            "relevancy": 0.3134919047355652
        }
    },
    {
        "id": "2308.11155",
        "title": "xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium",
        "abstract": "Neural force fields (NFFs) have gained prominence in computational chemistry\nas surrogate models, superseding quantum-chemistry calculations in ab initio\nmolecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset\nand its subsequent extension. These datasets predominantly comprise geometries\nfrom the equilibrium region of the ground electronic state potential energy\nsurface, sampling from direct adiabatic dynamics. However, many chemical\nreactions entail significant molecular deformations, notably bond breaking. We\ndemonstrate the constrained distribution of internal coordinates and energies\nin the MD17 datasets, underscoring their inadequacy for representing systems\nundergoing chemical reactions. Addressing this sampling limitation, we\nintroduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived\nfrom non-adiabatic dynamics. This dataset encompasses energies and forces\nascertained from both multireference wave function theory and density\nfunctional theory. Furthermore, its nuclear configuration spaces authentically\ndepict chemical reactions, making xxMD a more chemically relevant dataset. Our\nre-assessment of equivariant models on the xxMD datasets reveals notably higher\nmean absolute errors than those reported for MD17 and its variants. This\nobservation underscores the challenges faced in crafting a generalizable NFF\nmodel with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT\ndatasets are available at \\url{this https URL}.",
        "pdfLink": "https://arxiv.org/pdf/2308.11155.pdf",
        "metaData": {
            "relevancy": 0.2360538899898529
        }
    },
    {
        "id": "2308.11144",
        "title": "Exploring Unsupervised Cell Recognition with Prior Self-activation Maps",
        "abstract": "The success of supervised deep learning models on cell recognition tasks\nrelies on detailed annotations. Many previous works have managed to reduce the\ndependency on labels. However, considering the large number of cells contained\nin a patch, costly and inefficient labeling is still inevitable. To this end,\nwe explored label-free methods for cell recognition. Prior self-activation maps\n(PSM) are proposed to generate pseudo masks as training targets. To be\nspecific, an activation network is trained with self-supervised learning. The\ngradient information in the shallow layers of the network is aggregated to\ngenerate prior self-activation maps. Afterward, a semantic clustering module is\nthen introduced as a pipeline to transform PSMs to pixel-level semantic pseudo\nmasks for downstream tasks. We evaluated our method on two histological\ndatasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection).\nCompared with other fully-supervised and weakly-supervised methods, our method\ncan achieve competitive performance without any manual annotations. Our simple\nbut effective framework can also achieve multi-class cell detection which can\nnot be done by existing unsupervised methods. The results show the potential of\nPSMs that might inspire other research to deal with the hunger for labels in\nmedical area.",
        "pdfLink": "https://arxiv.org/pdf/2308.11144.pdf",
        "metaData": {
            "relevancy": 0.32491021156311034
        }
    },
    {
        "id": "2308.11136",
        "title": "Is There Any Social Principle for LLM-Based Agents?",
        "abstract": "Focus on Large Language Model based agents should involve more than\n\"human-centered\" alignment or application. We argue that more attention should\nbe paid to the agent itself and discuss the potential of social sciences for\nagents.",
        "pdfLink": "https://arxiv.org/pdf/2308.11136.pdf",
        "metaData": {
            "relevancy": 0.43722319006919863
        }
    },
    {
        "id": "2308.11131",
        "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
        "abstract": "With large language models (LLMs) achieving remarkable breakthroughs in\nnatural language processing (NLP) domains, LLM-enhanced recommender systems\nhave received much attention and have been actively explored currently. In this\npaper, we focus on adapting and empowering a pure large language model for\nzero-shot and few-shot recommendation tasks. First and foremost, we identify\nand formulate the lifelong sequential behavior incomprehension problem for LLMs\nin recommendation domains, i.e., LLMs fail to extract useful information from a\ntextual context of long user behavior sequence, even if the length of context\nis far from reaching the context limitation of LLMs. To address such an issue\nand improve the recommendation performance of LLMs, we propose a novel\nframework, namely Retrieval-enhanced Large Language models (ReLLa) for\nrecommendation tasks in both zero-shot and few-shot settings. For zero-shot\nrecommendation, we perform semantic user behavior retrieval (SUBR) to improve\nthe data quality of testing samples, which greatly reduces the difficulty for\nLLMs to extract the essential knowledge from user behavior sequences. As for\nfew-shot recommendation, we further design retrieval-enhanced instruction\ntuning (ReiT) by adopting SUBR as a data augmentation technique for training\nsamples. Specifically, we develop a mixed training dataset consisting of both\nthe original data samples and their retrieval-enhanced counterparts. We conduct\nextensive experiments on a real-world public dataset (i.e., MovieLens-1M) to\ndemonstrate the superiority of ReLLa compared with existing baseline models, as\nwell as its capability for lifelong sequential behavior comprehension.",
        "pdfLink": "https://arxiv.org/pdf/2308.11131.pdf",
        "metaData": {
            "relevancy": 0.5308103203773499
        }
    },
    {
        "id": "2308.11129",
        "title": "Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances",
        "abstract": "Graph transformers need strong inductive biases to derive meaningful\nattention scores. Yet, current proposals rarely address methods capturing\nlonger ranges, hierarchical structures, or community structures, as they appear\nin various graphs such as molecules, social networks, and citation networks. In\nthis paper, we propose a hierarchy-distance structural encoding (HDSE), which\nmodels a hierarchical distance between the nodes in a graph focusing on its\nmulti-level, hierarchical nature. In particular, this yields a framework which\ncan be flexibly integrated with existing graph transformers, allowing for\nsimultaneous application with other positional representations. Through\nextensive experiments on 12 real-world datasets, we demonstrate that our HDSE\nmethod successfully enhances various types of baseline transformers, achieving\nstate-of-the-art empirical performances on 10 benchmark datasets.",
        "pdfLink": "https://arxiv.org/pdf/2308.11129.pdf",
        "metaData": {
            "relevancy": 0.4676427125930786
        }
    },
    {
        "id": "2308.11111",
        "title": "CAME: Contrastive Automated Model Evaluation",
        "abstract": "The Automated Model Evaluation (AutoEval) framework entertains the\npossibility of evaluating a trained machine learning model without resorting to\na labeled testing set. Despite the promise and some decent results, the\nexisting AutoEval methods heavily rely on computing distribution shifts between\nthe unlabelled testing set and the training set. We believe this reliance on\nthe training set becomes another obstacle in shipping this technology to\nreal-world ML development. In this work, we propose Contrastive Automatic Model\nEvaluation (CAME), a novel AutoEval framework that is rid of involving training\nset in the loop. The core idea of CAME bases on a theoretical analysis which\nbonds the model performance with a contrastive loss. Further, with extensive\nempirical validation, we manage to set up a predictable relationship between\nthe two, simply by deducing on the unlabeled/unseen testing set. The resulting\nframework CAME establishes a new SOTA results for AutoEval by surpassing prior\nwork significantly.",
        "pdfLink": "https://arxiv.org/pdf/2308.11111.pdf",
        "metaData": {
            "relevancy": 0.37860485911369324
        }
    },
    {
        "id": "2308.11103",
        "title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models",
        "abstract": "Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.",
        "pdfLink": "https://arxiv.org/pdf/2308.11103.pdf",
        "metaData": {
            "relevancy": 0.35324689745903015
        }
    },
    {
        "id": "2308.11100",
        "title": "Using Early Exits for Fast Inference in Automatic Modulation Classification",
        "abstract": "Automatic modulation classification (AMC) plays a critical role in wireless\ncommunications by autonomously classifying signals transmitted over the radio\nspectrum. Deep learning (DL) techniques are increasingly being used for AMC due\nto their ability to extract complex wireless signal features. However, DL\nmodels are computationally intensive and incur high inference latencies. This\npaper proposes the application of early exiting (EE) techniques for DL models\nused for AMC to accelerate inference. We present and analyze four early exiting\narchitectures and a customized multi-branch training algorithm for this\nproblem. Through extensive experimentation, we show that signals with moderate\nto high signal-to-noise ratios (SNRs) are easier to classify, do not require\ndeep architectures, and can therefore leverage the proposed EE architectures.\nOur experimental results demonstrate that EE techniques can significantly\nreduce the inference speed of deep neural networks without sacrificing\nclassification accuracy. We also thoroughly study the trade-off between\nclassification accuracy and inference time when using these architectures. To\nthe best of our knowledge, this work represents the first attempt to apply\nearly exiting methods to AMC, providing a foundation for future research in\nthis area.",
        "pdfLink": "https://arxiv.org/pdf/2308.11100.pdf",
        "metaData": {
            "relevancy": 0.2913888394832611
        }
    },
    {
        "id": "2308.11093",
        "title": "Video OWL-ViT: Temporally-consistent open-world localization in video",
        "abstract": "We present an architecture and a training recipe that adapts pre-trained\nopen-world image models to localization in videos. Understanding the open\nvisual world (without being constrained by fixed label spaces) is crucial for\nmany real-world vision tasks. Contrastive pre-training on large image-text\ndatasets has recently led to significant improvements for image-level tasks.\nFor more structured tasks involving object localization applying pre-trained\nmodels is more challenging. This is particularly true for video tasks, where\ntask-specific data is limited. We show successful transfer of open-world models\nby building on the OWL-ViT open-vocabulary detection model and adapting it to\nvideo by adding a transformer decoder. The decoder propagates object\nrepresentations recurrently through time by using the output tokens for one\nframe as the object queries for the next. Our model is end-to-end trainable on\nvideo data and enjoys improved temporal consistency compared to\ntracking-by-detection baselines, while retaining the open-world capabilities of\nthe backbone detector. We evaluate our model on the challenging TAO-OW\nbenchmark and demonstrate that open-world capabilities, learned from\nlarge-scale image-text pre-training, can be transferred successfully to\nopen-world localization across diverse videos.",
        "pdfLink": "https://arxiv.org/pdf/2308.11093.pdf",
        "metaData": {
            "relevancy": 0.3879175901412964
        }
    },
    {
        "id": "2308.11070",
        "title": "Temporal-Distributed Backdoor Attack Against Video Based Action Recognition",
        "abstract": "Deep neural networks (DNNs) have achieved tremendous success in various\napplications including video action recognition, yet remain vulnerable to\nbackdoor attacks (Trojans). The backdoor-compromised model will mis-classify to\nthe target class chosen by the attacker when a test instance (from a non-target\nclass) is embedded with a specific trigger, while maintaining high accuracy on\nattack-free instances. Although there are extensive studies on backdoor attacks\nagainst image data, the susceptibility of video-based systems under backdoor\nattacks remains largely unexplored. Current studies are direct extensions of\napproaches proposed for image data, e.g., the triggers are\n\\textbf{independently} embedded within the frames, which tend to be detectable\nby existing defenses. In this paper, we introduce a \\textit{simple} yet\n\\textit{effective} backdoor attack against video data. Our proposed attack,\nadding perturbations in a transformed domain, plants an \\textbf{imperceptible,\ntemporally distributed} trigger across the video frames, and is shown to be\nresilient to existing defensive strategies. The effectiveness of the proposed\nattack is demonstrated by extensive experiments with various well-known models\non two video recognition benchmarks, UCF101 and HMDB51, and a sign language\nrecognition benchmark, Greek Sign Language (GSL) dataset. We delve into the\nimpact of several influential factors on our proposed attack and identify an\nintriguing effect termed \"collateral damage\" through extensive studies.",
        "pdfLink": "https://arxiv.org/pdf/2308.11070.pdf",
        "metaData": {
            "relevancy": 0.2730074167251587
        }
    },
    {
        "id": "2308.11068",
        "title": "Topological Graph Signal Compression",
        "abstract": "Recently emerged Topological Deep Learning (TDL) methods aim to extend\ncurrent Graph Neural Networks (GNN) by naturally processing higher-order\ninteractions, going beyond the pairwise relations and local neighborhoods\ndefined by graph representations. In this paper we propose a novel TDL-based\nmethod for compressing signals over graphs, consisting in two main steps:\nfirst, disjoint sets of higher-order structures are inferred based on the\noriginal signal --by clustering NN datapoints into K\u226aNK\\ll N collections; then,\na topological-inspired message passing gets a compressed representation of the\nsignal within those multi-element sets. Our results show that our framework\nimproves both standard GNN and feed-forward architectures in compressing\ntemporal link-based signals from two real-word Internet Service Provider\nNetworks' datasets --from 30%30\\% up to 90%90\\% better reconstruction errors\nacross all evaluation scenarios--, suggesting that it better captures and\nexploits spatial and temporal correlations over the whole graph-based network\nstructure.",
        "pdfLink": "https://arxiv.org/pdf/2308.11068.pdf",
        "metaData": {
            "relevancy": 0.3826634705066681
        }
    },
    {
        "id": "2308.11052",
        "title": "Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation",
        "abstract": "In recent years, several Weakly Supervised Semantic Segmentation (WS3)\nmethods have been proposed that use class activation maps (CAMs) generated by a\nclassifier to produce pseudo-ground truths for training segmentation models.\nWhile CAMs are good at highlighting discriminative regions (DR) of an image,\nthey are known to disregard regions of the object that do not contribute to the\nclassifier's prediction, termed non-discriminative regions (NDR). In contrast,\nattribution methods such as saliency maps provide an alternative approach for\nassigning a score to every pixel based on its contribution to the\nclassification prediction. This paper provides a comprehensive comparison\nbetween saliencies and CAMs for WS3. Our study includes multiple perspectives\non understanding their similarities and dissimilarities. Moreover, we provide\nnew evaluation metrics that perform a comprehensive assessment of WS3\nperformance of alternative methods w.r.t. CAMs. We demonstrate the\neffectiveness of saliencies in addressing the limitation of CAMs through our\nempirical studies on benchmark datasets. Furthermore, we propose random\ncropping as a stochastic aggregation technique that improves the performance of\nsaliency, making it a strong alternative to CAM for WS3.",
        "pdfLink": "https://arxiv.org/pdf/2308.11052.pdf",
        "metaData": {
            "relevancy": 0.32200402617454527
        }
    },
    {
        "id": "2308.11038",
        "title": "Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances",
        "abstract": "Logistic hubs play a pivotal role in the last-mile delivery distance; even a\nslight increment in distance negatively impacts the business of the e-commerce\nindustry while also increasing its carbon footprint. The growth of this\nindustry, particularly after Covid-19, has further intensified the need for\noptimized allocation of resources in an urban environment. In this study, we\nuse a hybrid approach to optimize the placement of logistic hubs. The approach\nsequentially employs different techniques. Initially, delivery points are\nclustered using K-Means in relation to their spatial locations. The clustering\nmethod utilizes road network distances as opposed to Euclidean distances.\nNon-road network-based approaches have been avoided since they lead to\nerroneous and misleading results. Finally, hubs are located using the P-Median\nmethod. The P-Median method also incorporates the number of deliveries and\npopulation as weights. Real-world delivery data from Muller and Phipps (M&P) is\nused to demonstrate the effectiveness of the approach. Serving deliveries from\nthe optimal hub locations results in the saving of 815 (10%) meters per\ndelivery.",
        "pdfLink": "https://arxiv.org/pdf/2308.11038.pdf",
        "metaData": {
            "relevancy": 0.14247372150421142
        }
    },
    {
        "id": "2308.11034",
        "title": "Digital Twin-Oriented Complex Networked Systems based on Heterogeneous node features and interaction rules",
        "abstract": "This study proposes an extendable modelling framework for Digital\nTwin-Oriented Complex Networked Systems (DT-CNSs) with a goal of generating\nnetworks that faithfully represent real systems. Modelling process focuses on\n(i) features of nodes and (ii) interaction rules for creating connections that\nare built based on individual node's preferences. We conduct experiments on\nsimulation-based DT-CNSs that incorporate various features and rules about\nnetwork growth and different transmissibilities related to an epidemic spread\non these networks. We present a case study on disaster resilience of social\nnetworks given an epidemic outbreak by investigating the infection occurrence\nwithin specific time and social distance. The experimental results show how\ndifferent levels of the structural and dynamics complexities, concerned with\nfeature diversity and flexibility of interaction rules respectively, influence\nnetwork growth and epidemic spread. The analysis revealed that, to achieve\nmaximum disaster resilience, mitigation policies should be targeted at nodes\nwith preferred features as they have higher infection risks and should be the\nfocus of the epidemic control.",
        "pdfLink": "https://arxiv.org/pdf/2308.11034.pdf",
        "metaData": {
            "relevancy": 0.2438647210597992
        }
    },
    {
        "id": "2308.11013",
        "title": "Personalized Event Prediction for Electronic Health Records",
        "abstract": "Clinical event sequences consist of hundreds of clinical events that\nrepresent records of patient care in time. Developing accurate predictive\nmodels of such sequences is of a great importance for supporting a variety of\nmodels for interpreting/classifying the current patient condition, or\npredicting adverse clinical events and outcomes, all aimed to improve patient\ncare. One important challenge of learning predictive models of clinical\nsequences is their patient-specific variability. Based on underlying clinical\nconditions, each patient's sequence may consist of different sets of clinical\nevents (observations, lab results, medications, procedures). Hence, simple\npopulation-wide models learned from event sequences for many different patients\nmay not accurately predict patient-specific dynamics of event sequences and\ntheir differences. To address the problem, we propose and investigate multiple\nnew event sequence prediction models and methods that let us better adjust the\nprediction for individual patients and their specific conditions. The methods\ndeveloped in this work pursue refinement of population-wide models to\nsubpopulations, self-adaptation, and a meta-level model switching that is able\nto adaptively select the model with the best chance to support the immediate\nprediction. We analyze and test the performance of these models on clinical\nevent sequences of patients in MIMIC-III database.",
        "pdfLink": "https://arxiv.org/pdf/2308.11013.pdf",
        "metaData": {
            "relevancy": 0.33119739294052125
        }
    },
    {
        "id": "2308.10999",
        "title": "Eigenvalue-based Incremental Spectral Clustering",
        "abstract": "Our previous experiments demonstrated that subsets collections of (short)\ndocuments (with several hundred entries) share a common normalized in some way\neigenvalue spectrum of combinatorial Laplacian. Based on this insight, we\npropose a method of incremental spectral clustering. The method consists of the\nfollowing steps: (1) split the data into manageable subsets, (2) cluster each\nof the subsets, (3) merge clusters from different subsets based on the\neigenvalue spectrum similarity to form clusters of the entire set. This method\ncan be especially useful for clustering methods of complexity strongly\nincreasing with the size of the data sample,like in case of typical spectral\nclustering. Experiments were performed showing that in fact the clustering and\nmerging the subsets yields clusters close to clustering the entire dataset.",
        "pdfLink": "https://arxiv.org/pdf/2308.10999.pdf",
        "metaData": {
            "relevancy": 0.22054551541805267
        }
    },
    {
        "id": "2308.10997",
        "title": "SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models",
        "abstract": "Modern text-to-image generation models produce high-quality images that are\nboth photorealistic and faithful to the text prompts. However, this quality\ncomes at significant computational cost: nearly all of these models are\niterative and require running inference multiple times with large models. This\niterative process is needed to ensure that different regions of the image are\nnot only aligned with the text prompt, but also compatible with each other. In\nthis work, we propose a light-weight approach to achieving this compatibility\nbetween different regions of an image, using a Markov Random Field (MRF) model.\nThis method is shown to work in conjunction with the recently proposed Muse\nmodel. The MRF encodes the compatibility among image tokens at different\nspatial locations and enables us to significantly reduce the required number of\nMuse prediction steps. Inference with the MRF is significantly cheaper, and its\nparameters can be quickly learned through back-propagation by modeling MRF\ninference as a differentiable neural-network layer. Our full model, SPEGTI,\nuses this proposed MRF model to speed up Muse by 1.5X with no loss in output\nimage quality.",
        "pdfLink": "https://arxiv.org/pdf/2308.10997.pdf",
        "metaData": {
            "relevancy": 0.37074061036109923
        }
    },
    {
        "id": "2308.10959",
        "title": "DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering",
        "abstract": "In this paper, we propose Docprompt for document question answering tasks\nwith powerful zero-shot and few-shot performance. We proposed a novel weakly\nsupervised data generation method, a novel multl-stage training method and a\nnovel understanding model & generation model ensemble method. Experiment\nresults show that the Docprompt model after continue pretrain significantly\noutperforms the existing strong baseline models on document question answering\ntasks. This method greatly improves the delivery efficiency and model\nperformance of document question answering customer projects, reducing\nannotation costs and labor costs. Our demo can be found at\nthis https URL.",
        "pdfLink": "https://arxiv.org/pdf/2308.10959.pdf",
        "metaData": {
            "relevancy": 0.48481454253196715
        }
    },
    {
        "id": "2308.10922",
        "title": "DataVinci: Learning Syntactic and Semantic String Repairs",
        "abstract": "String data is common in real-world datasets: 67.6% of values in a sample of\n1.8 million real Excel spreadsheets from the web were represented as text.\nSystems that successfully clean such string data can have a significant impact\non real users. While prior work has explored errors in string data, proposed\napproaches have often been limited to error detection or require that the user\nprovide annotations, examples, or constraints to fix the errors. Furthermore,\nthese systems have focused independently on syntactic errors or semantic errors\nin strings, but ignore that strings often contain both syntactic and semantic\nsubstrings. We introduce DataVinci, a fully unsupervised string data error\ndetection and repair system. DataVinci learns regular-expression-based patterns\nthat cover a majority of values in a column and reports values that do not\nsatisfy such patterns as data errors. DataVinci can automatically derive edits\nto the data error based on the majority patterns and constraints learned over\nother columns without the need for further user interaction. To handle strings\nwith both syntactic and semantic substrings, DataVinci uses an LLM to abstract\n(and re-concretize) portions of strings that are semantic prior to learning\nmajority patterns and deriving edits. Because not all data can result in\nmajority patterns, DataVinci leverages execution information from an existing\nprogram (which reads the target data) to identify and correct data repairs that\nwould not otherwise be identified. DataVinci outperforms 7 baselines on both\nerror detection and repair when evaluated on 4 existing and new benchmarks.",
        "pdfLink": "https://arxiv.org/pdf/2308.10922.pdf",
        "metaData": {
            "relevancy": 0.4239443004131317
        }
    },
    {
        "id": "2308.10918",
        "title": "Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge",
        "abstract": "Graph anomaly detection has attracted considerable attention in recent years.\nThis paper introduces a novel approach that leverages metapath-based\nsemi-supervised learning, addressing the limitations of previous methods. We\npresent a new framework, Metapath-based Semi-supervised Anomaly Detection\n(MSAD), incorporating GCN layers in both the encoder and decoder to efficiently\npropagate context information between abnormal and normal nodes. The design of\nmetapath-based context information and a specifically crafted anomaly community\nenhance the process of learning differences in structures and attributes, both\nglobally and locally. Through a comprehensive set of experiments conducted on\nseven real-world networks, this paper demonstrates the superiority of the MSAD\nmethod compared to state-of-the-art techniques. The promising results of this\nstudy pave the way for future investigations, focusing on the optimization and\nanalysis of metapath patterns to further enhance the effectiveness of anomaly\ndetection on attributed networks.",
        "pdfLink": "https://arxiv.org/pdf/2308.10918.pdf",
        "metaData": {
            "relevancy": 0.3745484709739685
        }
    },
    {
        "id": "2308.10916",
        "title": "Diffusion Model as Representation Learner",
        "abstract": "Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive\nresults on various generative tasks.Despite its promises, the learned\nrepresentations of pre-trained DPMs, however, have not been fully understood.\nIn this paper, we conduct an in-depth investigation of the representation power\nof DPMs, and propose a novel knowledge transfer method that leverages the\nknowledge acquired by generative DPMs for recognition tasks. Our study begins\nby examining the feature space of DPMs, revealing that DPMs are inherently\ndenoising autoencoders that balance the representation learning with\nregularizing model capacity. To this end, we introduce a novel knowledge\ntransfer paradigm named RepFusion. Our paradigm extracts representations at\ndifferent time steps from off-the-shelf DPMs and dynamically employs them as\nsupervision for student networks, in which the optimal time is determined\nthrough reinforcement learning. We evaluate our approach on several image\nclassification, semantic segmentation, and landmark detection benchmarks, and\ndemonstrate that it outperforms state-of-the-art methods. Our results uncover\nthe potential of DPMs as a powerful tool for representation learning and\nprovide insights into the usefulness of generative models beyond sample\ngeneration. The code is available at\n\\url{this https URL}.",
        "pdfLink": "https://arxiv.org/pdf/2308.10916.pdf",
        "metaData": {
            "relevancy": 0.46288915276527404
        }
    },
    {
        "id": "2308.10912",
        "title": "Explaining Emergence",
        "abstract": "Emergence is a pregnant property in various fields. It is the fact for a\nphenomenon to appear surprisingly and to be such that it seems at first sight\nthat it is not possible to predict its apparition. That is the reason why it\nhas often been said that emergence is a subjective property relative to the\nobserver. Some mathematical systems having very simple and deterministic rules\nnevertheless show emergent behavior. Studying these systems shed a new light on\nthe subject and allows to define a new concept, computational irreducibility,\nwhich deals with behaviors that even though they are totally deterministic\ncannot be predicted without simulating them. Computational irreducibility is\nthen a key for understanding emergent phenomena from an objective point of view\nthat does not need the mention of any observer.",
        "pdfLink": "https://arxiv.org/pdf/2308.10912.pdf",
        "metaData": {
            "relevancy": 0.22418230175971984
        }
    },
    {
        "id": "2308.10910",
        "title": "Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction",
        "abstract": "While multi-modal learning has been widely used for MRI reconstruction, it\nrelies on paired multi-modal data which is difficult to acquire in real\nclinical scenarios. Especially in the federated setting, the common situation\nis that several medical institutions only have single-modal data, termed the\nmodality missing issue. Therefore, it is infeasible to deploy a standard\nfederated learning framework in such conditions. In this paper, we propose a\nnovel communication-efficient federated learning framework, namely Fed-PMG, to\naddress the missing modality challenge in federated multi-modal MRI\nreconstruction. Specifically, we utilize a pseudo modality generation mechanism\nto recover the missing modality for each single-modal client by sharing the\ndistribution information of the amplitude spectrum in frequency space. However,\nthe step of sharing the original amplitude spectrum leads to heavy\ncommunication costs. To reduce the communication cost, we introduce a\nclustering scheme to project the set of amplitude spectrum into finite cluster\ncentroids, and share them among the clients. With such an elaborate design, our\napproach can effectively complete the missing modality within an acceptable\ncommunication cost. Extensive experiments demonstrate that our proposed method\ncan attain similar performance with the ideal scenario, i.e., all clients have\nthe full set of modalities. The source code will be released.",
        "pdfLink": "https://arxiv.org/pdf/2308.10910.pdf",
        "metaData": {
            "relevancy": 0.34380602836608887
        }
    },
    {
        "id": "2308.10905",
        "title": "Analyzing Quantization in TVM",
        "abstract": "There has been many papers in academic literature on quantizing weight\ntensors in deep learning models to reduce inference latency and memory\nfootprint. TVM also has the ability to quantize weights and support low-bit\ncomputations. Although quantization is typically expected to improve inference\ntime, in TVM, the performance of 8-bit quantization does not meet the\nexpectations. Typically, when applying 8-bit quantization to a deep learning\nmodel, it is usually expected to achieve around 50% of the full-precision\ninference time. However, in this particular case, not only does the quantized\nversion fail to achieve the desired performance boost, but it actually performs\nworse, resulting in an inference time that is about 2 times as slow as the\nnon-quantized version. In this project, we thoroughly investigate the reasons\nbehind the underperformance and assess the compatibility and optimization\nopportunities of 8-bit quantization in TVM. We discuss the optimization of two\ndifferent types of tasks: computation-bound and memory-bound, and provide a\ndetailed comparison of various optimization techniques in TVM. Through the\nidentification of performance issues, we have successfully improved\nquantization by addressing a bug in graph building. Furthermore, we analyze\nmultiple optimization strategies to achieve the optimal quantization result.\nThe best experiment achieves 163.88% improvement compared with the TVM compiled\nbaseline in inference time for the compute-bound task and 194.98% for the\nmemory-bound task.",
        "pdfLink": "https://arxiv.org/pdf/2308.10905.pdf",
        "metaData": {
            "relevancy": 0.35439454913139345
        }
    },
    {
        "id": "2308.10807",
        "title": "DynED: Dynamic Ensemble Diversification in Data Stream Classification",
        "abstract": "Ensemble methods are commonly used in classification due to their remarkable\nperformance. Achieving high accuracy in a data stream environment is a\nchallenging task considering disruptive changes in the data distribution, also\nknown as concept drift. A greater diversity of ensemble components is known to\nenhance prediction accuracy in such settings. Despite the diversity of\ncomponents within an ensemble, not all contribute as expected to its overall\nperformance. This necessitates a method for selecting components that exhibit\nhigh performance and diversity. We present a novel ensemble construction and\nmaintenance approach based on MMR (Maximal Marginal Relevance) that dynamically\ncombines the diversity and prediction accuracy of components during the process\nof structuring an ensemble. The experimental results on both four real and 11\nsynthetic datasets demonstrate that the proposed approach (DynED) provides a\nhigher average mean accuracy compared to the five state-of-the-art baselines.",
        "pdfLink": "https://arxiv.org/pdf/2308.10807.pdf",
        "metaData": {
            "relevancy": 0.265998899936676
        }
    },
    {
        "id": "2307.14071",
        "title": "Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching",
        "abstract": "Correlation based stereo matching has achieved outstanding performance, which\npursues cost volume between two feature maps. Unfortunately, current methods\nwith a fixed model do not work uniformly well across various datasets, greatly\nlimiting their real-world applicability. To tackle this issue, this paper\nproposes a new perspective to dynamically calculate correlation for robust\nstereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module\nis introduced to robustly adapt the same model for different scenarios.\nSpecifically, a variance-based uncertainty estimation is employed to adaptively\nadjust the sampling area during warping operation. Additionally, we improve the\ntraditional non-parametric warping with learnable parameters, such that the\nposition-specific weights can be learned. We show that by empowering the\nrecurrent network with the UGAC module, stereo matching can be exploited more\nrobustly and effectively. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury\ndatasets when employing the same fixed model over these datasets without any\nretraining procedure. To target real-time applications, we further design a\nlightweight model based on UGAC, which also outperforms other methods over\nKITTI benchmarks with only 0.6 M parameters.",
        "pdfLink": "https://arxiv.org/pdf/2307.14071.pdf",
        "metaData": {
            "relevancy": 0.2791115134954453
        }
    }
]