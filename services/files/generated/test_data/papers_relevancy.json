[
    {
        "id": "2309.00422",
        "title": "Declarative Reasoning on Explanations Using Constraint Logic Programming",
        "abstract": "Explaining opaque Machine Learning (ML) models is an increasingly relevant\nproblem. Current explanation in AI (XAI) methods suffer several shortcomings,\namong others an insufficient incorporation of background knowledge, and a lack\nof abstraction and interactivity with the user. We propose REASONX, an\nexplanation method based on Constraint Logic Programming (CLP). REASONX can\nprovide declarative, interactive explanations for decision trees, which can be\nthe ML models under analysis or global/local surrogate models of any black-box\nmodel. Users can express background or common sense knowledge using linear\nconstraints and MILP optimization over features of factual and contrastive\ninstances, and interact with the answer constraints at different levels of\nabstraction through constraint projection. We present here the architecture of\nREASONX, which consists of a Python layer, closer to the user, and a CLP layer.\nREASONX's core execution engine is a Prolog meta-program with declarative\nsemantics in terms of logic theories.",
        "pdfLink": "https://arxiv.org/pdf/2309.00422.pdf",
        "metaData": {
            "relevancy": 0.4529836177825928
        }
    },
    {
        "id": "2309.00357",
        "title": "Discrete Versus Continuous Algorithms in Dynamics of Affective Decision Making",
        "abstract": "The dynamics of affective decision making is considered for an intelligent\nnetwork composed of agents with different types of memory: long-term and\nshort-term memory. The consideration is based on probabilistic affective\ndecision theory, which takes into account the rational utility of alternatives\nas well as the emotional alternative attractiveness. The objective of this\npaper is the comparison of two multistep operational algorithms of the\nintelligent network: one based on discrete dynamics and the other on continuous\ndynamics. By means of numerical analysis, it is shown that, depending on the\nnetwork parameters, the characteristic probabilities for continuous and\ndiscrete operations can exhibit either close or drastically different behavior.\nThus, depending on which algorithm is employed, either discrete or continuous,\ntheoretical predictions can be rather different, which does not allow for a\nuniquely defined description of practical problems. This finding is important\nfor understanding which of the algorithms is more appropriate for the correct\nanalysis of decision-making tasks. A discussion is given, revealing that the\ndiscrete operation seems to be more realistic for describing intelligent\nnetworks as well as affective artificial intelligence.",
        "pdfLink": "https://arxiv.org/pdf/2309.00357.pdf",
        "metaData": {
            "relevancy": 0.30016403198242186
        }
    },
    {
        "id": "2309.00317",
        "title": "A Text-based Approach For Link Prediction on Wikipedia Articles",
        "abstract": "This paper present our work in the DSAA 2023 Challenge about Link Prediction\nfor Wikipedia Articles. We use traditional machine learning models with POS\ntags (part-of-speech tags) features extracted from text to train the\nclassification model for predicting whether two nodes has the link. Then, we\nuse these tags to test on various machine learning models. We obtained the\nresults by F1 score at 0.99999 and got 7th place in the competition. Our source\ncode is publicly available at this link:\nthis https URL",
        "pdfLink": "https://arxiv.org/pdf/2309.00317.pdf",
        "metaData": {
            "relevancy": 0.40424844622612
        }
    },
    {
        "id": "2309.00306",
        "title": "On the Aggregation of Rules for Knowledge Graph Completion",
        "abstract": "Rule learning approaches for knowledge graph completion are efficient,\ninterpretable and competitive to purely neural models. The rule aggregation\nproblem is concerned with finding one plausibility score for a candidate fact\nwhich was simultaneously predicted by multiple rules. Although the problem is\nubiquitous, as data-driven rule learning can result in noisy and large\nrulesets, it is underrepresented in the literature and its theoretical\nfoundations have not been studied before in this context. In this work, we\ndemonstrate that existing aggregation approaches can be expressed as marginal\ninference operations over the predicting rules. In particular, we show that the\ncommon Max-aggregation strategy, which scores candidates based on the rule with\nthe highest confidence, has a probabilistic interpretation. Finally, we propose\nan efficient and overlooked baseline which combines the previous strategies and\nis competitive to computationally more expensive approaches.",
        "pdfLink": "https://arxiv.org/pdf/2309.00306.pdf",
        "metaData": {
            "relevancy": 0.4675256609916687
        }
    },
    {
        "id": "2309.00300",
        "title": "Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance",
        "abstract": "Cognitive diagnosis aims to diagnose students' knowledge proficiencies based\non their response scores on exam questions, which is the basis of many domains\nsuch as computerized adaptive testing. Existing cognitive diagnosis models\n(CDMs) follow a proficiency-response paradigm, which views diagnostic results\nas learnable embeddings that are the cause of students' responses and learns\nthe diagnostic results through optimization. However, such a paradigm can\neasily lead to unidentifiable diagnostic results and the explainability\noverfitting problem, which is harmful to the quantification of students'\nlearning performance. To address these problems, we propose a novel\nidentifiable cognitive diagnosis framework. Specifically, we first propose a\nflexible diagnostic module which directly diagnose identifiable and explainable\nexaminee traits and question features from response logs. Next, we leverage a\ngeneral predictive module to reconstruct response logs from the diagnostic\nresults to ensure the preciseness of the latter. We furthermore propose an\nimplementation of the framework, i.e., ID-CDM, to demonstrate the availability\nof the former. Finally, we demonstrate the identifiability, explainability and\npreciseness of diagnostic results of ID-CDM through experiments on four public\nreal-world datasets.",
        "pdfLink": "https://arxiv.org/pdf/2309.00300.pdf",
        "metaData": {
            "relevancy": 0.4039309799671173
        }
    },
    {
        "id": "2309.00238",
        "title": "ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine Learning Models",
        "abstract": "Legal Judgment Prediction (LJP) aims to predict judgment outcomes based on\ncase description. Several researchers have developed techniques to assist\npotential clients by predicting the outcome in the legal profession. However,\nnone of the proposed techniques were implemented in Arabic, and only a few\nattempts were implemented in English, Chinese, and Hindi. In this paper, we\ndevelop a system that utilizes deep learning (DL) and natural language\nprocessing (NLP) techniques to predict the judgment outcome from Arabic case\nscripts, especially in cases of custody and annulment of marriage. This system\nwill assist judges and attorneys in improving their work and time efficiency\nwhile reducing sentencing disparity. In addition, it will help litigants,\nlawyers, and law students analyze the probable outcomes of any given case\nbefore trial. We use a different machine and deep learning models such as\nSupport Vector Machine (SVM), Logistic regression (LR), Long Short Term Memory\n(LSTM), and Bidirectional Long Short-Term Memory (BiLSTM) using representation\ntechniques such as TF-IDF and word2vec on the developed dataset. Experimental\nresults demonstrate that compared with the five baseline methods, the SVM model\nwith word2vec and LR with TF-IDF achieve the highest accuracy of 88% and 78% in\npredicting the judgment on custody cases and annulment of marriage,\nrespectively. Furthermore, the LR and SVM with word2vec and BiLSTM model with\nTF-IDF achieved the highest accuracy of 88% and 69% in predicting the\nprobability of outcomes on custody cases and annulment of marriage,\nrespectively.",
        "pdfLink": "https://arxiv.org/pdf/2309.00238.pdf",
        "metaData": {
            "relevancy": 0.42358185052871705
        }
    },
    {
        "id": "2309.00172",
        "title": "Detecting Evidence of Organization in groups by Trajectories",
        "abstract": "Effective detection of organizations is essential for fighting crime and\nmaintaining public safety, especially considering the limited human resources\nand tools to deal with each group that exhibits co-movement patterns. This\npaper focuses on solving the Network Structure Inference (NSI) challenge. Thus,\nwe introduce two new approaches to detect network structure inferences based on\nagent trajectories. The first approach is based on the evaluation of graph\nentropy, while the second considers the quality of clustering indices. To\nevaluate the effectiveness of the new approaches, we conducted experiments\nusing four scenario simulations based on the animal kingdom, available on the\nNetLogo platform: Ants, Wolf Sheep Predation, Flocking, and Ant Adaptation.\nFurthermore, we compare the results obtained with those of an approach\npreviously proposed in the literature, applying all methods to simulations of\nthe NetLogo platform. The results demonstrate that our new detection approaches\ncan more clearly identify the inferences of organizations or networks in the\nsimulated scenarios.",
        "pdfLink": "https://arxiv.org/pdf/2309.00172.pdf",
        "metaData": {
            "relevancy": 0.296273672580719
        }
    },
    {
        "id": "2309.00138",
        "title": "Fuzzy Approach for Audio-Video Emotion Recognition in Computer Games for Children",
        "abstract": "Computer games are widespread nowadays and enjoyed by people of all ages. But\nwhen it comes to kids, playing these games can be more than just fun, it is a\nway for them to develop important skills and build emotional intelligence.\nFacial expressions and sounds that kids produce during gameplay reflect their\nfeelings, thoughts, and moods. In this paper, we propose a novel framework that\nintegrates a fuzzy approach for the recognition of emotions through the\nanalysis of audio and video data. Our focus lies within the specific context of\ncomputer games tailored for children, aiming to enhance their overall user\nexperience. We use the FER dataset to detect facial emotions in video frames\nrecorded from the screen during the game. For the audio emotion recognition of\nsounds a kid produces during the game, we use CREMA-D, TESS, RAVDESS, and Savee\ndatasets. Next, a fuzzy inference system is used for the fusion of results.\nBesides this, our system can detect emotion stability and emotion diversity\nduring gameplay, which, together with prevailing emotion report, can serve as\nvaluable information for parents worrying about the effect of certain games on\ntheir kids. The proposed approach has shown promising results in the\npreliminary experiments we conducted, involving 3 different video games, namely\nfighting, racing, and logic games, and providing emotion-tracking results for\nkids in each game. Our study can contribute to the advancement of\nchild-oriented game development, which is not only engaging but also accounts\nfor children's cognitive and emotional states.",
        "pdfLink": "https://arxiv.org/pdf/2309.00138.pdf",
        "metaData": {
            "relevancy": 0.3479453921318054
        }
    },
    {
        "id": "2309.00135",
        "title": "Construction Grammar and Artificial Intelligence",
        "abstract": "In this chapter, we argue that it is highly beneficial for the contemporary\nconstruction grammarian to have a thorough understanding of the strong\nrelationship between the research fields of construction grammar and artificial\nintelligence. We start by unravelling the historical links between the two\nfields, showing that their relationship is rooted in a common attitude towards\nhuman communication and language. We then discuss the first direction of\ninfluence, focussing in particular on how insights and techniques from the\nfield of artificial intelligence play an important role in operationalising,\nvalidating and scaling constructionist approaches to language. We then proceed\nto the second direction of influence, highlighting the relevance of\nconstruction grammar insights and analyses to the artificial intelligence\nendeavour of building truly intelligent agents. We support our case with a\nvariety of illustrative examples and conclude that the further elaboration of\nthis relationship will play a key role in shaping the future of the field of\nconstruction grammar.",
        "pdfLink": "https://arxiv.org/pdf/2309.00135.pdf",
        "metaData": {
            "relevancy": 0.3461999177932739
        }
    },
    {
        "id": "2309.00029",
        "title": "Exploring the Potential of Large Language Models to Generate Formative Programming Feedback",
        "abstract": "Ever since the emergence of large language models (LLMs) and related\napplications, such as ChatGPT, its performance and error analysis for\nprogramming tasks have been subject to research. In this work-in-progress\npaper, we explore the potential of such LLMs for computing educators and\nlearners, as we analyze the feedback it generates to a given input containing\nprogram code. In particular, we aim at (1) exploring how an LLM like ChatGPT\nresponds to students seeking help with their introductory programming tasks,\nand (2) identifying feedback types in its responses. To achieve these goals, we\nused students' programming sequences from a dataset gathered within a CS1\ncourse as input for ChatGPT along with questions required to elicit feedback\nand correct solutions. The results show that ChatGPT performs reasonably well\nfor some of the introductory programming tasks and student errors, which means\nthat students can potentially benefit. However, educators should provide\nguidance on how to use the provided feedback, as it can contain misleading\ninformation for novices.",
        "pdfLink": "https://arxiv.org/pdf/2309.00029.pdf",
        "metaData": {
            "relevancy": 0.4788977146148682
        }
    },
    {
        "id": "2309.00615",
        "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
        "abstract": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with\n2D image, language, audio, and video. Guided by ImageBind, we construct a joint\nembedding space between 3D and multi-modalities, enabling many promising\napplications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D\nopen-world understanding. On top of this, we further present Point-LLM, the\nfirst 3D large language model (LLM) following 3D multi-modal instructions. By\nparameter-efficient fine-tuning techniques, Point-LLM injects the semantics of\nPoint-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction\ndata, but exhibits superior 3D and multi-modal question-answering capacity. We\nhope our work may cast a light on the community for extending 3D point clouds\nto multi-modality applications. Code is available at\nthis https URL.",
        "pdfLink": "https://arxiv.org/pdf/2309.00615.pdf",
        "metaData": {
            "relevancy": 0.43967133164405825
        }
    },
    {
        "id": "2309.00613",
        "title": "Iterative Multi-granular Image Editing using Diffusion Models",
        "abstract": "Recent advances in text-guided image synthesis has dramatically changed how\ncreative professionals generate artistic and aesthetically pleasing visual\nassets. To fully support such creative endeavors, the process should possess\nthe ability to: 1) iteratively edit the generations and 2) control the spatial\nreach of desired changes (global, local or anything in between). We formalize\nthis pragmatic problem setting as Iterative Multi-granular Editing. While there\nhas been substantial progress with diffusion-based models for image synthesis\nand editing, they are all one shot (i.e., no iterative editing capabilities)\nand do not naturally yield multi-granular control (i.e., covering the full\nspectrum of local-to-global edits). To overcome these drawbacks, we propose\nEMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent\niteration strategy, which re-purposes a pre-trained diffusion model to\nfacilitate iterative editing. This is complemented by a gradient control\noperation for multi-granular control. We introduce a new benchmark dataset to\nevaluate our newly proposed setting. We conduct exhaustive quantitatively and\nqualitatively evaluation against recent state-of-the-art approaches adapted to\nour task, to being out the mettle of EMILIE. We hope our work would attract\nattention to this newly identified, pragmatic problem setting.",
        "pdfLink": "https://arxiv.org/pdf/2309.00613.pdf",
        "metaData": {
            "relevancy": 0.35241888761520385
        }
    },
    {
        "id": "2309.00543",
        "title": "Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare",
        "abstract": "Deep learning models have shown promising predictive accuracy for time-series\nhealthcare applications. However, ensuring the robustness of these models is\nvital for building trustworthy AI systems. Existing research predominantly\nfocuses on robustness to synthetic adversarial examples, crafted by adding\nimperceptible perturbations to clean input data. However, these synthetic\nadversarial examples do not accurately reflect the most challenging real-world\nscenarios, especially in the context of healthcare data. Consequently,\nrobustness to synthetic adversarial examples may not necessarily translate to\nrobustness against naturally occurring adversarial examples, which is highly\ndesirable for trustworthy AI. We propose a method to curate datasets comprised\nof natural adversarial examples to evaluate model robustness. The method relies\non probabilistic labels obtained from automated weakly-supervised labeling that\ncombines noisy and cheap-to-obtain labeling heuristics. Based on these labels,\nour method adversarially orders the input data and uses this ordering to\nconstruct a sequence of increasingly adversarial datasets. Our evaluation on\nsix medical case studies and three non-medical case studies demonstrates the\nefficacy and statistical validity of our approach to generating naturally\nadversarial datasets",
        "pdfLink": "https://arxiv.org/pdf/2309.00543.pdf",
        "metaData": {
            "relevancy": 0.38196120262145994
        }
    },
    {
        "id": "2309.00480",
        "title": "Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network",
        "abstract": "The global navigation satellite systems (GNSS) play a vital role in transport\nsystems for accurate and consistent vehicle localization. However, GNSS\nobservations can be distorted due to multipath effects and non-line-of-sight\n(NLOS) receptions in challenging environments such as urban canyons. In such\ncases, traditional methods to classify and exclude faulty GNSS observations may\nfail, leading to unreliable state estimation and unsafe system operations. This\nwork proposes a Deep-Learning-based method to detect NLOS receptions and\npredict GNSS pseudorange errors by analyzing GNSS observations as a\nspatio-temporal modeling problem. Compared to previous works, we construct a\ntransformer-like attention mechanism to enhance the long short-term memory\n(LSTM) networks, improving model performance and generalization. For the\ntraining and evaluation of the proposed network, we used labeled datasets from\nthe cities of Hong Kong and Aachen. We also introduce a dataset generation\nprocess to label the GNSS observations using lidar maps. In experimental\nstudies, we compare the proposed network with a deep-learning-based model and\nclassical machine-learning models. Furthermore, we conduct ablation studies of\nour network components and integrate the NLOS detection with data\nout-of-distribution in a state estimator. As a result, our network presents\nimproved precision and recall ratios compared to other models. Additionally, we\nshow that the proposed method avoids trajectory divergence in real-world\nvehicle localization by classifying and excluding NLOS observations.",
        "pdfLink": "https://arxiv.org/pdf/2309.00480.pdf",
        "metaData": {
            "relevancy": 0.3139308273792267
        }
    },
    {
        "id": "2309.00464",
        "title": "A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection",
        "abstract": "The proliferation of Deep Neural Networks has resulted in machine learning\nsystems becoming increasingly more present in various real-world applications.\nConsequently, there is a growing demand for highly reliable models in these\ndomains, making the problem of uncertainty calibration pivotal, when\nconsidering the future of deep learning. This is especially true when\nconsidering object detection systems, that are commonly present in\nsafety-critical application such as autonomous driving and robotics. For this\nreason, this work presents a novel theoretical and practical framework to\nevaluate object detection systems in the context of uncertainty calibration.\nThe robustness of the proposed uncertainty calibration metrics is shown through\na series of representative experiments. Code for the proposed uncertainty\ncalibration metrics at:\nthis https URL.",
        "pdfLink": "https://arxiv.org/pdf/2309.00464.pdf",
        "metaData": {
            "relevancy": 0.22890349924564363
        }
    },
    {
        "id": "2309.00462",
        "title": "New metrics for analyzing continual learners",
        "abstract": "Deep neural networks have shown remarkable performance when trained on\nindependent and identically distributed data from a fixed set of classes.\nHowever, in real-world scenarios, it can be desirable to train models on a\ncontinuous stream of data where multiple classification tasks are presented\nsequentially. This scenario, known as Continual Learning (CL) poses challenges\nto standard learning algorithms which struggle to maintain knowledge of old\ntasks while learning new ones. This stability-plasticity dilemma remains\ncentral to CL and multiple metrics have been proposed to adequately measure\nstability and plasticity separately. However, none considers the increasing\ndifficulty of the classification task, which inherently results in performance\nloss for any model. In that sense, we analyze some limitations of current\nmetrics and identify the presence of setup-induced forgetting. Therefore, we\npropose new metrics that account for the task's increasing difficulty. Through\nexperiments on benchmark datasets, we demonstrate that our proposed metrics can\nprovide new insights into the stability-plasticity trade-off achieved by models\nin the continual learning environment.",
        "pdfLink": "https://arxiv.org/pdf/2309.00462.pdf",
        "metaData": {
            "relevancy": 0.43381611704826356
        }
    },
    {
        "id": "2309.00424",
        "title": "Learning Speech Representation From Contrastive Token-Acoustic Pretraining",
        "abstract": "For fine-grained generation and recognition tasks such as\nminimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic\nspeech recognition (ASR), the intermediate representations extracted from\nspeech should serve as a \"bridge\" between text and acoustic information,\ncontaining information from both modalities. The semantic content is\nemphasized, while the paralinguistic information such as speaker identity and\nacoustic details should be de-emphasized. However, existing methods for\nextracting fine-grained intermediate representations from speech suffer from\nissues of excessive redundancy and dimension explosion. Contrastive learning is\na good method for modeling intermediate representations from two modalities.\nHowever, existing contrastive learning methods in the audio field focus on\nextracting global descriptive information for downstream audio classification\ntasks, making them unsuitable for TTS, VC, and ASR tasks. To address these\nissues, we propose a method named \"Contrastive Token-Acoustic Pretraining\n(CTAP)\", which uses two encoders to bring phoneme and speech into a joint\nmultimodal space, learning how to connect phoneme and speech at the frame\nlevel. The CTAP model is trained on 210k speech and phoneme text pairs,\nachieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method\noffers a promising solution for fine-grained generation and recognition\ndownstream tasks in speech processing.",
        "pdfLink": "https://arxiv.org/pdf/2309.00424.pdf",
        "metaData": {
            "relevancy": 0.4354353129863739
        }
    },
    {
        "id": "2309.00417",
        "title": "Area-norm COBRA on Conditional Survival Prediction",
        "abstract": "The paper explores a different variation of combined regression strategy to\ncalculate the conditional survival function. We use regression based weak\nlearners to create the proposed ensemble technique. The proposed combined\nregression strategy uses proximity measure as area between two survival curves.\nThe proposed model shows a construction which ensures that it performs better\nthan the Random Survival Forest. The paper discusses a novel technique to\nselect the most important variable in the combined regression setup. We perform\na simulation study to show that our proposition for finding relevance of the\nvariables works quite well. We also use three real-life datasets to illustrate\nthe model.",
        "pdfLink": "https://arxiv.org/pdf/2309.00417.pdf",
        "metaData": {
            "relevancy": 0.20139218866825104
        }
    },
    {
        "id": "2309.00408",
        "title": "Boosting AND/OR-Based Computational Protein Design: Dynamic Heuristics and Generalizable UFO",
        "abstract": "Scientific computing has experienced a surge empowered by advancements in\ntechnologies such as neural networks. However, certain important tasks are less\namenable to these technologies, benefiting from innovations to traditional\ninference schemes. One such task is protein re-design. Recently a new re-design\nalgorithm, AOBB-K*, was introduced and was competitive with state-of-the-art\nBBK* on small protein re-design problems. However, AOBB-K* did not scale well.\nIn this work we focus on scaling up AOBB-K* and introduce three new versions:\nAOBB-K*-b (boosted), AOBB-K*-DH (with dynamic heuristics), and AOBB-K*-UFO\n(with underflow optimization) that significantly enhance scalability.",
        "pdfLink": "https://arxiv.org/pdf/2309.00408.pdf",
        "metaData": {
            "relevancy": 0.20797729194164277
        }
    },
    {
        "id": "2309.00385",
        "title": "Dense Voxel 3D Reconstruction Using a Monocular Event Camera",
        "abstract": "Event cameras are sensors inspired by biological systems that specialize in\ncapturing changes in brightness. These emerging cameras offer many advantages\nover conventional frame-based cameras, including high dynamic range, high frame\nrates, and extremely low power consumption. Due to these advantages, event\ncameras have increasingly been adapted in various fields, such as frame\ninterpolation, semantic segmentation, odometry, and SLAM. However, their\napplication in 3D reconstruction for VR applications is underexplored. Previous\nmethods in this field mainly focused on 3D reconstruction through depth map\nestimation. Methods that produce dense 3D reconstruction generally require\nmultiple cameras, while methods that utilize a single event camera can only\nproduce a semi-dense result. Other single-camera methods that can produce dense\n3D reconstruction rely on creating a pipeline that either incorporates the\naforementioned methods or other existing Structure from Motion (SfM) or\nMulti-view Stereo (MVS) methods. In this paper, we propose a novel approach for\nsolving dense 3D reconstruction using only a single event camera. To the best\nof our knowledge, our work is the first attempt in this regard. Our preliminary\nresults demonstrate that the proposed method can produce visually\ndistinguishable dense 3D reconstructions directly without requiring pipelines\nlike those used by existing methods. Additionally, we have created a synthetic\ndataset with 39,73939,739 object scans using an event camera simulator. This\ndataset will help accelerate other relevant research in this field.",
        "pdfLink": "https://arxiv.org/pdf/2309.00385.pdf",
        "metaData": {
            "relevancy": 0.19617518782615662
        }
    },
    {
        "id": "2309.00373",
        "title": "Scenario-based model predictive control of water reservoir systems",
        "abstract": "The optimal operation of water reservoir systems is a challenging task\ninvolving multiple conflicting objectives. The main source of complexity is the\npresence of the water inflow, which acts as an exogenous, highly uncertain\ndisturbance on the system. When model predictive control (MPC) is employed, the\noptimal water release is usually computed based on the (predicted) trajectory\nof the inflow. This choice may jeopardize the closed-loop performance when the\nactual inflow differs from its forecast. In this work, we consider - for the\nfirst time - a stochastic MPC approach for water reservoirs, in which the\ncontrol is optimized based on a set of plausible future inflows directly\ngenerated from past data. Such a scenario-based MPC strategy allows the\ncontroller to be more cautious, counteracting droughty periods (e.g., the lake\nlevel going below the dry limit) while at the same time guaranteeing that the\nagricultural water demand is satisfied. The method's effectiveness is validated\nthrough extensive Monte Carlo tests using actual inflow data from Lake Como,\nItaly.",
        "pdfLink": "https://arxiv.org/pdf/2309.00373.pdf",
        "metaData": {
            "relevancy": 0.24701079428195954
        }
    },
    {
        "id": "2309.00356",
        "title": "Explainable Active Learning for Preference Elicitation",
        "abstract": "Gaining insights into the preferences of new users and subsequently\npersonalizing recommendations necessitate managing user interactions\nintelligently, namely, posing pertinent questions to elicit valuable\ninformation effectively. In this study, our focus is on a specific scenario of\nthe cold-start problem, where the recommendation system lacks adequate user\npresence or access to other users' data is restricted, obstructing employing\nuser profiling methods utilizing existing data in the system. We employ Active\nLearning (AL) to solve the addressed problem with the objective of maximizing\ninformation acquisition with minimal user effort. AL operates for selecting\ninformative data from a large unlabeled set to inquire an oracle to label them\nand eventually updating a machine learning (ML) model. We operate AL in an\nintegrated process of unsupervised, semi-supervised, and supervised ML within\nan explanatory preference elicitation process. It harvests user feedback (given\nfor the system's explanations on the presented items) over informative samples\nto update an underlying ML model estimating user preferences. The designed user\ninteraction facilitates personalizing the system by incorporating user feedback\ninto the ML model and also enhances user trust by refining the system's\nexplanations on recommendations. We implement the proposed preference\nelicitation methodology for food recommendation. We conducted human experiments\nto assess its efficacy in the short term and also experimented with several AL\nstrategies over synthetic user profiles that we created for two food datasets,\naiming for long-term performance analysis. The experimental results demonstrate\nthe efficiency of the proposed preference elicitation with limited user-labeled\ndata while also enhancing user trust through accurate explanations.",
        "pdfLink": "https://arxiv.org/pdf/2309.00356.pdf",
        "metaData": {
            "relevancy": 0.47282610535621644
        }
    },
    {
        "id": "2309.00296",
        "title": "End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing",
        "abstract": "Reinforcement Learning (RL) has emerged as a transformative approach in the\ndomains of automation and robotics, offering powerful solutions to complex\nproblems that conventional methods struggle to address. In scenarios where the\nproblem definitions are elusive and challenging to quantify, learning-based\nsolutions such as RL become particularly valuable. One instance of such\ncomplexity can be found in the realm of car racing, a dynamic and unpredictable\nenvironment that demands sophisticated decision-making algorithms. This study\nfocuses on developing and training an RL agent to navigate a racing environment\nsolely using feedforward raw lidar and velocity data in a simulated context.\nThe agent's performance, trained in the simulation environment, is then\nexperimentally evaluated in a real-world racing scenario. This exploration\nunderlines the feasibility and potential benefits of RL algorithm enhancing\nautonomous racing performance, especially in the environments where prior map\ninformation is not available.",
        "pdfLink": "https://arxiv.org/pdf/2309.00296.pdf",
        "metaData": {
            "relevancy": 0.4766716241836548
        }
    },
    {
        "id": "2309.00267",
        "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
        "abstract": "Reinforcement learning from human feedback (RLHF) is effective at aligning\nlarge language models (LLMs) to human preferences, but gathering high quality\nhuman preference labels is a key bottleneck. We conduct a head-to-head\ncomparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where\npreferences are labeled by an off-the-shelf LLM in lieu of humans, and we find\nthat they result in similar improvements. On the task of summarization, human\nevaluators prefer generations from both RLAIF and RLHF over a baseline\nsupervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate\nRLAIF vs. RLHF summaries, humans prefer both at equal rates. These results\nsuggest that RLAIF can yield human-level performance, offering a potential\nsolution to the scalability limitations of RLHF.",
        "pdfLink": "https://arxiv.org/pdf/2309.00267.pdf",
        "metaData": {
            "relevancy": 0.5393457055091858
        }
    },
    {
        "id": "2309.00257",
        "title": "Leveraging Learning Metrics for Improved Federated Learning",
        "abstract": "Currently in the federated setting, no learning schemes leverage the emerging\nresearch of explainable artificial intelligence (XAI) in particular the novel\nlearning metrics that help determine how well a model is learning. One of these\nnovel learning metrics is termed `Effective Rank' (ER) which measures the\nShannon Entropy of the singular values of a matrix, thus enabling a metric\ndetermining how well a layer is mapping. By joining federated learning and the\nlearning metric, effective rank, this work will \\textbf{(1)} give the first\nfederated learning metric aggregation method \\textbf{(2)} show that effective\nrank is well-suited to federated problems by out-performing baseline Federated\nAveraging \\cite{konevcny2016federated} and \\textbf{(3)} develop a novel\nweight-aggregation scheme relying on effective rank.",
        "pdfLink": "https://arxiv.org/pdf/2309.00257.pdf",
        "metaData": {
            "relevancy": 0.39679927825927735
        }
    },
    {
        "id": "2309.00248",
        "title": "DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models",
        "abstract": "Generating high-quality labeled image datasets is crucial for training\naccurate and robust machine learning models in the field of computer vision.\nHowever, the process of manually labeling real images is often time-consuming\nand costly. To address these challenges associated with dataset generation, we\nintroduce \"DiffuGen,\" a simple and adaptable approach that harnesses the power\nof stable diffusion models to create labeled image datasets efficiently. By\nleveraging stable diffusion models, our approach not only ensures the quality\nof generated datasets but also provides a versatile solution for label\ngeneration. In this paper, we present the methodology behind DiffuGen, which\ncombines the capabilities of diffusion models with two distinct labeling\ntechniques: unsupervised and supervised. Distinctively, DiffuGen employs prompt\ntemplating for adaptable image generation and textual inversion to enhance\ndiffusion model capabilities.",
        "pdfLink": "https://arxiv.org/pdf/2309.00248.pdf",
        "metaData": {
            "relevancy": 0.35856795907020567
        }
    },
    {
        "id": "2309.00245",
        "title": "City electric power consumption forecasting based on big data & neural network under smart grid background",
        "abstract": "With the development of the electric power system, the smart grid has become\nan important part of the smart city. The rational transmission of electric\nenergy and the guarantee of power supply of the smart grid are very important\nto smart cities, smart cities can provide better services through smart grids.\nAmong them, predicting and judging city electric power consumption is closely\nrelated to electricity supply and regulation, the location of power plants, and\nthe control of electricity transmission losses. Based on big data, this paper\nestablishes a neural network and considers the influence of various nonlinear\nfactors on city electric power consumption. A model is established to realize\nthe prediction of power consumption. Based on the permutation importance test,\nan evaluation model of the influencing factors of city electric power\nconsumption is constructed to obtain the core characteristic values of city\nelectric power consumption prediction, which can provide an important reference\nfor electric power related industry.",
        "pdfLink": "https://arxiv.org/pdf/2309.00245.pdf",
        "metaData": {
            "relevancy": 0.154412841796875
        }
    },
    {
        "id": "2309.00240",
        "title": "FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking",
        "abstract": "Automatic fact-checking plays a crucial role in combating the spread of\nmisinformation. Large Language Models (LLMs) and Instruction-Following\nvariants, such as InstructGPT and Alpaca, have shown remarkable performance in\nvarious natural language processing tasks. However, their knowledge may not\nalways be up-to-date or sufficient, potentially leading to inaccuracies in\nfact-checking. To address this limitation, we propose combining the power of\ninstruction-following language models with external evidence retrieval to\nenhance fact-checking performance. Our approach involves leveraging search\nengines to retrieve relevant evidence for a given input claim. This external\nevidence serves as valuable supplementary information to augment the knowledge\nof the pretrained language model. Then, we instruct-tune an open-sourced\nlanguage model, called LLaMA, using this evidence, enabling it to predict the\nveracity of the input claim more accurately. To evaluate our method, we\nconducted experiments on two widely used fact-checking datasets: RAWFC and\nLIAR. The results demonstrate that our approach achieves state-of-the-art\nperformance in fact-checking tasks. By integrating external evidence, we bridge\nthe gap between the model's knowledge and the most up-to-date and sufficient\ncontext available, leading to improved fact-checking outcomes. Our findings\nhave implications for combating misinformation and promoting the dissemination\nof accurate information on online platforms. Our released materials are\naccessible at: this https URL.",
        "pdfLink": "https://arxiv.org/pdf/2309.00240.pdf",
        "metaData": {
            "relevancy": 0.5153089046478272
        }
    },
    {
        "id": "2309.00237",
        "title": "Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes",
        "abstract": "The development of large language models tailored for handling patients'\nclinical notes is often hindered by the limited accessibility and usability of\nthese notes due to strict privacy regulations. To address these challenges, we\nfirst create synthetic large-scale clinical notes using publicly available case\nreports extracted from biomedical literature. We then use these synthetic notes\nto train our specialized clinical large language model, Asclepius. While\nAsclepius is trained on synthetic data, we assess its potential performance in\nreal-world applications by evaluating it using real clinical notes. We\nbenchmark Asclepius against several other large language models, including\nGPT-3.5-turbo and other open-source alternatives. To further validate our\napproach using synthetic notes, we also compare Asclepius with its variants\ntrained on real clinical notes. Our findings convincingly demonstrate that\nsynthetic clinical notes can serve as viable substitutes for real ones when\nconstructing high-performing clinical language models. This conclusion is\nsupported by detailed evaluations conducted by both GPT-4 and medical\nprofessionals. All resources including weights, codes, and data used in the\ndevelopment of Asclepius are made publicly accessible for future research.",
        "pdfLink": "https://arxiv.org/pdf/2309.00237.pdf",
        "metaData": {
            "relevancy": 0.39934290647506715
        }
    },
    {
        "id": "2309.00208",
        "title": "Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea's Top 50 KOSPI Companies",
        "abstract": "In the rapidly advancing domain of artificial intelligence, state-of-the-art\nlanguage models such as OpenAI's GPT-3.5-turbo and GPT-4 offer unprecedented\nopportunities for automating complex tasks. This research paper delves into the\ncapabilities of these models for semantically analyzing corporate disclosures\nin the Korean context, specifically for timely disclosure. The study focuses on\nthe top 50 publicly traded companies listed on the Korean KOSPI, based on\nmarket capitalization, and scrutinizes their monthly disclosure summaries over\na period of 17 months. Each summary was assigned a sentiment rating on a scale\nranging from 1(very negative) to 5(very positive). To gauge the effectiveness\nof the language models, their sentiment ratings were compared with those\ngenerated by human experts. Our findings reveal a notable performance disparity\nbetween GPT-3.5-turbo and GPT-4, with the latter demonstrating significant\naccuracy in human evaluation tests. The Spearman correlation coefficient was\nregistered at 0.61, while the simple concordance rate was recorded at 0.82.\nThis research contributes valuable insights into the evaluative characteristics\nof GPT models, thereby laying the groundwork for future innovations in the\nfield of automated semantic monitoring.",
        "pdfLink": "https://arxiv.org/pdf/2309.00208.pdf",
        "metaData": {
            "relevancy": 0.34084596633911135
        }
    },
    {
        "id": "2309.00206",
        "title": "Gap and Overlap Detection in Automated Fiber Placement",
        "abstract": "The identification and correction of manufacturing defects, particularly gaps\nand overlaps, are crucial for ensuring high-quality composite parts produced\nthrough Automated Fiber Placement (AFP). These imperfections are the most\ncommonly observed issues that can significantly impact the overall quality of\nthe composite parts. Manual inspection is both time-consuming and\nlabor-intensive, making it an inefficient approach. To overcome this challenge,\nthe implementation of an automated defect detection system serves as the\noptimal solution. In this paper, we introduce a novel method that uses an\nOptical Coherence Tomography (OCT) sensor and computer vision techniques to\ndetect and locate gaps and overlaps in composite parts. Our approach involves\ngenerating a depth map image of the composite surface that highlights the\nelevation of composite tapes (or tows) on the surface. By detecting the\nboundaries of each tow, our algorithm can compare consecutive tows and identify\ngaps or overlaps that may exist between them. Any gaps or overlaps exceeding a\npredefined tolerance threshold are considered manufacturing defects. To\nevaluate the performance of our approach, we compare the detected defects with\nthe ground truth annotated by experts. The results demonstrate a high level of\naccuracy and efficiency in gap and overlap segmentation.",
        "pdfLink": "https://arxiv.org/pdf/2309.00206.pdf",
        "metaData": {
            "relevancy": 0.1180828183889389
        }
    },
    {
        "id": "2309.00201",
        "title": "Subjectivity in Unsupervised Machine Learning Model Selection",
        "abstract": "Model selection is a necessary step in unsupervised machine learning. Despite\nnumerous criteria and metrics, model selection remains subjective. A high\ndegree of subjectivity may lead to questions about repeatability and\nreproducibility of various machine learning studies and doubts about the\nrobustness of models deployed in the real world. Yet, the impact of modelers'\npreferences on model selection outcomes remains largely unexplored. This study\nuses the Hidden Markov Model as an example to investigate the subjectivity\ninvolved in model selection. We asked 33 participants and three Large Language\nModels (LLMs) to make model selections in three scenarios. Results revealed\nvariability and inconsistencies in both the participants' and the LLMs'\nchoices, especially when different criteria and metrics disagree. Sources of\nsubjectivity include varying opinions on the importance of different criteria\nand metrics, differing views on how parsimonious a model should be, and how the\nsize of a dataset should influence model selection. The results underscore the\nimportance of developing a more standardized way to document subjective choices\nmade in model selection processes.",
        "pdfLink": "https://arxiv.org/pdf/2309.00201.pdf",
        "metaData": {
            "relevancy": 0.4449512004852295
        }
    },
    {
        "id": "2309.00199",
        "title": "Diffusion Model with Clustering-based Conditioning for Food Image Generation",
        "abstract": "Image-based dietary assessment serves as an efficient and accurate solution\nfor recording and analyzing nutrition intake using eating occasion images as\ninput. Deep learning-based techniques are commonly used to perform image\nanalysis such as food classification, segmentation, and portion size\nestimation, which rely on large amounts of food images with annotations for\ntraining. However, such data dependency poses significant barriers to\nreal-world applications, because acquiring a substantial, diverse, and balanced\nset of food images can be challenging. One potential solution is to use\nsynthetic food images for data augmentation. Although existing work has\nexplored the use of generative adversarial networks (GAN) based structures for\ngeneration, the quality of synthetic food images still remains subpar. In\naddition, while diffusion-based generative models have shown promising results\nfor general image generation tasks, the generation of food images can be\nchallenging due to the substantial intra-class variance. In this paper, we\ninvestigate the generation of synthetic food images based on the conditional\ndiffusion model and propose an effective clustering-based training framework,\nnamed ClusDiff, for generating high-quality and representative food images. The\nproposed method is evaluated on the Food-101 dataset and shows improved\nperformance when compared with existing image generation works. We also\ndemonstrate that the synthetic food images generated by ClusDiff can help\naddress the severe class imbalance issue in long-tailed food classification\nusing the VFN-LT dataset.",
        "pdfLink": "https://arxiv.org/pdf/2309.00199.pdf",
        "metaData": {
            "relevancy": 0.2772971123456955
        }
    },
    {
        "id": "2309.00155",
        "title": "LLM in the Shell: Generative Honeypots",
        "abstract": "Honeypots are essential tools in cybersecurity. However, most of them (even\nthe high-interaction ones) lack the required realism to engage and fool human\nattackers. This limitation makes them easily discernible, hindering their\neffectiveness. This work introduces a novel method to create dynamic and\nrealistic software honeypots based on Large Language Models. Preliminary\nresults indicate that LLMs can create credible and dynamic honeypots capable of\naddressing important limitations of previous honeypots, such as deterministic\nresponses, lack of adaptability, etc. We evaluated the realism of each command\nby conducting an experiment with human attackers who needed to say if the\nanswer from the honeypot was fake or not. Our proposed honeypot, called shelLM,\nreached an accuracy rate of 0.92.",
        "pdfLink": "https://arxiv.org/pdf/2309.00155.pdf",
        "metaData": {
            "relevancy": 0.37944255471229554
        }
    },
    {
        "id": "2309.00136",
        "title": "Predicting Financial Market Trends using Time Series Analysis and Natural Language Processing",
        "abstract": "Forecasting financial market trends through time series analysis and natural\nlanguage processing poses a complex and demanding undertaking, owing to the\nnumerous variables that can influence stock prices. These variables encompass a\nspectrum of economic and political occurrences, as well as prevailing public\nattitudes. Recent research has indicated that the expression of public\nsentiments on social media platforms such as Twitter may have a noteworthy\nimpact on the determination of stock prices. The objective of this study was to\nassess the viability of Twitter sentiments as a tool for predicting stock\nprices of major corporations such as Tesla, Apple. Our study has revealed a\nrobust association between the emotions conveyed in tweets and fluctuations in\nstock prices. Our findings indicate that positivity, negativity, and\nsubjectivity are the primary determinants of fluctuations in stock prices. The\ndata was analyzed utilizing the Long-Short Term Memory neural network (LSTM)\nmodel, which is currently recognized as the leading methodology for predicting\nstock prices by incorporating Twitter sentiments and historical stock prices\ndata. The models utilized in our study demonstrated a high degree of\nreliability and yielded precise outcomes for the designated corporations. In\nsummary, this research emphasizes the significance of incorporating public\nopinions into the prediction of stock prices. The application of Time Series\nAnalysis and Natural Language Processing methodologies can yield significant\nscientific findings regarding financial market patterns, thereby facilitating\ninformed decision-making among investors. The results of our study indicate\nthat the utilization of Twitter sentiments can serve as a potent instrument for\nforecasting stock prices, and ought to be factored in when formulating\ninvestment strategies.",
        "pdfLink": "https://arxiv.org/pdf/2309.00136.pdf",
        "metaData": {
            "relevancy": 0.2800001442432404
        }
    },
    {
        "id": "2309.00096",
        "title": "Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation",
        "abstract": "Open-vocabulary semantic segmentation is a challenging task that requires\nsegmenting novel object categories at inference time. Recent works explore\nvision-language pre-training to handle this task, but suffer from unrealistic\nassumptions in practical scenarios, i.e., low-quality textual category names.\nFor example, this paradigm assumes that new textual categories will be\naccurately and completely provided, and exist in lexicons during pre-training.\nHowever, exceptions often happen when meet with ambiguity for brief or\nincomplete names, new words that are not present in the pre-trained lexicons,\nand difficult-to-describe categories for users. To address these issues, this\nwork proposes a novel decomposition-aggregation framework, inspired by human\ncognition in understanding new concepts. Specifically, in the decomposition\nstage, we decouple class names into diverse attribute descriptions to enrich\nsemantic contexts. Two attribute construction strategies are designed: using\nlarge language models for common categories, and involving manually labelling\nfor human-invented categories. In the aggregation stage, we group diverse\nattributes into an integrated global description, to form a discriminative\nclassifier that distinguishes the target object from others. One hierarchical\naggregation is further designed to achieve multi-level alignment and deep\nfusion between vision and text. The final result is obtained by computing the\nembedding similarity between aggregated attributes and images. To evaluate the\neffectiveness, we annotate three datasets with attribute descriptions, and\nconduct extensive experiments and ablation studies. The results show the\nsuperior performance of attribute decomposition-aggregation.",
        "pdfLink": "https://arxiv.org/pdf/2309.00096.pdf",
        "metaData": {
            "relevancy": 0.4333529770374298
        }
    },
    {
        "id": "2309.00087",
        "title": "Large language models in medicine: the potentials and pitfalls",
        "abstract": "Large language models (LLMs) have been applied to tasks in healthcare,\nranging from medical exam questions to responding to patient questions. With\nincreasing institutional partnerships between companies producing LLMs and\nhealthcare systems, real world clinical application is coming closer to\nreality. As these models gain traction, it is essential for healthcare\npractitioners to understand what LLMs are, their development, their current and\npotential applications, and the associated pitfalls when utilized in medicine.\nThis review and accompanying tutorial aim to give an overview of these topics\nto aid healthcare practitioners in understanding the rapidly changing landscape\nof LLMs as applied to medicine.",
        "pdfLink": "https://arxiv.org/pdf/2309.00087.pdf",
        "metaData": {
            "relevancy": 0.4222908020019531
        }
    },
    {
        "id": "2309.00082",
        "title": "RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability",
        "abstract": "Visual model-based RL methods typically encode image observations into\nlow-dimensional representations in a manner that does not eliminate redundant\ninformation. This leaves them susceptible to spurious variations -- changes in\ntask-irrelevant components such as background distractors or lighting\nconditions. In this paper, we propose a visual model-based RL method that\nlearns a latent representation resilient to such spurious variations. Our\ntraining objective encourages the representation to be maximally predictive of\ndynamics and reward, while constraining the information flow from the\nobservation to the latent representation. We demonstrate that this objective\nsignificantly bolsters the resilience of visual model-based RL methods to\nvisual distractors, allowing them to operate in dynamic environments. We then\nshow that while the learned encoder is resilient to spirious variations, it is\nnot invariant under significant distribution shift. To address this, we propose\na simple reward-free alignment procedure that enables test time adaptation of\nthe encoder. This allows for quick adaptation to widely differing environments\nwithout having to relearn the dynamics and policy. Our effort is a step towards\nmaking model-based RL a practical and useful tool for dynamic, diverse domains.\nWe show its effectiveness in simulation benchmarks with significant spurious\nvariations as well as a real-world egocentric navigation task with noisy TVs in\nthe background. Videos and code at this https URL.",
        "pdfLink": "https://arxiv.org/pdf/2309.00082.pdf",
        "metaData": {
            "relevancy": 0.48041156530380247
        }
    },
    {
        "id": "2309.00079",
        "title": "On the Implicit Bias of Adam",
        "abstract": "In previous literature, backward error analysis was used to find ordinary\ndifferential equations (ODEs) approximating the gradient descent trajectory. It\nwas found that finite step sizes implicitly regularize solutions because terms\nappearing in the ODEs penalize the two-norm of the loss gradients. We prove\nthat the existence of similar implicit regularization in RMSProp and Adam\ndepends on their hyperparameters and the training stage, but with a different\n\"norm\" involved: the corresponding ODE terms either penalize the (perturbed)\none-norm of the loss gradients or, on the contrary, hinder its decrease (the\nlatter case being typical). We also conduct numerical experiments and discuss\nhow the proven facts can influence generalization.",
        "pdfLink": "https://arxiv.org/pdf/2309.00079.pdf",
        "metaData": {
            "relevancy": 0.29007138013839723
        }
    },
    {
        "id": "2309.00071",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "abstract": "Rotary Position Embeddings (RoPE) have been shown to effectively encode\npositional information in transformer-based language models. However, these\nmodels fail to generalize past the sequence length they were trained on. We\npresent YaRN (Yet another RoPE extensioN method), a compute-efficient method to\nextend the context window of such models, requiring 10x less tokens and 2.5x\nless training steps than previous methods. Using YaRN, we show that LLaMA\nmodels can effectively utilize and extrapolate to context lengths much longer\nthan their original pre-training would allow, while also surpassing previous\nthe state-of-the-art at context window extension. In addition, we demonstrate\nthat YaRN exhibits the capability to extrapolate beyond the limited context of\na fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned\nusing YaRN with 64k and 128k context windows at\nthis https URL",
        "pdfLink": "https://arxiv.org/pdf/2309.00071.pdf",
        "metaData": {
            "relevancy": 0.41009137630462644
        }
    },
    {
        "id": "2309.00064",
        "title": "Ethical Framework for Harnessing the Power of AI in Healthcare and Beyond",
        "abstract": "In the past decade, the deployment of deep learning (Artificial Intelligence\n(AI)) methods has become pervasive across a spectrum of real-world\napplications, often in safety-critical contexts. This comprehensive research\narticle rigorously investigates the ethical dimensions intricately linked to\nthe rapid evolution of AI technologies, with a particular focus on the\nhealthcare domain. Delving deeply, it explores a multitude of facets including\ntransparency, adept data management, human oversight, educational imperatives,\nand international collaboration within the realm of AI advancement. Central to\nthis article is the proposition of a conscientious AI framework, meticulously\ncrafted to accentuate values of transparency, equity, answerability, and a\nhuman-centric orientation. The second contribution of the article is the\nin-depth and thorough discussion of the limitations inherent to AI systems. It\nastutely identifies potential biases and the intricate challenges of navigating\nmultifaceted contexts. Lastly, the article unequivocally accentuates the\npressing need for globally standardized AI ethics principles and frameworks.\nSimultaneously, it aptly illustrates the adaptability of the ethical framework\nproposed herein, positioned skillfully to surmount emergent challenges.",
        "pdfLink": "https://arxiv.org/pdf/2309.00064.pdf",
        "metaData": {
            "relevancy": 0.38950676918029786
        }
    },
    {
        "id": "2309.00035",
        "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
        "abstract": "Computer vision models have known performance disparities across attributes\nsuch as gender and skin tone. This means during tasks such as classification\nand detection, model performance differs for certain classes based on the\ndemographics of the people in the image. These disparities have been shown to\nexist, but until now there has not been a unified approach to measure these\ndifferences for common use-cases of computer vision models. We present a new\nbenchmark named FACET (FAirness in Computer Vision EvaluaTion), a large,\npublicly available evaluation set of 32k images for some of the most common\nvision tasks - image classification, object detection and segmentation. For\nevery image in FACET, we hired expert reviewers to manually annotate\nperson-related attributes such as perceived skin tone and hair type, manually\ndraw bounding boxes and label fine-grained person-related classes such as disk\njockey or guitarist. In addition, we use FACET to benchmark state-of-the-art\nvision models and present a deeper understanding of potential performance\ndisparities and challenges across sensitive demographic attributes. With the\nexhaustive annotations collected, we probe models using single demographics\nattributes as well as multiple attributes using an intersectional approach\n(e.g. hair color and perceived skin tone). Our results show that\nclassification, detection, segmentation, and visual grounding models exhibit\nperformance disparities across demographic attributes and intersections of\nattributes. These harms suggest that not all people represented in datasets\nreceive fair and equitable treatment in these vision tasks. We hope current and\nfuture results using our benchmark will contribute to fairer, more robust\nvision models. FACET is available publicly at this https URL",
        "pdfLink": "https://arxiv.org/pdf/2309.00035.pdf",
        "metaData": {
            "relevancy": 0.29245824813842775
        }
    },
    {
        "id": "2309.00018",
        "title": "Unsupervised discovery of Interpretable Visual Concepts",
        "abstract": "Providing interpretability of deep-learning models to non-experts, while\nfundamental for a responsible real-world usage, is challenging. Attribution\nmaps from xAI techniques, such as Integrated Gradients, are a typical example\nof a visualization technique containing a high level of information, but with\ndifficult interpretation. In this paper, we propose two methods, Maximum\nActivation Groups Extraction (MAGE) and Multiscale Interpretable Visualization\n(Ms-IV), to explain the model's decision, enhancing global interpretability.\nMAGE finds, for a given CNN, combinations of features which, globally, form a\nsemantic meaning, that we call concepts. We group these similar feature\npatterns by clustering in ``concepts'', that we visualize through Ms-IV. This\nlast method is inspired by Occlusion and Sensitivity analysis (incorporating\ncausality), and uses a novel metric, called Class-aware Order Correlation\n(CaOC), to globally evaluate the most important image regions according to the\nmodel's decision space. We compare our approach to xAI methods such as LIME and\nIntegrated Gradients. Experimental results evince the Ms-IV higher localization\nand faithfulness values. Finally, qualitative evaluation of combined MAGE and\nMs-IV demonstrate humans' ability to agree, based on the visualization, on the\ndecision of clusters' concepts; and, to detect, among a given set of networks,\nthe existence of bias.",
        "pdfLink": "https://arxiv.org/pdf/2309.00018.pdf",
        "metaData": {
            "relevancy": 0.45556307435035703
        }
    }
]