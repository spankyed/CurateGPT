var Zr=Object.defineProperty;var Wr=(a,i,o)=>i in a?Zr(a,i,{enumerable:!0,configurable:!0,writable:!0,value:o}):a[i]=o;var Jn=(a,i,o)=>(Wr(a,typeof i!="symbol"?i+"":i,o),o),Co=(a,i,o)=>{if(!i.has(a))throw TypeError("Cannot "+o)};var St=(a,i,o)=>(Co(a,i,"read from private field"),o?o.call(a):i.get(a)),Yt=(a,i,o)=>{if(i.has(a))throw TypeError("Cannot add the same private member more than once");i instanceof WeakSet?i.add(a):i.set(a,o)},wn=(a,i,o,s)=>(Co(a,i,"write to private field"),s?s.call(a,o):i.set(a,o),o);var ao=(a,i,o,s)=>({set _($){wn(a,i,$,o)},get _(){return St(a,i,s)}}),un=(a,i,o)=>(Co(a,i,"access private method"),o);function _mergeNamespaces(a,i){for(var o=0;o<i.length;o++){const s=i[o];if(typeof s!="string"&&!Array.isArray(s)){for(const $ in s)if($!=="default"&&!($ in a)){const j=Object.getOwnPropertyDescriptor(s,$);j&&Object.defineProperty(a,$,j.get?j:{enumerable:!0,get:()=>s[$]})}}}return Object.freeze(Object.defineProperty(a,Symbol.toStringTag,{value:"Module"}))}(function(){const i=document.createElement("link").relList;if(i&&i.supports&&i.supports("modulepreload"))return;for(const $ of document.querySelectorAll('link[rel="modulepreload"]'))s($);new MutationObserver($=>{for(const j of $)if(j.type==="childList")for(const _e of j.addedNodes)_e.tagName==="LINK"&&_e.rel==="modulepreload"&&s(_e)}).observe(document,{childList:!0,subtree:!0});function o($){const j={};return $.integrity&&(j.integrity=$.integrity),$.referrerPolicy&&(j.referrerPolicy=$.referrerPolicy),$.crossOrigin==="use-credentials"?j.credentials="include":$.crossOrigin==="anonymous"?j.credentials="omit":j.credentials="same-origin",j}function s($){if($.ep)return;$.ep=!0;const j=o($);fetch($.href,j)}})();var commonjsGlobal=typeof globalThis<"u"?globalThis:typeof window<"u"?window:typeof global<"u"?global:typeof self<"u"?self:{};function getDefaultExportFromCjs(a){return a&&a.__esModule&&Object.prototype.hasOwnProperty.call(a,"default")?a.default:a}function getAugmentedNamespace(a){if(a.__esModule)return a;var i=a.default;if(typeof i=="function"){var o=function s(){return this instanceof s?Reflect.construct(i,arguments,this.constructor):i.apply(this,arguments)};o.prototype=i.prototype}else o={};return Object.defineProperty(o,"__esModule",{value:!0}),Object.keys(a).forEach(function(s){var $=Object.getOwnPropertyDescriptor(a,s);Object.defineProperty(o,s,$.get?$:{enumerable:!0,get:function(){return a[s]}})}),o}var jsxRuntime={exports:{}},reactJsxRuntime_production_min={},react={exports:{}},react_production_min={};/**
 * @license React
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var l$3=Symbol.for("react.element"),n$3=Symbol.for("react.portal"),p$4=Symbol.for("react.fragment"),q$3=Symbol.for("react.strict_mode"),r$2=Symbol.for("react.profiler"),t$2=Symbol.for("react.provider"),u$1=Symbol.for("react.context"),v$3=Symbol.for("react.forward_ref"),w$1=Symbol.for("react.suspense"),x$1=Symbol.for("react.memo"),y$1=Symbol.for("react.lazy"),z$2=Symbol.iterator;function A$2(a){return a===null||typeof a!="object"?null:(a=z$2&&a[z$2]||a["@@iterator"],typeof a=="function"?a:null)}var B$1={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},C$1=Object.assign,D$1={};function E$1(a,i,o){this.props=a,this.context=i,this.refs=D$1,this.updater=o||B$1}E$1.prototype.isReactComponent={};E$1.prototype.setState=function(a,i){if(typeof a!="object"&&typeof a!="function"&&a!=null)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,a,i,"setState")};E$1.prototype.forceUpdate=function(a){this.updater.enqueueForceUpdate(this,a,"forceUpdate")};function F(){}F.prototype=E$1.prototype;function G$1(a,i,o){this.props=a,this.context=i,this.refs=D$1,this.updater=o||B$1}var H$1=G$1.prototype=new F;H$1.constructor=G$1;C$1(H$1,E$1.prototype);H$1.isPureReactComponent=!0;var I$1=Array.isArray,J=Object.prototype.hasOwnProperty,K$1={current:null},L$1={key:!0,ref:!0,__self:!0,__source:!0};function M$1(a,i,o){var s,$={},j=null,_e=null;if(i!=null)for(s in i.ref!==void 0&&(_e=i.ref),i.key!==void 0&&(j=""+i.key),i)J.call(i,s)&&!L$1.hasOwnProperty(s)&&($[s]=i[s]);var et=arguments.length-2;if(et===1)$.children=o;else if(1<et){for(var tt=Array(et),nt=0;nt<et;nt++)tt[nt]=arguments[nt+2];$.children=tt}if(a&&a.defaultProps)for(s in et=a.defaultProps,et)$[s]===void 0&&($[s]=et[s]);return{$$typeof:l$3,type:a,key:j,ref:_e,props:$,_owner:K$1.current}}function N$1(a,i){return{$$typeof:l$3,type:a.type,key:i,ref:a.ref,props:a.props,_owner:a._owner}}function O$1(a){return typeof a=="object"&&a!==null&&a.$$typeof===l$3}function escape$1(a){var i={"=":"=0",":":"=2"};return"$"+a.replace(/[=:]/g,function(o){return i[o]})}var P$1=/\/+/g;function Q$1(a,i){return typeof a=="object"&&a!==null&&a.key!=null?escape$1(""+a.key):i.toString(36)}function R$1(a,i,o,s,$){var j=typeof a;(j==="undefined"||j==="boolean")&&(a=null);var _e=!1;if(a===null)_e=!0;else switch(j){case"string":case"number":_e=!0;break;case"object":switch(a.$$typeof){case l$3:case n$3:_e=!0}}if(_e)return _e=a,$=$(_e),a=s===""?"."+Q$1(_e,0):s,I$1($)?(o="",a!=null&&(o=a.replace(P$1,"$&/")+"/"),R$1($,i,o,"",function(nt){return nt})):$!=null&&(O$1($)&&($=N$1($,o+(!$.key||_e&&_e.key===$.key?"":(""+$.key).replace(P$1,"$&/")+"/")+a)),i.push($)),1;if(_e=0,s=s===""?".":s+":",I$1(a))for(var et=0;et<a.length;et++){j=a[et];var tt=s+Q$1(j,et);_e+=R$1(j,i,o,tt,$)}else if(tt=A$2(a),typeof tt=="function")for(a=tt.call(a),et=0;!(j=a.next()).done;)j=j.value,tt=s+Q$1(j,et++),_e+=R$1(j,i,o,tt,$);else if(j==="object")throw i=String(a),Error("Objects are not valid as a React child (found: "+(i==="[object Object]"?"object with keys {"+Object.keys(a).join(", ")+"}":i)+"). If you meant to render a collection of children, use an array instead.");return _e}function S$1(a,i,o){if(a==null)return a;var s=[],$=0;return R$1(a,s,"","",function(j){return i.call(o,j,$++)}),s}function T$1(a){if(a._status===-1){var i=a._result;i=i(),i.then(function(o){(a._status===0||a._status===-1)&&(a._status=1,a._result=o)},function(o){(a._status===0||a._status===-1)&&(a._status=2,a._result=o)}),a._status===-1&&(a._status=0,a._result=i)}if(a._status===1)return a._result.default;throw a._result}var U$1={current:null},V$1={transition:null},W$1={ReactCurrentDispatcher:U$1,ReactCurrentBatchConfig:V$1,ReactCurrentOwner:K$1};react_production_min.Children={map:S$1,forEach:function(a,i,o){S$1(a,function(){i.apply(this,arguments)},o)},count:function(a){var i=0;return S$1(a,function(){i++}),i},toArray:function(a){return S$1(a,function(i){return i})||[]},only:function(a){if(!O$1(a))throw Error("React.Children.only expected to receive a single React element child.");return a}};react_production_min.Component=E$1;react_production_min.Fragment=p$4;react_production_min.Profiler=r$2;react_production_min.PureComponent=G$1;react_production_min.StrictMode=q$3;react_production_min.Suspense=w$1;react_production_min.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=W$1;react_production_min.cloneElement=function(a,i,o){if(a==null)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+a+".");var s=C$1({},a.props),$=a.key,j=a.ref,_e=a._owner;if(i!=null){if(i.ref!==void 0&&(j=i.ref,_e=K$1.current),i.key!==void 0&&($=""+i.key),a.type&&a.type.defaultProps)var et=a.type.defaultProps;for(tt in i)J.call(i,tt)&&!L$1.hasOwnProperty(tt)&&(s[tt]=i[tt]===void 0&&et!==void 0?et[tt]:i[tt])}var tt=arguments.length-2;if(tt===1)s.children=o;else if(1<tt){et=Array(tt);for(var nt=0;nt<tt;nt++)et[nt]=arguments[nt+2];s.children=et}return{$$typeof:l$3,type:a.type,key:$,ref:j,props:s,_owner:_e}};react_production_min.createContext=function(a){return a={$$typeof:u$1,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null},a.Provider={$$typeof:t$2,_context:a},a.Consumer=a};react_production_min.createElement=M$1;react_production_min.createFactory=function(a){var i=M$1.bind(null,a);return i.type=a,i};react_production_min.createRef=function(){return{current:null}};react_production_min.forwardRef=function(a){return{$$typeof:v$3,render:a}};react_production_min.isValidElement=O$1;react_production_min.lazy=function(a){return{$$typeof:y$1,_payload:{_status:-1,_result:a},_init:T$1}};react_production_min.memo=function(a,i){return{$$typeof:x$1,type:a,compare:i===void 0?null:i}};react_production_min.startTransition=function(a){var i=V$1.transition;V$1.transition={};try{a()}finally{V$1.transition=i}};react_production_min.unstable_act=function(){throw Error("act(...) is not supported in production builds of React.")};react_production_min.useCallback=function(a,i){return U$1.current.useCallback(a,i)};react_production_min.useContext=function(a){return U$1.current.useContext(a)};react_production_min.useDebugValue=function(){};react_production_min.useDeferredValue=function(a){return U$1.current.useDeferredValue(a)};react_production_min.useEffect=function(a,i){return U$1.current.useEffect(a,i)};react_production_min.useId=function(){return U$1.current.useId()};react_production_min.useImperativeHandle=function(a,i,o){return U$1.current.useImperativeHandle(a,i,o)};react_production_min.useInsertionEffect=function(a,i){return U$1.current.useInsertionEffect(a,i)};react_production_min.useLayoutEffect=function(a,i){return U$1.current.useLayoutEffect(a,i)};react_production_min.useMemo=function(a,i){return U$1.current.useMemo(a,i)};react_production_min.useReducer=function(a,i,o){return U$1.current.useReducer(a,i,o)};react_production_min.useRef=function(a){return U$1.current.useRef(a)};react_production_min.useState=function(a){return U$1.current.useState(a)};react_production_min.useSyncExternalStore=function(a,i,o){return U$1.current.useSyncExternalStore(a,i,o)};react_production_min.useTransition=function(){return U$1.current.useTransition()};react_production_min.version="18.2.0";react.exports=react_production_min;var reactExports=react.exports;const React$1=getDefaultExportFromCjs(reactExports),React$2=_mergeNamespaces({__proto__:null,default:React$1},[reactExports]);/**
 * @license React
 * react-jsx-runtime.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var f$2=reactExports,k$2=Symbol.for("react.element"),l$2=Symbol.for("react.fragment"),m$3=Object.prototype.hasOwnProperty,n$2=f$2.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,p$3={key:!0,ref:!0,__self:!0,__source:!0};function q$2(a,i,o){var s,$={},j=null,_e=null;o!==void 0&&(j=""+o),i.key!==void 0&&(j=""+i.key),i.ref!==void 0&&(_e=i.ref);for(s in i)m$3.call(i,s)&&!p$3.hasOwnProperty(s)&&($[s]=i[s]);if(a&&a.defaultProps)for(s in i=a.defaultProps,i)$[s]===void 0&&($[s]=i[s]);return{$$typeof:k$2,type:a,key:j,ref:_e,props:$,_owner:n$2.current}}reactJsxRuntime_production_min.Fragment=l$2;reactJsxRuntime_production_min.jsx=q$2;reactJsxRuntime_production_min.jsxs=q$2;jsxRuntime.exports=reactJsxRuntime_production_min;var jsxRuntimeExports=jsxRuntime.exports,reactDom={exports:{}},reactDom_production_min={},scheduler={exports:{}},scheduler_production_min={};/**
 * @license React
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */(function(a){function i(Ct,jt){var Zt=Ct.length;Ct.push(jt);e:for(;0<Zt;){var Xt=Zt-1>>>1,sn=Ct[Xt];if(0<$(sn,jt))Ct[Xt]=jt,Ct[Zt]=sn,Zt=Xt;else break e}}function o(Ct){return Ct.length===0?null:Ct[0]}function s(Ct){if(Ct.length===0)return null;var jt=Ct[0],Zt=Ct.pop();if(Zt!==jt){Ct[0]=Zt;e:for(var Xt=0,sn=Ct.length,Ft=sn>>>1;Xt<Ft;){var wt=2*(Xt+1)-1,kt=Ct[wt],At=wt+1,Pt=Ct[At];if(0>$(kt,Zt))At<sn&&0>$(Pt,kt)?(Ct[Xt]=Pt,Ct[At]=Zt,Xt=At):(Ct[Xt]=kt,Ct[wt]=Zt,Xt=wt);else if(At<sn&&0>$(Pt,Zt))Ct[Xt]=Pt,Ct[At]=Zt,Xt=At;else break e}}return jt}function $(Ct,jt){var Zt=Ct.sortIndex-jt.sortIndex;return Zt!==0?Zt:Ct.id-jt.id}if(typeof performance=="object"&&typeof performance.now=="function"){var j=performance;a.unstable_now=function(){return j.now()}}else{var _e=Date,et=_e.now();a.unstable_now=function(){return _e.now()-et}}var tt=[],nt=[],at=1,it=null,st=3,lt=!1,ct=!1,rt=!1,ut=typeof setTimeout=="function"?setTimeout:null,ot=typeof clearTimeout=="function"?clearTimeout:null,dt=typeof setImmediate<"u"?setImmediate:null;typeof navigator<"u"&&navigator.scheduling!==void 0&&navigator.scheduling.isInputPending!==void 0&&navigator.scheduling.isInputPending.bind(navigator.scheduling);function pt(Ct){for(var jt=o(nt);jt!==null;){if(jt.callback===null)s(nt);else if(jt.startTime<=Ct)s(nt),jt.sortIndex=jt.expirationTime,i(tt,jt);else break;jt=o(nt)}}function mt(Ct){if(rt=!1,pt(Ct),!ct)if(o(tt)!==null)ct=!0,Dt(ft);else{var jt=o(nt);jt!==null&&It(mt,jt.startTime-Ct)}}function ft(Ct,jt){ct=!1,rt&&(rt=!1,ot(bt),bt=-1),lt=!0;var Zt=st;try{for(pt(jt),it=o(tt);it!==null&&(!(it.expirationTime>jt)||Ct&&!vt());){var Xt=it.callback;if(typeof Xt=="function"){it.callback=null,st=it.priorityLevel;var sn=Xt(it.expirationTime<=jt);jt=a.unstable_now(),typeof sn=="function"?it.callback=sn:it===o(tt)&&s(tt),pt(jt)}else s(tt);it=o(tt)}if(it!==null)var Ft=!0;else{var wt=o(nt);wt!==null&&It(mt,wt.startTime-jt),Ft=!1}return Ft}finally{it=null,st=Zt,lt=!1}}var ht=!1,yt=null,bt=-1,gt=5,xt=-1;function vt(){return!(a.unstable_now()-xt<gt)}function Lt(){if(yt!==null){var Ct=a.unstable_now();xt=Ct;var jt=!0;try{jt=yt(!0,Ct)}finally{jt?$t():(ht=!1,yt=null)}}else ht=!1}var $t;if(typeof dt=="function")$t=function(){dt(Lt)};else if(typeof MessageChannel<"u"){var Tt=new MessageChannel,Et=Tt.port2;Tt.port1.onmessage=Lt,$t=function(){Et.postMessage(null)}}else $t=function(){ut(Lt,0)};function Dt(Ct){yt=Ct,ht||(ht=!0,$t())}function It(Ct,jt){bt=ut(function(){Ct(a.unstable_now())},jt)}a.unstable_IdlePriority=5,a.unstable_ImmediatePriority=1,a.unstable_LowPriority=4,a.unstable_NormalPriority=3,a.unstable_Profiling=null,a.unstable_UserBlockingPriority=2,a.unstable_cancelCallback=function(Ct){Ct.callback=null},a.unstable_continueExecution=function(){ct||lt||(ct=!0,Dt(ft))},a.unstable_forceFrameRate=function(Ct){0>Ct||125<Ct?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):gt=0<Ct?Math.floor(1e3/Ct):5},a.unstable_getCurrentPriorityLevel=function(){return st},a.unstable_getFirstCallbackNode=function(){return o(tt)},a.unstable_next=function(Ct){switch(st){case 1:case 2:case 3:var jt=3;break;default:jt=st}var Zt=st;st=jt;try{return Ct()}finally{st=Zt}},a.unstable_pauseExecution=function(){},a.unstable_requestPaint=function(){},a.unstable_runWithPriority=function(Ct,jt){switch(Ct){case 1:case 2:case 3:case 4:case 5:break;default:Ct=3}var Zt=st;st=Ct;try{return jt()}finally{st=Zt}},a.unstable_scheduleCallback=function(Ct,jt,Zt){var Xt=a.unstable_now();switch(typeof Zt=="object"&&Zt!==null?(Zt=Zt.delay,Zt=typeof Zt=="number"&&0<Zt?Xt+Zt:Xt):Zt=Xt,Ct){case 1:var sn=-1;break;case 2:sn=250;break;case 5:sn=1073741823;break;case 4:sn=1e4;break;default:sn=5e3}return sn=Zt+sn,Ct={id:at++,callback:jt,priorityLevel:Ct,startTime:Zt,expirationTime:sn,sortIndex:-1},Zt>Xt?(Ct.sortIndex=Zt,i(nt,Ct),o(tt)===null&&Ct===o(nt)&&(rt?(ot(bt),bt=-1):rt=!0,It(mt,Zt-Xt))):(Ct.sortIndex=sn,i(tt,Ct),ct||lt||(ct=!0,Dt(ft))),Ct},a.unstable_shouldYield=vt,a.unstable_wrapCallback=function(Ct){var jt=st;return function(){var Zt=st;st=jt;try{return Ct.apply(this,arguments)}finally{st=Zt}}}})(scheduler_production_min);scheduler.exports=scheduler_production_min;var schedulerExports=scheduler.exports;/**
 * @license React
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var aa=reactExports,ca=schedulerExports;function p$2(a){for(var i="https://reactjs.org/docs/error-decoder.html?invariant="+a,o=1;o<arguments.length;o++)i+="&args[]="+encodeURIComponent(arguments[o]);return"Minified React error #"+a+"; visit "+i+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var da=new Set,ea={};function fa(a,i){ha(a,i),ha(a+"Capture",i)}function ha(a,i){for(ea[a]=i,a=0;a<i.length;a++)da.add(i[a])}var ia=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),ja=Object.prototype.hasOwnProperty,ka=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,la={},ma={};function oa(a){return ja.call(ma,a)?!0:ja.call(la,a)?!1:ka.test(a)?ma[a]=!0:(la[a]=!0,!1)}function pa(a,i,o,s){if(o!==null&&o.type===0)return!1;switch(typeof i){case"function":case"symbol":return!0;case"boolean":return s?!1:o!==null?!o.acceptsBooleans:(a=a.toLowerCase().slice(0,5),a!=="data-"&&a!=="aria-");default:return!1}}function qa(a,i,o,s){if(i===null||typeof i>"u"||pa(a,i,o,s))return!0;if(s)return!1;if(o!==null)switch(o.type){case 3:return!i;case 4:return i===!1;case 5:return isNaN(i);case 6:return isNaN(i)||1>i}return!1}function v$2(a,i,o,s,$,j,_e){this.acceptsBooleans=i===2||i===3||i===4,this.attributeName=s,this.attributeNamespace=$,this.mustUseProperty=o,this.propertyName=a,this.type=i,this.sanitizeURL=j,this.removeEmptyString=_e}var z$1={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(a){z$1[a]=new v$2(a,0,!1,a,null,!1,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(a){var i=a[0];z$1[i]=new v$2(i,1,!1,a[1],null,!1,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(a){z$1[a]=new v$2(a,2,!1,a.toLowerCase(),null,!1,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(a){z$1[a]=new v$2(a,2,!1,a,null,!1,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(a){z$1[a]=new v$2(a,3,!1,a.toLowerCase(),null,!1,!1)});["checked","multiple","muted","selected"].forEach(function(a){z$1[a]=new v$2(a,3,!0,a,null,!1,!1)});["capture","download"].forEach(function(a){z$1[a]=new v$2(a,4,!1,a,null,!1,!1)});["cols","rows","size","span"].forEach(function(a){z$1[a]=new v$2(a,6,!1,a,null,!1,!1)});["rowSpan","start"].forEach(function(a){z$1[a]=new v$2(a,5,!1,a.toLowerCase(),null,!1,!1)});var ra=/[\-:]([a-z])/g;function sa(a){return a[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(a){var i=a.replace(ra,sa);z$1[i]=new v$2(i,1,!1,a,null,!1,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(a){var i=a.replace(ra,sa);z$1[i]=new v$2(i,1,!1,a,"http://www.w3.org/1999/xlink",!1,!1)});["xml:base","xml:lang","xml:space"].forEach(function(a){var i=a.replace(ra,sa);z$1[i]=new v$2(i,1,!1,a,"http://www.w3.org/XML/1998/namespace",!1,!1)});["tabIndex","crossOrigin"].forEach(function(a){z$1[a]=new v$2(a,1,!1,a.toLowerCase(),null,!1,!1)});z$1.xlinkHref=new v$2("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1);["src","href","action","formAction"].forEach(function(a){z$1[a]=new v$2(a,1,!1,a.toLowerCase(),null,!0,!0)});function ta(a,i,o,s){var $=z$1.hasOwnProperty(i)?z$1[i]:null;($!==null?$.type!==0:s||!(2<i.length)||i[0]!=="o"&&i[0]!=="O"||i[1]!=="n"&&i[1]!=="N")&&(qa(i,o,$,s)&&(o=null),s||$===null?oa(i)&&(o===null?a.removeAttribute(i):a.setAttribute(i,""+o)):$.mustUseProperty?a[$.propertyName]=o===null?$.type===3?!1:"":o:(i=$.attributeName,s=$.attributeNamespace,o===null?a.removeAttribute(i):($=$.type,o=$===3||$===4&&o===!0?"":""+o,s?a.setAttributeNS(s,i,o):a.setAttribute(i,o))))}var ua=aa.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,va=Symbol.for("react.element"),wa=Symbol.for("react.portal"),ya=Symbol.for("react.fragment"),za=Symbol.for("react.strict_mode"),Aa=Symbol.for("react.profiler"),Ba=Symbol.for("react.provider"),Ca=Symbol.for("react.context"),Da=Symbol.for("react.forward_ref"),Ea=Symbol.for("react.suspense"),Fa=Symbol.for("react.suspense_list"),Ga=Symbol.for("react.memo"),Ha=Symbol.for("react.lazy"),Ia=Symbol.for("react.offscreen"),Ja=Symbol.iterator;function Ka(a){return a===null||typeof a!="object"?null:(a=Ja&&a[Ja]||a["@@iterator"],typeof a=="function"?a:null)}var A$1=Object.assign,La;function Ma(a){if(La===void 0)try{throw Error()}catch(o){var i=o.stack.trim().match(/\n( *(at )?)/);La=i&&i[1]||""}return`
`+La+a}var Na=!1;function Oa(a,i){if(!a||Na)return"";Na=!0;var o=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(i)if(i=function(){throw Error()},Object.defineProperty(i.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(i,[])}catch(nt){var s=nt}Reflect.construct(a,[],i)}else{try{i.call()}catch(nt){s=nt}a.call(i.prototype)}else{try{throw Error()}catch(nt){s=nt}a()}}catch(nt){if(nt&&s&&typeof nt.stack=="string"){for(var $=nt.stack.split(`
`),j=s.stack.split(`
`),_e=$.length-1,et=j.length-1;1<=_e&&0<=et&&$[_e]!==j[et];)et--;for(;1<=_e&&0<=et;_e--,et--)if($[_e]!==j[et]){if(_e!==1||et!==1)do if(_e--,et--,0>et||$[_e]!==j[et]){var tt=`
`+$[_e].replace(" at new "," at ");return a.displayName&&tt.includes("<anonymous>")&&(tt=tt.replace("<anonymous>",a.displayName)),tt}while(1<=_e&&0<=et);break}}}finally{Na=!1,Error.prepareStackTrace=o}return(a=a?a.displayName||a.name:"")?Ma(a):""}function Pa(a){switch(a.tag){case 5:return Ma(a.type);case 16:return Ma("Lazy");case 13:return Ma("Suspense");case 19:return Ma("SuspenseList");case 0:case 2:case 15:return a=Oa(a.type,!1),a;case 11:return a=Oa(a.type.render,!1),a;case 1:return a=Oa(a.type,!0),a;default:return""}}function Qa(a){if(a==null)return null;if(typeof a=="function")return a.displayName||a.name||null;if(typeof a=="string")return a;switch(a){case ya:return"Fragment";case wa:return"Portal";case Aa:return"Profiler";case za:return"StrictMode";case Ea:return"Suspense";case Fa:return"SuspenseList"}if(typeof a=="object")switch(a.$$typeof){case Ca:return(a.displayName||"Context")+".Consumer";case Ba:return(a._context.displayName||"Context")+".Provider";case Da:var i=a.render;return a=a.displayName,a||(a=i.displayName||i.name||"",a=a!==""?"ForwardRef("+a+")":"ForwardRef"),a;case Ga:return i=a.displayName||null,i!==null?i:Qa(a.type)||"Memo";case Ha:i=a._payload,a=a._init;try{return Qa(a(i))}catch{}}return null}function Ra(a){var i=a.type;switch(a.tag){case 24:return"Cache";case 9:return(i.displayName||"Context")+".Consumer";case 10:return(i._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return a=i.render,a=a.displayName||a.name||"",i.displayName||(a!==""?"ForwardRef("+a+")":"ForwardRef");case 7:return"Fragment";case 5:return i;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return Qa(i);case 8:return i===za?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if(typeof i=="function")return i.displayName||i.name||null;if(typeof i=="string")return i}return null}function Sa(a){switch(typeof a){case"boolean":case"number":case"string":case"undefined":return a;case"object":return a;default:return""}}function Ta(a){var i=a.type;return(a=a.nodeName)&&a.toLowerCase()==="input"&&(i==="checkbox"||i==="radio")}function Ua(a){var i=Ta(a)?"checked":"value",o=Object.getOwnPropertyDescriptor(a.constructor.prototype,i),s=""+a[i];if(!a.hasOwnProperty(i)&&typeof o<"u"&&typeof o.get=="function"&&typeof o.set=="function"){var $=o.get,j=o.set;return Object.defineProperty(a,i,{configurable:!0,get:function(){return $.call(this)},set:function(_e){s=""+_e,j.call(this,_e)}}),Object.defineProperty(a,i,{enumerable:o.enumerable}),{getValue:function(){return s},setValue:function(_e){s=""+_e},stopTracking:function(){a._valueTracker=null,delete a[i]}}}}function Va(a){a._valueTracker||(a._valueTracker=Ua(a))}function Wa(a){if(!a)return!1;var i=a._valueTracker;if(!i)return!0;var o=i.getValue(),s="";return a&&(s=Ta(a)?a.checked?"true":"false":a.value),a=s,a!==o?(i.setValue(a),!0):!1}function Xa(a){if(a=a||(typeof document<"u"?document:void 0),typeof a>"u")return null;try{return a.activeElement||a.body}catch{return a.body}}function Ya(a,i){var o=i.checked;return A$1({},i,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:o??a._wrapperState.initialChecked})}function Za(a,i){var o=i.defaultValue==null?"":i.defaultValue,s=i.checked!=null?i.checked:i.defaultChecked;o=Sa(i.value!=null?i.value:o),a._wrapperState={initialChecked:s,initialValue:o,controlled:i.type==="checkbox"||i.type==="radio"?i.checked!=null:i.value!=null}}function ab(a,i){i=i.checked,i!=null&&ta(a,"checked",i,!1)}function bb(a,i){ab(a,i);var o=Sa(i.value),s=i.type;if(o!=null)s==="number"?(o===0&&a.value===""||a.value!=o)&&(a.value=""+o):a.value!==""+o&&(a.value=""+o);else if(s==="submit"||s==="reset"){a.removeAttribute("value");return}i.hasOwnProperty("value")?cb(a,i.type,o):i.hasOwnProperty("defaultValue")&&cb(a,i.type,Sa(i.defaultValue)),i.checked==null&&i.defaultChecked!=null&&(a.defaultChecked=!!i.defaultChecked)}function db(a,i,o){if(i.hasOwnProperty("value")||i.hasOwnProperty("defaultValue")){var s=i.type;if(!(s!=="submit"&&s!=="reset"||i.value!==void 0&&i.value!==null))return;i=""+a._wrapperState.initialValue,o||i===a.value||(a.value=i),a.defaultValue=i}o=a.name,o!==""&&(a.name=""),a.defaultChecked=!!a._wrapperState.initialChecked,o!==""&&(a.name=o)}function cb(a,i,o){(i!=="number"||Xa(a.ownerDocument)!==a)&&(o==null?a.defaultValue=""+a._wrapperState.initialValue:a.defaultValue!==""+o&&(a.defaultValue=""+o))}var eb=Array.isArray;function fb(a,i,o,s){if(a=a.options,i){i={};for(var $=0;$<o.length;$++)i["$"+o[$]]=!0;for(o=0;o<a.length;o++)$=i.hasOwnProperty("$"+a[o].value),a[o].selected!==$&&(a[o].selected=$),$&&s&&(a[o].defaultSelected=!0)}else{for(o=""+Sa(o),i=null,$=0;$<a.length;$++){if(a[$].value===o){a[$].selected=!0,s&&(a[$].defaultSelected=!0);return}i!==null||a[$].disabled||(i=a[$])}i!==null&&(i.selected=!0)}}function gb(a,i){if(i.dangerouslySetInnerHTML!=null)throw Error(p$2(91));return A$1({},i,{value:void 0,defaultValue:void 0,children:""+a._wrapperState.initialValue})}function hb(a,i){var o=i.value;if(o==null){if(o=i.children,i=i.defaultValue,o!=null){if(i!=null)throw Error(p$2(92));if(eb(o)){if(1<o.length)throw Error(p$2(93));o=o[0]}i=o}i==null&&(i=""),o=i}a._wrapperState={initialValue:Sa(o)}}function ib(a,i){var o=Sa(i.value),s=Sa(i.defaultValue);o!=null&&(o=""+o,o!==a.value&&(a.value=o),i.defaultValue==null&&a.defaultValue!==o&&(a.defaultValue=o)),s!=null&&(a.defaultValue=""+s)}function jb(a){var i=a.textContent;i===a._wrapperState.initialValue&&i!==""&&i!==null&&(a.value=i)}function kb(a){switch(a){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function lb(a,i){return a==null||a==="http://www.w3.org/1999/xhtml"?kb(i):a==="http://www.w3.org/2000/svg"&&i==="foreignObject"?"http://www.w3.org/1999/xhtml":a}var mb,nb=function(a){return typeof MSApp<"u"&&MSApp.execUnsafeLocalFunction?function(i,o,s,$){MSApp.execUnsafeLocalFunction(function(){return a(i,o,s,$)})}:a}(function(a,i){if(a.namespaceURI!=="http://www.w3.org/2000/svg"||"innerHTML"in a)a.innerHTML=i;else{for(mb=mb||document.createElement("div"),mb.innerHTML="<svg>"+i.valueOf().toString()+"</svg>",i=mb.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;i.firstChild;)a.appendChild(i.firstChild)}});function ob(a,i){if(i){var o=a.firstChild;if(o&&o===a.lastChild&&o.nodeType===3){o.nodeValue=i;return}}a.textContent=i}var pb={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},qb=["Webkit","ms","Moz","O"];Object.keys(pb).forEach(function(a){qb.forEach(function(i){i=i+a.charAt(0).toUpperCase()+a.substring(1),pb[i]=pb[a]})});function rb(a,i,o){return i==null||typeof i=="boolean"||i===""?"":o||typeof i!="number"||i===0||pb.hasOwnProperty(a)&&pb[a]?(""+i).trim():i+"px"}function sb(a,i){a=a.style;for(var o in i)if(i.hasOwnProperty(o)){var s=o.indexOf("--")===0,$=rb(o,i[o],s);o==="float"&&(o="cssFloat"),s?a.setProperty(o,$):a[o]=$}}var tb=A$1({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function ub(a,i){if(i){if(tb[a]&&(i.children!=null||i.dangerouslySetInnerHTML!=null))throw Error(p$2(137,a));if(i.dangerouslySetInnerHTML!=null){if(i.children!=null)throw Error(p$2(60));if(typeof i.dangerouslySetInnerHTML!="object"||!("__html"in i.dangerouslySetInnerHTML))throw Error(p$2(61))}if(i.style!=null&&typeof i.style!="object")throw Error(p$2(62))}}function vb(a,i){if(a.indexOf("-")===-1)return typeof i.is=="string";switch(a){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var wb=null;function xb(a){return a=a.target||a.srcElement||window,a.correspondingUseElement&&(a=a.correspondingUseElement),a.nodeType===3?a.parentNode:a}var yb=null,zb=null,Ab=null;function Bb(a){if(a=Cb(a)){if(typeof yb!="function")throw Error(p$2(280));var i=a.stateNode;i&&(i=Db(i),yb(a.stateNode,a.type,i))}}function Eb(a){zb?Ab?Ab.push(a):Ab=[a]:zb=a}function Fb(){if(zb){var a=zb,i=Ab;if(Ab=zb=null,Bb(a),i)for(a=0;a<i.length;a++)Bb(i[a])}}function Gb(a,i){return a(i)}function Hb(){}var Ib=!1;function Jb(a,i,o){if(Ib)return a(i,o);Ib=!0;try{return Gb(a,i,o)}finally{Ib=!1,(zb!==null||Ab!==null)&&(Hb(),Fb())}}function Kb(a,i){var o=a.stateNode;if(o===null)return null;var s=Db(o);if(s===null)return null;o=s[i];e:switch(i){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(s=!s.disabled)||(a=a.type,s=!(a==="button"||a==="input"||a==="select"||a==="textarea")),a=!s;break e;default:a=!1}if(a)return null;if(o&&typeof o!="function")throw Error(p$2(231,i,typeof o));return o}var Lb=!1;if(ia)try{var Mb={};Object.defineProperty(Mb,"passive",{get:function(){Lb=!0}}),window.addEventListener("test",Mb,Mb),window.removeEventListener("test",Mb,Mb)}catch{Lb=!1}function Nb(a,i,o,s,$,j,_e,et,tt){var nt=Array.prototype.slice.call(arguments,3);try{i.apply(o,nt)}catch(at){this.onError(at)}}var Ob=!1,Pb=null,Qb=!1,Rb=null,Sb={onError:function(a){Ob=!0,Pb=a}};function Tb(a,i,o,s,$,j,_e,et,tt){Ob=!1,Pb=null,Nb.apply(Sb,arguments)}function Ub(a,i,o,s,$,j,_e,et,tt){if(Tb.apply(this,arguments),Ob){if(Ob){var nt=Pb;Ob=!1,Pb=null}else throw Error(p$2(198));Qb||(Qb=!0,Rb=nt)}}function Vb(a){var i=a,o=a;if(a.alternate)for(;i.return;)i=i.return;else{a=i;do i=a,i.flags&4098&&(o=i.return),a=i.return;while(a)}return i.tag===3?o:null}function Wb(a){if(a.tag===13){var i=a.memoizedState;if(i===null&&(a=a.alternate,a!==null&&(i=a.memoizedState)),i!==null)return i.dehydrated}return null}function Xb(a){if(Vb(a)!==a)throw Error(p$2(188))}function Yb(a){var i=a.alternate;if(!i){if(i=Vb(a),i===null)throw Error(p$2(188));return i!==a?null:a}for(var o=a,s=i;;){var $=o.return;if($===null)break;var j=$.alternate;if(j===null){if(s=$.return,s!==null){o=s;continue}break}if($.child===j.child){for(j=$.child;j;){if(j===o)return Xb($),a;if(j===s)return Xb($),i;j=j.sibling}throw Error(p$2(188))}if(o.return!==s.return)o=$,s=j;else{for(var _e=!1,et=$.child;et;){if(et===o){_e=!0,o=$,s=j;break}if(et===s){_e=!0,s=$,o=j;break}et=et.sibling}if(!_e){for(et=j.child;et;){if(et===o){_e=!0,o=j,s=$;break}if(et===s){_e=!0,s=j,o=$;break}et=et.sibling}if(!_e)throw Error(p$2(189))}}if(o.alternate!==s)throw Error(p$2(190))}if(o.tag!==3)throw Error(p$2(188));return o.stateNode.current===o?a:i}function Zb(a){return a=Yb(a),a!==null?$b(a):null}function $b(a){if(a.tag===5||a.tag===6)return a;for(a=a.child;a!==null;){var i=$b(a);if(i!==null)return i;a=a.sibling}return null}var ac=ca.unstable_scheduleCallback,bc=ca.unstable_cancelCallback,cc=ca.unstable_shouldYield,dc=ca.unstable_requestPaint,B=ca.unstable_now,ec=ca.unstable_getCurrentPriorityLevel,fc=ca.unstable_ImmediatePriority,gc=ca.unstable_UserBlockingPriority,hc=ca.unstable_NormalPriority,ic=ca.unstable_LowPriority,jc=ca.unstable_IdlePriority,kc=null,lc=null;function mc(a){if(lc&&typeof lc.onCommitFiberRoot=="function")try{lc.onCommitFiberRoot(kc,a,void 0,(a.current.flags&128)===128)}catch{}}var oc=Math.clz32?Math.clz32:nc,pc=Math.log,qc=Math.LN2;function nc(a){return a>>>=0,a===0?32:31-(pc(a)/qc|0)|0}var rc=64,sc=4194304;function tc(a){switch(a&-a){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return a&4194240;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return a&130023424;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return a}}function uc(a,i){var o=a.pendingLanes;if(o===0)return 0;var s=0,$=a.suspendedLanes,j=a.pingedLanes,_e=o&268435455;if(_e!==0){var et=_e&~$;et!==0?s=tc(et):(j&=_e,j!==0&&(s=tc(j)))}else _e=o&~$,_e!==0?s=tc(_e):j!==0&&(s=tc(j));if(s===0)return 0;if(i!==0&&i!==s&&!(i&$)&&($=s&-s,j=i&-i,$>=j||$===16&&(j&4194240)!==0))return i;if(s&4&&(s|=o&16),i=a.entangledLanes,i!==0)for(a=a.entanglements,i&=s;0<i;)o=31-oc(i),$=1<<o,s|=a[o],i&=~$;return s}function vc(a,i){switch(a){case 1:case 2:case 4:return i+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return i+5e3;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return-1;case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function wc(a,i){for(var o=a.suspendedLanes,s=a.pingedLanes,$=a.expirationTimes,j=a.pendingLanes;0<j;){var _e=31-oc(j),et=1<<_e,tt=$[_e];tt===-1?(!(et&o)||et&s)&&($[_e]=vc(et,i)):tt<=i&&(a.expiredLanes|=et),j&=~et}}function xc(a){return a=a.pendingLanes&-1073741825,a!==0?a:a&1073741824?1073741824:0}function yc(){var a=rc;return rc<<=1,!(rc&4194240)&&(rc=64),a}function zc(a){for(var i=[],o=0;31>o;o++)i.push(a);return i}function Ac(a,i,o){a.pendingLanes|=i,i!==536870912&&(a.suspendedLanes=0,a.pingedLanes=0),a=a.eventTimes,i=31-oc(i),a[i]=o}function Bc(a,i){var o=a.pendingLanes&~i;a.pendingLanes=i,a.suspendedLanes=0,a.pingedLanes=0,a.expiredLanes&=i,a.mutableReadLanes&=i,a.entangledLanes&=i,i=a.entanglements;var s=a.eventTimes;for(a=a.expirationTimes;0<o;){var $=31-oc(o),j=1<<$;i[$]=0,s[$]=-1,a[$]=-1,o&=~j}}function Cc(a,i){var o=a.entangledLanes|=i;for(a=a.entanglements;o;){var s=31-oc(o),$=1<<s;$&i|a[s]&i&&(a[s]|=i),o&=~$}}var C=0;function Dc(a){return a&=-a,1<a?4<a?a&268435455?16:536870912:4:1}var Ec,Fc,Gc,Hc,Ic,Jc=!1,Kc=[],Lc=null,Mc=null,Nc=null,Oc=new Map,Pc=new Map,Qc=[],Rc="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function Sc(a,i){switch(a){case"focusin":case"focusout":Lc=null;break;case"dragenter":case"dragleave":Mc=null;break;case"mouseover":case"mouseout":Nc=null;break;case"pointerover":case"pointerout":Oc.delete(i.pointerId);break;case"gotpointercapture":case"lostpointercapture":Pc.delete(i.pointerId)}}function Tc(a,i,o,s,$,j){return a===null||a.nativeEvent!==j?(a={blockedOn:i,domEventName:o,eventSystemFlags:s,nativeEvent:j,targetContainers:[$]},i!==null&&(i=Cb(i),i!==null&&Fc(i)),a):(a.eventSystemFlags|=s,i=a.targetContainers,$!==null&&i.indexOf($)===-1&&i.push($),a)}function Uc(a,i,o,s,$){switch(i){case"focusin":return Lc=Tc(Lc,a,i,o,s,$),!0;case"dragenter":return Mc=Tc(Mc,a,i,o,s,$),!0;case"mouseover":return Nc=Tc(Nc,a,i,o,s,$),!0;case"pointerover":var j=$.pointerId;return Oc.set(j,Tc(Oc.get(j)||null,a,i,o,s,$)),!0;case"gotpointercapture":return j=$.pointerId,Pc.set(j,Tc(Pc.get(j)||null,a,i,o,s,$)),!0}return!1}function Vc(a){var i=Wc(a.target);if(i!==null){var o=Vb(i);if(o!==null){if(i=o.tag,i===13){if(i=Wb(o),i!==null){a.blockedOn=i,Ic(a.priority,function(){Gc(o)});return}}else if(i===3&&o.stateNode.current.memoizedState.isDehydrated){a.blockedOn=o.tag===3?o.stateNode.containerInfo:null;return}}}a.blockedOn=null}function Xc(a){if(a.blockedOn!==null)return!1;for(var i=a.targetContainers;0<i.length;){var o=Yc(a.domEventName,a.eventSystemFlags,i[0],a.nativeEvent);if(o===null){o=a.nativeEvent;var s=new o.constructor(o.type,o);wb=s,o.target.dispatchEvent(s),wb=null}else return i=Cb(o),i!==null&&Fc(i),a.blockedOn=o,!1;i.shift()}return!0}function Zc(a,i,o){Xc(a)&&o.delete(i)}function $c(){Jc=!1,Lc!==null&&Xc(Lc)&&(Lc=null),Mc!==null&&Xc(Mc)&&(Mc=null),Nc!==null&&Xc(Nc)&&(Nc=null),Oc.forEach(Zc),Pc.forEach(Zc)}function ad(a,i){a.blockedOn===i&&(a.blockedOn=null,Jc||(Jc=!0,ca.unstable_scheduleCallback(ca.unstable_NormalPriority,$c)))}function bd(a){function i($){return ad($,a)}if(0<Kc.length){ad(Kc[0],a);for(var o=1;o<Kc.length;o++){var s=Kc[o];s.blockedOn===a&&(s.blockedOn=null)}}for(Lc!==null&&ad(Lc,a),Mc!==null&&ad(Mc,a),Nc!==null&&ad(Nc,a),Oc.forEach(i),Pc.forEach(i),o=0;o<Qc.length;o++)s=Qc[o],s.blockedOn===a&&(s.blockedOn=null);for(;0<Qc.length&&(o=Qc[0],o.blockedOn===null);)Vc(o),o.blockedOn===null&&Qc.shift()}var cd=ua.ReactCurrentBatchConfig,dd=!0;function ed(a,i,o,s){var $=C,j=cd.transition;cd.transition=null;try{C=1,fd(a,i,o,s)}finally{C=$,cd.transition=j}}function gd(a,i,o,s){var $=C,j=cd.transition;cd.transition=null;try{C=4,fd(a,i,o,s)}finally{C=$,cd.transition=j}}function fd(a,i,o,s){if(dd){var $=Yc(a,i,o,s);if($===null)hd(a,i,s,id,o),Sc(a,s);else if(Uc($,a,i,o,s))s.stopPropagation();else if(Sc(a,s),i&4&&-1<Rc.indexOf(a)){for(;$!==null;){var j=Cb($);if(j!==null&&Ec(j),j=Yc(a,i,o,s),j===null&&hd(a,i,s,id,o),j===$)break;$=j}$!==null&&s.stopPropagation()}else hd(a,i,s,null,o)}}var id=null;function Yc(a,i,o,s){if(id=null,a=xb(s),a=Wc(a),a!==null)if(i=Vb(a),i===null)a=null;else if(o=i.tag,o===13){if(a=Wb(i),a!==null)return a;a=null}else if(o===3){if(i.stateNode.current.memoizedState.isDehydrated)return i.tag===3?i.stateNode.containerInfo:null;a=null}else i!==a&&(a=null);return id=a,null}function jd(a){switch(a){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(ec()){case fc:return 1;case gc:return 4;case hc:case ic:return 16;case jc:return 536870912;default:return 16}default:return 16}}var kd=null,ld=null,md=null;function nd(){if(md)return md;var a,i=ld,o=i.length,s,$="value"in kd?kd.value:kd.textContent,j=$.length;for(a=0;a<o&&i[a]===$[a];a++);var _e=o-a;for(s=1;s<=_e&&i[o-s]===$[j-s];s++);return md=$.slice(a,1<s?1-s:void 0)}function od(a){var i=a.keyCode;return"charCode"in a?(a=a.charCode,a===0&&i===13&&(a=13)):a=i,a===10&&(a=13),32<=a||a===13?a:0}function pd(){return!0}function qd(){return!1}function rd(a){function i(o,s,$,j,_e){this._reactName=o,this._targetInst=$,this.type=s,this.nativeEvent=j,this.target=_e,this.currentTarget=null;for(var et in a)a.hasOwnProperty(et)&&(o=a[et],this[et]=o?o(j):j[et]);return this.isDefaultPrevented=(j.defaultPrevented!=null?j.defaultPrevented:j.returnValue===!1)?pd:qd,this.isPropagationStopped=qd,this}return A$1(i.prototype,{preventDefault:function(){this.defaultPrevented=!0;var o=this.nativeEvent;o&&(o.preventDefault?o.preventDefault():typeof o.returnValue!="unknown"&&(o.returnValue=!1),this.isDefaultPrevented=pd)},stopPropagation:function(){var o=this.nativeEvent;o&&(o.stopPropagation?o.stopPropagation():typeof o.cancelBubble!="unknown"&&(o.cancelBubble=!0),this.isPropagationStopped=pd)},persist:function(){},isPersistent:pd}),i}var sd={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(a){return a.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},td=rd(sd),ud=A$1({},sd,{view:0,detail:0}),vd=rd(ud),wd,xd,yd,Ad=A$1({},ud,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:zd,button:0,buttons:0,relatedTarget:function(a){return a.relatedTarget===void 0?a.fromElement===a.srcElement?a.toElement:a.fromElement:a.relatedTarget},movementX:function(a){return"movementX"in a?a.movementX:(a!==yd&&(yd&&a.type==="mousemove"?(wd=a.screenX-yd.screenX,xd=a.screenY-yd.screenY):xd=wd=0,yd=a),wd)},movementY:function(a){return"movementY"in a?a.movementY:xd}}),Bd=rd(Ad),Cd=A$1({},Ad,{dataTransfer:0}),Dd=rd(Cd),Ed=A$1({},ud,{relatedTarget:0}),Fd=rd(Ed),Gd=A$1({},sd,{animationName:0,elapsedTime:0,pseudoElement:0}),Hd=rd(Gd),Id=A$1({},sd,{clipboardData:function(a){return"clipboardData"in a?a.clipboardData:window.clipboardData}}),Jd=rd(Id),Kd=A$1({},sd,{data:0}),Ld=rd(Kd),Md={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Nd={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},Od={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Pd(a){var i=this.nativeEvent;return i.getModifierState?i.getModifierState(a):(a=Od[a])?!!i[a]:!1}function zd(){return Pd}var Qd=A$1({},ud,{key:function(a){if(a.key){var i=Md[a.key]||a.key;if(i!=="Unidentified")return i}return a.type==="keypress"?(a=od(a),a===13?"Enter":String.fromCharCode(a)):a.type==="keydown"||a.type==="keyup"?Nd[a.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:zd,charCode:function(a){return a.type==="keypress"?od(a):0},keyCode:function(a){return a.type==="keydown"||a.type==="keyup"?a.keyCode:0},which:function(a){return a.type==="keypress"?od(a):a.type==="keydown"||a.type==="keyup"?a.keyCode:0}}),Rd=rd(Qd),Sd=A$1({},Ad,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),Td=rd(Sd),Ud=A$1({},ud,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:zd}),Vd=rd(Ud),Wd=A$1({},sd,{propertyName:0,elapsedTime:0,pseudoElement:0}),Xd=rd(Wd),Yd=A$1({},Ad,{deltaX:function(a){return"deltaX"in a?a.deltaX:"wheelDeltaX"in a?-a.wheelDeltaX:0},deltaY:function(a){return"deltaY"in a?a.deltaY:"wheelDeltaY"in a?-a.wheelDeltaY:"wheelDelta"in a?-a.wheelDelta:0},deltaZ:0,deltaMode:0}),Zd=rd(Yd),$d=[9,13,27,32],ae=ia&&"CompositionEvent"in window,be=null;ia&&"documentMode"in document&&(be=document.documentMode);var ce=ia&&"TextEvent"in window&&!be,de=ia&&(!ae||be&&8<be&&11>=be),ee=" ",fe=!1;function ge(a,i){switch(a){case"keyup":return $d.indexOf(i.keyCode)!==-1;case"keydown":return i.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function he(a){return a=a.detail,typeof a=="object"&&"data"in a?a.data:null}var ie=!1;function je(a,i){switch(a){case"compositionend":return he(i);case"keypress":return i.which!==32?null:(fe=!0,ee);case"textInput":return a=i.data,a===ee&&fe?null:a;default:return null}}function ke(a,i){if(ie)return a==="compositionend"||!ae&&ge(a,i)?(a=nd(),md=ld=kd=null,ie=!1,a):null;switch(a){case"paste":return null;case"keypress":if(!(i.ctrlKey||i.altKey||i.metaKey)||i.ctrlKey&&i.altKey){if(i.char&&1<i.char.length)return i.char;if(i.which)return String.fromCharCode(i.which)}return null;case"compositionend":return de&&i.locale!=="ko"?null:i.data;default:return null}}var le={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function me(a){var i=a&&a.nodeName&&a.nodeName.toLowerCase();return i==="input"?!!le[a.type]:i==="textarea"}function ne(a,i,o,s){Eb(s),i=oe(i,"onChange"),0<i.length&&(o=new td("onChange","change",null,o,s),a.push({event:o,listeners:i}))}var pe=null,qe=null;function re(a){se(a,0)}function te(a){var i=ue(a);if(Wa(i))return a}function ve(a,i){if(a==="change")return i}var we=!1;if(ia){var xe;if(ia){var ye="oninput"in document;if(!ye){var ze=document.createElement("div");ze.setAttribute("oninput","return;"),ye=typeof ze.oninput=="function"}xe=ye}else xe=!1;we=xe&&(!document.documentMode||9<document.documentMode)}function Ae(){pe&&(pe.detachEvent("onpropertychange",Be),qe=pe=null)}function Be(a){if(a.propertyName==="value"&&te(qe)){var i=[];ne(i,qe,a,xb(a)),Jb(re,i)}}function Ce(a,i,o){a==="focusin"?(Ae(),pe=i,qe=o,pe.attachEvent("onpropertychange",Be)):a==="focusout"&&Ae()}function De(a){if(a==="selectionchange"||a==="keyup"||a==="keydown")return te(qe)}function Ee(a,i){if(a==="click")return te(i)}function Fe(a,i){if(a==="input"||a==="change")return te(i)}function Ge(a,i){return a===i&&(a!==0||1/a===1/i)||a!==a&&i!==i}var He=typeof Object.is=="function"?Object.is:Ge;function Ie(a,i){if(He(a,i))return!0;if(typeof a!="object"||a===null||typeof i!="object"||i===null)return!1;var o=Object.keys(a),s=Object.keys(i);if(o.length!==s.length)return!1;for(s=0;s<o.length;s++){var $=o[s];if(!ja.call(i,$)||!He(a[$],i[$]))return!1}return!0}function Je(a){for(;a&&a.firstChild;)a=a.firstChild;return a}function Ke(a,i){var o=Je(a);a=0;for(var s;o;){if(o.nodeType===3){if(s=a+o.textContent.length,a<=i&&s>=i)return{node:o,offset:i-a};a=s}e:{for(;o;){if(o.nextSibling){o=o.nextSibling;break e}o=o.parentNode}o=void 0}o=Je(o)}}function Le(a,i){return a&&i?a===i?!0:a&&a.nodeType===3?!1:i&&i.nodeType===3?Le(a,i.parentNode):"contains"in a?a.contains(i):a.compareDocumentPosition?!!(a.compareDocumentPosition(i)&16):!1:!1}function Me(){for(var a=window,i=Xa();i instanceof a.HTMLIFrameElement;){try{var o=typeof i.contentWindow.location.href=="string"}catch{o=!1}if(o)a=i.contentWindow;else break;i=Xa(a.document)}return i}function Ne(a){var i=a&&a.nodeName&&a.nodeName.toLowerCase();return i&&(i==="input"&&(a.type==="text"||a.type==="search"||a.type==="tel"||a.type==="url"||a.type==="password")||i==="textarea"||a.contentEditable==="true")}function Oe(a){var i=Me(),o=a.focusedElem,s=a.selectionRange;if(i!==o&&o&&o.ownerDocument&&Le(o.ownerDocument.documentElement,o)){if(s!==null&&Ne(o)){if(i=s.start,a=s.end,a===void 0&&(a=i),"selectionStart"in o)o.selectionStart=i,o.selectionEnd=Math.min(a,o.value.length);else if(a=(i=o.ownerDocument||document)&&i.defaultView||window,a.getSelection){a=a.getSelection();var $=o.textContent.length,j=Math.min(s.start,$);s=s.end===void 0?j:Math.min(s.end,$),!a.extend&&j>s&&($=s,s=j,j=$),$=Ke(o,j);var _e=Ke(o,s);$&&_e&&(a.rangeCount!==1||a.anchorNode!==$.node||a.anchorOffset!==$.offset||a.focusNode!==_e.node||a.focusOffset!==_e.offset)&&(i=i.createRange(),i.setStart($.node,$.offset),a.removeAllRanges(),j>s?(a.addRange(i),a.extend(_e.node,_e.offset)):(i.setEnd(_e.node,_e.offset),a.addRange(i)))}}for(i=[],a=o;a=a.parentNode;)a.nodeType===1&&i.push({element:a,left:a.scrollLeft,top:a.scrollTop});for(typeof o.focus=="function"&&o.focus(),o=0;o<i.length;o++)a=i[o],a.element.scrollLeft=a.left,a.element.scrollTop=a.top}}var Pe=ia&&"documentMode"in document&&11>=document.documentMode,Qe=null,Re=null,Se=null,Te=!1;function Ue(a,i,o){var s=o.window===o?o.document:o.nodeType===9?o:o.ownerDocument;Te||Qe==null||Qe!==Xa(s)||(s=Qe,"selectionStart"in s&&Ne(s)?s={start:s.selectionStart,end:s.selectionEnd}:(s=(s.ownerDocument&&s.ownerDocument.defaultView||window).getSelection(),s={anchorNode:s.anchorNode,anchorOffset:s.anchorOffset,focusNode:s.focusNode,focusOffset:s.focusOffset}),Se&&Ie(Se,s)||(Se=s,s=oe(Re,"onSelect"),0<s.length&&(i=new td("onSelect","select",null,i,o),a.push({event:i,listeners:s}),i.target=Qe)))}function Ve(a,i){var o={};return o[a.toLowerCase()]=i.toLowerCase(),o["Webkit"+a]="webkit"+i,o["Moz"+a]="moz"+i,o}var We={animationend:Ve("Animation","AnimationEnd"),animationiteration:Ve("Animation","AnimationIteration"),animationstart:Ve("Animation","AnimationStart"),transitionend:Ve("Transition","TransitionEnd")},Xe={},Ye={};ia&&(Ye=document.createElement("div").style,"AnimationEvent"in window||(delete We.animationend.animation,delete We.animationiteration.animation,delete We.animationstart.animation),"TransitionEvent"in window||delete We.transitionend.transition);function Ze(a){if(Xe[a])return Xe[a];if(!We[a])return a;var i=We[a],o;for(o in i)if(i.hasOwnProperty(o)&&o in Ye)return Xe[a]=i[o];return a}var $e=Ze("animationend"),af=Ze("animationiteration"),bf=Ze("animationstart"),cf=Ze("transitionend"),df=new Map,ef="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function ff(a,i){df.set(a,i),fa(i,[a])}for(var gf=0;gf<ef.length;gf++){var hf=ef[gf],jf=hf.toLowerCase(),kf=hf[0].toUpperCase()+hf.slice(1);ff(jf,"on"+kf)}ff($e,"onAnimationEnd");ff(af,"onAnimationIteration");ff(bf,"onAnimationStart");ff("dblclick","onDoubleClick");ff("focusin","onFocus");ff("focusout","onBlur");ff(cf,"onTransitionEnd");ha("onMouseEnter",["mouseout","mouseover"]);ha("onMouseLeave",["mouseout","mouseover"]);ha("onPointerEnter",["pointerout","pointerover"]);ha("onPointerLeave",["pointerout","pointerover"]);fa("onChange","change click focusin focusout input keydown keyup selectionchange".split(" "));fa("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" "));fa("onBeforeInput",["compositionend","keypress","textInput","paste"]);fa("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" "));fa("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" "));fa("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var lf="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),mf=new Set("cancel close invalid load scroll toggle".split(" ").concat(lf));function nf(a,i,o){var s=a.type||"unknown-event";a.currentTarget=o,Ub(s,i,void 0,a),a.currentTarget=null}function se(a,i){i=(i&4)!==0;for(var o=0;o<a.length;o++){var s=a[o],$=s.event;s=s.listeners;e:{var j=void 0;if(i)for(var _e=s.length-1;0<=_e;_e--){var et=s[_e],tt=et.instance,nt=et.currentTarget;if(et=et.listener,tt!==j&&$.isPropagationStopped())break e;nf($,et,nt),j=tt}else for(_e=0;_e<s.length;_e++){if(et=s[_e],tt=et.instance,nt=et.currentTarget,et=et.listener,tt!==j&&$.isPropagationStopped())break e;nf($,et,nt),j=tt}}}if(Qb)throw a=Rb,Qb=!1,Rb=null,a}function D(a,i){var o=i[of];o===void 0&&(o=i[of]=new Set);var s=a+"__bubble";o.has(s)||(pf(i,a,2,!1),o.add(s))}function qf(a,i,o){var s=0;i&&(s|=4),pf(o,a,s,i)}var rf="_reactListening"+Math.random().toString(36).slice(2);function sf(a){if(!a[rf]){a[rf]=!0,da.forEach(function(o){o!=="selectionchange"&&(mf.has(o)||qf(o,!1,a),qf(o,!0,a))});var i=a.nodeType===9?a:a.ownerDocument;i===null||i[rf]||(i[rf]=!0,qf("selectionchange",!1,i))}}function pf(a,i,o,s){switch(jd(i)){case 1:var $=ed;break;case 4:$=gd;break;default:$=fd}o=$.bind(null,i,o,a),$=void 0,!Lb||i!=="touchstart"&&i!=="touchmove"&&i!=="wheel"||($=!0),s?$!==void 0?a.addEventListener(i,o,{capture:!0,passive:$}):a.addEventListener(i,o,!0):$!==void 0?a.addEventListener(i,o,{passive:$}):a.addEventListener(i,o,!1)}function hd(a,i,o,s,$){var j=s;if(!(i&1)&&!(i&2)&&s!==null)e:for(;;){if(s===null)return;var _e=s.tag;if(_e===3||_e===4){var et=s.stateNode.containerInfo;if(et===$||et.nodeType===8&&et.parentNode===$)break;if(_e===4)for(_e=s.return;_e!==null;){var tt=_e.tag;if((tt===3||tt===4)&&(tt=_e.stateNode.containerInfo,tt===$||tt.nodeType===8&&tt.parentNode===$))return;_e=_e.return}for(;et!==null;){if(_e=Wc(et),_e===null)return;if(tt=_e.tag,tt===5||tt===6){s=j=_e;continue e}et=et.parentNode}}s=s.return}Jb(function(){var nt=j,at=xb(o),it=[];e:{var st=df.get(a);if(st!==void 0){var lt=td,ct=a;switch(a){case"keypress":if(od(o)===0)break e;case"keydown":case"keyup":lt=Rd;break;case"focusin":ct="focus",lt=Fd;break;case"focusout":ct="blur",lt=Fd;break;case"beforeblur":case"afterblur":lt=Fd;break;case"click":if(o.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":lt=Bd;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":lt=Dd;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":lt=Vd;break;case $e:case af:case bf:lt=Hd;break;case cf:lt=Xd;break;case"scroll":lt=vd;break;case"wheel":lt=Zd;break;case"copy":case"cut":case"paste":lt=Jd;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":lt=Td}var rt=(i&4)!==0,ut=!rt&&a==="scroll",ot=rt?st!==null?st+"Capture":null:st;rt=[];for(var dt=nt,pt;dt!==null;){pt=dt;var mt=pt.stateNode;if(pt.tag===5&&mt!==null&&(pt=mt,ot!==null&&(mt=Kb(dt,ot),mt!=null&&rt.push(tf(dt,mt,pt)))),ut)break;dt=dt.return}0<rt.length&&(st=new lt(st,ct,null,o,at),it.push({event:st,listeners:rt}))}}if(!(i&7)){e:{if(st=a==="mouseover"||a==="pointerover",lt=a==="mouseout"||a==="pointerout",st&&o!==wb&&(ct=o.relatedTarget||o.fromElement)&&(Wc(ct)||ct[uf]))break e;if((lt||st)&&(st=at.window===at?at:(st=at.ownerDocument)?st.defaultView||st.parentWindow:window,lt?(ct=o.relatedTarget||o.toElement,lt=nt,ct=ct?Wc(ct):null,ct!==null&&(ut=Vb(ct),ct!==ut||ct.tag!==5&&ct.tag!==6)&&(ct=null)):(lt=null,ct=nt),lt!==ct)){if(rt=Bd,mt="onMouseLeave",ot="onMouseEnter",dt="mouse",(a==="pointerout"||a==="pointerover")&&(rt=Td,mt="onPointerLeave",ot="onPointerEnter",dt="pointer"),ut=lt==null?st:ue(lt),pt=ct==null?st:ue(ct),st=new rt(mt,dt+"leave",lt,o,at),st.target=ut,st.relatedTarget=pt,mt=null,Wc(at)===nt&&(rt=new rt(ot,dt+"enter",ct,o,at),rt.target=pt,rt.relatedTarget=ut,mt=rt),ut=mt,lt&&ct)t:{for(rt=lt,ot=ct,dt=0,pt=rt;pt;pt=vf(pt))dt++;for(pt=0,mt=ot;mt;mt=vf(mt))pt++;for(;0<dt-pt;)rt=vf(rt),dt--;for(;0<pt-dt;)ot=vf(ot),pt--;for(;dt--;){if(rt===ot||ot!==null&&rt===ot.alternate)break t;rt=vf(rt),ot=vf(ot)}rt=null}else rt=null;lt!==null&&wf(it,st,lt,rt,!1),ct!==null&&ut!==null&&wf(it,ut,ct,rt,!0)}}e:{if(st=nt?ue(nt):window,lt=st.nodeName&&st.nodeName.toLowerCase(),lt==="select"||lt==="input"&&st.type==="file")var ft=ve;else if(me(st))if(we)ft=Fe;else{ft=De;var ht=Ce}else(lt=st.nodeName)&&lt.toLowerCase()==="input"&&(st.type==="checkbox"||st.type==="radio")&&(ft=Ee);if(ft&&(ft=ft(a,nt))){ne(it,ft,o,at);break e}ht&&ht(a,st,nt),a==="focusout"&&(ht=st._wrapperState)&&ht.controlled&&st.type==="number"&&cb(st,"number",st.value)}switch(ht=nt?ue(nt):window,a){case"focusin":(me(ht)||ht.contentEditable==="true")&&(Qe=ht,Re=nt,Se=null);break;case"focusout":Se=Re=Qe=null;break;case"mousedown":Te=!0;break;case"contextmenu":case"mouseup":case"dragend":Te=!1,Ue(it,o,at);break;case"selectionchange":if(Pe)break;case"keydown":case"keyup":Ue(it,o,at)}var yt;if(ae)e:{switch(a){case"compositionstart":var bt="onCompositionStart";break e;case"compositionend":bt="onCompositionEnd";break e;case"compositionupdate":bt="onCompositionUpdate";break e}bt=void 0}else ie?ge(a,o)&&(bt="onCompositionEnd"):a==="keydown"&&o.keyCode===229&&(bt="onCompositionStart");bt&&(de&&o.locale!=="ko"&&(ie||bt!=="onCompositionStart"?bt==="onCompositionEnd"&&ie&&(yt=nd()):(kd=at,ld="value"in kd?kd.value:kd.textContent,ie=!0)),ht=oe(nt,bt),0<ht.length&&(bt=new Ld(bt,a,null,o,at),it.push({event:bt,listeners:ht}),yt?bt.data=yt:(yt=he(o),yt!==null&&(bt.data=yt)))),(yt=ce?je(a,o):ke(a,o))&&(nt=oe(nt,"onBeforeInput"),0<nt.length&&(at=new Ld("onBeforeInput","beforeinput",null,o,at),it.push({event:at,listeners:nt}),at.data=yt))}se(it,i)})}function tf(a,i,o){return{instance:a,listener:i,currentTarget:o}}function oe(a,i){for(var o=i+"Capture",s=[];a!==null;){var $=a,j=$.stateNode;$.tag===5&&j!==null&&($=j,j=Kb(a,o),j!=null&&s.unshift(tf(a,j,$)),j=Kb(a,i),j!=null&&s.push(tf(a,j,$))),a=a.return}return s}function vf(a){if(a===null)return null;do a=a.return;while(a&&a.tag!==5);return a||null}function wf(a,i,o,s,$){for(var j=i._reactName,_e=[];o!==null&&o!==s;){var et=o,tt=et.alternate,nt=et.stateNode;if(tt!==null&&tt===s)break;et.tag===5&&nt!==null&&(et=nt,$?(tt=Kb(o,j),tt!=null&&_e.unshift(tf(o,tt,et))):$||(tt=Kb(o,j),tt!=null&&_e.push(tf(o,tt,et)))),o=o.return}_e.length!==0&&a.push({event:i,listeners:_e})}var xf=/\r\n?/g,yf=/\u0000|\uFFFD/g;function zf(a){return(typeof a=="string"?a:""+a).replace(xf,`
`).replace(yf,"")}function Af(a,i,o){if(i=zf(i),zf(a)!==i&&o)throw Error(p$2(425))}function Bf(){}var Cf=null,Df=null;function Ef(a,i){return a==="textarea"||a==="noscript"||typeof i.children=="string"||typeof i.children=="number"||typeof i.dangerouslySetInnerHTML=="object"&&i.dangerouslySetInnerHTML!==null&&i.dangerouslySetInnerHTML.__html!=null}var Ff=typeof setTimeout=="function"?setTimeout:void 0,Gf=typeof clearTimeout=="function"?clearTimeout:void 0,Hf=typeof Promise=="function"?Promise:void 0,Jf=typeof queueMicrotask=="function"?queueMicrotask:typeof Hf<"u"?function(a){return Hf.resolve(null).then(a).catch(If)}:Ff;function If(a){setTimeout(function(){throw a})}function Kf(a,i){var o=i,s=0;do{var $=o.nextSibling;if(a.removeChild(o),$&&$.nodeType===8)if(o=$.data,o==="/$"){if(s===0){a.removeChild($),bd(i);return}s--}else o!=="$"&&o!=="$?"&&o!=="$!"||s++;o=$}while(o);bd(i)}function Lf(a){for(;a!=null;a=a.nextSibling){var i=a.nodeType;if(i===1||i===3)break;if(i===8){if(i=a.data,i==="$"||i==="$!"||i==="$?")break;if(i==="/$")return null}}return a}function Mf(a){a=a.previousSibling;for(var i=0;a;){if(a.nodeType===8){var o=a.data;if(o==="$"||o==="$!"||o==="$?"){if(i===0)return a;i--}else o==="/$"&&i++}a=a.previousSibling}return null}var Nf=Math.random().toString(36).slice(2),Of="__reactFiber$"+Nf,Pf="__reactProps$"+Nf,uf="__reactContainer$"+Nf,of="__reactEvents$"+Nf,Qf="__reactListeners$"+Nf,Rf="__reactHandles$"+Nf;function Wc(a){var i=a[Of];if(i)return i;for(var o=a.parentNode;o;){if(i=o[uf]||o[Of]){if(o=i.alternate,i.child!==null||o!==null&&o.child!==null)for(a=Mf(a);a!==null;){if(o=a[Of])return o;a=Mf(a)}return i}a=o,o=a.parentNode}return null}function Cb(a){return a=a[Of]||a[uf],!a||a.tag!==5&&a.tag!==6&&a.tag!==13&&a.tag!==3?null:a}function ue(a){if(a.tag===5||a.tag===6)return a.stateNode;throw Error(p$2(33))}function Db(a){return a[Pf]||null}var Sf=[],Tf=-1;function Uf(a){return{current:a}}function E(a){0>Tf||(a.current=Sf[Tf],Sf[Tf]=null,Tf--)}function G(a,i){Tf++,Sf[Tf]=a.current,a.current=i}var Vf={},H=Uf(Vf),Wf=Uf(!1),Xf=Vf;function Yf(a,i){var o=a.type.contextTypes;if(!o)return Vf;var s=a.stateNode;if(s&&s.__reactInternalMemoizedUnmaskedChildContext===i)return s.__reactInternalMemoizedMaskedChildContext;var $={},j;for(j in o)$[j]=i[j];return s&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=i,a.__reactInternalMemoizedMaskedChildContext=$),$}function Zf(a){return a=a.childContextTypes,a!=null}function $f(){E(Wf),E(H)}function ag(a,i,o){if(H.current!==Vf)throw Error(p$2(168));G(H,i),G(Wf,o)}function bg(a,i,o){var s=a.stateNode;if(i=i.childContextTypes,typeof s.getChildContext!="function")return o;s=s.getChildContext();for(var $ in s)if(!($ in i))throw Error(p$2(108,Ra(a)||"Unknown",$));return A$1({},o,s)}function cg(a){return a=(a=a.stateNode)&&a.__reactInternalMemoizedMergedChildContext||Vf,Xf=H.current,G(H,a),G(Wf,Wf.current),!0}function dg(a,i,o){var s=a.stateNode;if(!s)throw Error(p$2(169));o?(a=bg(a,i,Xf),s.__reactInternalMemoizedMergedChildContext=a,E(Wf),E(H),G(H,a)):E(Wf),G(Wf,o)}var eg=null,fg=!1,gg=!1;function hg(a){eg===null?eg=[a]:eg.push(a)}function ig(a){fg=!0,hg(a)}function jg(){if(!gg&&eg!==null){gg=!0;var a=0,i=C;try{var o=eg;for(C=1;a<o.length;a++){var s=o[a];do s=s(!0);while(s!==null)}eg=null,fg=!1}catch($){throw eg!==null&&(eg=eg.slice(a+1)),ac(fc,jg),$}finally{C=i,gg=!1}}return null}var kg=[],lg=0,mg=null,ng=0,og=[],pg=0,qg=null,rg=1,sg="";function tg(a,i){kg[lg++]=ng,kg[lg++]=mg,mg=a,ng=i}function ug(a,i,o){og[pg++]=rg,og[pg++]=sg,og[pg++]=qg,qg=a;var s=rg;a=sg;var $=32-oc(s)-1;s&=~(1<<$),o+=1;var j=32-oc(i)+$;if(30<j){var _e=$-$%5;j=(s&(1<<_e)-1).toString(32),s>>=_e,$-=_e,rg=1<<32-oc(i)+$|o<<$|s,sg=j+a}else rg=1<<j|o<<$|s,sg=a}function vg(a){a.return!==null&&(tg(a,1),ug(a,1,0))}function wg(a){for(;a===mg;)mg=kg[--lg],kg[lg]=null,ng=kg[--lg],kg[lg]=null;for(;a===qg;)qg=og[--pg],og[pg]=null,sg=og[--pg],og[pg]=null,rg=og[--pg],og[pg]=null}var xg=null,yg=null,I=!1,zg=null;function Ag(a,i){var o=Bg(5,null,null,0);o.elementType="DELETED",o.stateNode=i,o.return=a,i=a.deletions,i===null?(a.deletions=[o],a.flags|=16):i.push(o)}function Cg(a,i){switch(a.tag){case 5:var o=a.type;return i=i.nodeType!==1||o.toLowerCase()!==i.nodeName.toLowerCase()?null:i,i!==null?(a.stateNode=i,xg=a,yg=Lf(i.firstChild),!0):!1;case 6:return i=a.pendingProps===""||i.nodeType!==3?null:i,i!==null?(a.stateNode=i,xg=a,yg=null,!0):!1;case 13:return i=i.nodeType!==8?null:i,i!==null?(o=qg!==null?{id:rg,overflow:sg}:null,a.memoizedState={dehydrated:i,treeContext:o,retryLane:1073741824},o=Bg(18,null,null,0),o.stateNode=i,o.return=a,a.child=o,xg=a,yg=null,!0):!1;default:return!1}}function Dg(a){return(a.mode&1)!==0&&(a.flags&128)===0}function Eg(a){if(I){var i=yg;if(i){var o=i;if(!Cg(a,i)){if(Dg(a))throw Error(p$2(418));i=Lf(o.nextSibling);var s=xg;i&&Cg(a,i)?Ag(s,o):(a.flags=a.flags&-4097|2,I=!1,xg=a)}}else{if(Dg(a))throw Error(p$2(418));a.flags=a.flags&-4097|2,I=!1,xg=a}}}function Fg(a){for(a=a.return;a!==null&&a.tag!==5&&a.tag!==3&&a.tag!==13;)a=a.return;xg=a}function Gg(a){if(a!==xg)return!1;if(!I)return Fg(a),I=!0,!1;var i;if((i=a.tag!==3)&&!(i=a.tag!==5)&&(i=a.type,i=i!=="head"&&i!=="body"&&!Ef(a.type,a.memoizedProps)),i&&(i=yg)){if(Dg(a))throw Hg(),Error(p$2(418));for(;i;)Ag(a,i),i=Lf(i.nextSibling)}if(Fg(a),a.tag===13){if(a=a.memoizedState,a=a!==null?a.dehydrated:null,!a)throw Error(p$2(317));e:{for(a=a.nextSibling,i=0;a;){if(a.nodeType===8){var o=a.data;if(o==="/$"){if(i===0){yg=Lf(a.nextSibling);break e}i--}else o!=="$"&&o!=="$!"&&o!=="$?"||i++}a=a.nextSibling}yg=null}}else yg=xg?Lf(a.stateNode.nextSibling):null;return!0}function Hg(){for(var a=yg;a;)a=Lf(a.nextSibling)}function Ig(){yg=xg=null,I=!1}function Jg(a){zg===null?zg=[a]:zg.push(a)}var Kg=ua.ReactCurrentBatchConfig;function Lg(a,i){if(a&&a.defaultProps){i=A$1({},i),a=a.defaultProps;for(var o in a)i[o]===void 0&&(i[o]=a[o]);return i}return i}var Mg=Uf(null),Ng=null,Og=null,Pg=null;function Qg(){Pg=Og=Ng=null}function Rg(a){var i=Mg.current;E(Mg),a._currentValue=i}function Sg(a,i,o){for(;a!==null;){var s=a.alternate;if((a.childLanes&i)!==i?(a.childLanes|=i,s!==null&&(s.childLanes|=i)):s!==null&&(s.childLanes&i)!==i&&(s.childLanes|=i),a===o)break;a=a.return}}function Tg(a,i){Ng=a,Pg=Og=null,a=a.dependencies,a!==null&&a.firstContext!==null&&(a.lanes&i&&(Ug=!0),a.firstContext=null)}function Vg(a){var i=a._currentValue;if(Pg!==a)if(a={context:a,memoizedValue:i,next:null},Og===null){if(Ng===null)throw Error(p$2(308));Og=a,Ng.dependencies={lanes:0,firstContext:a}}else Og=Og.next=a;return i}var Wg=null;function Xg(a){Wg===null?Wg=[a]:Wg.push(a)}function Yg(a,i,o,s){var $=i.interleaved;return $===null?(o.next=o,Xg(i)):(o.next=$.next,$.next=o),i.interleaved=o,Zg(a,s)}function Zg(a,i){a.lanes|=i;var o=a.alternate;for(o!==null&&(o.lanes|=i),o=a,a=a.return;a!==null;)a.childLanes|=i,o=a.alternate,o!==null&&(o.childLanes|=i),o=a,a=a.return;return o.tag===3?o.stateNode:null}var $g=!1;function ah(a){a.updateQueue={baseState:a.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function bh(a,i){a=a.updateQueue,i.updateQueue===a&&(i.updateQueue={baseState:a.baseState,firstBaseUpdate:a.firstBaseUpdate,lastBaseUpdate:a.lastBaseUpdate,shared:a.shared,effects:a.effects})}function ch(a,i){return{eventTime:a,lane:i,tag:0,payload:null,callback:null,next:null}}function dh(a,i,o){var s=a.updateQueue;if(s===null)return null;if(s=s.shared,K&2){var $=s.pending;return $===null?i.next=i:(i.next=$.next,$.next=i),s.pending=i,Zg(a,o)}return $=s.interleaved,$===null?(i.next=i,Xg(s)):(i.next=$.next,$.next=i),s.interleaved=i,Zg(a,o)}function eh(a,i,o){if(i=i.updateQueue,i!==null&&(i=i.shared,(o&4194240)!==0)){var s=i.lanes;s&=a.pendingLanes,o|=s,i.lanes=o,Cc(a,o)}}function fh(a,i){var o=a.updateQueue,s=a.alternate;if(s!==null&&(s=s.updateQueue,o===s)){var $=null,j=null;if(o=o.firstBaseUpdate,o!==null){do{var _e={eventTime:o.eventTime,lane:o.lane,tag:o.tag,payload:o.payload,callback:o.callback,next:null};j===null?$=j=_e:j=j.next=_e,o=o.next}while(o!==null);j===null?$=j=i:j=j.next=i}else $=j=i;o={baseState:s.baseState,firstBaseUpdate:$,lastBaseUpdate:j,shared:s.shared,effects:s.effects},a.updateQueue=o;return}a=o.lastBaseUpdate,a===null?o.firstBaseUpdate=i:a.next=i,o.lastBaseUpdate=i}function gh(a,i,o,s){var $=a.updateQueue;$g=!1;var j=$.firstBaseUpdate,_e=$.lastBaseUpdate,et=$.shared.pending;if(et!==null){$.shared.pending=null;var tt=et,nt=tt.next;tt.next=null,_e===null?j=nt:_e.next=nt,_e=tt;var at=a.alternate;at!==null&&(at=at.updateQueue,et=at.lastBaseUpdate,et!==_e&&(et===null?at.firstBaseUpdate=nt:et.next=nt,at.lastBaseUpdate=tt))}if(j!==null){var it=$.baseState;_e=0,at=nt=tt=null,et=j;do{var st=et.lane,lt=et.eventTime;if((s&st)===st){at!==null&&(at=at.next={eventTime:lt,lane:0,tag:et.tag,payload:et.payload,callback:et.callback,next:null});e:{var ct=a,rt=et;switch(st=i,lt=o,rt.tag){case 1:if(ct=rt.payload,typeof ct=="function"){it=ct.call(lt,it,st);break e}it=ct;break e;case 3:ct.flags=ct.flags&-65537|128;case 0:if(ct=rt.payload,st=typeof ct=="function"?ct.call(lt,it,st):ct,st==null)break e;it=A$1({},it,st);break e;case 2:$g=!0}}et.callback!==null&&et.lane!==0&&(a.flags|=64,st=$.effects,st===null?$.effects=[et]:st.push(et))}else lt={eventTime:lt,lane:st,tag:et.tag,payload:et.payload,callback:et.callback,next:null},at===null?(nt=at=lt,tt=it):at=at.next=lt,_e|=st;if(et=et.next,et===null){if(et=$.shared.pending,et===null)break;st=et,et=st.next,st.next=null,$.lastBaseUpdate=st,$.shared.pending=null}}while(!0);if(at===null&&(tt=it),$.baseState=tt,$.firstBaseUpdate=nt,$.lastBaseUpdate=at,i=$.shared.interleaved,i!==null){$=i;do _e|=$.lane,$=$.next;while($!==i)}else j===null&&($.shared.lanes=0);hh|=_e,a.lanes=_e,a.memoizedState=it}}function ih(a,i,o){if(a=i.effects,i.effects=null,a!==null)for(i=0;i<a.length;i++){var s=a[i],$=s.callback;if($!==null){if(s.callback=null,s=o,typeof $!="function")throw Error(p$2(191,$));$.call(s)}}}var jh=new aa.Component().refs;function kh(a,i,o,s){i=a.memoizedState,o=o(s,i),o=o==null?i:A$1({},i,o),a.memoizedState=o,a.lanes===0&&(a.updateQueue.baseState=o)}var nh={isMounted:function(a){return(a=a._reactInternals)?Vb(a)===a:!1},enqueueSetState:function(a,i,o){a=a._reactInternals;var s=L(),$=lh(a),j=ch(s,$);j.payload=i,o!=null&&(j.callback=o),i=dh(a,j,$),i!==null&&(mh(i,a,$,s),eh(i,a,$))},enqueueReplaceState:function(a,i,o){a=a._reactInternals;var s=L(),$=lh(a),j=ch(s,$);j.tag=1,j.payload=i,o!=null&&(j.callback=o),i=dh(a,j,$),i!==null&&(mh(i,a,$,s),eh(i,a,$))},enqueueForceUpdate:function(a,i){a=a._reactInternals;var o=L(),s=lh(a),$=ch(o,s);$.tag=2,i!=null&&($.callback=i),i=dh(a,$,s),i!==null&&(mh(i,a,s,o),eh(i,a,s))}};function oh(a,i,o,s,$,j,_e){return a=a.stateNode,typeof a.shouldComponentUpdate=="function"?a.shouldComponentUpdate(s,j,_e):i.prototype&&i.prototype.isPureReactComponent?!Ie(o,s)||!Ie($,j):!0}function ph(a,i,o){var s=!1,$=Vf,j=i.contextType;return typeof j=="object"&&j!==null?j=Vg(j):($=Zf(i)?Xf:H.current,s=i.contextTypes,j=(s=s!=null)?Yf(a,$):Vf),i=new i(o,j),a.memoizedState=i.state!==null&&i.state!==void 0?i.state:null,i.updater=nh,a.stateNode=i,i._reactInternals=a,s&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=$,a.__reactInternalMemoizedMaskedChildContext=j),i}function qh(a,i,o,s){a=i.state,typeof i.componentWillReceiveProps=="function"&&i.componentWillReceiveProps(o,s),typeof i.UNSAFE_componentWillReceiveProps=="function"&&i.UNSAFE_componentWillReceiveProps(o,s),i.state!==a&&nh.enqueueReplaceState(i,i.state,null)}function rh(a,i,o,s){var $=a.stateNode;$.props=o,$.state=a.memoizedState,$.refs=jh,ah(a);var j=i.contextType;typeof j=="object"&&j!==null?$.context=Vg(j):(j=Zf(i)?Xf:H.current,$.context=Yf(a,j)),$.state=a.memoizedState,j=i.getDerivedStateFromProps,typeof j=="function"&&(kh(a,i,j,o),$.state=a.memoizedState),typeof i.getDerivedStateFromProps=="function"||typeof $.getSnapshotBeforeUpdate=="function"||typeof $.UNSAFE_componentWillMount!="function"&&typeof $.componentWillMount!="function"||(i=$.state,typeof $.componentWillMount=="function"&&$.componentWillMount(),typeof $.UNSAFE_componentWillMount=="function"&&$.UNSAFE_componentWillMount(),i!==$.state&&nh.enqueueReplaceState($,$.state,null),gh(a,o,$,s),$.state=a.memoizedState),typeof $.componentDidMount=="function"&&(a.flags|=4194308)}function sh(a,i,o){if(a=o.ref,a!==null&&typeof a!="function"&&typeof a!="object"){if(o._owner){if(o=o._owner,o){if(o.tag!==1)throw Error(p$2(309));var s=o.stateNode}if(!s)throw Error(p$2(147,a));var $=s,j=""+a;return i!==null&&i.ref!==null&&typeof i.ref=="function"&&i.ref._stringRef===j?i.ref:(i=function(_e){var et=$.refs;et===jh&&(et=$.refs={}),_e===null?delete et[j]:et[j]=_e},i._stringRef=j,i)}if(typeof a!="string")throw Error(p$2(284));if(!o._owner)throw Error(p$2(290,a))}return a}function th(a,i){throw a=Object.prototype.toString.call(i),Error(p$2(31,a==="[object Object]"?"object with keys {"+Object.keys(i).join(", ")+"}":a))}function uh(a){var i=a._init;return i(a._payload)}function vh(a){function i(ot,dt){if(a){var pt=ot.deletions;pt===null?(ot.deletions=[dt],ot.flags|=16):pt.push(dt)}}function o(ot,dt){if(!a)return null;for(;dt!==null;)i(ot,dt),dt=dt.sibling;return null}function s(ot,dt){for(ot=new Map;dt!==null;)dt.key!==null?ot.set(dt.key,dt):ot.set(dt.index,dt),dt=dt.sibling;return ot}function $(ot,dt){return ot=wh(ot,dt),ot.index=0,ot.sibling=null,ot}function j(ot,dt,pt){return ot.index=pt,a?(pt=ot.alternate,pt!==null?(pt=pt.index,pt<dt?(ot.flags|=2,dt):pt):(ot.flags|=2,dt)):(ot.flags|=1048576,dt)}function _e(ot){return a&&ot.alternate===null&&(ot.flags|=2),ot}function et(ot,dt,pt,mt){return dt===null||dt.tag!==6?(dt=xh(pt,ot.mode,mt),dt.return=ot,dt):(dt=$(dt,pt),dt.return=ot,dt)}function tt(ot,dt,pt,mt){var ft=pt.type;return ft===ya?at(ot,dt,pt.props.children,mt,pt.key):dt!==null&&(dt.elementType===ft||typeof ft=="object"&&ft!==null&&ft.$$typeof===Ha&&uh(ft)===dt.type)?(mt=$(dt,pt.props),mt.ref=sh(ot,dt,pt),mt.return=ot,mt):(mt=yh(pt.type,pt.key,pt.props,null,ot.mode,mt),mt.ref=sh(ot,dt,pt),mt.return=ot,mt)}function nt(ot,dt,pt,mt){return dt===null||dt.tag!==4||dt.stateNode.containerInfo!==pt.containerInfo||dt.stateNode.implementation!==pt.implementation?(dt=zh(pt,ot.mode,mt),dt.return=ot,dt):(dt=$(dt,pt.children||[]),dt.return=ot,dt)}function at(ot,dt,pt,mt,ft){return dt===null||dt.tag!==7?(dt=Ah(pt,ot.mode,mt,ft),dt.return=ot,dt):(dt=$(dt,pt),dt.return=ot,dt)}function it(ot,dt,pt){if(typeof dt=="string"&&dt!==""||typeof dt=="number")return dt=xh(""+dt,ot.mode,pt),dt.return=ot,dt;if(typeof dt=="object"&&dt!==null){switch(dt.$$typeof){case va:return pt=yh(dt.type,dt.key,dt.props,null,ot.mode,pt),pt.ref=sh(ot,null,dt),pt.return=ot,pt;case wa:return dt=zh(dt,ot.mode,pt),dt.return=ot,dt;case Ha:var mt=dt._init;return it(ot,mt(dt._payload),pt)}if(eb(dt)||Ka(dt))return dt=Ah(dt,ot.mode,pt,null),dt.return=ot,dt;th(ot,dt)}return null}function st(ot,dt,pt,mt){var ft=dt!==null?dt.key:null;if(typeof pt=="string"&&pt!==""||typeof pt=="number")return ft!==null?null:et(ot,dt,""+pt,mt);if(typeof pt=="object"&&pt!==null){switch(pt.$$typeof){case va:return pt.key===ft?tt(ot,dt,pt,mt):null;case wa:return pt.key===ft?nt(ot,dt,pt,mt):null;case Ha:return ft=pt._init,st(ot,dt,ft(pt._payload),mt)}if(eb(pt)||Ka(pt))return ft!==null?null:at(ot,dt,pt,mt,null);th(ot,pt)}return null}function lt(ot,dt,pt,mt,ft){if(typeof mt=="string"&&mt!==""||typeof mt=="number")return ot=ot.get(pt)||null,et(dt,ot,""+mt,ft);if(typeof mt=="object"&&mt!==null){switch(mt.$$typeof){case va:return ot=ot.get(mt.key===null?pt:mt.key)||null,tt(dt,ot,mt,ft);case wa:return ot=ot.get(mt.key===null?pt:mt.key)||null,nt(dt,ot,mt,ft);case Ha:var ht=mt._init;return lt(ot,dt,pt,ht(mt._payload),ft)}if(eb(mt)||Ka(mt))return ot=ot.get(pt)||null,at(dt,ot,mt,ft,null);th(dt,mt)}return null}function ct(ot,dt,pt,mt){for(var ft=null,ht=null,yt=dt,bt=dt=0,gt=null;yt!==null&&bt<pt.length;bt++){yt.index>bt?(gt=yt,yt=null):gt=yt.sibling;var xt=st(ot,yt,pt[bt],mt);if(xt===null){yt===null&&(yt=gt);break}a&&yt&&xt.alternate===null&&i(ot,yt),dt=j(xt,dt,bt),ht===null?ft=xt:ht.sibling=xt,ht=xt,yt=gt}if(bt===pt.length)return o(ot,yt),I&&tg(ot,bt),ft;if(yt===null){for(;bt<pt.length;bt++)yt=it(ot,pt[bt],mt),yt!==null&&(dt=j(yt,dt,bt),ht===null?ft=yt:ht.sibling=yt,ht=yt);return I&&tg(ot,bt),ft}for(yt=s(ot,yt);bt<pt.length;bt++)gt=lt(yt,ot,bt,pt[bt],mt),gt!==null&&(a&&gt.alternate!==null&&yt.delete(gt.key===null?bt:gt.key),dt=j(gt,dt,bt),ht===null?ft=gt:ht.sibling=gt,ht=gt);return a&&yt.forEach(function(vt){return i(ot,vt)}),I&&tg(ot,bt),ft}function rt(ot,dt,pt,mt){var ft=Ka(pt);if(typeof ft!="function")throw Error(p$2(150));if(pt=ft.call(pt),pt==null)throw Error(p$2(151));for(var ht=ft=null,yt=dt,bt=dt=0,gt=null,xt=pt.next();yt!==null&&!xt.done;bt++,xt=pt.next()){yt.index>bt?(gt=yt,yt=null):gt=yt.sibling;var vt=st(ot,yt,xt.value,mt);if(vt===null){yt===null&&(yt=gt);break}a&&yt&&vt.alternate===null&&i(ot,yt),dt=j(vt,dt,bt),ht===null?ft=vt:ht.sibling=vt,ht=vt,yt=gt}if(xt.done)return o(ot,yt),I&&tg(ot,bt),ft;if(yt===null){for(;!xt.done;bt++,xt=pt.next())xt=it(ot,xt.value,mt),xt!==null&&(dt=j(xt,dt,bt),ht===null?ft=xt:ht.sibling=xt,ht=xt);return I&&tg(ot,bt),ft}for(yt=s(ot,yt);!xt.done;bt++,xt=pt.next())xt=lt(yt,ot,bt,xt.value,mt),xt!==null&&(a&&xt.alternate!==null&&yt.delete(xt.key===null?bt:xt.key),dt=j(xt,dt,bt),ht===null?ft=xt:ht.sibling=xt,ht=xt);return a&&yt.forEach(function(Lt){return i(ot,Lt)}),I&&tg(ot,bt),ft}function ut(ot,dt,pt,mt){if(typeof pt=="object"&&pt!==null&&pt.type===ya&&pt.key===null&&(pt=pt.props.children),typeof pt=="object"&&pt!==null){switch(pt.$$typeof){case va:e:{for(var ft=pt.key,ht=dt;ht!==null;){if(ht.key===ft){if(ft=pt.type,ft===ya){if(ht.tag===7){o(ot,ht.sibling),dt=$(ht,pt.props.children),dt.return=ot,ot=dt;break e}}else if(ht.elementType===ft||typeof ft=="object"&&ft!==null&&ft.$$typeof===Ha&&uh(ft)===ht.type){o(ot,ht.sibling),dt=$(ht,pt.props),dt.ref=sh(ot,ht,pt),dt.return=ot,ot=dt;break e}o(ot,ht);break}else i(ot,ht);ht=ht.sibling}pt.type===ya?(dt=Ah(pt.props.children,ot.mode,mt,pt.key),dt.return=ot,ot=dt):(mt=yh(pt.type,pt.key,pt.props,null,ot.mode,mt),mt.ref=sh(ot,dt,pt),mt.return=ot,ot=mt)}return _e(ot);case wa:e:{for(ht=pt.key;dt!==null;){if(dt.key===ht)if(dt.tag===4&&dt.stateNode.containerInfo===pt.containerInfo&&dt.stateNode.implementation===pt.implementation){o(ot,dt.sibling),dt=$(dt,pt.children||[]),dt.return=ot,ot=dt;break e}else{o(ot,dt);break}else i(ot,dt);dt=dt.sibling}dt=zh(pt,ot.mode,mt),dt.return=ot,ot=dt}return _e(ot);case Ha:return ht=pt._init,ut(ot,dt,ht(pt._payload),mt)}if(eb(pt))return ct(ot,dt,pt,mt);if(Ka(pt))return rt(ot,dt,pt,mt);th(ot,pt)}return typeof pt=="string"&&pt!==""||typeof pt=="number"?(pt=""+pt,dt!==null&&dt.tag===6?(o(ot,dt.sibling),dt=$(dt,pt),dt.return=ot,ot=dt):(o(ot,dt),dt=xh(pt,ot.mode,mt),dt.return=ot,ot=dt),_e(ot)):o(ot,dt)}return ut}var Bh=vh(!0),Ch=vh(!1),Dh={},Eh=Uf(Dh),Fh=Uf(Dh),Gh=Uf(Dh);function Hh(a){if(a===Dh)throw Error(p$2(174));return a}function Ih(a,i){switch(G(Gh,i),G(Fh,a),G(Eh,Dh),a=i.nodeType,a){case 9:case 11:i=(i=i.documentElement)?i.namespaceURI:lb(null,"");break;default:a=a===8?i.parentNode:i,i=a.namespaceURI||null,a=a.tagName,i=lb(i,a)}E(Eh),G(Eh,i)}function Jh(){E(Eh),E(Fh),E(Gh)}function Kh(a){Hh(Gh.current);var i=Hh(Eh.current),o=lb(i,a.type);i!==o&&(G(Fh,a),G(Eh,o))}function Lh(a){Fh.current===a&&(E(Eh),E(Fh))}var M=Uf(0);function Mh(a){for(var i=a;i!==null;){if(i.tag===13){var o=i.memoizedState;if(o!==null&&(o=o.dehydrated,o===null||o.data==="$?"||o.data==="$!"))return i}else if(i.tag===19&&i.memoizedProps.revealOrder!==void 0){if(i.flags&128)return i}else if(i.child!==null){i.child.return=i,i=i.child;continue}if(i===a)break;for(;i.sibling===null;){if(i.return===null||i.return===a)return null;i=i.return}i.sibling.return=i.return,i=i.sibling}return null}var Nh=[];function Oh(){for(var a=0;a<Nh.length;a++)Nh[a]._workInProgressVersionPrimary=null;Nh.length=0}var Ph=ua.ReactCurrentDispatcher,Qh=ua.ReactCurrentBatchConfig,Rh=0,N=null,O=null,P=null,Sh=!1,Th=!1,Uh=0,Vh=0;function Q(){throw Error(p$2(321))}function Wh(a,i){if(i===null)return!1;for(var o=0;o<i.length&&o<a.length;o++)if(!He(a[o],i[o]))return!1;return!0}function Xh(a,i,o,s,$,j){if(Rh=j,N=i,i.memoizedState=null,i.updateQueue=null,i.lanes=0,Ph.current=a===null||a.memoizedState===null?Yh:Zh,a=o(s,$),Th){j=0;do{if(Th=!1,Uh=0,25<=j)throw Error(p$2(301));j+=1,P=O=null,i.updateQueue=null,Ph.current=$h,a=o(s,$)}while(Th)}if(Ph.current=ai,i=O!==null&&O.next!==null,Rh=0,P=O=N=null,Sh=!1,i)throw Error(p$2(300));return a}function bi(){var a=Uh!==0;return Uh=0,a}function ci(){var a={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return P===null?N.memoizedState=P=a:P=P.next=a,P}function di(){if(O===null){var a=N.alternate;a=a!==null?a.memoizedState:null}else a=O.next;var i=P===null?N.memoizedState:P.next;if(i!==null)P=i,O=a;else{if(a===null)throw Error(p$2(310));O=a,a={memoizedState:O.memoizedState,baseState:O.baseState,baseQueue:O.baseQueue,queue:O.queue,next:null},P===null?N.memoizedState=P=a:P=P.next=a}return P}function ei(a,i){return typeof i=="function"?i(a):i}function fi(a){var i=di(),o=i.queue;if(o===null)throw Error(p$2(311));o.lastRenderedReducer=a;var s=O,$=s.baseQueue,j=o.pending;if(j!==null){if($!==null){var _e=$.next;$.next=j.next,j.next=_e}s.baseQueue=$=j,o.pending=null}if($!==null){j=$.next,s=s.baseState;var et=_e=null,tt=null,nt=j;do{var at=nt.lane;if((Rh&at)===at)tt!==null&&(tt=tt.next={lane:0,action:nt.action,hasEagerState:nt.hasEagerState,eagerState:nt.eagerState,next:null}),s=nt.hasEagerState?nt.eagerState:a(s,nt.action);else{var it={lane:at,action:nt.action,hasEagerState:nt.hasEagerState,eagerState:nt.eagerState,next:null};tt===null?(et=tt=it,_e=s):tt=tt.next=it,N.lanes|=at,hh|=at}nt=nt.next}while(nt!==null&&nt!==j);tt===null?_e=s:tt.next=et,He(s,i.memoizedState)||(Ug=!0),i.memoizedState=s,i.baseState=_e,i.baseQueue=tt,o.lastRenderedState=s}if(a=o.interleaved,a!==null){$=a;do j=$.lane,N.lanes|=j,hh|=j,$=$.next;while($!==a)}else $===null&&(o.lanes=0);return[i.memoizedState,o.dispatch]}function gi(a){var i=di(),o=i.queue;if(o===null)throw Error(p$2(311));o.lastRenderedReducer=a;var s=o.dispatch,$=o.pending,j=i.memoizedState;if($!==null){o.pending=null;var _e=$=$.next;do j=a(j,_e.action),_e=_e.next;while(_e!==$);He(j,i.memoizedState)||(Ug=!0),i.memoizedState=j,i.baseQueue===null&&(i.baseState=j),o.lastRenderedState=j}return[j,s]}function hi(){}function ii(a,i){var o=N,s=di(),$=i(),j=!He(s.memoizedState,$);if(j&&(s.memoizedState=$,Ug=!0),s=s.queue,ji(ki.bind(null,o,s,a),[a]),s.getSnapshot!==i||j||P!==null&&P.memoizedState.tag&1){if(o.flags|=2048,li(9,mi.bind(null,o,s,$,i),void 0,null),R===null)throw Error(p$2(349));Rh&30||ni(o,i,$)}return $}function ni(a,i,o){a.flags|=16384,a={getSnapshot:i,value:o},i=N.updateQueue,i===null?(i={lastEffect:null,stores:null},N.updateQueue=i,i.stores=[a]):(o=i.stores,o===null?i.stores=[a]:o.push(a))}function mi(a,i,o,s){i.value=o,i.getSnapshot=s,oi(i)&&pi(a)}function ki(a,i,o){return o(function(){oi(i)&&pi(a)})}function oi(a){var i=a.getSnapshot;a=a.value;try{var o=i();return!He(a,o)}catch{return!0}}function pi(a){var i=Zg(a,1);i!==null&&mh(i,a,1,-1)}function qi(a){var i=ci();return typeof a=="function"&&(a=a()),i.memoizedState=i.baseState=a,a={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:ei,lastRenderedState:a},i.queue=a,a=a.dispatch=ri.bind(null,N,a),[i.memoizedState,a]}function li(a,i,o,s){return a={tag:a,create:i,destroy:o,deps:s,next:null},i=N.updateQueue,i===null?(i={lastEffect:null,stores:null},N.updateQueue=i,i.lastEffect=a.next=a):(o=i.lastEffect,o===null?i.lastEffect=a.next=a:(s=o.next,o.next=a,a.next=s,i.lastEffect=a)),a}function si(){return di().memoizedState}function ti(a,i,o,s){var $=ci();N.flags|=a,$.memoizedState=li(1|i,o,void 0,s===void 0?null:s)}function ui(a,i,o,s){var $=di();s=s===void 0?null:s;var j=void 0;if(O!==null){var _e=O.memoizedState;if(j=_e.destroy,s!==null&&Wh(s,_e.deps)){$.memoizedState=li(i,o,j,s);return}}N.flags|=a,$.memoizedState=li(1|i,o,j,s)}function vi(a,i){return ti(8390656,8,a,i)}function ji(a,i){return ui(2048,8,a,i)}function wi(a,i){return ui(4,2,a,i)}function xi(a,i){return ui(4,4,a,i)}function yi(a,i){if(typeof i=="function")return a=a(),i(a),function(){i(null)};if(i!=null)return a=a(),i.current=a,function(){i.current=null}}function zi(a,i,o){return o=o!=null?o.concat([a]):null,ui(4,4,yi.bind(null,i,a),o)}function Ai(){}function Bi(a,i){var o=di();i=i===void 0?null:i;var s=o.memoizedState;return s!==null&&i!==null&&Wh(i,s[1])?s[0]:(o.memoizedState=[a,i],a)}function Ci(a,i){var o=di();i=i===void 0?null:i;var s=o.memoizedState;return s!==null&&i!==null&&Wh(i,s[1])?s[0]:(a=a(),o.memoizedState=[a,i],a)}function Di(a,i,o){return Rh&21?(He(o,i)||(o=yc(),N.lanes|=o,hh|=o,a.baseState=!0),i):(a.baseState&&(a.baseState=!1,Ug=!0),a.memoizedState=o)}function Ei(a,i){var o=C;C=o!==0&&4>o?o:4,a(!0);var s=Qh.transition;Qh.transition={};try{a(!1),i()}finally{C=o,Qh.transition=s}}function Fi(){return di().memoizedState}function Gi(a,i,o){var s=lh(a);if(o={lane:s,action:o,hasEagerState:!1,eagerState:null,next:null},Hi(a))Ii(i,o);else if(o=Yg(a,i,o,s),o!==null){var $=L();mh(o,a,s,$),Ji(o,i,s)}}function ri(a,i,o){var s=lh(a),$={lane:s,action:o,hasEagerState:!1,eagerState:null,next:null};if(Hi(a))Ii(i,$);else{var j=a.alternate;if(a.lanes===0&&(j===null||j.lanes===0)&&(j=i.lastRenderedReducer,j!==null))try{var _e=i.lastRenderedState,et=j(_e,o);if($.hasEagerState=!0,$.eagerState=et,He(et,_e)){var tt=i.interleaved;tt===null?($.next=$,Xg(i)):($.next=tt.next,tt.next=$),i.interleaved=$;return}}catch{}finally{}o=Yg(a,i,$,s),o!==null&&($=L(),mh(o,a,s,$),Ji(o,i,s))}}function Hi(a){var i=a.alternate;return a===N||i!==null&&i===N}function Ii(a,i){Th=Sh=!0;var o=a.pending;o===null?i.next=i:(i.next=o.next,o.next=i),a.pending=i}function Ji(a,i,o){if(o&4194240){var s=i.lanes;s&=a.pendingLanes,o|=s,i.lanes=o,Cc(a,o)}}var ai={readContext:Vg,useCallback:Q,useContext:Q,useEffect:Q,useImperativeHandle:Q,useInsertionEffect:Q,useLayoutEffect:Q,useMemo:Q,useReducer:Q,useRef:Q,useState:Q,useDebugValue:Q,useDeferredValue:Q,useTransition:Q,useMutableSource:Q,useSyncExternalStore:Q,useId:Q,unstable_isNewReconciler:!1},Yh={readContext:Vg,useCallback:function(a,i){return ci().memoizedState=[a,i===void 0?null:i],a},useContext:Vg,useEffect:vi,useImperativeHandle:function(a,i,o){return o=o!=null?o.concat([a]):null,ti(4194308,4,yi.bind(null,i,a),o)},useLayoutEffect:function(a,i){return ti(4194308,4,a,i)},useInsertionEffect:function(a,i){return ti(4,2,a,i)},useMemo:function(a,i){var o=ci();return i=i===void 0?null:i,a=a(),o.memoizedState=[a,i],a},useReducer:function(a,i,o){var s=ci();return i=o!==void 0?o(i):i,s.memoizedState=s.baseState=i,a={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:a,lastRenderedState:i},s.queue=a,a=a.dispatch=Gi.bind(null,N,a),[s.memoizedState,a]},useRef:function(a){var i=ci();return a={current:a},i.memoizedState=a},useState:qi,useDebugValue:Ai,useDeferredValue:function(a){return ci().memoizedState=a},useTransition:function(){var a=qi(!1),i=a[0];return a=Ei.bind(null,a[1]),ci().memoizedState=a,[i,a]},useMutableSource:function(){},useSyncExternalStore:function(a,i,o){var s=N,$=ci();if(I){if(o===void 0)throw Error(p$2(407));o=o()}else{if(o=i(),R===null)throw Error(p$2(349));Rh&30||ni(s,i,o)}$.memoizedState=o;var j={value:o,getSnapshot:i};return $.queue=j,vi(ki.bind(null,s,j,a),[a]),s.flags|=2048,li(9,mi.bind(null,s,j,o,i),void 0,null),o},useId:function(){var a=ci(),i=R.identifierPrefix;if(I){var o=sg,s=rg;o=(s&~(1<<32-oc(s)-1)).toString(32)+o,i=":"+i+"R"+o,o=Uh++,0<o&&(i+="H"+o.toString(32)),i+=":"}else o=Vh++,i=":"+i+"r"+o.toString(32)+":";return a.memoizedState=i},unstable_isNewReconciler:!1},Zh={readContext:Vg,useCallback:Bi,useContext:Vg,useEffect:ji,useImperativeHandle:zi,useInsertionEffect:wi,useLayoutEffect:xi,useMemo:Ci,useReducer:fi,useRef:si,useState:function(){return fi(ei)},useDebugValue:Ai,useDeferredValue:function(a){var i=di();return Di(i,O.memoizedState,a)},useTransition:function(){var a=fi(ei)[0],i=di().memoizedState;return[a,i]},useMutableSource:hi,useSyncExternalStore:ii,useId:Fi,unstable_isNewReconciler:!1},$h={readContext:Vg,useCallback:Bi,useContext:Vg,useEffect:ji,useImperativeHandle:zi,useInsertionEffect:wi,useLayoutEffect:xi,useMemo:Ci,useReducer:gi,useRef:si,useState:function(){return gi(ei)},useDebugValue:Ai,useDeferredValue:function(a){var i=di();return O===null?i.memoizedState=a:Di(i,O.memoizedState,a)},useTransition:function(){var a=gi(ei)[0],i=di().memoizedState;return[a,i]},useMutableSource:hi,useSyncExternalStore:ii,useId:Fi,unstable_isNewReconciler:!1};function Ki(a,i){try{var o="",s=i;do o+=Pa(s),s=s.return;while(s);var $=o}catch(j){$=`
Error generating stack: `+j.message+`
`+j.stack}return{value:a,source:i,stack:$,digest:null}}function Li(a,i,o){return{value:a,source:null,stack:o??null,digest:i??null}}function Mi(a,i){try{console.error(i.value)}catch(o){setTimeout(function(){throw o})}}var Ni=typeof WeakMap=="function"?WeakMap:Map;function Oi(a,i,o){o=ch(-1,o),o.tag=3,o.payload={element:null};var s=i.value;return o.callback=function(){Pi||(Pi=!0,Qi=s),Mi(a,i)},o}function Ri(a,i,o){o=ch(-1,o),o.tag=3;var s=a.type.getDerivedStateFromError;if(typeof s=="function"){var $=i.value;o.payload=function(){return s($)},o.callback=function(){Mi(a,i)}}var j=a.stateNode;return j!==null&&typeof j.componentDidCatch=="function"&&(o.callback=function(){Mi(a,i),typeof s!="function"&&(Si===null?Si=new Set([this]):Si.add(this));var _e=i.stack;this.componentDidCatch(i.value,{componentStack:_e!==null?_e:""})}),o}function Ti(a,i,o){var s=a.pingCache;if(s===null){s=a.pingCache=new Ni;var $=new Set;s.set(i,$)}else $=s.get(i),$===void 0&&($=new Set,s.set(i,$));$.has(o)||($.add(o),a=Ui.bind(null,a,i,o),i.then(a,a))}function Vi(a){do{var i;if((i=a.tag===13)&&(i=a.memoizedState,i=i!==null?i.dehydrated!==null:!0),i)return a;a=a.return}while(a!==null);return null}function Wi(a,i,o,s,$){return a.mode&1?(a.flags|=65536,a.lanes=$,a):(a===i?a.flags|=65536:(a.flags|=128,o.flags|=131072,o.flags&=-52805,o.tag===1&&(o.alternate===null?o.tag=17:(i=ch(-1,1),i.tag=2,dh(o,i,1))),o.lanes|=1),a)}var Xi=ua.ReactCurrentOwner,Ug=!1;function Yi(a,i,o,s){i.child=a===null?Ch(i,null,o,s):Bh(i,a.child,o,s)}function Zi(a,i,o,s,$){o=o.render;var j=i.ref;return Tg(i,$),s=Xh(a,i,o,s,j,$),o=bi(),a!==null&&!Ug?(i.updateQueue=a.updateQueue,i.flags&=-2053,a.lanes&=~$,$i(a,i,$)):(I&&o&&vg(i),i.flags|=1,Yi(a,i,s,$),i.child)}function aj(a,i,o,s,$){if(a===null){var j=o.type;return typeof j=="function"&&!bj(j)&&j.defaultProps===void 0&&o.compare===null&&o.defaultProps===void 0?(i.tag=15,i.type=j,cj(a,i,j,s,$)):(a=yh(o.type,null,s,i,i.mode,$),a.ref=i.ref,a.return=i,i.child=a)}if(j=a.child,!(a.lanes&$)){var _e=j.memoizedProps;if(o=o.compare,o=o!==null?o:Ie,o(_e,s)&&a.ref===i.ref)return $i(a,i,$)}return i.flags|=1,a=wh(j,s),a.ref=i.ref,a.return=i,i.child=a}function cj(a,i,o,s,$){if(a!==null){var j=a.memoizedProps;if(Ie(j,s)&&a.ref===i.ref)if(Ug=!1,i.pendingProps=s=j,(a.lanes&$)!==0)a.flags&131072&&(Ug=!0);else return i.lanes=a.lanes,$i(a,i,$)}return dj(a,i,o,s,$)}function ej(a,i,o){var s=i.pendingProps,$=s.children,j=a!==null?a.memoizedState:null;if(s.mode==="hidden")if(!(i.mode&1))i.memoizedState={baseLanes:0,cachePool:null,transitions:null},G(fj,gj),gj|=o;else{if(!(o&1073741824))return a=j!==null?j.baseLanes|o:o,i.lanes=i.childLanes=1073741824,i.memoizedState={baseLanes:a,cachePool:null,transitions:null},i.updateQueue=null,G(fj,gj),gj|=a,null;i.memoizedState={baseLanes:0,cachePool:null,transitions:null},s=j!==null?j.baseLanes:o,G(fj,gj),gj|=s}else j!==null?(s=j.baseLanes|o,i.memoizedState=null):s=o,G(fj,gj),gj|=s;return Yi(a,i,$,o),i.child}function hj(a,i){var o=i.ref;(a===null&&o!==null||a!==null&&a.ref!==o)&&(i.flags|=512,i.flags|=2097152)}function dj(a,i,o,s,$){var j=Zf(o)?Xf:H.current;return j=Yf(i,j),Tg(i,$),o=Xh(a,i,o,s,j,$),s=bi(),a!==null&&!Ug?(i.updateQueue=a.updateQueue,i.flags&=-2053,a.lanes&=~$,$i(a,i,$)):(I&&s&&vg(i),i.flags|=1,Yi(a,i,o,$),i.child)}function ij(a,i,o,s,$){if(Zf(o)){var j=!0;cg(i)}else j=!1;if(Tg(i,$),i.stateNode===null)jj(a,i),ph(i,o,s),rh(i,o,s,$),s=!0;else if(a===null){var _e=i.stateNode,et=i.memoizedProps;_e.props=et;var tt=_e.context,nt=o.contextType;typeof nt=="object"&&nt!==null?nt=Vg(nt):(nt=Zf(o)?Xf:H.current,nt=Yf(i,nt));var at=o.getDerivedStateFromProps,it=typeof at=="function"||typeof _e.getSnapshotBeforeUpdate=="function";it||typeof _e.UNSAFE_componentWillReceiveProps!="function"&&typeof _e.componentWillReceiveProps!="function"||(et!==s||tt!==nt)&&qh(i,_e,s,nt),$g=!1;var st=i.memoizedState;_e.state=st,gh(i,s,_e,$),tt=i.memoizedState,et!==s||st!==tt||Wf.current||$g?(typeof at=="function"&&(kh(i,o,at,s),tt=i.memoizedState),(et=$g||oh(i,o,et,s,st,tt,nt))?(it||typeof _e.UNSAFE_componentWillMount!="function"&&typeof _e.componentWillMount!="function"||(typeof _e.componentWillMount=="function"&&_e.componentWillMount(),typeof _e.UNSAFE_componentWillMount=="function"&&_e.UNSAFE_componentWillMount()),typeof _e.componentDidMount=="function"&&(i.flags|=4194308)):(typeof _e.componentDidMount=="function"&&(i.flags|=4194308),i.memoizedProps=s,i.memoizedState=tt),_e.props=s,_e.state=tt,_e.context=nt,s=et):(typeof _e.componentDidMount=="function"&&(i.flags|=4194308),s=!1)}else{_e=i.stateNode,bh(a,i),et=i.memoizedProps,nt=i.type===i.elementType?et:Lg(i.type,et),_e.props=nt,it=i.pendingProps,st=_e.context,tt=o.contextType,typeof tt=="object"&&tt!==null?tt=Vg(tt):(tt=Zf(o)?Xf:H.current,tt=Yf(i,tt));var lt=o.getDerivedStateFromProps;(at=typeof lt=="function"||typeof _e.getSnapshotBeforeUpdate=="function")||typeof _e.UNSAFE_componentWillReceiveProps!="function"&&typeof _e.componentWillReceiveProps!="function"||(et!==it||st!==tt)&&qh(i,_e,s,tt),$g=!1,st=i.memoizedState,_e.state=st,gh(i,s,_e,$);var ct=i.memoizedState;et!==it||st!==ct||Wf.current||$g?(typeof lt=="function"&&(kh(i,o,lt,s),ct=i.memoizedState),(nt=$g||oh(i,o,nt,s,st,ct,tt)||!1)?(at||typeof _e.UNSAFE_componentWillUpdate!="function"&&typeof _e.componentWillUpdate!="function"||(typeof _e.componentWillUpdate=="function"&&_e.componentWillUpdate(s,ct,tt),typeof _e.UNSAFE_componentWillUpdate=="function"&&_e.UNSAFE_componentWillUpdate(s,ct,tt)),typeof _e.componentDidUpdate=="function"&&(i.flags|=4),typeof _e.getSnapshotBeforeUpdate=="function"&&(i.flags|=1024)):(typeof _e.componentDidUpdate!="function"||et===a.memoizedProps&&st===a.memoizedState||(i.flags|=4),typeof _e.getSnapshotBeforeUpdate!="function"||et===a.memoizedProps&&st===a.memoizedState||(i.flags|=1024),i.memoizedProps=s,i.memoizedState=ct),_e.props=s,_e.state=ct,_e.context=tt,s=nt):(typeof _e.componentDidUpdate!="function"||et===a.memoizedProps&&st===a.memoizedState||(i.flags|=4),typeof _e.getSnapshotBeforeUpdate!="function"||et===a.memoizedProps&&st===a.memoizedState||(i.flags|=1024),s=!1)}return kj(a,i,o,s,j,$)}function kj(a,i,o,s,$,j){hj(a,i);var _e=(i.flags&128)!==0;if(!s&&!_e)return $&&dg(i,o,!1),$i(a,i,j);s=i.stateNode,Xi.current=i;var et=_e&&typeof o.getDerivedStateFromError!="function"?null:s.render();return i.flags|=1,a!==null&&_e?(i.child=Bh(i,a.child,null,j),i.child=Bh(i,null,et,j)):Yi(a,i,et,j),i.memoizedState=s.state,$&&dg(i,o,!0),i.child}function lj(a){var i=a.stateNode;i.pendingContext?ag(a,i.pendingContext,i.pendingContext!==i.context):i.context&&ag(a,i.context,!1),Ih(a,i.containerInfo)}function mj(a,i,o,s,$){return Ig(),Jg($),i.flags|=256,Yi(a,i,o,s),i.child}var nj={dehydrated:null,treeContext:null,retryLane:0};function oj(a){return{baseLanes:a,cachePool:null,transitions:null}}function pj(a,i,o){var s=i.pendingProps,$=M.current,j=!1,_e=(i.flags&128)!==0,et;if((et=_e)||(et=a!==null&&a.memoizedState===null?!1:($&2)!==0),et?(j=!0,i.flags&=-129):(a===null||a.memoizedState!==null)&&($|=1),G(M,$&1),a===null)return Eg(i),a=i.memoizedState,a!==null&&(a=a.dehydrated,a!==null)?(i.mode&1?a.data==="$!"?i.lanes=8:i.lanes=1073741824:i.lanes=1,null):(_e=s.children,a=s.fallback,j?(s=i.mode,j=i.child,_e={mode:"hidden",children:_e},!(s&1)&&j!==null?(j.childLanes=0,j.pendingProps=_e):j=qj(_e,s,0,null),a=Ah(a,s,o,null),j.return=i,a.return=i,j.sibling=a,i.child=j,i.child.memoizedState=oj(o),i.memoizedState=nj,a):rj(i,_e));if($=a.memoizedState,$!==null&&(et=$.dehydrated,et!==null))return sj(a,i,_e,s,et,$,o);if(j){j=s.fallback,_e=i.mode,$=a.child,et=$.sibling;var tt={mode:"hidden",children:s.children};return!(_e&1)&&i.child!==$?(s=i.child,s.childLanes=0,s.pendingProps=tt,i.deletions=null):(s=wh($,tt),s.subtreeFlags=$.subtreeFlags&14680064),et!==null?j=wh(et,j):(j=Ah(j,_e,o,null),j.flags|=2),j.return=i,s.return=i,s.sibling=j,i.child=s,s=j,j=i.child,_e=a.child.memoizedState,_e=_e===null?oj(o):{baseLanes:_e.baseLanes|o,cachePool:null,transitions:_e.transitions},j.memoizedState=_e,j.childLanes=a.childLanes&~o,i.memoizedState=nj,s}return j=a.child,a=j.sibling,s=wh(j,{mode:"visible",children:s.children}),!(i.mode&1)&&(s.lanes=o),s.return=i,s.sibling=null,a!==null&&(o=i.deletions,o===null?(i.deletions=[a],i.flags|=16):o.push(a)),i.child=s,i.memoizedState=null,s}function rj(a,i){return i=qj({mode:"visible",children:i},a.mode,0,null),i.return=a,a.child=i}function tj(a,i,o,s){return s!==null&&Jg(s),Bh(i,a.child,null,o),a=rj(i,i.pendingProps.children),a.flags|=2,i.memoizedState=null,a}function sj(a,i,o,s,$,j,_e){if(o)return i.flags&256?(i.flags&=-257,s=Li(Error(p$2(422))),tj(a,i,_e,s)):i.memoizedState!==null?(i.child=a.child,i.flags|=128,null):(j=s.fallback,$=i.mode,s=qj({mode:"visible",children:s.children},$,0,null),j=Ah(j,$,_e,null),j.flags|=2,s.return=i,j.return=i,s.sibling=j,i.child=s,i.mode&1&&Bh(i,a.child,null,_e),i.child.memoizedState=oj(_e),i.memoizedState=nj,j);if(!(i.mode&1))return tj(a,i,_e,null);if($.data==="$!"){if(s=$.nextSibling&&$.nextSibling.dataset,s)var et=s.dgst;return s=et,j=Error(p$2(419)),s=Li(j,s,void 0),tj(a,i,_e,s)}if(et=(_e&a.childLanes)!==0,Ug||et){if(s=R,s!==null){switch(_e&-_e){case 4:$=2;break;case 16:$=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:$=32;break;case 536870912:$=268435456;break;default:$=0}$=$&(s.suspendedLanes|_e)?0:$,$!==0&&$!==j.retryLane&&(j.retryLane=$,Zg(a,$),mh(s,a,$,-1))}return uj(),s=Li(Error(p$2(421))),tj(a,i,_e,s)}return $.data==="$?"?(i.flags|=128,i.child=a.child,i=vj.bind(null,a),$._reactRetry=i,null):(a=j.treeContext,yg=Lf($.nextSibling),xg=i,I=!0,zg=null,a!==null&&(og[pg++]=rg,og[pg++]=sg,og[pg++]=qg,rg=a.id,sg=a.overflow,qg=i),i=rj(i,s.children),i.flags|=4096,i)}function wj(a,i,o){a.lanes|=i;var s=a.alternate;s!==null&&(s.lanes|=i),Sg(a.return,i,o)}function xj(a,i,o,s,$){var j=a.memoizedState;j===null?a.memoizedState={isBackwards:i,rendering:null,renderingStartTime:0,last:s,tail:o,tailMode:$}:(j.isBackwards=i,j.rendering=null,j.renderingStartTime=0,j.last=s,j.tail=o,j.tailMode=$)}function yj(a,i,o){var s=i.pendingProps,$=s.revealOrder,j=s.tail;if(Yi(a,i,s.children,o),s=M.current,s&2)s=s&1|2,i.flags|=128;else{if(a!==null&&a.flags&128)e:for(a=i.child;a!==null;){if(a.tag===13)a.memoizedState!==null&&wj(a,o,i);else if(a.tag===19)wj(a,o,i);else if(a.child!==null){a.child.return=a,a=a.child;continue}if(a===i)break e;for(;a.sibling===null;){if(a.return===null||a.return===i)break e;a=a.return}a.sibling.return=a.return,a=a.sibling}s&=1}if(G(M,s),!(i.mode&1))i.memoizedState=null;else switch($){case"forwards":for(o=i.child,$=null;o!==null;)a=o.alternate,a!==null&&Mh(a)===null&&($=o),o=o.sibling;o=$,o===null?($=i.child,i.child=null):($=o.sibling,o.sibling=null),xj(i,!1,$,o,j);break;case"backwards":for(o=null,$=i.child,i.child=null;$!==null;){if(a=$.alternate,a!==null&&Mh(a)===null){i.child=$;break}a=$.sibling,$.sibling=o,o=$,$=a}xj(i,!0,o,null,j);break;case"together":xj(i,!1,null,null,void 0);break;default:i.memoizedState=null}return i.child}function jj(a,i){!(i.mode&1)&&a!==null&&(a.alternate=null,i.alternate=null,i.flags|=2)}function $i(a,i,o){if(a!==null&&(i.dependencies=a.dependencies),hh|=i.lanes,!(o&i.childLanes))return null;if(a!==null&&i.child!==a.child)throw Error(p$2(153));if(i.child!==null){for(a=i.child,o=wh(a,a.pendingProps),i.child=o,o.return=i;a.sibling!==null;)a=a.sibling,o=o.sibling=wh(a,a.pendingProps),o.return=i;o.sibling=null}return i.child}function zj(a,i,o){switch(i.tag){case 3:lj(i),Ig();break;case 5:Kh(i);break;case 1:Zf(i.type)&&cg(i);break;case 4:Ih(i,i.stateNode.containerInfo);break;case 10:var s=i.type._context,$=i.memoizedProps.value;G(Mg,s._currentValue),s._currentValue=$;break;case 13:if(s=i.memoizedState,s!==null)return s.dehydrated!==null?(G(M,M.current&1),i.flags|=128,null):o&i.child.childLanes?pj(a,i,o):(G(M,M.current&1),a=$i(a,i,o),a!==null?a.sibling:null);G(M,M.current&1);break;case 19:if(s=(o&i.childLanes)!==0,a.flags&128){if(s)return yj(a,i,o);i.flags|=128}if($=i.memoizedState,$!==null&&($.rendering=null,$.tail=null,$.lastEffect=null),G(M,M.current),s)break;return null;case 22:case 23:return i.lanes=0,ej(a,i,o)}return $i(a,i,o)}var Aj,Bj,Cj,Dj;Aj=function(a,i){for(var o=i.child;o!==null;){if(o.tag===5||o.tag===6)a.appendChild(o.stateNode);else if(o.tag!==4&&o.child!==null){o.child.return=o,o=o.child;continue}if(o===i)break;for(;o.sibling===null;){if(o.return===null||o.return===i)return;o=o.return}o.sibling.return=o.return,o=o.sibling}};Bj=function(){};Cj=function(a,i,o,s){var $=a.memoizedProps;if($!==s){a=i.stateNode,Hh(Eh.current);var j=null;switch(o){case"input":$=Ya(a,$),s=Ya(a,s),j=[];break;case"select":$=A$1({},$,{value:void 0}),s=A$1({},s,{value:void 0}),j=[];break;case"textarea":$=gb(a,$),s=gb(a,s),j=[];break;default:typeof $.onClick!="function"&&typeof s.onClick=="function"&&(a.onclick=Bf)}ub(o,s);var _e;o=null;for(nt in $)if(!s.hasOwnProperty(nt)&&$.hasOwnProperty(nt)&&$[nt]!=null)if(nt==="style"){var et=$[nt];for(_e in et)et.hasOwnProperty(_e)&&(o||(o={}),o[_e]="")}else nt!=="dangerouslySetInnerHTML"&&nt!=="children"&&nt!=="suppressContentEditableWarning"&&nt!=="suppressHydrationWarning"&&nt!=="autoFocus"&&(ea.hasOwnProperty(nt)?j||(j=[]):(j=j||[]).push(nt,null));for(nt in s){var tt=s[nt];if(et=$!=null?$[nt]:void 0,s.hasOwnProperty(nt)&&tt!==et&&(tt!=null||et!=null))if(nt==="style")if(et){for(_e in et)!et.hasOwnProperty(_e)||tt&&tt.hasOwnProperty(_e)||(o||(o={}),o[_e]="");for(_e in tt)tt.hasOwnProperty(_e)&&et[_e]!==tt[_e]&&(o||(o={}),o[_e]=tt[_e])}else o||(j||(j=[]),j.push(nt,o)),o=tt;else nt==="dangerouslySetInnerHTML"?(tt=tt?tt.__html:void 0,et=et?et.__html:void 0,tt!=null&&et!==tt&&(j=j||[]).push(nt,tt)):nt==="children"?typeof tt!="string"&&typeof tt!="number"||(j=j||[]).push(nt,""+tt):nt!=="suppressContentEditableWarning"&&nt!=="suppressHydrationWarning"&&(ea.hasOwnProperty(nt)?(tt!=null&&nt==="onScroll"&&D("scroll",a),j||et===tt||(j=[])):(j=j||[]).push(nt,tt))}o&&(j=j||[]).push("style",o);var nt=j;(i.updateQueue=nt)&&(i.flags|=4)}};Dj=function(a,i,o,s){o!==s&&(i.flags|=4)};function Ej(a,i){if(!I)switch(a.tailMode){case"hidden":i=a.tail;for(var o=null;i!==null;)i.alternate!==null&&(o=i),i=i.sibling;o===null?a.tail=null:o.sibling=null;break;case"collapsed":o=a.tail;for(var s=null;o!==null;)o.alternate!==null&&(s=o),o=o.sibling;s===null?i||a.tail===null?a.tail=null:a.tail.sibling=null:s.sibling=null}}function S(a){var i=a.alternate!==null&&a.alternate.child===a.child,o=0,s=0;if(i)for(var $=a.child;$!==null;)o|=$.lanes|$.childLanes,s|=$.subtreeFlags&14680064,s|=$.flags&14680064,$.return=a,$=$.sibling;else for($=a.child;$!==null;)o|=$.lanes|$.childLanes,s|=$.subtreeFlags,s|=$.flags,$.return=a,$=$.sibling;return a.subtreeFlags|=s,a.childLanes=o,i}function Fj(a,i,o){var s=i.pendingProps;switch(wg(i),i.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return S(i),null;case 1:return Zf(i.type)&&$f(),S(i),null;case 3:return s=i.stateNode,Jh(),E(Wf),E(H),Oh(),s.pendingContext&&(s.context=s.pendingContext,s.pendingContext=null),(a===null||a.child===null)&&(Gg(i)?i.flags|=4:a===null||a.memoizedState.isDehydrated&&!(i.flags&256)||(i.flags|=1024,zg!==null&&(Gj(zg),zg=null))),Bj(a,i),S(i),null;case 5:Lh(i);var $=Hh(Gh.current);if(o=i.type,a!==null&&i.stateNode!=null)Cj(a,i,o,s,$),a.ref!==i.ref&&(i.flags|=512,i.flags|=2097152);else{if(!s){if(i.stateNode===null)throw Error(p$2(166));return S(i),null}if(a=Hh(Eh.current),Gg(i)){s=i.stateNode,o=i.type;var j=i.memoizedProps;switch(s[Of]=i,s[Pf]=j,a=(i.mode&1)!==0,o){case"dialog":D("cancel",s),D("close",s);break;case"iframe":case"object":case"embed":D("load",s);break;case"video":case"audio":for($=0;$<lf.length;$++)D(lf[$],s);break;case"source":D("error",s);break;case"img":case"image":case"link":D("error",s),D("load",s);break;case"details":D("toggle",s);break;case"input":Za(s,j),D("invalid",s);break;case"select":s._wrapperState={wasMultiple:!!j.multiple},D("invalid",s);break;case"textarea":hb(s,j),D("invalid",s)}ub(o,j),$=null;for(var _e in j)if(j.hasOwnProperty(_e)){var et=j[_e];_e==="children"?typeof et=="string"?s.textContent!==et&&(j.suppressHydrationWarning!==!0&&Af(s.textContent,et,a),$=["children",et]):typeof et=="number"&&s.textContent!==""+et&&(j.suppressHydrationWarning!==!0&&Af(s.textContent,et,a),$=["children",""+et]):ea.hasOwnProperty(_e)&&et!=null&&_e==="onScroll"&&D("scroll",s)}switch(o){case"input":Va(s),db(s,j,!0);break;case"textarea":Va(s),jb(s);break;case"select":case"option":break;default:typeof j.onClick=="function"&&(s.onclick=Bf)}s=$,i.updateQueue=s,s!==null&&(i.flags|=4)}else{_e=$.nodeType===9?$:$.ownerDocument,a==="http://www.w3.org/1999/xhtml"&&(a=kb(o)),a==="http://www.w3.org/1999/xhtml"?o==="script"?(a=_e.createElement("div"),a.innerHTML="<script><\/script>",a=a.removeChild(a.firstChild)):typeof s.is=="string"?a=_e.createElement(o,{is:s.is}):(a=_e.createElement(o),o==="select"&&(_e=a,s.multiple?_e.multiple=!0:s.size&&(_e.size=s.size))):a=_e.createElementNS(a,o),a[Of]=i,a[Pf]=s,Aj(a,i,!1,!1),i.stateNode=a;e:{switch(_e=vb(o,s),o){case"dialog":D("cancel",a),D("close",a),$=s;break;case"iframe":case"object":case"embed":D("load",a),$=s;break;case"video":case"audio":for($=0;$<lf.length;$++)D(lf[$],a);$=s;break;case"source":D("error",a),$=s;break;case"img":case"image":case"link":D("error",a),D("load",a),$=s;break;case"details":D("toggle",a),$=s;break;case"input":Za(a,s),$=Ya(a,s),D("invalid",a);break;case"option":$=s;break;case"select":a._wrapperState={wasMultiple:!!s.multiple},$=A$1({},s,{value:void 0}),D("invalid",a);break;case"textarea":hb(a,s),$=gb(a,s),D("invalid",a);break;default:$=s}ub(o,$),et=$;for(j in et)if(et.hasOwnProperty(j)){var tt=et[j];j==="style"?sb(a,tt):j==="dangerouslySetInnerHTML"?(tt=tt?tt.__html:void 0,tt!=null&&nb(a,tt)):j==="children"?typeof tt=="string"?(o!=="textarea"||tt!=="")&&ob(a,tt):typeof tt=="number"&&ob(a,""+tt):j!=="suppressContentEditableWarning"&&j!=="suppressHydrationWarning"&&j!=="autoFocus"&&(ea.hasOwnProperty(j)?tt!=null&&j==="onScroll"&&D("scroll",a):tt!=null&&ta(a,j,tt,_e))}switch(o){case"input":Va(a),db(a,s,!1);break;case"textarea":Va(a),jb(a);break;case"option":s.value!=null&&a.setAttribute("value",""+Sa(s.value));break;case"select":a.multiple=!!s.multiple,j=s.value,j!=null?fb(a,!!s.multiple,j,!1):s.defaultValue!=null&&fb(a,!!s.multiple,s.defaultValue,!0);break;default:typeof $.onClick=="function"&&(a.onclick=Bf)}switch(o){case"button":case"input":case"select":case"textarea":s=!!s.autoFocus;break e;case"img":s=!0;break e;default:s=!1}}s&&(i.flags|=4)}i.ref!==null&&(i.flags|=512,i.flags|=2097152)}return S(i),null;case 6:if(a&&i.stateNode!=null)Dj(a,i,a.memoizedProps,s);else{if(typeof s!="string"&&i.stateNode===null)throw Error(p$2(166));if(o=Hh(Gh.current),Hh(Eh.current),Gg(i)){if(s=i.stateNode,o=i.memoizedProps,s[Of]=i,(j=s.nodeValue!==o)&&(a=xg,a!==null))switch(a.tag){case 3:Af(s.nodeValue,o,(a.mode&1)!==0);break;case 5:a.memoizedProps.suppressHydrationWarning!==!0&&Af(s.nodeValue,o,(a.mode&1)!==0)}j&&(i.flags|=4)}else s=(o.nodeType===9?o:o.ownerDocument).createTextNode(s),s[Of]=i,i.stateNode=s}return S(i),null;case 13:if(E(M),s=i.memoizedState,a===null||a.memoizedState!==null&&a.memoizedState.dehydrated!==null){if(I&&yg!==null&&i.mode&1&&!(i.flags&128))Hg(),Ig(),i.flags|=98560,j=!1;else if(j=Gg(i),s!==null&&s.dehydrated!==null){if(a===null){if(!j)throw Error(p$2(318));if(j=i.memoizedState,j=j!==null?j.dehydrated:null,!j)throw Error(p$2(317));j[Of]=i}else Ig(),!(i.flags&128)&&(i.memoizedState=null),i.flags|=4;S(i),j=!1}else zg!==null&&(Gj(zg),zg=null),j=!0;if(!j)return i.flags&65536?i:null}return i.flags&128?(i.lanes=o,i):(s=s!==null,s!==(a!==null&&a.memoizedState!==null)&&s&&(i.child.flags|=8192,i.mode&1&&(a===null||M.current&1?T===0&&(T=3):uj())),i.updateQueue!==null&&(i.flags|=4),S(i),null);case 4:return Jh(),Bj(a,i),a===null&&sf(i.stateNode.containerInfo),S(i),null;case 10:return Rg(i.type._context),S(i),null;case 17:return Zf(i.type)&&$f(),S(i),null;case 19:if(E(M),j=i.memoizedState,j===null)return S(i),null;if(s=(i.flags&128)!==0,_e=j.rendering,_e===null)if(s)Ej(j,!1);else{if(T!==0||a!==null&&a.flags&128)for(a=i.child;a!==null;){if(_e=Mh(a),_e!==null){for(i.flags|=128,Ej(j,!1),s=_e.updateQueue,s!==null&&(i.updateQueue=s,i.flags|=4),i.subtreeFlags=0,s=o,o=i.child;o!==null;)j=o,a=s,j.flags&=14680066,_e=j.alternate,_e===null?(j.childLanes=0,j.lanes=a,j.child=null,j.subtreeFlags=0,j.memoizedProps=null,j.memoizedState=null,j.updateQueue=null,j.dependencies=null,j.stateNode=null):(j.childLanes=_e.childLanes,j.lanes=_e.lanes,j.child=_e.child,j.subtreeFlags=0,j.deletions=null,j.memoizedProps=_e.memoizedProps,j.memoizedState=_e.memoizedState,j.updateQueue=_e.updateQueue,j.type=_e.type,a=_e.dependencies,j.dependencies=a===null?null:{lanes:a.lanes,firstContext:a.firstContext}),o=o.sibling;return G(M,M.current&1|2),i.child}a=a.sibling}j.tail!==null&&B()>Hj&&(i.flags|=128,s=!0,Ej(j,!1),i.lanes=4194304)}else{if(!s)if(a=Mh(_e),a!==null){if(i.flags|=128,s=!0,o=a.updateQueue,o!==null&&(i.updateQueue=o,i.flags|=4),Ej(j,!0),j.tail===null&&j.tailMode==="hidden"&&!_e.alternate&&!I)return S(i),null}else 2*B()-j.renderingStartTime>Hj&&o!==1073741824&&(i.flags|=128,s=!0,Ej(j,!1),i.lanes=4194304);j.isBackwards?(_e.sibling=i.child,i.child=_e):(o=j.last,o!==null?o.sibling=_e:i.child=_e,j.last=_e)}return j.tail!==null?(i=j.tail,j.rendering=i,j.tail=i.sibling,j.renderingStartTime=B(),i.sibling=null,o=M.current,G(M,s?o&1|2:o&1),i):(S(i),null);case 22:case 23:return Ij(),s=i.memoizedState!==null,a!==null&&a.memoizedState!==null!==s&&(i.flags|=8192),s&&i.mode&1?gj&1073741824&&(S(i),i.subtreeFlags&6&&(i.flags|=8192)):S(i),null;case 24:return null;case 25:return null}throw Error(p$2(156,i.tag))}function Jj(a,i){switch(wg(i),i.tag){case 1:return Zf(i.type)&&$f(),a=i.flags,a&65536?(i.flags=a&-65537|128,i):null;case 3:return Jh(),E(Wf),E(H),Oh(),a=i.flags,a&65536&&!(a&128)?(i.flags=a&-65537|128,i):null;case 5:return Lh(i),null;case 13:if(E(M),a=i.memoizedState,a!==null&&a.dehydrated!==null){if(i.alternate===null)throw Error(p$2(340));Ig()}return a=i.flags,a&65536?(i.flags=a&-65537|128,i):null;case 19:return E(M),null;case 4:return Jh(),null;case 10:return Rg(i.type._context),null;case 22:case 23:return Ij(),null;case 24:return null;default:return null}}var Kj=!1,U=!1,Lj=typeof WeakSet=="function"?WeakSet:Set,V=null;function Mj(a,i){var o=a.ref;if(o!==null)if(typeof o=="function")try{o(null)}catch(s){W(a,i,s)}else o.current=null}function Nj(a,i,o){try{o()}catch(s){W(a,i,s)}}var Oj=!1;function Pj(a,i){if(Cf=dd,a=Me(),Ne(a)){if("selectionStart"in a)var o={start:a.selectionStart,end:a.selectionEnd};else e:{o=(o=a.ownerDocument)&&o.defaultView||window;var s=o.getSelection&&o.getSelection();if(s&&s.rangeCount!==0){o=s.anchorNode;var $=s.anchorOffset,j=s.focusNode;s=s.focusOffset;try{o.nodeType,j.nodeType}catch{o=null;break e}var _e=0,et=-1,tt=-1,nt=0,at=0,it=a,st=null;t:for(;;){for(var lt;it!==o||$!==0&&it.nodeType!==3||(et=_e+$),it!==j||s!==0&&it.nodeType!==3||(tt=_e+s),it.nodeType===3&&(_e+=it.nodeValue.length),(lt=it.firstChild)!==null;)st=it,it=lt;for(;;){if(it===a)break t;if(st===o&&++nt===$&&(et=_e),st===j&&++at===s&&(tt=_e),(lt=it.nextSibling)!==null)break;it=st,st=it.parentNode}it=lt}o=et===-1||tt===-1?null:{start:et,end:tt}}else o=null}o=o||{start:0,end:0}}else o=null;for(Df={focusedElem:a,selectionRange:o},dd=!1,V=i;V!==null;)if(i=V,a=i.child,(i.subtreeFlags&1028)!==0&&a!==null)a.return=i,V=a;else for(;V!==null;){i=V;try{var ct=i.alternate;if(i.flags&1024)switch(i.tag){case 0:case 11:case 15:break;case 1:if(ct!==null){var rt=ct.memoizedProps,ut=ct.memoizedState,ot=i.stateNode,dt=ot.getSnapshotBeforeUpdate(i.elementType===i.type?rt:Lg(i.type,rt),ut);ot.__reactInternalSnapshotBeforeUpdate=dt}break;case 3:var pt=i.stateNode.containerInfo;pt.nodeType===1?pt.textContent="":pt.nodeType===9&&pt.documentElement&&pt.removeChild(pt.documentElement);break;case 5:case 6:case 4:case 17:break;default:throw Error(p$2(163))}}catch(mt){W(i,i.return,mt)}if(a=i.sibling,a!==null){a.return=i.return,V=a;break}V=i.return}return ct=Oj,Oj=!1,ct}function Qj(a,i,o){var s=i.updateQueue;if(s=s!==null?s.lastEffect:null,s!==null){var $=s=s.next;do{if(($.tag&a)===a){var j=$.destroy;$.destroy=void 0,j!==void 0&&Nj(i,o,j)}$=$.next}while($!==s)}}function Rj(a,i){if(i=i.updateQueue,i=i!==null?i.lastEffect:null,i!==null){var o=i=i.next;do{if((o.tag&a)===a){var s=o.create;o.destroy=s()}o=o.next}while(o!==i)}}function Sj(a){var i=a.ref;if(i!==null){var o=a.stateNode;switch(a.tag){case 5:a=o;break;default:a=o}typeof i=="function"?i(a):i.current=a}}function Tj(a){var i=a.alternate;i!==null&&(a.alternate=null,Tj(i)),a.child=null,a.deletions=null,a.sibling=null,a.tag===5&&(i=a.stateNode,i!==null&&(delete i[Of],delete i[Pf],delete i[of],delete i[Qf],delete i[Rf])),a.stateNode=null,a.return=null,a.dependencies=null,a.memoizedProps=null,a.memoizedState=null,a.pendingProps=null,a.stateNode=null,a.updateQueue=null}function Uj(a){return a.tag===5||a.tag===3||a.tag===4}function Vj(a){e:for(;;){for(;a.sibling===null;){if(a.return===null||Uj(a.return))return null;a=a.return}for(a.sibling.return=a.return,a=a.sibling;a.tag!==5&&a.tag!==6&&a.tag!==18;){if(a.flags&2||a.child===null||a.tag===4)continue e;a.child.return=a,a=a.child}if(!(a.flags&2))return a.stateNode}}function Wj(a,i,o){var s=a.tag;if(s===5||s===6)a=a.stateNode,i?o.nodeType===8?o.parentNode.insertBefore(a,i):o.insertBefore(a,i):(o.nodeType===8?(i=o.parentNode,i.insertBefore(a,o)):(i=o,i.appendChild(a)),o=o._reactRootContainer,o!=null||i.onclick!==null||(i.onclick=Bf));else if(s!==4&&(a=a.child,a!==null))for(Wj(a,i,o),a=a.sibling;a!==null;)Wj(a,i,o),a=a.sibling}function Xj(a,i,o){var s=a.tag;if(s===5||s===6)a=a.stateNode,i?o.insertBefore(a,i):o.appendChild(a);else if(s!==4&&(a=a.child,a!==null))for(Xj(a,i,o),a=a.sibling;a!==null;)Xj(a,i,o),a=a.sibling}var X=null,Yj=!1;function Zj(a,i,o){for(o=o.child;o!==null;)ak(a,i,o),o=o.sibling}function ak(a,i,o){if(lc&&typeof lc.onCommitFiberUnmount=="function")try{lc.onCommitFiberUnmount(kc,o)}catch{}switch(o.tag){case 5:U||Mj(o,i);case 6:var s=X,$=Yj;X=null,Zj(a,i,o),X=s,Yj=$,X!==null&&(Yj?(a=X,o=o.stateNode,a.nodeType===8?a.parentNode.removeChild(o):a.removeChild(o)):X.removeChild(o.stateNode));break;case 18:X!==null&&(Yj?(a=X,o=o.stateNode,a.nodeType===8?Kf(a.parentNode,o):a.nodeType===1&&Kf(a,o),bd(a)):Kf(X,o.stateNode));break;case 4:s=X,$=Yj,X=o.stateNode.containerInfo,Yj=!0,Zj(a,i,o),X=s,Yj=$;break;case 0:case 11:case 14:case 15:if(!U&&(s=o.updateQueue,s!==null&&(s=s.lastEffect,s!==null))){$=s=s.next;do{var j=$,_e=j.destroy;j=j.tag,_e!==void 0&&(j&2||j&4)&&Nj(o,i,_e),$=$.next}while($!==s)}Zj(a,i,o);break;case 1:if(!U&&(Mj(o,i),s=o.stateNode,typeof s.componentWillUnmount=="function"))try{s.props=o.memoizedProps,s.state=o.memoizedState,s.componentWillUnmount()}catch(et){W(o,i,et)}Zj(a,i,o);break;case 21:Zj(a,i,o);break;case 22:o.mode&1?(U=(s=U)||o.memoizedState!==null,Zj(a,i,o),U=s):Zj(a,i,o);break;default:Zj(a,i,o)}}function bk(a){var i=a.updateQueue;if(i!==null){a.updateQueue=null;var o=a.stateNode;o===null&&(o=a.stateNode=new Lj),i.forEach(function(s){var $=ck.bind(null,a,s);o.has(s)||(o.add(s),s.then($,$))})}}function dk(a,i){var o=i.deletions;if(o!==null)for(var s=0;s<o.length;s++){var $=o[s];try{var j=a,_e=i,et=_e;e:for(;et!==null;){switch(et.tag){case 5:X=et.stateNode,Yj=!1;break e;case 3:X=et.stateNode.containerInfo,Yj=!0;break e;case 4:X=et.stateNode.containerInfo,Yj=!0;break e}et=et.return}if(X===null)throw Error(p$2(160));ak(j,_e,$),X=null,Yj=!1;var tt=$.alternate;tt!==null&&(tt.return=null),$.return=null}catch(nt){W($,i,nt)}}if(i.subtreeFlags&12854)for(i=i.child;i!==null;)ek(i,a),i=i.sibling}function ek(a,i){var o=a.alternate,s=a.flags;switch(a.tag){case 0:case 11:case 14:case 15:if(dk(i,a),fk(a),s&4){try{Qj(3,a,a.return),Rj(3,a)}catch(rt){W(a,a.return,rt)}try{Qj(5,a,a.return)}catch(rt){W(a,a.return,rt)}}break;case 1:dk(i,a),fk(a),s&512&&o!==null&&Mj(o,o.return);break;case 5:if(dk(i,a),fk(a),s&512&&o!==null&&Mj(o,o.return),a.flags&32){var $=a.stateNode;try{ob($,"")}catch(rt){W(a,a.return,rt)}}if(s&4&&($=a.stateNode,$!=null)){var j=a.memoizedProps,_e=o!==null?o.memoizedProps:j,et=a.type,tt=a.updateQueue;if(a.updateQueue=null,tt!==null)try{et==="input"&&j.type==="radio"&&j.name!=null&&ab($,j),vb(et,_e);var nt=vb(et,j);for(_e=0;_e<tt.length;_e+=2){var at=tt[_e],it=tt[_e+1];at==="style"?sb($,it):at==="dangerouslySetInnerHTML"?nb($,it):at==="children"?ob($,it):ta($,at,it,nt)}switch(et){case"input":bb($,j);break;case"textarea":ib($,j);break;case"select":var st=$._wrapperState.wasMultiple;$._wrapperState.wasMultiple=!!j.multiple;var lt=j.value;lt!=null?fb($,!!j.multiple,lt,!1):st!==!!j.multiple&&(j.defaultValue!=null?fb($,!!j.multiple,j.defaultValue,!0):fb($,!!j.multiple,j.multiple?[]:"",!1))}$[Pf]=j}catch(rt){W(a,a.return,rt)}}break;case 6:if(dk(i,a),fk(a),s&4){if(a.stateNode===null)throw Error(p$2(162));$=a.stateNode,j=a.memoizedProps;try{$.nodeValue=j}catch(rt){W(a,a.return,rt)}}break;case 3:if(dk(i,a),fk(a),s&4&&o!==null&&o.memoizedState.isDehydrated)try{bd(i.containerInfo)}catch(rt){W(a,a.return,rt)}break;case 4:dk(i,a),fk(a);break;case 13:dk(i,a),fk(a),$=a.child,$.flags&8192&&(j=$.memoizedState!==null,$.stateNode.isHidden=j,!j||$.alternate!==null&&$.alternate.memoizedState!==null||(gk=B())),s&4&&bk(a);break;case 22:if(at=o!==null&&o.memoizedState!==null,a.mode&1?(U=(nt=U)||at,dk(i,a),U=nt):dk(i,a),fk(a),s&8192){if(nt=a.memoizedState!==null,(a.stateNode.isHidden=nt)&&!at&&a.mode&1)for(V=a,at=a.child;at!==null;){for(it=V=at;V!==null;){switch(st=V,lt=st.child,st.tag){case 0:case 11:case 14:case 15:Qj(4,st,st.return);break;case 1:Mj(st,st.return);var ct=st.stateNode;if(typeof ct.componentWillUnmount=="function"){s=st,o=st.return;try{i=s,ct.props=i.memoizedProps,ct.state=i.memoizedState,ct.componentWillUnmount()}catch(rt){W(s,o,rt)}}break;case 5:Mj(st,st.return);break;case 22:if(st.memoizedState!==null){hk(it);continue}}lt!==null?(lt.return=st,V=lt):hk(it)}at=at.sibling}e:for(at=null,it=a;;){if(it.tag===5){if(at===null){at=it;try{$=it.stateNode,nt?(j=$.style,typeof j.setProperty=="function"?j.setProperty("display","none","important"):j.display="none"):(et=it.stateNode,tt=it.memoizedProps.style,_e=tt!=null&&tt.hasOwnProperty("display")?tt.display:null,et.style.display=rb("display",_e))}catch(rt){W(a,a.return,rt)}}}else if(it.tag===6){if(at===null)try{it.stateNode.nodeValue=nt?"":it.memoizedProps}catch(rt){W(a,a.return,rt)}}else if((it.tag!==22&&it.tag!==23||it.memoizedState===null||it===a)&&it.child!==null){it.child.return=it,it=it.child;continue}if(it===a)break e;for(;it.sibling===null;){if(it.return===null||it.return===a)break e;at===it&&(at=null),it=it.return}at===it&&(at=null),it.sibling.return=it.return,it=it.sibling}}break;case 19:dk(i,a),fk(a),s&4&&bk(a);break;case 21:break;default:dk(i,a),fk(a)}}function fk(a){var i=a.flags;if(i&2){try{e:{for(var o=a.return;o!==null;){if(Uj(o)){var s=o;break e}o=o.return}throw Error(p$2(160))}switch(s.tag){case 5:var $=s.stateNode;s.flags&32&&(ob($,""),s.flags&=-33);var j=Vj(a);Xj(a,j,$);break;case 3:case 4:var _e=s.stateNode.containerInfo,et=Vj(a);Wj(a,et,_e);break;default:throw Error(p$2(161))}}catch(tt){W(a,a.return,tt)}a.flags&=-3}i&4096&&(a.flags&=-4097)}function ik(a,i,o){V=a,jk(a)}function jk(a,i,o){for(var s=(a.mode&1)!==0;V!==null;){var $=V,j=$.child;if($.tag===22&&s){var _e=$.memoizedState!==null||Kj;if(!_e){var et=$.alternate,tt=et!==null&&et.memoizedState!==null||U;et=Kj;var nt=U;if(Kj=_e,(U=tt)&&!nt)for(V=$;V!==null;)_e=V,tt=_e.child,_e.tag===22&&_e.memoizedState!==null?kk($):tt!==null?(tt.return=_e,V=tt):kk($);for(;j!==null;)V=j,jk(j),j=j.sibling;V=$,Kj=et,U=nt}lk(a)}else $.subtreeFlags&8772&&j!==null?(j.return=$,V=j):lk(a)}}function lk(a){for(;V!==null;){var i=V;if(i.flags&8772){var o=i.alternate;try{if(i.flags&8772)switch(i.tag){case 0:case 11:case 15:U||Rj(5,i);break;case 1:var s=i.stateNode;if(i.flags&4&&!U)if(o===null)s.componentDidMount();else{var $=i.elementType===i.type?o.memoizedProps:Lg(i.type,o.memoizedProps);s.componentDidUpdate($,o.memoizedState,s.__reactInternalSnapshotBeforeUpdate)}var j=i.updateQueue;j!==null&&ih(i,j,s);break;case 3:var _e=i.updateQueue;if(_e!==null){if(o=null,i.child!==null)switch(i.child.tag){case 5:o=i.child.stateNode;break;case 1:o=i.child.stateNode}ih(i,_e,o)}break;case 5:var et=i.stateNode;if(o===null&&i.flags&4){o=et;var tt=i.memoizedProps;switch(i.type){case"button":case"input":case"select":case"textarea":tt.autoFocus&&o.focus();break;case"img":tt.src&&(o.src=tt.src)}}break;case 6:break;case 4:break;case 12:break;case 13:if(i.memoizedState===null){var nt=i.alternate;if(nt!==null){var at=nt.memoizedState;if(at!==null){var it=at.dehydrated;it!==null&&bd(it)}}}break;case 19:case 17:case 21:case 22:case 23:case 25:break;default:throw Error(p$2(163))}U||i.flags&512&&Sj(i)}catch(st){W(i,i.return,st)}}if(i===a){V=null;break}if(o=i.sibling,o!==null){o.return=i.return,V=o;break}V=i.return}}function hk(a){for(;V!==null;){var i=V;if(i===a){V=null;break}var o=i.sibling;if(o!==null){o.return=i.return,V=o;break}V=i.return}}function kk(a){for(;V!==null;){var i=V;try{switch(i.tag){case 0:case 11:case 15:var o=i.return;try{Rj(4,i)}catch(tt){W(i,o,tt)}break;case 1:var s=i.stateNode;if(typeof s.componentDidMount=="function"){var $=i.return;try{s.componentDidMount()}catch(tt){W(i,$,tt)}}var j=i.return;try{Sj(i)}catch(tt){W(i,j,tt)}break;case 5:var _e=i.return;try{Sj(i)}catch(tt){W(i,_e,tt)}}}catch(tt){W(i,i.return,tt)}if(i===a){V=null;break}var et=i.sibling;if(et!==null){et.return=i.return,V=et;break}V=i.return}}var mk=Math.ceil,nk=ua.ReactCurrentDispatcher,ok=ua.ReactCurrentOwner,pk=ua.ReactCurrentBatchConfig,K=0,R=null,Y=null,Z=0,gj=0,fj=Uf(0),T=0,qk=null,hh=0,rk=0,sk=0,tk=null,uk=null,gk=0,Hj=1/0,vk=null,Pi=!1,Qi=null,Si=null,wk=!1,xk=null,yk=0,zk=0,Ak=null,Bk=-1,Ck=0;function L(){return K&6?B():Bk!==-1?Bk:Bk=B()}function lh(a){return a.mode&1?K&2&&Z!==0?Z&-Z:Kg.transition!==null?(Ck===0&&(Ck=yc()),Ck):(a=C,a!==0||(a=window.event,a=a===void 0?16:jd(a.type)),a):1}function mh(a,i,o,s){if(50<zk)throw zk=0,Ak=null,Error(p$2(185));Ac(a,o,s),(!(K&2)||a!==R)&&(a===R&&(!(K&2)&&(rk|=o),T===4&&Dk(a,Z)),Ek(a,s),o===1&&K===0&&!(i.mode&1)&&(Hj=B()+500,fg&&jg()))}function Ek(a,i){var o=a.callbackNode;wc(a,i);var s=uc(a,a===R?Z:0);if(s===0)o!==null&&bc(o),a.callbackNode=null,a.callbackPriority=0;else if(i=s&-s,a.callbackPriority!==i){if(o!=null&&bc(o),i===1)a.tag===0?ig(Fk.bind(null,a)):hg(Fk.bind(null,a)),Jf(function(){!(K&6)&&jg()}),o=null;else{switch(Dc(s)){case 1:o=fc;break;case 4:o=gc;break;case 16:o=hc;break;case 536870912:o=jc;break;default:o=hc}o=Gk(o,Hk.bind(null,a))}a.callbackPriority=i,a.callbackNode=o}}function Hk(a,i){if(Bk=-1,Ck=0,K&6)throw Error(p$2(327));var o=a.callbackNode;if(Ik()&&a.callbackNode!==o)return null;var s=uc(a,a===R?Z:0);if(s===0)return null;if(s&30||s&a.expiredLanes||i)i=Jk(a,s);else{i=s;var $=K;K|=2;var j=Kk();(R!==a||Z!==i)&&(vk=null,Hj=B()+500,Lk(a,i));do try{Mk();break}catch(et){Nk(a,et)}while(!0);Qg(),nk.current=j,K=$,Y!==null?i=0:(R=null,Z=0,i=T)}if(i!==0){if(i===2&&($=xc(a),$!==0&&(s=$,i=Ok(a,$))),i===1)throw o=qk,Lk(a,0),Dk(a,s),Ek(a,B()),o;if(i===6)Dk(a,s);else{if($=a.current.alternate,!(s&30)&&!Pk($)&&(i=Jk(a,s),i===2&&(j=xc(a),j!==0&&(s=j,i=Ok(a,j))),i===1))throw o=qk,Lk(a,0),Dk(a,s),Ek(a,B()),o;switch(a.finishedWork=$,a.finishedLanes=s,i){case 0:case 1:throw Error(p$2(345));case 2:Qk(a,uk,vk);break;case 3:if(Dk(a,s),(s&130023424)===s&&(i=gk+500-B(),10<i)){if(uc(a,0)!==0)break;if($=a.suspendedLanes,($&s)!==s){L(),a.pingedLanes|=a.suspendedLanes&$;break}a.timeoutHandle=Ff(Qk.bind(null,a,uk,vk),i);break}Qk(a,uk,vk);break;case 4:if(Dk(a,s),(s&4194240)===s)break;for(i=a.eventTimes,$=-1;0<s;){var _e=31-oc(s);j=1<<_e,_e=i[_e],_e>$&&($=_e),s&=~j}if(s=$,s=B()-s,s=(120>s?120:480>s?480:1080>s?1080:1920>s?1920:3e3>s?3e3:4320>s?4320:1960*mk(s/1960))-s,10<s){a.timeoutHandle=Ff(Qk.bind(null,a,uk,vk),s);break}Qk(a,uk,vk);break;case 5:Qk(a,uk,vk);break;default:throw Error(p$2(329))}}}return Ek(a,B()),a.callbackNode===o?Hk.bind(null,a):null}function Ok(a,i){var o=tk;return a.current.memoizedState.isDehydrated&&(Lk(a,i).flags|=256),a=Jk(a,i),a!==2&&(i=uk,uk=o,i!==null&&Gj(i)),a}function Gj(a){uk===null?uk=a:uk.push.apply(uk,a)}function Pk(a){for(var i=a;;){if(i.flags&16384){var o=i.updateQueue;if(o!==null&&(o=o.stores,o!==null))for(var s=0;s<o.length;s++){var $=o[s],j=$.getSnapshot;$=$.value;try{if(!He(j(),$))return!1}catch{return!1}}}if(o=i.child,i.subtreeFlags&16384&&o!==null)o.return=i,i=o;else{if(i===a)break;for(;i.sibling===null;){if(i.return===null||i.return===a)return!0;i=i.return}i.sibling.return=i.return,i=i.sibling}}return!0}function Dk(a,i){for(i&=~sk,i&=~rk,a.suspendedLanes|=i,a.pingedLanes&=~i,a=a.expirationTimes;0<i;){var o=31-oc(i),s=1<<o;a[o]=-1,i&=~s}}function Fk(a){if(K&6)throw Error(p$2(327));Ik();var i=uc(a,0);if(!(i&1))return Ek(a,B()),null;var o=Jk(a,i);if(a.tag!==0&&o===2){var s=xc(a);s!==0&&(i=s,o=Ok(a,s))}if(o===1)throw o=qk,Lk(a,0),Dk(a,i),Ek(a,B()),o;if(o===6)throw Error(p$2(345));return a.finishedWork=a.current.alternate,a.finishedLanes=i,Qk(a,uk,vk),Ek(a,B()),null}function Rk(a,i){var o=K;K|=1;try{return a(i)}finally{K=o,K===0&&(Hj=B()+500,fg&&jg())}}function Sk(a){xk!==null&&xk.tag===0&&!(K&6)&&Ik();var i=K;K|=1;var o=pk.transition,s=C;try{if(pk.transition=null,C=1,a)return a()}finally{C=s,pk.transition=o,K=i,!(K&6)&&jg()}}function Ij(){gj=fj.current,E(fj)}function Lk(a,i){a.finishedWork=null,a.finishedLanes=0;var o=a.timeoutHandle;if(o!==-1&&(a.timeoutHandle=-1,Gf(o)),Y!==null)for(o=Y.return;o!==null;){var s=o;switch(wg(s),s.tag){case 1:s=s.type.childContextTypes,s!=null&&$f();break;case 3:Jh(),E(Wf),E(H),Oh();break;case 5:Lh(s);break;case 4:Jh();break;case 13:E(M);break;case 19:E(M);break;case 10:Rg(s.type._context);break;case 22:case 23:Ij()}o=o.return}if(R=a,Y=a=wh(a.current,null),Z=gj=i,T=0,qk=null,sk=rk=hh=0,uk=tk=null,Wg!==null){for(i=0;i<Wg.length;i++)if(o=Wg[i],s=o.interleaved,s!==null){o.interleaved=null;var $=s.next,j=o.pending;if(j!==null){var _e=j.next;j.next=$,s.next=_e}o.pending=s}Wg=null}return a}function Nk(a,i){do{var o=Y;try{if(Qg(),Ph.current=ai,Sh){for(var s=N.memoizedState;s!==null;){var $=s.queue;$!==null&&($.pending=null),s=s.next}Sh=!1}if(Rh=0,P=O=N=null,Th=!1,Uh=0,ok.current=null,o===null||o.return===null){T=1,qk=i,Y=null;break}e:{var j=a,_e=o.return,et=o,tt=i;if(i=Z,et.flags|=32768,tt!==null&&typeof tt=="object"&&typeof tt.then=="function"){var nt=tt,at=et,it=at.tag;if(!(at.mode&1)&&(it===0||it===11||it===15)){var st=at.alternate;st?(at.updateQueue=st.updateQueue,at.memoizedState=st.memoizedState,at.lanes=st.lanes):(at.updateQueue=null,at.memoizedState=null)}var lt=Vi(_e);if(lt!==null){lt.flags&=-257,Wi(lt,_e,et,j,i),lt.mode&1&&Ti(j,nt,i),i=lt,tt=nt;var ct=i.updateQueue;if(ct===null){var rt=new Set;rt.add(tt),i.updateQueue=rt}else ct.add(tt);break e}else{if(!(i&1)){Ti(j,nt,i),uj();break e}tt=Error(p$2(426))}}else if(I&&et.mode&1){var ut=Vi(_e);if(ut!==null){!(ut.flags&65536)&&(ut.flags|=256),Wi(ut,_e,et,j,i),Jg(Ki(tt,et));break e}}j=tt=Ki(tt,et),T!==4&&(T=2),tk===null?tk=[j]:tk.push(j),j=_e;do{switch(j.tag){case 3:j.flags|=65536,i&=-i,j.lanes|=i;var ot=Oi(j,tt,i);fh(j,ot);break e;case 1:et=tt;var dt=j.type,pt=j.stateNode;if(!(j.flags&128)&&(typeof dt.getDerivedStateFromError=="function"||pt!==null&&typeof pt.componentDidCatch=="function"&&(Si===null||!Si.has(pt)))){j.flags|=65536,i&=-i,j.lanes|=i;var mt=Ri(j,et,i);fh(j,mt);break e}}j=j.return}while(j!==null)}Tk(o)}catch(ft){i=ft,Y===o&&o!==null&&(Y=o=o.return);continue}break}while(!0)}function Kk(){var a=nk.current;return nk.current=ai,a===null?ai:a}function uj(){(T===0||T===3||T===2)&&(T=4),R===null||!(hh&268435455)&&!(rk&268435455)||Dk(R,Z)}function Jk(a,i){var o=K;K|=2;var s=Kk();(R!==a||Z!==i)&&(vk=null,Lk(a,i));do try{Uk();break}catch($){Nk(a,$)}while(!0);if(Qg(),K=o,nk.current=s,Y!==null)throw Error(p$2(261));return R=null,Z=0,T}function Uk(){for(;Y!==null;)Vk(Y)}function Mk(){for(;Y!==null&&!cc();)Vk(Y)}function Vk(a){var i=Wk(a.alternate,a,gj);a.memoizedProps=a.pendingProps,i===null?Tk(a):Y=i,ok.current=null}function Tk(a){var i=a;do{var o=i.alternate;if(a=i.return,i.flags&32768){if(o=Jj(o,i),o!==null){o.flags&=32767,Y=o;return}if(a!==null)a.flags|=32768,a.subtreeFlags=0,a.deletions=null;else{T=6,Y=null;return}}else if(o=Fj(o,i,gj),o!==null){Y=o;return}if(i=i.sibling,i!==null){Y=i;return}Y=i=a}while(i!==null);T===0&&(T=5)}function Qk(a,i,o){var s=C,$=pk.transition;try{pk.transition=null,C=1,Xk(a,i,o,s)}finally{pk.transition=$,C=s}return null}function Xk(a,i,o,s){do Ik();while(xk!==null);if(K&6)throw Error(p$2(327));o=a.finishedWork;var $=a.finishedLanes;if(o===null)return null;if(a.finishedWork=null,a.finishedLanes=0,o===a.current)throw Error(p$2(177));a.callbackNode=null,a.callbackPriority=0;var j=o.lanes|o.childLanes;if(Bc(a,j),a===R&&(Y=R=null,Z=0),!(o.subtreeFlags&2064)&&!(o.flags&2064)||wk||(wk=!0,Gk(hc,function(){return Ik(),null})),j=(o.flags&15990)!==0,o.subtreeFlags&15990||j){j=pk.transition,pk.transition=null;var _e=C;C=1;var et=K;K|=4,ok.current=null,Pj(a,o),ek(o,a),Oe(Df),dd=!!Cf,Df=Cf=null,a.current=o,ik(o),dc(),K=et,C=_e,pk.transition=j}else a.current=o;if(wk&&(wk=!1,xk=a,yk=$),j=a.pendingLanes,j===0&&(Si=null),mc(o.stateNode),Ek(a,B()),i!==null)for(s=a.onRecoverableError,o=0;o<i.length;o++)$=i[o],s($.value,{componentStack:$.stack,digest:$.digest});if(Pi)throw Pi=!1,a=Qi,Qi=null,a;return yk&1&&a.tag!==0&&Ik(),j=a.pendingLanes,j&1?a===Ak?zk++:(zk=0,Ak=a):zk=0,jg(),null}function Ik(){if(xk!==null){var a=Dc(yk),i=pk.transition,o=C;try{if(pk.transition=null,C=16>a?16:a,xk===null)var s=!1;else{if(a=xk,xk=null,yk=0,K&6)throw Error(p$2(331));var $=K;for(K|=4,V=a.current;V!==null;){var j=V,_e=j.child;if(V.flags&16){var et=j.deletions;if(et!==null){for(var tt=0;tt<et.length;tt++){var nt=et[tt];for(V=nt;V!==null;){var at=V;switch(at.tag){case 0:case 11:case 15:Qj(8,at,j)}var it=at.child;if(it!==null)it.return=at,V=it;else for(;V!==null;){at=V;var st=at.sibling,lt=at.return;if(Tj(at),at===nt){V=null;break}if(st!==null){st.return=lt,V=st;break}V=lt}}}var ct=j.alternate;if(ct!==null){var rt=ct.child;if(rt!==null){ct.child=null;do{var ut=rt.sibling;rt.sibling=null,rt=ut}while(rt!==null)}}V=j}}if(j.subtreeFlags&2064&&_e!==null)_e.return=j,V=_e;else e:for(;V!==null;){if(j=V,j.flags&2048)switch(j.tag){case 0:case 11:case 15:Qj(9,j,j.return)}var ot=j.sibling;if(ot!==null){ot.return=j.return,V=ot;break e}V=j.return}}var dt=a.current;for(V=dt;V!==null;){_e=V;var pt=_e.child;if(_e.subtreeFlags&2064&&pt!==null)pt.return=_e,V=pt;else e:for(_e=dt;V!==null;){if(et=V,et.flags&2048)try{switch(et.tag){case 0:case 11:case 15:Rj(9,et)}}catch(ft){W(et,et.return,ft)}if(et===_e){V=null;break e}var mt=et.sibling;if(mt!==null){mt.return=et.return,V=mt;break e}V=et.return}}if(K=$,jg(),lc&&typeof lc.onPostCommitFiberRoot=="function")try{lc.onPostCommitFiberRoot(kc,a)}catch{}s=!0}return s}finally{C=o,pk.transition=i}}return!1}function Yk(a,i,o){i=Ki(o,i),i=Oi(a,i,1),a=dh(a,i,1),i=L(),a!==null&&(Ac(a,1,i),Ek(a,i))}function W(a,i,o){if(a.tag===3)Yk(a,a,o);else for(;i!==null;){if(i.tag===3){Yk(i,a,o);break}else if(i.tag===1){var s=i.stateNode;if(typeof i.type.getDerivedStateFromError=="function"||typeof s.componentDidCatch=="function"&&(Si===null||!Si.has(s))){a=Ki(o,a),a=Ri(i,a,1),i=dh(i,a,1),a=L(),i!==null&&(Ac(i,1,a),Ek(i,a));break}}i=i.return}}function Ui(a,i,o){var s=a.pingCache;s!==null&&s.delete(i),i=L(),a.pingedLanes|=a.suspendedLanes&o,R===a&&(Z&o)===o&&(T===4||T===3&&(Z&130023424)===Z&&500>B()-gk?Lk(a,0):sk|=o),Ek(a,i)}function Zk(a,i){i===0&&(a.mode&1?(i=sc,sc<<=1,!(sc&130023424)&&(sc=4194304)):i=1);var o=L();a=Zg(a,i),a!==null&&(Ac(a,i,o),Ek(a,o))}function vj(a){var i=a.memoizedState,o=0;i!==null&&(o=i.retryLane),Zk(a,o)}function ck(a,i){var o=0;switch(a.tag){case 13:var s=a.stateNode,$=a.memoizedState;$!==null&&(o=$.retryLane);break;case 19:s=a.stateNode;break;default:throw Error(p$2(314))}s!==null&&s.delete(i),Zk(a,o)}var Wk;Wk=function(a,i,o){if(a!==null)if(a.memoizedProps!==i.pendingProps||Wf.current)Ug=!0;else{if(!(a.lanes&o)&&!(i.flags&128))return Ug=!1,zj(a,i,o);Ug=!!(a.flags&131072)}else Ug=!1,I&&i.flags&1048576&&ug(i,ng,i.index);switch(i.lanes=0,i.tag){case 2:var s=i.type;jj(a,i),a=i.pendingProps;var $=Yf(i,H.current);Tg(i,o),$=Xh(null,i,s,a,$,o);var j=bi();return i.flags|=1,typeof $=="object"&&$!==null&&typeof $.render=="function"&&$.$$typeof===void 0?(i.tag=1,i.memoizedState=null,i.updateQueue=null,Zf(s)?(j=!0,cg(i)):j=!1,i.memoizedState=$.state!==null&&$.state!==void 0?$.state:null,ah(i),$.updater=nh,i.stateNode=$,$._reactInternals=i,rh(i,s,a,o),i=kj(null,i,s,!0,j,o)):(i.tag=0,I&&j&&vg(i),Yi(null,i,$,o),i=i.child),i;case 16:s=i.elementType;e:{switch(jj(a,i),a=i.pendingProps,$=s._init,s=$(s._payload),i.type=s,$=i.tag=$k(s),a=Lg(s,a),$){case 0:i=dj(null,i,s,a,o);break e;case 1:i=ij(null,i,s,a,o);break e;case 11:i=Zi(null,i,s,a,o);break e;case 14:i=aj(null,i,s,Lg(s.type,a),o);break e}throw Error(p$2(306,s,""))}return i;case 0:return s=i.type,$=i.pendingProps,$=i.elementType===s?$:Lg(s,$),dj(a,i,s,$,o);case 1:return s=i.type,$=i.pendingProps,$=i.elementType===s?$:Lg(s,$),ij(a,i,s,$,o);case 3:e:{if(lj(i),a===null)throw Error(p$2(387));s=i.pendingProps,j=i.memoizedState,$=j.element,bh(a,i),gh(i,s,null,o);var _e=i.memoizedState;if(s=_e.element,j.isDehydrated)if(j={element:s,isDehydrated:!1,cache:_e.cache,pendingSuspenseBoundaries:_e.pendingSuspenseBoundaries,transitions:_e.transitions},i.updateQueue.baseState=j,i.memoizedState=j,i.flags&256){$=Ki(Error(p$2(423)),i),i=mj(a,i,s,o,$);break e}else if(s!==$){$=Ki(Error(p$2(424)),i),i=mj(a,i,s,o,$);break e}else for(yg=Lf(i.stateNode.containerInfo.firstChild),xg=i,I=!0,zg=null,o=Ch(i,null,s,o),i.child=o;o;)o.flags=o.flags&-3|4096,o=o.sibling;else{if(Ig(),s===$){i=$i(a,i,o);break e}Yi(a,i,s,o)}i=i.child}return i;case 5:return Kh(i),a===null&&Eg(i),s=i.type,$=i.pendingProps,j=a!==null?a.memoizedProps:null,_e=$.children,Ef(s,$)?_e=null:j!==null&&Ef(s,j)&&(i.flags|=32),hj(a,i),Yi(a,i,_e,o),i.child;case 6:return a===null&&Eg(i),null;case 13:return pj(a,i,o);case 4:return Ih(i,i.stateNode.containerInfo),s=i.pendingProps,a===null?i.child=Bh(i,null,s,o):Yi(a,i,s,o),i.child;case 11:return s=i.type,$=i.pendingProps,$=i.elementType===s?$:Lg(s,$),Zi(a,i,s,$,o);case 7:return Yi(a,i,i.pendingProps,o),i.child;case 8:return Yi(a,i,i.pendingProps.children,o),i.child;case 12:return Yi(a,i,i.pendingProps.children,o),i.child;case 10:e:{if(s=i.type._context,$=i.pendingProps,j=i.memoizedProps,_e=$.value,G(Mg,s._currentValue),s._currentValue=_e,j!==null)if(He(j.value,_e)){if(j.children===$.children&&!Wf.current){i=$i(a,i,o);break e}}else for(j=i.child,j!==null&&(j.return=i);j!==null;){var et=j.dependencies;if(et!==null){_e=j.child;for(var tt=et.firstContext;tt!==null;){if(tt.context===s){if(j.tag===1){tt=ch(-1,o&-o),tt.tag=2;var nt=j.updateQueue;if(nt!==null){nt=nt.shared;var at=nt.pending;at===null?tt.next=tt:(tt.next=at.next,at.next=tt),nt.pending=tt}}j.lanes|=o,tt=j.alternate,tt!==null&&(tt.lanes|=o),Sg(j.return,o,i),et.lanes|=o;break}tt=tt.next}}else if(j.tag===10)_e=j.type===i.type?null:j.child;else if(j.tag===18){if(_e=j.return,_e===null)throw Error(p$2(341));_e.lanes|=o,et=_e.alternate,et!==null&&(et.lanes|=o),Sg(_e,o,i),_e=j.sibling}else _e=j.child;if(_e!==null)_e.return=j;else for(_e=j;_e!==null;){if(_e===i){_e=null;break}if(j=_e.sibling,j!==null){j.return=_e.return,_e=j;break}_e=_e.return}j=_e}Yi(a,i,$.children,o),i=i.child}return i;case 9:return $=i.type,s=i.pendingProps.children,Tg(i,o),$=Vg($),s=s($),i.flags|=1,Yi(a,i,s,o),i.child;case 14:return s=i.type,$=Lg(s,i.pendingProps),$=Lg(s.type,$),aj(a,i,s,$,o);case 15:return cj(a,i,i.type,i.pendingProps,o);case 17:return s=i.type,$=i.pendingProps,$=i.elementType===s?$:Lg(s,$),jj(a,i),i.tag=1,Zf(s)?(a=!0,cg(i)):a=!1,Tg(i,o),ph(i,s,$),rh(i,s,$,o),kj(null,i,s,!0,a,o);case 19:return yj(a,i,o);case 22:return ej(a,i,o)}throw Error(p$2(156,i.tag))};function Gk(a,i){return ac(a,i)}function al(a,i,o,s){this.tag=a,this.key=o,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=i,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=s,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function Bg(a,i,o,s){return new al(a,i,o,s)}function bj(a){return a=a.prototype,!(!a||!a.isReactComponent)}function $k(a){if(typeof a=="function")return bj(a)?1:0;if(a!=null){if(a=a.$$typeof,a===Da)return 11;if(a===Ga)return 14}return 2}function wh(a,i){var o=a.alternate;return o===null?(o=Bg(a.tag,i,a.key,a.mode),o.elementType=a.elementType,o.type=a.type,o.stateNode=a.stateNode,o.alternate=a,a.alternate=o):(o.pendingProps=i,o.type=a.type,o.flags=0,o.subtreeFlags=0,o.deletions=null),o.flags=a.flags&14680064,o.childLanes=a.childLanes,o.lanes=a.lanes,o.child=a.child,o.memoizedProps=a.memoizedProps,o.memoizedState=a.memoizedState,o.updateQueue=a.updateQueue,i=a.dependencies,o.dependencies=i===null?null:{lanes:i.lanes,firstContext:i.firstContext},o.sibling=a.sibling,o.index=a.index,o.ref=a.ref,o}function yh(a,i,o,s,$,j){var _e=2;if(s=a,typeof a=="function")bj(a)&&(_e=1);else if(typeof a=="string")_e=5;else e:switch(a){case ya:return Ah(o.children,$,j,i);case za:_e=8,$|=8;break;case Aa:return a=Bg(12,o,i,$|2),a.elementType=Aa,a.lanes=j,a;case Ea:return a=Bg(13,o,i,$),a.elementType=Ea,a.lanes=j,a;case Fa:return a=Bg(19,o,i,$),a.elementType=Fa,a.lanes=j,a;case Ia:return qj(o,$,j,i);default:if(typeof a=="object"&&a!==null)switch(a.$$typeof){case Ba:_e=10;break e;case Ca:_e=9;break e;case Da:_e=11;break e;case Ga:_e=14;break e;case Ha:_e=16,s=null;break e}throw Error(p$2(130,a==null?a:typeof a,""))}return i=Bg(_e,o,i,$),i.elementType=a,i.type=s,i.lanes=j,i}function Ah(a,i,o,s){return a=Bg(7,a,s,i),a.lanes=o,a}function qj(a,i,o,s){return a=Bg(22,a,s,i),a.elementType=Ia,a.lanes=o,a.stateNode={isHidden:!1},a}function xh(a,i,o){return a=Bg(6,a,null,i),a.lanes=o,a}function zh(a,i,o){return i=Bg(4,a.children!==null?a.children:[],a.key,i),i.lanes=o,i.stateNode={containerInfo:a.containerInfo,pendingChildren:null,implementation:a.implementation},i}function bl(a,i,o,s,$){this.tag=i,this.containerInfo=a,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=zc(0),this.expirationTimes=zc(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=zc(0),this.identifierPrefix=s,this.onRecoverableError=$,this.mutableSourceEagerHydrationData=null}function cl(a,i,o,s,$,j,_e,et,tt){return a=new bl(a,i,o,et,tt),i===1?(i=1,j===!0&&(i|=8)):i=0,j=Bg(3,null,null,i),a.current=j,j.stateNode=a,j.memoizedState={element:s,isDehydrated:o,cache:null,transitions:null,pendingSuspenseBoundaries:null},ah(j),a}function dl(a,i,o){var s=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:wa,key:s==null?null:""+s,children:a,containerInfo:i,implementation:o}}function el(a){if(!a)return Vf;a=a._reactInternals;e:{if(Vb(a)!==a||a.tag!==1)throw Error(p$2(170));var i=a;do{switch(i.tag){case 3:i=i.stateNode.context;break e;case 1:if(Zf(i.type)){i=i.stateNode.__reactInternalMemoizedMergedChildContext;break e}}i=i.return}while(i!==null);throw Error(p$2(171))}if(a.tag===1){var o=a.type;if(Zf(o))return bg(a,o,i)}return i}function fl(a,i,o,s,$,j,_e,et,tt){return a=cl(o,s,!0,a,$,j,_e,et,tt),a.context=el(null),o=a.current,s=L(),$=lh(o),j=ch(s,$),j.callback=i??null,dh(o,j,$),a.current.lanes=$,Ac(a,$,s),Ek(a,s),a}function gl(a,i,o,s){var $=i.current,j=L(),_e=lh($);return o=el(o),i.context===null?i.context=o:i.pendingContext=o,i=ch(j,_e),i.payload={element:a},s=s===void 0?null:s,s!==null&&(i.callback=s),a=dh($,i,_e),a!==null&&(mh(a,$,_e,j),eh(a,$,_e)),_e}function hl(a){if(a=a.current,!a.child)return null;switch(a.child.tag){case 5:return a.child.stateNode;default:return a.child.stateNode}}function il(a,i){if(a=a.memoizedState,a!==null&&a.dehydrated!==null){var o=a.retryLane;a.retryLane=o!==0&&o<i?o:i}}function jl(a,i){il(a,i),(a=a.alternate)&&il(a,i)}function kl(){return null}var ll=typeof reportError=="function"?reportError:function(a){console.error(a)};function ml(a){this._internalRoot=a}nl.prototype.render=ml.prototype.render=function(a){var i=this._internalRoot;if(i===null)throw Error(p$2(409));gl(a,i,null,null)};nl.prototype.unmount=ml.prototype.unmount=function(){var a=this._internalRoot;if(a!==null){this._internalRoot=null;var i=a.containerInfo;Sk(function(){gl(null,a,null,null)}),i[uf]=null}};function nl(a){this._internalRoot=a}nl.prototype.unstable_scheduleHydration=function(a){if(a){var i=Hc();a={blockedOn:null,target:a,priority:i};for(var o=0;o<Qc.length&&i!==0&&i<Qc[o].priority;o++);Qc.splice(o,0,a),o===0&&Vc(a)}};function ol(a){return!(!a||a.nodeType!==1&&a.nodeType!==9&&a.nodeType!==11)}function pl(a){return!(!a||a.nodeType!==1&&a.nodeType!==9&&a.nodeType!==11&&(a.nodeType!==8||a.nodeValue!==" react-mount-point-unstable "))}function ql(){}function rl(a,i,o,s,$){if($){if(typeof s=="function"){var j=s;s=function(){var nt=hl(_e);j.call(nt)}}var _e=fl(i,s,a,0,null,!1,!1,"",ql);return a._reactRootContainer=_e,a[uf]=_e.current,sf(a.nodeType===8?a.parentNode:a),Sk(),_e}for(;$=a.lastChild;)a.removeChild($);if(typeof s=="function"){var et=s;s=function(){var nt=hl(tt);et.call(nt)}}var tt=cl(a,0,!1,null,null,!1,!1,"",ql);return a._reactRootContainer=tt,a[uf]=tt.current,sf(a.nodeType===8?a.parentNode:a),Sk(function(){gl(i,tt,o,s)}),tt}function sl(a,i,o,s,$){var j=o._reactRootContainer;if(j){var _e=j;if(typeof $=="function"){var et=$;$=function(){var tt=hl(_e);et.call(tt)}}gl(i,_e,a,$)}else _e=rl(o,i,a,$,s);return hl(_e)}Ec=function(a){switch(a.tag){case 3:var i=a.stateNode;if(i.current.memoizedState.isDehydrated){var o=tc(i.pendingLanes);o!==0&&(Cc(i,o|1),Ek(i,B()),!(K&6)&&(Hj=B()+500,jg()))}break;case 13:Sk(function(){var s=Zg(a,1);if(s!==null){var $=L();mh(s,a,1,$)}}),jl(a,1)}};Fc=function(a){if(a.tag===13){var i=Zg(a,134217728);if(i!==null){var o=L();mh(i,a,134217728,o)}jl(a,134217728)}};Gc=function(a){if(a.tag===13){var i=lh(a),o=Zg(a,i);if(o!==null){var s=L();mh(o,a,i,s)}jl(a,i)}};Hc=function(){return C};Ic=function(a,i){var o=C;try{return C=a,i()}finally{C=o}};yb=function(a,i,o){switch(i){case"input":if(bb(a,o),i=o.name,o.type==="radio"&&i!=null){for(o=a;o.parentNode;)o=o.parentNode;for(o=o.querySelectorAll("input[name="+JSON.stringify(""+i)+'][type="radio"]'),i=0;i<o.length;i++){var s=o[i];if(s!==a&&s.form===a.form){var $=Db(s);if(!$)throw Error(p$2(90));Wa(s),bb(s,$)}}}break;case"textarea":ib(a,o);break;case"select":i=o.value,i!=null&&fb(a,!!o.multiple,i,!1)}};Gb=Rk;Hb=Sk;var tl={usingClientEntryPoint:!1,Events:[Cb,ue,Db,Eb,Fb,Rk]},ul={findFiberByHostInstance:Wc,bundleType:0,version:"18.2.0",rendererPackageName:"react-dom"},vl={bundleType:ul.bundleType,version:ul.version,rendererPackageName:ul.rendererPackageName,rendererConfig:ul.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:ua.ReactCurrentDispatcher,findHostInstanceByFiber:function(a){return a=Zb(a),a===null?null:a.stateNode},findFiberByHostInstance:ul.findFiberByHostInstance||kl,findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.2.0-next-9e3b772b8-20220608"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var wl=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!wl.isDisabled&&wl.supportsFiber)try{kc=wl.inject(vl),lc=wl}catch{}}reactDom_production_min.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=tl;reactDom_production_min.createPortal=function(a,i){var o=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!ol(i))throw Error(p$2(200));return dl(a,i,null,o)};reactDom_production_min.createRoot=function(a,i){if(!ol(a))throw Error(p$2(299));var o=!1,s="",$=ll;return i!=null&&(i.unstable_strictMode===!0&&(o=!0),i.identifierPrefix!==void 0&&(s=i.identifierPrefix),i.onRecoverableError!==void 0&&($=i.onRecoverableError)),i=cl(a,1,!1,null,null,o,!1,s,$),a[uf]=i.current,sf(a.nodeType===8?a.parentNode:a),new ml(i)};reactDom_production_min.findDOMNode=function(a){if(a==null)return null;if(a.nodeType===1)return a;var i=a._reactInternals;if(i===void 0)throw typeof a.render=="function"?Error(p$2(188)):(a=Object.keys(a).join(","),Error(p$2(268,a)));return a=Zb(i),a=a===null?null:a.stateNode,a};reactDom_production_min.flushSync=function(a){return Sk(a)};reactDom_production_min.hydrate=function(a,i,o){if(!pl(i))throw Error(p$2(200));return sl(null,a,i,!0,o)};reactDom_production_min.hydrateRoot=function(a,i,o){if(!ol(a))throw Error(p$2(405));var s=o!=null&&o.hydratedSources||null,$=!1,j="",_e=ll;if(o!=null&&(o.unstable_strictMode===!0&&($=!0),o.identifierPrefix!==void 0&&(j=o.identifierPrefix),o.onRecoverableError!==void 0&&(_e=o.onRecoverableError)),i=fl(i,null,a,1,o??null,$,!1,j,_e),a[uf]=i.current,sf(a),s)for(a=0;a<s.length;a++)o=s[a],$=o._getVersion,$=$(o._source),i.mutableSourceEagerHydrationData==null?i.mutableSourceEagerHydrationData=[o,$]:i.mutableSourceEagerHydrationData.push(o,$);return new nl(i)};reactDom_production_min.render=function(a,i,o){if(!pl(i))throw Error(p$2(200));return sl(null,a,i,!1,o)};reactDom_production_min.unmountComponentAtNode=function(a){if(!pl(a))throw Error(p$2(40));return a._reactRootContainer?(Sk(function(){sl(null,null,a,!1,function(){a._reactRootContainer=null,a[uf]=null})}),!0):!1};reactDom_production_min.unstable_batchedUpdates=Rk;reactDom_production_min.unstable_renderSubtreeIntoContainer=function(a,i,o,s){if(!pl(o))throw Error(p$2(200));if(a==null||a._reactInternals===void 0)throw Error(p$2(38));return sl(a,i,o,!1,s)};reactDom_production_min.version="18.2.0-next-9e3b772b8-20220608";function checkDCE(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(checkDCE)}catch(a){console.error(a)}}checkDCE(),reactDom.exports=reactDom_production_min;var reactDomExports=reactDom.exports;const ReactDOM=getDefaultExportFromCjs(reactDomExports),ReactDOM$1=_mergeNamespaces({__proto__:null,default:ReactDOM},[reactDomExports]);var createRoot,m$2=reactDomExports;createRoot=m$2.createRoot,m$2.hydrateRoot;/**
 * @remix-run/router v1.15.3
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function _extends$4(){return _extends$4=Object.assign?Object.assign.bind():function(a){for(var i=1;i<arguments.length;i++){var o=arguments[i];for(var s in o)Object.prototype.hasOwnProperty.call(o,s)&&(a[s]=o[s])}return a},_extends$4.apply(this,arguments)}var Action;(function(a){a.Pop="POP",a.Push="PUSH",a.Replace="REPLACE"})(Action||(Action={}));const PopStateEventType="popstate";function createBrowserHistory(a){a===void 0&&(a={});function i(s,$){let{pathname:j,search:_e,hash:et}=s.location;return createLocation("",{pathname:j,search:_e,hash:et},$.state&&$.state.usr||null,$.state&&$.state.key||"default")}function o(s,$){return typeof $=="string"?$:createPath($)}return getUrlBasedHistory(i,o,null,a)}function invariant$1(a,i){if(a===!1||a===null||typeof a>"u")throw new Error(i)}function warning$2(a,i){if(!a){typeof console<"u"&&console.warn(i);try{throw new Error(i)}catch{}}}function createKey(){return Math.random().toString(36).substr(2,8)}function getHistoryState(a,i){return{usr:a.state,key:a.key,idx:i}}function createLocation(a,i,o,s){return o===void 0&&(o=null),_extends$4({pathname:typeof a=="string"?a:a.pathname,search:"",hash:""},typeof i=="string"?parsePath(i):i,{state:o,key:i&&i.key||s||createKey()})}function createPath(a){let{pathname:i="/",search:o="",hash:s=""}=a;return o&&o!=="?"&&(i+=o.charAt(0)==="?"?o:"?"+o),s&&s!=="#"&&(i+=s.charAt(0)==="#"?s:"#"+s),i}function parsePath(a){let i={};if(a){let o=a.indexOf("#");o>=0&&(i.hash=a.substr(o),a=a.substr(0,o));let s=a.indexOf("?");s>=0&&(i.search=a.substr(s),a=a.substr(0,s)),a&&(i.pathname=a)}return i}function getUrlBasedHistory(a,i,o,s){s===void 0&&(s={});let{window:$=document.defaultView,v5Compat:j=!1}=s,_e=$.history,et=Action.Pop,tt=null,nt=at();nt==null&&(nt=0,_e.replaceState(_extends$4({},_e.state,{idx:nt}),""));function at(){return(_e.state||{idx:null}).idx}function it(){et=Action.Pop;let ut=at(),ot=ut==null?null:ut-nt;nt=ut,tt&&tt({action:et,location:rt.location,delta:ot})}function st(ut,ot){et=Action.Push;let dt=createLocation(rt.location,ut,ot);o&&o(dt,ut),nt=at()+1;let pt=getHistoryState(dt,nt),mt=rt.createHref(dt);try{_e.pushState(pt,"",mt)}catch(ft){if(ft instanceof DOMException&&ft.name==="DataCloneError")throw ft;$.location.assign(mt)}j&&tt&&tt({action:et,location:rt.location,delta:1})}function lt(ut,ot){et=Action.Replace;let dt=createLocation(rt.location,ut,ot);o&&o(dt,ut),nt=at();let pt=getHistoryState(dt,nt),mt=rt.createHref(dt);_e.replaceState(pt,"",mt),j&&tt&&tt({action:et,location:rt.location,delta:0})}function ct(ut){let ot=$.location.origin!=="null"?$.location.origin:$.location.href,dt=typeof ut=="string"?ut:createPath(ut);return dt=dt.replace(/ $/,"%20"),invariant$1(ot,"No window.location.(origin|href) available to create URL for href: "+dt),new URL(dt,ot)}let rt={get action(){return et},get location(){return a($,_e)},listen(ut){if(tt)throw new Error("A history only accepts one active listener");return $.addEventListener(PopStateEventType,it),tt=ut,()=>{$.removeEventListener(PopStateEventType,it),tt=null}},createHref(ut){return i($,ut)},createURL:ct,encodeLocation(ut){let ot=ct(ut);return{pathname:ot.pathname,search:ot.search,hash:ot.hash}},push:st,replace:lt,go(ut){return _e.go(ut)}};return rt}var ResultType;(function(a){a.data="data",a.deferred="deferred",a.redirect="redirect",a.error="error"})(ResultType||(ResultType={}));const immutableRouteKeys=new Set(["lazy","caseSensitive","path","id","index","children"]);function isIndexRoute(a){return a.index===!0}function convertRoutesToDataRoutes(a,i,o,s){return o===void 0&&(o=[]),s===void 0&&(s={}),a.map(($,j)=>{let _e=[...o,j],et=typeof $.id=="string"?$.id:_e.join("-");if(invariant$1($.index!==!0||!$.children,"Cannot specify children on an index route"),invariant$1(!s[et],'Found a route id collision on id "'+et+`".  Route id's must be globally unique within Data Router usages`),isIndexRoute($)){let tt=_extends$4({},$,i($),{id:et});return s[et]=tt,tt}else{let tt=_extends$4({},$,i($),{id:et,children:void 0});return s[et]=tt,$.children&&(tt.children=convertRoutesToDataRoutes($.children,i,_e,s)),tt}})}function matchRoutes(a,i,o){o===void 0&&(o="/");let s=typeof i=="string"?parsePath(i):i,$=stripBasename(s.pathname||"/",o);if($==null)return null;let j=flattenRoutes(a);rankRouteBranches(j);let _e=null;for(let et=0;_e==null&&et<j.length;++et){let tt=decodePath($);_e=matchRouteBranch(j[et],tt)}return _e}function convertRouteMatchToUiMatch(a,i){let{route:o,pathname:s,params:$}=a;return{id:o.id,pathname:s,params:$,data:i[o.id],handle:o.handle}}function flattenRoutes(a,i,o,s){i===void 0&&(i=[]),o===void 0&&(o=[]),s===void 0&&(s="");let $=(j,_e,et)=>{let tt={relativePath:et===void 0?j.path||"":et,caseSensitive:j.caseSensitive===!0,childrenIndex:_e,route:j};tt.relativePath.startsWith("/")&&(invariant$1(tt.relativePath.startsWith(s),'Absolute route path "'+tt.relativePath+'" nested under path '+('"'+s+'" is not valid. An absolute child route path ')+"must start with the combined path of all its parent routes."),tt.relativePath=tt.relativePath.slice(s.length));let nt=joinPaths([s,tt.relativePath]),at=o.concat(tt);j.children&&j.children.length>0&&(invariant$1(j.index!==!0,"Index routes must not have child routes. Please remove "+('all child routes from route path "'+nt+'".')),flattenRoutes(j.children,i,at,nt)),!(j.path==null&&!j.index)&&i.push({path:nt,score:computeScore(nt,j.index),routesMeta:at})};return a.forEach((j,_e)=>{var et;if(j.path===""||!((et=j.path)!=null&&et.includes("?")))$(j,_e);else for(let tt of explodeOptionalSegments(j.path))$(j,_e,tt)}),i}function explodeOptionalSegments(a){let i=a.split("/");if(i.length===0)return[];let[o,...s]=i,$=o.endsWith("?"),j=o.replace(/\?$/,"");if(s.length===0)return $?[j,""]:[j];let _e=explodeOptionalSegments(s.join("/")),et=[];return et.push(..._e.map(tt=>tt===""?j:[j,tt].join("/"))),$&&et.push(..._e),et.map(tt=>a.startsWith("/")&&tt===""?"/":tt)}function rankRouteBranches(a){a.sort((i,o)=>i.score!==o.score?o.score-i.score:compareIndexes(i.routesMeta.map(s=>s.childrenIndex),o.routesMeta.map(s=>s.childrenIndex)))}const paramRe=/^:[\w-]+$/,dynamicSegmentValue=3,indexRouteValue=2,emptySegmentValue=1,staticSegmentValue=10,splatPenalty=-2,isSplat=a=>a==="*";function computeScore(a,i){let o=a.split("/"),s=o.length;return o.some(isSplat)&&(s+=splatPenalty),i&&(s+=indexRouteValue),o.filter($=>!isSplat($)).reduce(($,j)=>$+(paramRe.test(j)?dynamicSegmentValue:j===""?emptySegmentValue:staticSegmentValue),s)}function compareIndexes(a,i){return a.length===i.length&&a.slice(0,-1).every((s,$)=>s===i[$])?a[a.length-1]-i[i.length-1]:0}function matchRouteBranch(a,i){let{routesMeta:o}=a,s={},$="/",j=[];for(let _e=0;_e<o.length;++_e){let et=o[_e],tt=_e===o.length-1,nt=$==="/"?i:i.slice($.length)||"/",at=matchPath({path:et.relativePath,caseSensitive:et.caseSensitive,end:tt},nt);if(!at)return null;Object.assign(s,at.params);let it=et.route;j.push({params:s,pathname:joinPaths([$,at.pathname]),pathnameBase:normalizePathname(joinPaths([$,at.pathnameBase])),route:it}),at.pathnameBase!=="/"&&($=joinPaths([$,at.pathnameBase]))}return j}function matchPath(a,i){typeof a=="string"&&(a={path:a,caseSensitive:!1,end:!0});let[o,s]=compilePath(a.path,a.caseSensitive,a.end),$=i.match(o);if(!$)return null;let j=$[0],_e=j.replace(/(.)\/+$/,"$1"),et=$.slice(1);return{params:s.reduce((nt,at,it)=>{let{paramName:st,isOptional:lt}=at;if(st==="*"){let rt=et[it]||"";_e=j.slice(0,j.length-rt.length).replace(/(.)\/+$/,"$1")}const ct=et[it];return lt&&!ct?nt[st]=void 0:nt[st]=(ct||"").replace(/%2F/g,"/"),nt},{}),pathname:j,pathnameBase:_e,pattern:a}}function compilePath(a,i,o){i===void 0&&(i=!1),o===void 0&&(o=!0),warning$2(a==="*"||!a.endsWith("*")||a.endsWith("/*"),'Route path "'+a+'" will be treated as if it were '+('"'+a.replace(/\*$/,"/*")+'" because the `*` character must ')+"always follow a `/` in the pattern. To get rid of this warning, "+('please change the route path to "'+a.replace(/\*$/,"/*")+'".'));let s=[],$="^"+a.replace(/\/*\*?$/,"").replace(/^\/*/,"/").replace(/[\\.*+^${}|()[\]]/g,"\\$&").replace(/\/:([\w-]+)(\?)?/g,(_e,et,tt)=>(s.push({paramName:et,isOptional:tt!=null}),tt?"/?([^\\/]+)?":"/([^\\/]+)"));return a.endsWith("*")?(s.push({paramName:"*"}),$+=a==="*"||a==="/*"?"(.*)$":"(?:\\/(.+)|\\/*)$"):o?$+="\\/*$":a!==""&&a!=="/"&&($+="(?:(?=\\/|$))"),[new RegExp($,i?void 0:"i"),s]}function decodePath(a){try{return a.split("/").map(i=>decodeURIComponent(i).replace(/\//g,"%2F")).join("/")}catch(i){return warning$2(!1,'The URL path "'+a+'" could not be decoded because it is is a malformed URL segment. This is probably due to a bad percent '+("encoding ("+i+").")),a}}function stripBasename(a,i){if(i==="/")return a;if(!a.toLowerCase().startsWith(i.toLowerCase()))return null;let o=i.endsWith("/")?i.length-1:i.length,s=a.charAt(o);return s&&s!=="/"?null:a.slice(o)||"/"}function resolvePath(a,i){i===void 0&&(i="/");let{pathname:o,search:s="",hash:$=""}=typeof a=="string"?parsePath(a):a;return{pathname:o?o.startsWith("/")?o:resolvePathname(o,i):i,search:normalizeSearch(s),hash:normalizeHash($)}}function resolvePathname(a,i){let o=i.replace(/\/+$/,"").split("/");return a.split("/").forEach($=>{$===".."?o.length>1&&o.pop():$!=="."&&o.push($)}),o.length>1?o.join("/"):"/"}function getInvalidPathError(a,i,o,s){return"Cannot include a '"+a+"' character in a manually specified "+("`to."+i+"` field ["+JSON.stringify(s)+"].  Please separate it out to the ")+("`to."+o+"` field. Alternatively you may provide the full path as ")+'a string in <Link to="..."> and the router will parse it for you.'}function getPathContributingMatches(a){return a.filter((i,o)=>o===0||i.route.path&&i.route.path.length>0)}function getResolveToMatches(a,i){let o=getPathContributingMatches(a);return i?o.map((s,$)=>$===a.length-1?s.pathname:s.pathnameBase):o.map(s=>s.pathnameBase)}function resolveTo(a,i,o,s){s===void 0&&(s=!1);let $;typeof a=="string"?$=parsePath(a):($=_extends$4({},a),invariant$1(!$.pathname||!$.pathname.includes("?"),getInvalidPathError("?","pathname","search",$)),invariant$1(!$.pathname||!$.pathname.includes("#"),getInvalidPathError("#","pathname","hash",$)),invariant$1(!$.search||!$.search.includes("#"),getInvalidPathError("#","search","hash",$)));let j=a===""||$.pathname==="",_e=j?"/":$.pathname,et;if(_e==null)et=o;else{let it=i.length-1;if(!s&&_e.startsWith("..")){let st=_e.split("/");for(;st[0]==="..";)st.shift(),it-=1;$.pathname=st.join("/")}et=it>=0?i[it]:"/"}let tt=resolvePath($,et),nt=_e&&_e!=="/"&&_e.endsWith("/"),at=(j||_e===".")&&o.endsWith("/");return!tt.pathname.endsWith("/")&&(nt||at)&&(tt.pathname+="/"),tt}const joinPaths=a=>a.join("/").replace(/\/\/+/g,"/"),normalizePathname=a=>a.replace(/\/+$/,"").replace(/^\/*/,"/"),normalizeSearch=a=>!a||a==="?"?"":a.startsWith("?")?a:"?"+a,normalizeHash=a=>!a||a==="#"?"":a.startsWith("#")?a:"#"+a;class ErrorResponseImpl{constructor(i,o,s,$){$===void 0&&($=!1),this.status=i,this.statusText=o||"",this.internal=$,s instanceof Error?(this.data=s.toString(),this.error=s):this.data=s}}function isRouteErrorResponse(a){return a!=null&&typeof a.status=="number"&&typeof a.statusText=="string"&&typeof a.internal=="boolean"&&"data"in a}const validMutationMethodsArr=["post","put","patch","delete"],validMutationMethods=new Set(validMutationMethodsArr),validRequestMethodsArr=["get",...validMutationMethodsArr],validRequestMethods=new Set(validRequestMethodsArr),redirectStatusCodes=new Set([301,302,303,307,308]),redirectPreserveMethodStatusCodes=new Set([307,308]),IDLE_NAVIGATION={state:"idle",location:void 0,formMethod:void 0,formAction:void 0,formEncType:void 0,formData:void 0,json:void 0,text:void 0},IDLE_FETCHER={state:"idle",data:void 0,formMethod:void 0,formAction:void 0,formEncType:void 0,formData:void 0,json:void 0,text:void 0},IDLE_BLOCKER={state:"unblocked",proceed:void 0,reset:void 0,location:void 0},ABSOLUTE_URL_REGEX$1=/^(?:[a-z][a-z0-9+.-]*:|\/\/)/i,defaultMapRouteProperties=a=>({hasErrorBoundary:!!a.hasErrorBoundary}),TRANSITIONS_STORAGE_KEY="remix-router-transitions";function createRouter(a){const i=a.window?a.window:typeof window<"u"?window:void 0,o=typeof i<"u"&&typeof i.document<"u"&&typeof i.document.createElement<"u",s=!o;invariant$1(a.routes.length>0,"You must provide a non-empty routes array to createRouter");let $;if(a.mapRouteProperties)$=a.mapRouteProperties;else if(a.detectErrorBoundary){let Ht=a.detectErrorBoundary;$=Kt=>({hasErrorBoundary:Ht(Kt)})}else $=defaultMapRouteProperties;let j={},_e=convertRoutesToDataRoutes(a.routes,$,void 0,j),et,tt=a.basename||"/",nt=_extends$4({v7_fetcherPersist:!1,v7_normalizeFormMethod:!1,v7_partialHydration:!1,v7_prependBasename:!1,v7_relativeSplatPath:!1},a.future),at=null,it=new Set,st=null,lt=null,ct=null,rt=a.hydrationData!=null,ut=matchRoutes(_e,a.history.location,tt),ot=null;if(ut==null){let Ht=getInternalRouterError(404,{pathname:a.history.location.pathname}),{matches:Kt,route:nn}=getShortCircuitMatches(_e);ut=Kt,ot={[nn.id]:Ht}}let dt,pt=ut.some(Ht=>Ht.route.lazy),mt=ut.some(Ht=>Ht.route.loader);if(pt)dt=!1;else if(!mt)dt=!0;else if(nt.v7_partialHydration){let Ht=a.hydrationData?a.hydrationData.loaderData:null,Kt=a.hydrationData?a.hydrationData.errors:null,nn=kn=>kn.route.loader?kn.route.loader.hydrate===!0?!1:Ht&&Ht[kn.route.id]!==void 0||Kt&&Kt[kn.route.id]!==void 0:!0;if(Kt){let kn=ut.findIndex(Rn=>Kt[Rn.route.id]!==void 0);dt=ut.slice(0,kn+1).every(nn)}else dt=ut.every(nn)}else dt=a.hydrationData!=null;let ft,ht={historyAction:a.history.action,location:a.history.location,matches:ut,initialized:dt,navigation:IDLE_NAVIGATION,restoreScrollPosition:a.hydrationData!=null?!1:null,preventScrollReset:!1,revalidation:"idle",loaderData:a.hydrationData&&a.hydrationData.loaderData||{},actionData:a.hydrationData&&a.hydrationData.actionData||null,errors:a.hydrationData&&a.hydrationData.errors||ot,fetchers:new Map,blockers:new Map},yt=Action.Pop,bt=!1,gt,xt=!1,vt=new Map,Lt=null,$t=!1,Tt=!1,Et=[],Dt=[],It=new Map,Ct=0,jt=-1,Zt=new Map,Xt=new Set,sn=new Map,Ft=new Map,wt=new Set,kt=new Map,At=new Map,Pt=!1;function Mt(){if(at=a.history.listen(Ht=>{let{action:Kt,location:nn,delta:kn}=Ht;if(Pt){Pt=!1;return}warning$2(At.size===0||kn!=null,"You are trying to use a blocker on a POP navigation to a location that was not created by @remix-run/router. This will fail silently in production. This can happen if you are navigating outside the router via `window.history.pushState`/`window.location.hash` instead of using router navigation APIs.  This can also happen if you are using createHashRouter and the user manually changes the URL.");let Rn=dn({currentLocation:ht.location,nextLocation:nn,historyAction:Kt});if(Rn&&kn!=null){Pt=!0,a.history.go(kn*-1),bn(Rn,{state:"blocked",location:nn,proceed(){bn(Rn,{state:"proceeding",proceed:void 0,reset:void 0,location:nn}),a.history.go(kn)},reset(){let Un=new Map(ht.blockers);Un.set(Rn,IDLE_BLOCKER),zt({blockers:Un})}});return}return tn(Kt,nn)}),o){restoreAppliedTransitions(i,vt);let Ht=()=>persistAppliedTransitions(i,vt);i.addEventListener("pagehide",Ht),Lt=()=>i.removeEventListener("pagehide",Ht)}return ht.initialized||tn(Action.Pop,ht.location,{initialHydration:!0}),ft}function Ot(){at&&at(),Lt&&Lt(),it.clear(),gt&&gt.abort(),ht.fetchers.forEach((Ht,Kt)=>Mn(Kt)),ht.blockers.forEach((Ht,Kt)=>Zn(Kt))}function Bt(Ht){return it.add(Ht),()=>it.delete(Ht)}function zt(Ht,Kt){Kt===void 0&&(Kt={}),ht=_extends$4({},ht,Ht);let nn=[],kn=[];nt.v7_fetcherPersist&&ht.fetchers.forEach((Rn,Un)=>{Rn.state==="idle"&&(wt.has(Un)?kn.push(Un):nn.push(Un))}),[...it].forEach(Rn=>Rn(ht,{deletedFetchers:kn,unstable_viewTransitionOpts:Kt.viewTransitionOpts,unstable_flushSync:Kt.flushSync===!0})),nt.v7_fetcherPersist&&(nn.forEach(Rn=>ht.fetchers.delete(Rn)),kn.forEach(Rn=>Mn(Rn)))}function Gt(Ht,Kt,nn){var kn,Rn;let{flushSync:Un}=nn===void 0?{}:nn,Tn=ht.actionData!=null&&ht.navigation.formMethod!=null&&isMutationMethod(ht.navigation.formMethod)&&ht.navigation.state==="loading"&&((kn=Ht.state)==null?void 0:kn._isRedirect)!==!0,Nn;Kt.actionData?Object.keys(Kt.actionData).length>0?Nn=Kt.actionData:Nn=null:Tn?Nn=ht.actionData:Nn=null;let Cn=Kt.loaderData?mergeLoaderData(ht.loaderData,Kt.loaderData,Kt.matches||[],Kt.errors):ht.loaderData,Ut=ht.blockers;Ut.size>0&&(Ut=new Map(Ut),Ut.forEach((Vt,Qt)=>Ut.set(Qt,IDLE_BLOCKER)));let Rt=bt===!0||ht.navigation.formMethod!=null&&isMutationMethod(ht.navigation.formMethod)&&((Rn=Ht.state)==null?void 0:Rn._isRedirect)!==!0;et&&(_e=et,et=void 0),$t||yt===Action.Pop||(yt===Action.Push?a.history.push(Ht,Ht.state):yt===Action.Replace&&a.history.replace(Ht,Ht.state));let Nt;if(yt===Action.Pop){let Vt=vt.get(ht.location.pathname);Vt&&Vt.has(Ht.pathname)?Nt={currentLocation:ht.location,nextLocation:Ht}:vt.has(Ht.pathname)&&(Nt={currentLocation:Ht,nextLocation:ht.location})}else if(xt){let Vt=vt.get(ht.location.pathname);Vt?Vt.add(Ht.pathname):(Vt=new Set([Ht.pathname]),vt.set(ht.location.pathname,Vt)),Nt={currentLocation:ht.location,nextLocation:Ht}}zt(_extends$4({},Kt,{actionData:Nn,loaderData:Cn,historyAction:yt,location:Ht,initialized:!0,navigation:IDLE_NAVIGATION,revalidation:"idle",restoreScrollPosition:Yn(Ht,Kt.matches||ht.matches),preventScrollReset:Rt,blockers:Ut}),{viewTransitionOpts:Nt,flushSync:Un===!0}),yt=Action.Pop,bt=!1,xt=!1,$t=!1,Tt=!1,Et=[],Dt=[]}async function Wt(Ht,Kt){if(typeof Ht=="number"){a.history.go(Ht);return}let nn=normalizeTo(ht.location,ht.matches,tt,nt.v7_prependBasename,Ht,nt.v7_relativeSplatPath,Kt==null?void 0:Kt.fromRouteId,Kt==null?void 0:Kt.relative),{path:kn,submission:Rn,error:Un}=normalizeNavigateOptions(nt.v7_normalizeFormMethod,!1,nn,Kt),Tn=ht.location,Nn=createLocation(ht.location,kn,Kt&&Kt.state);Nn=_extends$4({},Nn,a.history.encodeLocation(Nn));let Cn=Kt&&Kt.replace!=null?Kt.replace:void 0,Ut=Action.Push;Cn===!0?Ut=Action.Replace:Cn===!1||Rn!=null&&isMutationMethod(Rn.formMethod)&&Rn.formAction===ht.location.pathname+ht.location.search&&(Ut=Action.Replace);let Rt=Kt&&"preventScrollReset"in Kt?Kt.preventScrollReset===!0:void 0,Nt=(Kt&&Kt.unstable_flushSync)===!0,Vt=dn({currentLocation:Tn,nextLocation:Nn,historyAction:Ut});if(Vt){bn(Vt,{state:"blocked",location:Nn,proceed(){bn(Vt,{state:"proceeding",proceed:void 0,reset:void 0,location:Nn}),Wt(Ht,Kt)},reset(){let Qt=new Map(ht.blockers);Qt.set(Vt,IDLE_BLOCKER),zt({blockers:Qt})}});return}return await tn(Ut,Nn,{submission:Rn,pendingError:Un,preventScrollReset:Rt,replace:Kt&&Kt.replace,enableViewTransition:Kt&&Kt.unstable_viewTransition,flushSync:Nt})}function qt(){if(en(),zt({revalidation:"loading"}),ht.navigation.state!=="submitting"){if(ht.navigation.state==="idle"){tn(ht.historyAction,ht.location,{startUninterruptedRevalidation:!0});return}tn(yt||ht.historyAction,ht.navigation.location,{overrideNavigation:ht.navigation})}}async function tn(Ht,Kt,nn){gt&&gt.abort(),gt=null,yt=Ht,$t=(nn&&nn.startUninterruptedRevalidation)===!0,Xn(ht.location,ht.matches),bt=(nn&&nn.preventScrollReset)===!0,xt=(nn&&nn.enableViewTransition)===!0;let kn=et||_e,Rn=nn&&nn.overrideNavigation,Un=matchRoutes(kn,Kt,tt),Tn=(nn&&nn.flushSync)===!0;if(!Un){let Qt=getInternalRouterError(404,{pathname:Kt.pathname}),{matches:rn,route:fn}=getShortCircuitMatches(kn);an(),Gt(Kt,{matches:rn,loaderData:{},errors:{[fn.id]:Qt}},{flushSync:Tn});return}if(ht.initialized&&!Tt&&isHashChangeOnly(ht.location,Kt)&&!(nn&&nn.submission&&isMutationMethod(nn.submission.formMethod))){Gt(Kt,{matches:Un},{flushSync:Tn});return}gt=new AbortController;let Nn=createClientSideRequest(a.history,Kt,gt.signal,nn&&nn.submission),Cn,Ut;if(nn&&nn.pendingError)Ut={[findNearestBoundary(Un).route.id]:nn.pendingError};else if(nn&&nn.submission&&isMutationMethod(nn.submission.formMethod)){let Qt=await ln(Nn,Kt,nn.submission,Un,{replace:nn.replace,flushSync:Tn});if(Qt.shortCircuited)return;Cn=Qt.pendingActionData,Ut=Qt.pendingActionError,Rn=getLoadingNavigation(Kt,nn.submission),Tn=!1,Nn=new Request(Nn.url,{signal:Nn.signal})}let{shortCircuited:Rt,loaderData:Nt,errors:Vt}=await gn(Nn,Kt,Un,Rn,nn&&nn.submission,nn&&nn.fetcherSubmission,nn&&nn.replace,nn&&nn.initialHydration===!0,Tn,Cn,Ut);Rt||(gt=null,Gt(Kt,_extends$4({matches:Un},Cn?{actionData:Cn}:{},{loaderData:Nt,errors:Vt})))}async function ln(Ht,Kt,nn,kn,Rn){Rn===void 0&&(Rn={}),en();let Un=getSubmittingNavigation(Kt,nn);zt({navigation:Un},{flushSync:Rn.flushSync===!0});let Tn,Nn=getTargetMatch(kn,Kt);if(!Nn.route.action&&!Nn.route.lazy)Tn={type:ResultType.error,error:getInternalRouterError(405,{method:Ht.method,pathname:Kt.pathname,routeId:Nn.route.id})};else if(Tn=await callLoaderOrAction("action",Ht,Nn,kn,j,$,tt,nt.v7_relativeSplatPath),Ht.signal.aborted)return{shortCircuited:!0};if(isRedirectResult(Tn)){let Cn;return Rn&&Rn.replace!=null?Cn=Rn.replace:Cn=Tn.location===ht.location.pathname+ht.location.search,await xn(ht,Tn,{submission:nn,replace:Cn}),{shortCircuited:!0}}if(isErrorResult(Tn)){let Cn=findNearestBoundary(kn,Nn.route.id);return(Rn&&Rn.replace)!==!0&&(yt=Action.Push),{pendingActionData:{},pendingActionError:{[Cn.route.id]:Tn.error}}}if(isDeferredResult(Tn))throw getInternalRouterError(400,{type:"defer-action"});return{pendingActionData:{[Nn.route.id]:Tn.data}}}async function gn(Ht,Kt,nn,kn,Rn,Un,Tn,Nn,Cn,Ut,Rt){let Nt=kn||getLoadingNavigation(Kt,Rn),Vt=Rn||Un||getSubmissionFromNavigation(Nt),Qt=et||_e,[rn,fn]=getMatchesToLoad(a.history,ht,nn,Vt,Kt,nt.v7_partialHydration&&Nn===!0,Tt,Et,Dt,wt,sn,Xt,Qt,tt,Ut,Rt);if(an(Gn=>!(nn&&nn.some(qn=>qn.route.id===Gn))||rn&&rn.some(qn=>qn.route.id===Gn)),jt=++Ct,rn.length===0&&fn.length===0){let Gn=Hn();return Gt(Kt,_extends$4({matches:nn,loaderData:{},errors:Rt||null},Ut?{actionData:Ut}:{},Gn?{fetchers:new Map(ht.fetchers)}:{}),{flushSync:Cn}),{shortCircuited:!0}}if(!$t&&(!nt.v7_partialHydration||!Nn)){fn.forEach(qn=>{let Qn=ht.fetchers.get(qn.key),na=getLoadingFetcher(void 0,Qn?Qn.data:void 0);ht.fetchers.set(qn.key,na)});let Gn=Ut||ht.actionData;zt(_extends$4({navigation:Nt},Gn?Object.keys(Gn).length===0?{actionData:null}:{actionData:Gn}:{},fn.length>0?{fetchers:new Map(ht.fetchers)}:{}),{flushSync:Cn})}fn.forEach(Gn=>{It.has(Gn.key)&&En(Gn.key),Gn.controller&&It.set(Gn.key,Gn.controller)});let Ln=()=>fn.forEach(Gn=>En(Gn.key));gt&&gt.signal.addEventListener("abort",Ln);let{results:zn,loaderResults:on,fetcherResults:mn}=await hn(ht.matches,nn,rn,fn,Ht);if(Ht.signal.aborted)return{shortCircuited:!0};gt&&gt.signal.removeEventListener("abort",Ln),fn.forEach(Gn=>It.delete(Gn.key));let Sn=findRedirect(zn);if(Sn){if(Sn.idx>=rn.length){let Gn=fn[Sn.idx-rn.length].key;Xt.add(Gn)}return await xn(ht,Sn.result,{replace:Tn}),{shortCircuited:!0}}let{loaderData:An,errors:Fn}=processLoaderData(ht,nn,rn,on,Rt,fn,mn,kt);kt.forEach((Gn,qn)=>{Gn.subscribe(Qn=>{(Qn||Gn.done)&&kt.delete(qn)})}),nt.v7_partialHydration&&Nn&&ht.errors&&Object.entries(ht.errors).filter(Gn=>{let[qn]=Gn;return!rn.some(Qn=>Qn.route.id===qn)}).forEach(Gn=>{let[qn,Qn]=Gn;Fn=Object.assign(Fn||{},{[qn]:Qn})});let jn=Hn(),Vn=Wn(jt),Kn=jn||Vn||fn.length>0;return _extends$4({loaderData:An,errors:Fn},Kn?{fetchers:new Map(ht.fetchers)}:{})}function yn(Ht,Kt,nn,kn){if(s)throw new Error("router.fetch() was called during the server render, but it shouldn't be. You are likely calling a useFetcher() method in the body of your component. Try moving it to a useEffect or a callback.");It.has(Ht)&&En(Ht);let Rn=(kn&&kn.unstable_flushSync)===!0,Un=et||_e,Tn=normalizeTo(ht.location,ht.matches,tt,nt.v7_prependBasename,nn,nt.v7_relativeSplatPath,Kt,kn==null?void 0:kn.relative),Nn=matchRoutes(Un,Tn,tt);if(!Nn){vn(Ht,Kt,getInternalRouterError(404,{pathname:Tn}),{flushSync:Rn});return}let{path:Cn,submission:Ut,error:Rt}=normalizeNavigateOptions(nt.v7_normalizeFormMethod,!0,Tn,kn);if(Rt){vn(Ht,Kt,Rt,{flushSync:Rn});return}let Nt=getTargetMatch(Nn,Cn);if(bt=(kn&&kn.preventScrollReset)===!0,Ut&&isMutationMethod(Ut.formMethod)){Pn(Ht,Kt,Cn,Nt,Nn,Rn,Ut);return}sn.set(Ht,{routeId:Kt,path:Cn}),cn(Ht,Kt,Cn,Nt,Nn,Rn,Ut)}async function Pn(Ht,Kt,nn,kn,Rn,Un,Tn){if(en(),sn.delete(Ht),!kn.route.action&&!kn.route.lazy){let qn=getInternalRouterError(405,{method:Tn.formMethod,pathname:nn,routeId:Kt});vn(Ht,Kt,qn,{flushSync:Un});return}let Nn=ht.fetchers.get(Ht);Jt(Ht,getSubmittingFetcher(Tn,Nn),{flushSync:Un});let Cn=new AbortController,Ut=createClientSideRequest(a.history,nn,Cn.signal,Tn);It.set(Ht,Cn);let Rt=Ct,Nt=await callLoaderOrAction("action",Ut,kn,Rn,j,$,tt,nt.v7_relativeSplatPath);if(Ut.signal.aborted){It.get(Ht)===Cn&&It.delete(Ht);return}if(nt.v7_fetcherPersist&&wt.has(Ht)){if(isRedirectResult(Nt)||isErrorResult(Nt)){Jt(Ht,getDoneFetcher(void 0));return}}else{if(isRedirectResult(Nt))if(It.delete(Ht),jt>Rt){Jt(Ht,getDoneFetcher(void 0));return}else return Xt.add(Ht),Jt(Ht,getLoadingFetcher(Tn)),xn(ht,Nt,{fetcherSubmission:Tn});if(isErrorResult(Nt)){vn(Ht,Kt,Nt.error);return}}if(isDeferredResult(Nt))throw getInternalRouterError(400,{type:"defer-action"});let Vt=ht.navigation.location||ht.location,Qt=createClientSideRequest(a.history,Vt,Cn.signal),rn=et||_e,fn=ht.navigation.state!=="idle"?matchRoutes(rn,ht.navigation.location,tt):ht.matches;invariant$1(fn,"Didn't find any matches after fetcher action");let Ln=++Ct;Zt.set(Ht,Ln);let zn=getLoadingFetcher(Tn,Nt.data);ht.fetchers.set(Ht,zn);let[on,mn]=getMatchesToLoad(a.history,ht,fn,Tn,Vt,!1,Tt,Et,Dt,wt,sn,Xt,rn,tt,{[kn.route.id]:Nt.data},void 0);mn.filter(qn=>qn.key!==Ht).forEach(qn=>{let Qn=qn.key,na=ht.fetchers.get(Qn),ba=getLoadingFetcher(void 0,na?na.data:void 0);ht.fetchers.set(Qn,ba),It.has(Qn)&&En(Qn),qn.controller&&It.set(Qn,qn.controller)}),zt({fetchers:new Map(ht.fetchers)});let Sn=()=>mn.forEach(qn=>En(qn.key));Cn.signal.addEventListener("abort",Sn);let{results:An,loaderResults:Fn,fetcherResults:jn}=await hn(ht.matches,fn,on,mn,Qt);if(Cn.signal.aborted)return;Cn.signal.removeEventListener("abort",Sn),Zt.delete(Ht),It.delete(Ht),mn.forEach(qn=>It.delete(qn.key));let Vn=findRedirect(An);if(Vn){if(Vn.idx>=on.length){let qn=mn[Vn.idx-on.length].key;Xt.add(qn)}return xn(ht,Vn.result)}let{loaderData:Kn,errors:Gn}=processLoaderData(ht,ht.matches,on,Fn,void 0,mn,jn,kt);if(ht.fetchers.has(Ht)){let qn=getDoneFetcher(Nt.data);ht.fetchers.set(Ht,qn)}Wn(Ln),ht.navigation.state==="loading"&&Ln>jt?(invariant$1(yt,"Expected pending action"),gt&&gt.abort(),Gt(ht.navigation.location,{matches:fn,loaderData:Kn,errors:Gn,fetchers:new Map(ht.fetchers)})):(zt({errors:Gn,loaderData:mergeLoaderData(ht.loaderData,Kn,fn,Gn),fetchers:new Map(ht.fetchers)}),Tt=!1)}async function cn(Ht,Kt,nn,kn,Rn,Un,Tn){let Nn=ht.fetchers.get(Ht);Jt(Ht,getLoadingFetcher(Tn,Nn?Nn.data:void 0),{flushSync:Un});let Cn=new AbortController,Ut=createClientSideRequest(a.history,nn,Cn.signal);It.set(Ht,Cn);let Rt=Ct,Nt=await callLoaderOrAction("loader",Ut,kn,Rn,j,$,tt,nt.v7_relativeSplatPath);if(isDeferredResult(Nt)&&(Nt=await resolveDeferredData(Nt,Ut.signal,!0)||Nt),It.get(Ht)===Cn&&It.delete(Ht),!Ut.signal.aborted){if(wt.has(Ht)){Jt(Ht,getDoneFetcher(void 0));return}if(isRedirectResult(Nt))if(jt>Rt){Jt(Ht,getDoneFetcher(void 0));return}else{Xt.add(Ht),await xn(ht,Nt);return}if(isErrorResult(Nt)){vn(Ht,Kt,Nt.error);return}invariant$1(!isDeferredResult(Nt),"Unhandled fetcher deferred data"),Jt(Ht,getDoneFetcher(Nt.data))}}async function xn(Ht,Kt,nn){let{submission:kn,fetcherSubmission:Rn,replace:Un}=nn===void 0?{}:nn;Kt.revalidate&&(Tt=!0);let Tn=createLocation(Ht.location,Kt.location,{_isRedirect:!0});if(invariant$1(Tn,"Expected a location on the redirect navigation"),o){let Vt=!1;if(Kt.reloadDocument)Vt=!0;else if(ABSOLUTE_URL_REGEX$1.test(Kt.location)){const Qt=a.history.createURL(Kt.location);Vt=Qt.origin!==i.location.origin||stripBasename(Qt.pathname,tt)==null}if(Vt){Un?i.location.replace(Kt.location):i.location.assign(Kt.location);return}}gt=null;let Nn=Un===!0?Action.Replace:Action.Push,{formMethod:Cn,formAction:Ut,formEncType:Rt}=Ht.navigation;!kn&&!Rn&&Cn&&Ut&&Rt&&(kn=getSubmissionFromNavigation(Ht.navigation));let Nt=kn||Rn;if(redirectPreserveMethodStatusCodes.has(Kt.status)&&Nt&&isMutationMethod(Nt.formMethod))await tn(Nn,Tn,{submission:_extends$4({},Nt,{formAction:Kt.location}),preventScrollReset:bt});else{let Vt=getLoadingNavigation(Tn,kn);await tn(Nn,Tn,{overrideNavigation:Vt,fetcherSubmission:Rn,preventScrollReset:bt})}}async function hn(Ht,Kt,nn,kn,Rn){let Un=await Promise.all([...nn.map(Cn=>callLoaderOrAction("loader",Rn,Cn,Kt,j,$,tt,nt.v7_relativeSplatPath)),...kn.map(Cn=>Cn.matches&&Cn.match&&Cn.controller?callLoaderOrAction("loader",createClientSideRequest(a.history,Cn.path,Cn.controller.signal),Cn.match,Cn.matches,j,$,tt,nt.v7_relativeSplatPath):{type:ResultType.error,error:getInternalRouterError(404,{pathname:Cn.path})})]),Tn=Un.slice(0,nn.length),Nn=Un.slice(nn.length);return await Promise.all([resolveDeferredResults(Ht,nn,Tn,Tn.map(()=>Rn.signal),!1,ht.loaderData),resolveDeferredResults(Ht,kn.map(Cn=>Cn.match),Nn,kn.map(Cn=>Cn.controller?Cn.controller.signal:null),!0)]),{results:Un,loaderResults:Tn,fetcherResults:Nn}}function en(){Tt=!0,Et.push(...an()),sn.forEach((Ht,Kt)=>{It.has(Kt)&&(Dt.push(Kt),En(Kt))})}function Jt(Ht,Kt,nn){nn===void 0&&(nn={}),ht.fetchers.set(Ht,Kt),zt({fetchers:new Map(ht.fetchers)},{flushSync:(nn&&nn.flushSync)===!0})}function vn(Ht,Kt,nn,kn){kn===void 0&&(kn={});let Rn=findNearestBoundary(ht.matches,Kt);Mn(Ht),zt({errors:{[Rn.route.id]:nn},fetchers:new Map(ht.fetchers)},{flushSync:(kn&&kn.flushSync)===!0})}function $n(Ht){return nt.v7_fetcherPersist&&(Ft.set(Ht,(Ft.get(Ht)||0)+1),wt.has(Ht)&&wt.delete(Ht)),ht.fetchers.get(Ht)||IDLE_FETCHER}function Mn(Ht){let Kt=ht.fetchers.get(Ht);It.has(Ht)&&!(Kt&&Kt.state==="loading"&&Zt.has(Ht))&&En(Ht),sn.delete(Ht),Zt.delete(Ht),Xt.delete(Ht),wt.delete(Ht),ht.fetchers.delete(Ht)}function On(Ht){if(nt.v7_fetcherPersist){let Kt=(Ft.get(Ht)||0)-1;Kt<=0?(Ft.delete(Ht),wt.add(Ht)):Ft.set(Ht,Kt)}else Mn(Ht);zt({fetchers:new Map(ht.fetchers)})}function En(Ht){let Kt=It.get(Ht);invariant$1(Kt,"Expected fetch controller: "+Ht),Kt.abort(),It.delete(Ht)}function Bn(Ht){for(let Kt of Ht){let nn=$n(Kt),kn=getDoneFetcher(nn.data);ht.fetchers.set(Kt,kn)}}function Hn(){let Ht=[],Kt=!1;for(let nn of Xt){let kn=ht.fetchers.get(nn);invariant$1(kn,"Expected fetcher: "+nn),kn.state==="loading"&&(Xt.delete(nn),Ht.push(nn),Kt=!0)}return Bn(Ht),Kt}function Wn(Ht){let Kt=[];for(let[nn,kn]of Zt)if(kn<Ht){let Rn=ht.fetchers.get(nn);invariant$1(Rn,"Expected fetcher: "+nn),Rn.state==="loading"&&(En(nn),Zt.delete(nn),Kt.push(nn))}return Bn(Kt),Kt.length>0}function _n(Ht,Kt){let nn=ht.blockers.get(Ht)||IDLE_BLOCKER;return At.get(Ht)!==Kt&&At.set(Ht,Kt),nn}function Zn(Ht){ht.blockers.delete(Ht),At.delete(Ht)}function bn(Ht,Kt){let nn=ht.blockers.get(Ht)||IDLE_BLOCKER;invariant$1(nn.state==="unblocked"&&Kt.state==="blocked"||nn.state==="blocked"&&Kt.state==="blocked"||nn.state==="blocked"&&Kt.state==="proceeding"||nn.state==="blocked"&&Kt.state==="unblocked"||nn.state==="proceeding"&&Kt.state==="unblocked","Invalid blocker state transition: "+nn.state+" -> "+Kt.state);let kn=new Map(ht.blockers);kn.set(Ht,Kt),zt({blockers:kn})}function dn(Ht){let{currentLocation:Kt,nextLocation:nn,historyAction:kn}=Ht;if(At.size===0)return;At.size>1&&warning$2(!1,"A router only supports one blocker at a time");let Rn=Array.from(At.entries()),[Un,Tn]=Rn[Rn.length-1],Nn=ht.blockers.get(Un);if(!(Nn&&Nn.state==="proceeding")&&Tn({currentLocation:Kt,nextLocation:nn,historyAction:kn}))return Un}function an(Ht){let Kt=[];return kt.forEach((nn,kn)=>{(!Ht||Ht(kn))&&(nn.cancel(),Kt.push(kn),kt.delete(kn))}),Kt}function In(Ht,Kt,nn){if(st=Ht,ct=Kt,lt=nn||null,!rt&&ht.navigation===IDLE_NAVIGATION){rt=!0;let kn=Yn(ht.location,ht.matches);kn!=null&&zt({restoreScrollPosition:kn})}return()=>{st=null,ct=null,lt=null}}function Dn(Ht,Kt){return lt&&lt(Ht,Kt.map(kn=>convertRouteMatchToUiMatch(kn,ht.loaderData)))||Ht.key}function Xn(Ht,Kt){if(st&&ct){let nn=Dn(Ht,Kt);st[nn]=ct()}}function Yn(Ht,Kt){if(st){let nn=Dn(Ht,Kt),kn=st[nn];if(typeof kn=="number")return kn}return null}function pn(Ht){j={},et=convertRoutesToDataRoutes(Ht,$,void 0,j)}return ft={get basename(){return tt},get future(){return nt},get state(){return ht},get routes(){return _e},get window(){return i},initialize:Mt,subscribe:Bt,enableScrollRestoration:In,navigate:Wt,fetch:yn,revalidate:qt,createHref:Ht=>a.history.createHref(Ht),encodeLocation:Ht=>a.history.encodeLocation(Ht),getFetcher:$n,deleteFetcher:On,dispose:Ot,getBlocker:_n,deleteBlocker:Zn,_internalFetchControllers:It,_internalActiveDeferreds:kt,_internalSetRoutes:pn},ft}function isSubmissionNavigation(a){return a!=null&&("formData"in a&&a.formData!=null||"body"in a&&a.body!==void 0)}function normalizeTo(a,i,o,s,$,j,_e,et){let tt,nt;if(_e){tt=[];for(let it of i)if(tt.push(it),it.route.id===_e){nt=it;break}}else tt=i,nt=i[i.length-1];let at=resolveTo($||".",getResolveToMatches(tt,j),stripBasename(a.pathname,o)||a.pathname,et==="path");return $==null&&(at.search=a.search,at.hash=a.hash),($==null||$===""||$===".")&&nt&&nt.route.index&&!hasNakedIndexQuery(at.search)&&(at.search=at.search?at.search.replace(/^\?/,"?index&"):"?index"),s&&o!=="/"&&(at.pathname=at.pathname==="/"?o:joinPaths([o,at.pathname])),createPath(at)}function normalizeNavigateOptions(a,i,o,s){if(!s||!isSubmissionNavigation(s))return{path:o};if(s.formMethod&&!isValidMethod(s.formMethod))return{path:o,error:getInternalRouterError(405,{method:s.formMethod})};let $=()=>({path:o,error:getInternalRouterError(400,{type:"invalid-body"})}),j=s.formMethod||"get",_e=a?j.toUpperCase():j.toLowerCase(),et=stripHashFromPath(o);if(s.body!==void 0){if(s.formEncType==="text/plain"){if(!isMutationMethod(_e))return $();let st=typeof s.body=="string"?s.body:s.body instanceof FormData||s.body instanceof URLSearchParams?Array.from(s.body.entries()).reduce((lt,ct)=>{let[rt,ut]=ct;return""+lt+rt+"="+ut+`
`},""):String(s.body);return{path:o,submission:{formMethod:_e,formAction:et,formEncType:s.formEncType,formData:void 0,json:void 0,text:st}}}else if(s.formEncType==="application/json"){if(!isMutationMethod(_e))return $();try{let st=typeof s.body=="string"?JSON.parse(s.body):s.body;return{path:o,submission:{formMethod:_e,formAction:et,formEncType:s.formEncType,formData:void 0,json:st,text:void 0}}}catch{return $()}}}invariant$1(typeof FormData=="function","FormData is not available in this environment");let tt,nt;if(s.formData)tt=convertFormDataToSearchParams(s.formData),nt=s.formData;else if(s.body instanceof FormData)tt=convertFormDataToSearchParams(s.body),nt=s.body;else if(s.body instanceof URLSearchParams)tt=s.body,nt=convertSearchParamsToFormData(tt);else if(s.body==null)tt=new URLSearchParams,nt=new FormData;else try{tt=new URLSearchParams(s.body),nt=convertSearchParamsToFormData(tt)}catch{return $()}let at={formMethod:_e,formAction:et,formEncType:s&&s.formEncType||"application/x-www-form-urlencoded",formData:nt,json:void 0,text:void 0};if(isMutationMethod(at.formMethod))return{path:o,submission:at};let it=parsePath(o);return i&&it.search&&hasNakedIndexQuery(it.search)&&tt.append("index",""),it.search="?"+tt,{path:createPath(it),submission:at}}function getLoaderMatchesUntilBoundary(a,i){let o=a;if(i){let s=a.findIndex($=>$.route.id===i);s>=0&&(o=a.slice(0,s))}return o}function getMatchesToLoad(a,i,o,s,$,j,_e,et,tt,nt,at,it,st,lt,ct,rt){let ut=rt?Object.values(rt)[0]:ct?Object.values(ct)[0]:void 0,ot=a.createURL(i.location),dt=a.createURL($),pt=rt?Object.keys(rt)[0]:void 0,ft=getLoaderMatchesUntilBoundary(o,pt).filter((yt,bt)=>{let{route:gt}=yt;if(gt.lazy)return!0;if(gt.loader==null)return!1;if(j)return gt.loader.hydrate?!0:i.loaderData[gt.id]===void 0&&(!i.errors||i.errors[gt.id]===void 0);if(isNewLoader(i.loaderData,i.matches[bt],yt)||et.some(Lt=>Lt===yt.route.id))return!0;let xt=i.matches[bt],vt=yt;return shouldRevalidateLoader(yt,_extends$4({currentUrl:ot,currentParams:xt.params,nextUrl:dt,nextParams:vt.params},s,{actionResult:ut,defaultShouldRevalidate:_e||ot.pathname+ot.search===dt.pathname+dt.search||ot.search!==dt.search||isNewRouteInstance(xt,vt)}))}),ht=[];return at.forEach((yt,bt)=>{if(j||!o.some($t=>$t.route.id===yt.routeId)||nt.has(bt))return;let gt=matchRoutes(st,yt.path,lt);if(!gt){ht.push({key:bt,routeId:yt.routeId,path:yt.path,matches:null,match:null,controller:null});return}let xt=i.fetchers.get(bt),vt=getTargetMatch(gt,yt.path),Lt=!1;it.has(bt)?Lt=!1:tt.includes(bt)?Lt=!0:xt&&xt.state!=="idle"&&xt.data===void 0?Lt=_e:Lt=shouldRevalidateLoader(vt,_extends$4({currentUrl:ot,currentParams:i.matches[i.matches.length-1].params,nextUrl:dt,nextParams:o[o.length-1].params},s,{actionResult:ut,defaultShouldRevalidate:_e})),Lt&&ht.push({key:bt,routeId:yt.routeId,path:yt.path,matches:gt,match:vt,controller:new AbortController})}),[ft,ht]}function isNewLoader(a,i,o){let s=!i||o.route.id!==i.route.id,$=a[o.route.id]===void 0;return s||$}function isNewRouteInstance(a,i){let o=a.route.path;return a.pathname!==i.pathname||o!=null&&o.endsWith("*")&&a.params["*"]!==i.params["*"]}function shouldRevalidateLoader(a,i){if(a.route.shouldRevalidate){let o=a.route.shouldRevalidate(i);if(typeof o=="boolean")return o}return i.defaultShouldRevalidate}async function loadLazyRouteModule(a,i,o){if(!a.lazy)return;let s=await a.lazy();if(!a.lazy)return;let $=o[a.id];invariant$1($,"No route found in manifest");let j={};for(let _e in s){let tt=$[_e]!==void 0&&_e!=="hasErrorBoundary";warning$2(!tt,'Route "'+$.id+'" has a static property "'+_e+'" defined but its lazy function is also returning a value for this property. '+('The lazy route property "'+_e+'" will be ignored.')),!tt&&!immutableRouteKeys.has(_e)&&(j[_e]=s[_e])}Object.assign($,j),Object.assign($,_extends$4({},i($),{lazy:void 0}))}async function callLoaderOrAction(a,i,o,s,$,j,_e,et,tt){tt===void 0&&(tt={});let nt,at,it,st=rt=>{let ut,ot=new Promise((dt,pt)=>ut=pt);return it=()=>ut(),i.signal.addEventListener("abort",it),Promise.race([rt({request:i,params:o.params,context:tt.requestContext}),ot])};try{let rt=o.route[a];if(o.route.lazy)if(rt){let ut,ot=await Promise.all([st(rt).catch(dt=>{ut=dt}),loadLazyRouteModule(o.route,j,$)]);if(ut)throw ut;at=ot[0]}else if(await loadLazyRouteModule(o.route,j,$),rt=o.route[a],rt)at=await st(rt);else if(a==="action"){let ut=new URL(i.url),ot=ut.pathname+ut.search;throw getInternalRouterError(405,{method:i.method,pathname:ot,routeId:o.route.id})}else return{type:ResultType.data,data:void 0};else if(rt)at=await st(rt);else{let ut=new URL(i.url),ot=ut.pathname+ut.search;throw getInternalRouterError(404,{pathname:ot})}invariant$1(at!==void 0,"You defined "+(a==="action"?"an action":"a loader")+" for route "+('"'+o.route.id+"\" but didn't return anything from your `"+a+"` ")+"function. Please return a value or `null`.")}catch(rt){nt=ResultType.error,at=rt}finally{it&&i.signal.removeEventListener("abort",it)}if(isResponse(at)){let rt=at.status;if(redirectStatusCodes.has(rt)){let ot=at.headers.get("Location");if(invariant$1(ot,"Redirects returned/thrown from loaders/actions must have a Location header"),!ABSOLUTE_URL_REGEX$1.test(ot))ot=normalizeTo(new URL(i.url),s.slice(0,s.indexOf(o)+1),_e,!0,ot,et);else if(!tt.isStaticRequest){let dt=new URL(i.url),pt=ot.startsWith("//")?new URL(dt.protocol+ot):new URL(ot),mt=stripBasename(pt.pathname,_e)!=null;pt.origin===dt.origin&&mt&&(ot=pt.pathname+pt.search+pt.hash)}if(tt.isStaticRequest)throw at.headers.set("Location",ot),at;return{type:ResultType.redirect,status:rt,location:ot,revalidate:at.headers.get("X-Remix-Revalidate")!==null,reloadDocument:at.headers.get("X-Remix-Reload-Document")!==null}}if(tt.isRouteRequest)throw{type:nt===ResultType.error?ResultType.error:ResultType.data,response:at};let ut;try{let ot=at.headers.get("Content-Type");ot&&/\bapplication\/json\b/.test(ot)?at.body==null?ut=null:ut=await at.json():ut=await at.text()}catch(ot){return{type:ResultType.error,error:ot}}return nt===ResultType.error?{type:nt,error:new ErrorResponseImpl(rt,at.statusText,ut),headers:at.headers}:{type:ResultType.data,data:ut,statusCode:at.status,headers:at.headers}}if(nt===ResultType.error)return{type:nt,error:at};if(isDeferredData(at)){var lt,ct;return{type:ResultType.deferred,deferredData:at,statusCode:(lt=at.init)==null?void 0:lt.status,headers:((ct=at.init)==null?void 0:ct.headers)&&new Headers(at.init.headers)}}return{type:ResultType.data,data:at}}function createClientSideRequest(a,i,o,s){let $=a.createURL(stripHashFromPath(i)).toString(),j={signal:o};if(s&&isMutationMethod(s.formMethod)){let{formMethod:_e,formEncType:et}=s;j.method=_e.toUpperCase(),et==="application/json"?(j.headers=new Headers({"Content-Type":et}),j.body=JSON.stringify(s.json)):et==="text/plain"?j.body=s.text:et==="application/x-www-form-urlencoded"&&s.formData?j.body=convertFormDataToSearchParams(s.formData):j.body=s.formData}return new Request($,j)}function convertFormDataToSearchParams(a){let i=new URLSearchParams;for(let[o,s]of a.entries())i.append(o,typeof s=="string"?s:s.name);return i}function convertSearchParamsToFormData(a){let i=new FormData;for(let[o,s]of a.entries())i.append(o,s);return i}function processRouteLoaderData(a,i,o,s,$){let j={},_e=null,et,tt=!1,nt={};return o.forEach((at,it)=>{let st=i[it].route.id;if(invariant$1(!isRedirectResult(at),"Cannot handle redirect results in processLoaderData"),isErrorResult(at)){let lt=findNearestBoundary(a,st),ct=at.error;s&&(ct=Object.values(s)[0],s=void 0),_e=_e||{},_e[lt.route.id]==null&&(_e[lt.route.id]=ct),j[st]=void 0,tt||(tt=!0,et=isRouteErrorResponse(at.error)?at.error.status:500),at.headers&&(nt[st]=at.headers)}else isDeferredResult(at)?($.set(st,at.deferredData),j[st]=at.deferredData.data):j[st]=at.data,at.statusCode!=null&&at.statusCode!==200&&!tt&&(et=at.statusCode),at.headers&&(nt[st]=at.headers)}),s&&(_e=s,j[Object.keys(s)[0]]=void 0),{loaderData:j,errors:_e,statusCode:et||200,loaderHeaders:nt}}function processLoaderData(a,i,o,s,$,j,_e,et){let{loaderData:tt,errors:nt}=processRouteLoaderData(i,o,s,$,et);for(let at=0;at<j.length;at++){let{key:it,match:st,controller:lt}=j[at];invariant$1(_e!==void 0&&_e[at]!==void 0,"Did not find corresponding fetcher result");let ct=_e[at];if(!(lt&&lt.signal.aborted))if(isErrorResult(ct)){let rt=findNearestBoundary(a.matches,st==null?void 0:st.route.id);nt&&nt[rt.route.id]||(nt=_extends$4({},nt,{[rt.route.id]:ct.error})),a.fetchers.delete(it)}else if(isRedirectResult(ct))invariant$1(!1,"Unhandled fetcher revalidation redirect");else if(isDeferredResult(ct))invariant$1(!1,"Unhandled fetcher deferred data");else{let rt=getDoneFetcher(ct.data);a.fetchers.set(it,rt)}}return{loaderData:tt,errors:nt}}function mergeLoaderData(a,i,o,s){let $=_extends$4({},i);for(let j of o){let _e=j.route.id;if(i.hasOwnProperty(_e)?i[_e]!==void 0&&($[_e]=i[_e]):a[_e]!==void 0&&j.route.loader&&($[_e]=a[_e]),s&&s.hasOwnProperty(_e))break}return $}function findNearestBoundary(a,i){return(i?a.slice(0,a.findIndex(s=>s.route.id===i)+1):[...a]).reverse().find(s=>s.route.hasErrorBoundary===!0)||a[0]}function getShortCircuitMatches(a){let i=a.length===1?a[0]:a.find(o=>o.index||!o.path||o.path==="/")||{id:"__shim-error-route__"};return{matches:[{params:{},pathname:"",pathnameBase:"",route:i}],route:i}}function getInternalRouterError(a,i){let{pathname:o,routeId:s,method:$,type:j}=i===void 0?{}:i,_e="Unknown Server Error",et="Unknown @remix-run/router error";return a===400?(_e="Bad Request",$&&o&&s?et="You made a "+$+' request to "'+o+'" but '+('did not provide a `loader` for route "'+s+'", ')+"so there is no way to handle the request.":j==="defer-action"?et="defer() is not supported in actions":j==="invalid-body"&&(et="Unable to encode submission body")):a===403?(_e="Forbidden",et='Route "'+s+'" does not match URL "'+o+'"'):a===404?(_e="Not Found",et='No route matches URL "'+o+'"'):a===405&&(_e="Method Not Allowed",$&&o&&s?et="You made a "+$.toUpperCase()+' request to "'+o+'" but '+('did not provide an `action` for route "'+s+'", ')+"so there is no way to handle the request.":$&&(et='Invalid request method "'+$.toUpperCase()+'"')),new ErrorResponseImpl(a||500,_e,new Error(et),!0)}function findRedirect(a){for(let i=a.length-1;i>=0;i--){let o=a[i];if(isRedirectResult(o))return{result:o,idx:i}}}function stripHashFromPath(a){let i=typeof a=="string"?parsePath(a):a;return createPath(_extends$4({},i,{hash:""}))}function isHashChangeOnly(a,i){return a.pathname!==i.pathname||a.search!==i.search?!1:a.hash===""?i.hash!=="":a.hash===i.hash?!0:i.hash!==""}function isDeferredResult(a){return a.type===ResultType.deferred}function isErrorResult(a){return a.type===ResultType.error}function isRedirectResult(a){return(a&&a.type)===ResultType.redirect}function isDeferredData(a){let i=a;return i&&typeof i=="object"&&typeof i.data=="object"&&typeof i.subscribe=="function"&&typeof i.cancel=="function"&&typeof i.resolveData=="function"}function isResponse(a){return a!=null&&typeof a.status=="number"&&typeof a.statusText=="string"&&typeof a.headers=="object"&&typeof a.body<"u"}function isValidMethod(a){return validRequestMethods.has(a.toLowerCase())}function isMutationMethod(a){return validMutationMethods.has(a.toLowerCase())}async function resolveDeferredResults(a,i,o,s,$,j){for(let _e=0;_e<o.length;_e++){let et=o[_e],tt=i[_e];if(!tt)continue;let nt=a.find(it=>it.route.id===tt.route.id),at=nt!=null&&!isNewRouteInstance(nt,tt)&&(j&&j[tt.route.id])!==void 0;if(isDeferredResult(et)&&($||at)){let it=s[_e];invariant$1(it,"Expected an AbortSignal for revalidating fetcher deferred result"),await resolveDeferredData(et,it,$).then(st=>{st&&(o[_e]=st||o[_e])})}}}async function resolveDeferredData(a,i,o){if(o===void 0&&(o=!1),!await a.deferredData.resolveData(i)){if(o)try{return{type:ResultType.data,data:a.deferredData.unwrappedData}}catch($){return{type:ResultType.error,error:$}}return{type:ResultType.data,data:a.deferredData.data}}}function hasNakedIndexQuery(a){return new URLSearchParams(a).getAll("index").some(i=>i==="")}function getTargetMatch(a,i){let o=typeof i=="string"?parsePath(i).search:i.search;if(a[a.length-1].route.index&&hasNakedIndexQuery(o||""))return a[a.length-1];let s=getPathContributingMatches(a);return s[s.length-1]}function getSubmissionFromNavigation(a){let{formMethod:i,formAction:o,formEncType:s,text:$,formData:j,json:_e}=a;if(!(!i||!o||!s)){if($!=null)return{formMethod:i,formAction:o,formEncType:s,formData:void 0,json:void 0,text:$};if(j!=null)return{formMethod:i,formAction:o,formEncType:s,formData:j,json:void 0,text:void 0};if(_e!==void 0)return{formMethod:i,formAction:o,formEncType:s,formData:void 0,json:_e,text:void 0}}}function getLoadingNavigation(a,i){return i?{state:"loading",location:a,formMethod:i.formMethod,formAction:i.formAction,formEncType:i.formEncType,formData:i.formData,json:i.json,text:i.text}:{state:"loading",location:a,formMethod:void 0,formAction:void 0,formEncType:void 0,formData:void 0,json:void 0,text:void 0}}function getSubmittingNavigation(a,i){return{state:"submitting",location:a,formMethod:i.formMethod,formAction:i.formAction,formEncType:i.formEncType,formData:i.formData,json:i.json,text:i.text}}function getLoadingFetcher(a,i){return a?{state:"loading",formMethod:a.formMethod,formAction:a.formAction,formEncType:a.formEncType,formData:a.formData,json:a.json,text:a.text,data:i}:{state:"loading",formMethod:void 0,formAction:void 0,formEncType:void 0,formData:void 0,json:void 0,text:void 0,data:i}}function getSubmittingFetcher(a,i){return{state:"submitting",formMethod:a.formMethod,formAction:a.formAction,formEncType:a.formEncType,formData:a.formData,json:a.json,text:a.text,data:i?i.data:void 0}}function getDoneFetcher(a){return{state:"idle",formMethod:void 0,formAction:void 0,formEncType:void 0,formData:void 0,json:void 0,text:void 0,data:a}}function restoreAppliedTransitions(a,i){try{let o=a.sessionStorage.getItem(TRANSITIONS_STORAGE_KEY);if(o){let s=JSON.parse(o);for(let[$,j]of Object.entries(s||{}))j&&Array.isArray(j)&&i.set($,new Set(j||[]))}}catch{}}function persistAppliedTransitions(a,i){if(i.size>0){let o={};for(let[s,$]of i)o[s]=[...$];try{a.sessionStorage.setItem(TRANSITIONS_STORAGE_KEY,JSON.stringify(o))}catch(s){warning$2(!1,"Failed to save applied view transitions in sessionStorage ("+s+").")}}}/**
 * React Router v6.22.3
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function _extends$3(){return _extends$3=Object.assign?Object.assign.bind():function(a){for(var i=1;i<arguments.length;i++){var o=arguments[i];for(var s in o)Object.prototype.hasOwnProperty.call(o,s)&&(a[s]=o[s])}return a},_extends$3.apply(this,arguments)}const DataRouterContext=reactExports.createContext(null),DataRouterStateContext=reactExports.createContext(null),NavigationContext=reactExports.createContext(null),LocationContext=reactExports.createContext(null),RouteContext=reactExports.createContext({outlet:null,matches:[],isDataRoute:!1}),RouteErrorContext=reactExports.createContext(null);function useHref(a,i){let{relative:o}=i===void 0?{}:i;useInRouterContext()||invariant$1(!1);let{basename:s,navigator:$}=reactExports.useContext(NavigationContext),{hash:j,pathname:_e,search:et}=useResolvedPath(a,{relative:o}),tt=_e;return s!=="/"&&(tt=_e==="/"?s:joinPaths([s,_e])),$.createHref({pathname:tt,search:et,hash:j})}function useInRouterContext(){return reactExports.useContext(LocationContext)!=null}function useLocation(){return useInRouterContext()||invariant$1(!1),reactExports.useContext(LocationContext).location}function useIsomorphicLayoutEffect(a){reactExports.useContext(NavigationContext).static||reactExports.useLayoutEffect(a)}function useNavigate(){let{isDataRoute:a}=reactExports.useContext(RouteContext);return a?useNavigateStable():useNavigateUnstable()}function useNavigateUnstable(){useInRouterContext()||invariant$1(!1);let a=reactExports.useContext(DataRouterContext),{basename:i,future:o,navigator:s}=reactExports.useContext(NavigationContext),{matches:$}=reactExports.useContext(RouteContext),{pathname:j}=useLocation(),_e=JSON.stringify(getResolveToMatches($,o.v7_relativeSplatPath)),et=reactExports.useRef(!1);return useIsomorphicLayoutEffect(()=>{et.current=!0}),reactExports.useCallback(function(nt,at){if(at===void 0&&(at={}),!et.current)return;if(typeof nt=="number"){s.go(nt);return}let it=resolveTo(nt,JSON.parse(_e),j,at.relative==="path");a==null&&i!=="/"&&(it.pathname=it.pathname==="/"?i:joinPaths([i,it.pathname])),(at.replace?s.replace:s.push)(it,at.state,at)},[i,s,_e,j,a])}const OutletContext=reactExports.createContext(null);function useOutlet(a){let i=reactExports.useContext(RouteContext).outlet;return i&&reactExports.createElement(OutletContext.Provider,{value:a},i)}function useParams(){let{matches:a}=reactExports.useContext(RouteContext),i=a[a.length-1];return i?i.params:{}}function useResolvedPath(a,i){let{relative:o}=i===void 0?{}:i,{future:s}=reactExports.useContext(NavigationContext),{matches:$}=reactExports.useContext(RouteContext),{pathname:j}=useLocation(),_e=JSON.stringify(getResolveToMatches($,s.v7_relativeSplatPath));return reactExports.useMemo(()=>resolveTo(a,JSON.parse(_e),j,o==="path"),[a,_e,j,o])}function useRoutesImpl(a,i,o,s){useInRouterContext()||invariant$1(!1);let{navigator:$}=reactExports.useContext(NavigationContext),{matches:j}=reactExports.useContext(RouteContext),_e=j[j.length-1],et=_e?_e.params:{};_e&&_e.pathname;let tt=_e?_e.pathnameBase:"/";_e&&_e.route;let nt=useLocation(),at;if(i){var it;let ut=typeof i=="string"?parsePath(i):i;tt==="/"||(it=ut.pathname)!=null&&it.startsWith(tt)||invariant$1(!1),at=ut}else at=nt;let st=at.pathname||"/",lt=st;if(tt!=="/"){let ut=tt.replace(/^\//,"").split("/");lt="/"+st.replace(/^\//,"").split("/").slice(ut.length).join("/")}let ct=matchRoutes(a,{pathname:lt}),rt=_renderMatches(ct&&ct.map(ut=>Object.assign({},ut,{params:Object.assign({},et,ut.params),pathname:joinPaths([tt,$.encodeLocation?$.encodeLocation(ut.pathname).pathname:ut.pathname]),pathnameBase:ut.pathnameBase==="/"?tt:joinPaths([tt,$.encodeLocation?$.encodeLocation(ut.pathnameBase).pathname:ut.pathnameBase])})),j,o,s);return i&&rt?reactExports.createElement(LocationContext.Provider,{value:{location:_extends$3({pathname:"/",search:"",hash:"",state:null,key:"default"},at),navigationType:Action.Pop}},rt):rt}function DefaultErrorComponent(){let a=useRouteError(),i=isRouteErrorResponse(a)?a.status+" "+a.statusText:a instanceof Error?a.message:JSON.stringify(a),o=a instanceof Error?a.stack:null,$={padding:"0.5rem",backgroundColor:"rgba(200,200,200, 0.5)"};return reactExports.createElement(reactExports.Fragment,null,reactExports.createElement("h2",null,"Unexpected Application Error!"),reactExports.createElement("h3",{style:{fontStyle:"italic"}},i),o?reactExports.createElement("pre",{style:$},o):null,null)}const defaultErrorElement=reactExports.createElement(DefaultErrorComponent,null);class RenderErrorBoundary extends reactExports.Component{constructor(i){super(i),this.state={location:i.location,revalidation:i.revalidation,error:i.error}}static getDerivedStateFromError(i){return{error:i}}static getDerivedStateFromProps(i,o){return o.location!==i.location||o.revalidation!=="idle"&&i.revalidation==="idle"?{error:i.error,location:i.location,revalidation:i.revalidation}:{error:i.error!==void 0?i.error:o.error,location:o.location,revalidation:i.revalidation||o.revalidation}}componentDidCatch(i,o){console.error("React Router caught the following error during render",i,o)}render(){return this.state.error!==void 0?reactExports.createElement(RouteContext.Provider,{value:this.props.routeContext},reactExports.createElement(RouteErrorContext.Provider,{value:this.state.error,children:this.props.component})):this.props.children}}function RenderedRoute(a){let{routeContext:i,match:o,children:s}=a,$=reactExports.useContext(DataRouterContext);return $&&$.static&&$.staticContext&&(o.route.errorElement||o.route.ErrorBoundary)&&($.staticContext._deepestRenderedBoundaryId=o.route.id),reactExports.createElement(RouteContext.Provider,{value:i},s)}function _renderMatches(a,i,o,s){var $;if(i===void 0&&(i=[]),o===void 0&&(o=null),s===void 0&&(s=null),a==null){var j;if((j=o)!=null&&j.errors)a=o.matches;else return null}let _e=a,et=($=o)==null?void 0:$.errors;if(et!=null){let at=_e.findIndex(it=>it.route.id&&(et==null?void 0:et[it.route.id]));at>=0||invariant$1(!1),_e=_e.slice(0,Math.min(_e.length,at+1))}let tt=!1,nt=-1;if(o&&s&&s.v7_partialHydration)for(let at=0;at<_e.length;at++){let it=_e[at];if((it.route.HydrateFallback||it.route.hydrateFallbackElement)&&(nt=at),it.route.id){let{loaderData:st,errors:lt}=o,ct=it.route.loader&&st[it.route.id]===void 0&&(!lt||lt[it.route.id]===void 0);if(it.route.lazy||ct){tt=!0,nt>=0?_e=_e.slice(0,nt+1):_e=[_e[0]];break}}}return _e.reduceRight((at,it,st)=>{let lt,ct=!1,rt=null,ut=null;o&&(lt=et&&it.route.id?et[it.route.id]:void 0,rt=it.route.errorElement||defaultErrorElement,tt&&(nt<0&&st===0?(warningOnce("route-fallback",!1),ct=!0,ut=null):nt===st&&(ct=!0,ut=it.route.hydrateFallbackElement||null)));let ot=i.concat(_e.slice(0,st+1)),dt=()=>{let pt;return lt?pt=rt:ct?pt=ut:it.route.Component?pt=reactExports.createElement(it.route.Component,null):it.route.element?pt=it.route.element:pt=at,reactExports.createElement(RenderedRoute,{match:it,routeContext:{outlet:at,matches:ot,isDataRoute:o!=null},children:pt})};return o&&(it.route.ErrorBoundary||it.route.errorElement||st===0)?reactExports.createElement(RenderErrorBoundary,{location:o.location,revalidation:o.revalidation,component:rt,error:lt,children:dt(),routeContext:{outlet:null,matches:ot,isDataRoute:!0}}):dt()},null)}var DataRouterHook$1=function(a){return a.UseBlocker="useBlocker",a.UseRevalidator="useRevalidator",a.UseNavigateStable="useNavigate",a}(DataRouterHook$1||{}),DataRouterStateHook$1=function(a){return a.UseBlocker="useBlocker",a.UseLoaderData="useLoaderData",a.UseActionData="useActionData",a.UseRouteError="useRouteError",a.UseNavigation="useNavigation",a.UseRouteLoaderData="useRouteLoaderData",a.UseMatches="useMatches",a.UseRevalidator="useRevalidator",a.UseNavigateStable="useNavigate",a.UseRouteId="useRouteId",a}(DataRouterStateHook$1||{});function useDataRouterContext(a){let i=reactExports.useContext(DataRouterContext);return i||invariant$1(!1),i}function useDataRouterState(a){let i=reactExports.useContext(DataRouterStateContext);return i||invariant$1(!1),i}function useRouteContext(a){let i=reactExports.useContext(RouteContext);return i||invariant$1(!1),i}function useCurrentRouteId(a){let i=useRouteContext(),o=i.matches[i.matches.length-1];return o.route.id||invariant$1(!1),o.route.id}function useRouteError(){var a;let i=reactExports.useContext(RouteErrorContext),o=useDataRouterState(DataRouterStateHook$1.UseRouteError),s=useCurrentRouteId(DataRouterStateHook$1.UseRouteError);return i!==void 0?i:(a=o.errors)==null?void 0:a[s]}function useNavigateStable(){let{router:a}=useDataRouterContext(DataRouterHook$1.UseNavigateStable),i=useCurrentRouteId(DataRouterStateHook$1.UseNavigateStable),o=reactExports.useRef(!1);return useIsomorphicLayoutEffect(()=>{o.current=!0}),reactExports.useCallback(function($,j){j===void 0&&(j={}),o.current&&(typeof $=="number"?a.navigate($):a.navigate($,_extends$3({fromRouteId:i},j)))},[a,i])}const alreadyWarned={};function warningOnce(a,i,o){!i&&!alreadyWarned[a]&&(alreadyWarned[a]=!0)}function Navigate(a){let{to:i,replace:o,state:s,relative:$}=a;useInRouterContext()||invariant$1(!1);let{future:j,static:_e}=reactExports.useContext(NavigationContext),{matches:et}=reactExports.useContext(RouteContext),{pathname:tt}=useLocation(),nt=useNavigate(),at=resolveTo(i,getResolveToMatches(et,j.v7_relativeSplatPath),tt,$==="path"),it=JSON.stringify(at);return reactExports.useEffect(()=>nt(JSON.parse(it),{replace:o,state:s,relative:$}),[nt,it,$,o,s]),null}function Outlet(a){return useOutlet(a.context)}function Router(a){let{basename:i="/",children:o=null,location:s,navigationType:$=Action.Pop,navigator:j,static:_e=!1,future:et}=a;useInRouterContext()&&invariant$1(!1);let tt=i.replace(/^\/*/,"/"),nt=reactExports.useMemo(()=>({basename:tt,navigator:j,static:_e,future:_extends$3({v7_relativeSplatPath:!1},et)}),[tt,et,j,_e]);typeof s=="string"&&(s=parsePath(s));let{pathname:at="/",search:it="",hash:st="",state:lt=null,key:ct="default"}=s,rt=reactExports.useMemo(()=>{let ut=stripBasename(at,tt);return ut==null?null:{location:{pathname:ut,search:it,hash:st,state:lt,key:ct},navigationType:$}},[tt,at,it,st,lt,ct,$]);return rt==null?null:reactExports.createElement(NavigationContext.Provider,{value:nt},reactExports.createElement(LocationContext.Provider,{children:o,value:rt}))}new Promise(()=>{});function mapRouteProperties(a){let i={hasErrorBoundary:a.ErrorBoundary!=null||a.errorElement!=null};return a.Component&&Object.assign(i,{element:reactExports.createElement(a.Component),Component:void 0}),a.HydrateFallback&&Object.assign(i,{hydrateFallbackElement:reactExports.createElement(a.HydrateFallback),HydrateFallback:void 0}),a.ErrorBoundary&&Object.assign(i,{errorElement:reactExports.createElement(a.ErrorBoundary),ErrorBoundary:void 0}),i}/**
 * React Router DOM v6.22.3
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function _extends$2(){return _extends$2=Object.assign?Object.assign.bind():function(a){for(var i=1;i<arguments.length;i++){var o=arguments[i];for(var s in o)Object.prototype.hasOwnProperty.call(o,s)&&(a[s]=o[s])}return a},_extends$2.apply(this,arguments)}function _objectWithoutPropertiesLoose$1(a,i){if(a==null)return{};var o={},s=Object.keys(a),$,j;for(j=0;j<s.length;j++)$=s[j],!(i.indexOf($)>=0)&&(o[$]=a[$]);return o}function isModifiedEvent(a){return!!(a.metaKey||a.altKey||a.ctrlKey||a.shiftKey)}function shouldProcessLinkClick(a,i){return a.button===0&&(!i||i==="_self")&&!isModifiedEvent(a)}const _excluded$1X=["onClick","relative","reloadDocument","replace","state","target","to","preventScrollReset","unstable_viewTransition"],REACT_ROUTER_VERSION="6";try{window.__reactRouterVersion=REACT_ROUTER_VERSION}catch{}function createBrowserRouter(a,i){return createRouter({basename:i==null?void 0:i.basename,future:_extends$2({},i==null?void 0:i.future,{v7_prependBasename:!0}),history:createBrowserHistory({window:i==null?void 0:i.window}),hydrationData:(i==null?void 0:i.hydrationData)||parseHydrationData(),routes:a,mapRouteProperties,window:i==null?void 0:i.window}).initialize()}function parseHydrationData(){var a;let i=(a=window)==null?void 0:a.__staticRouterHydrationData;return i&&i.errors&&(i=_extends$2({},i,{errors:deserializeErrors(i.errors)})),i}function deserializeErrors(a){if(!a)return null;let i=Object.entries(a),o={};for(let[s,$]of i)if($&&$.__type==="RouteErrorResponse")o[s]=new ErrorResponseImpl($.status,$.statusText,$.data,$.internal===!0);else if($&&$.__type==="Error"){if($.__subType){let j=window[$.__subType];if(typeof j=="function")try{let _e=new j($.message);_e.stack="",o[s]=_e}catch{}}if(o[s]==null){let j=new Error($.message);j.stack="",o[s]=j}}else o[s]=$;return o}const ViewTransitionContext=reactExports.createContext({isTransitioning:!1}),FetchersContext=reactExports.createContext(new Map),START_TRANSITION="startTransition",startTransitionImpl=React$2[START_TRANSITION],FLUSH_SYNC="flushSync",flushSyncImpl=ReactDOM$1[FLUSH_SYNC];function startTransitionSafe(a){startTransitionImpl?startTransitionImpl(a):a()}function flushSyncSafe(a){flushSyncImpl?flushSyncImpl(a):a()}class Deferred{constructor(){this.status="pending",this.promise=new Promise((i,o)=>{this.resolve=s=>{this.status==="pending"&&(this.status="resolved",i(s))},this.reject=s=>{this.status==="pending"&&(this.status="rejected",o(s))}})}}function RouterProvider(a){let{fallbackElement:i,router:o,future:s}=a,[$,j]=reactExports.useState(o.state),[_e,et]=reactExports.useState(),[tt,nt]=reactExports.useState({isTransitioning:!1}),[at,it]=reactExports.useState(),[st,lt]=reactExports.useState(),[ct,rt]=reactExports.useState(),ut=reactExports.useRef(new Map),{v7_startTransition:ot}=s||{},dt=reactExports.useCallback(yt=>{ot?startTransitionSafe(yt):yt()},[ot]),pt=reactExports.useCallback((yt,bt)=>{let{deletedFetchers:gt,unstable_flushSync:xt,unstable_viewTransitionOpts:vt}=bt;gt.forEach($t=>ut.current.delete($t)),yt.fetchers.forEach(($t,Tt)=>{$t.data!==void 0&&ut.current.set(Tt,$t.data)});let Lt=o.window==null||typeof o.window.document.startViewTransition!="function";if(!vt||Lt){xt?flushSyncSafe(()=>j(yt)):dt(()=>j(yt));return}if(xt){flushSyncSafe(()=>{st&&(at&&at.resolve(),st.skipTransition()),nt({isTransitioning:!0,flushSync:!0,currentLocation:vt.currentLocation,nextLocation:vt.nextLocation})});let $t=o.window.document.startViewTransition(()=>{flushSyncSafe(()=>j(yt))});$t.finished.finally(()=>{flushSyncSafe(()=>{it(void 0),lt(void 0),et(void 0),nt({isTransitioning:!1})})}),flushSyncSafe(()=>lt($t));return}st?(at&&at.resolve(),st.skipTransition(),rt({state:yt,currentLocation:vt.currentLocation,nextLocation:vt.nextLocation})):(et(yt),nt({isTransitioning:!0,flushSync:!1,currentLocation:vt.currentLocation,nextLocation:vt.nextLocation}))},[o.window,st,at,ut,dt]);reactExports.useLayoutEffect(()=>o.subscribe(pt),[o,pt]),reactExports.useEffect(()=>{tt.isTransitioning&&!tt.flushSync&&it(new Deferred)},[tt]),reactExports.useEffect(()=>{if(at&&_e&&o.window){let yt=_e,bt=at.promise,gt=o.window.document.startViewTransition(async()=>{dt(()=>j(yt)),await bt});gt.finished.finally(()=>{it(void 0),lt(void 0),et(void 0),nt({isTransitioning:!1})}),lt(gt)}},[dt,_e,at,o.window]),reactExports.useEffect(()=>{at&&_e&&$.location.key===_e.location.key&&at.resolve()},[at,st,$.location,_e]),reactExports.useEffect(()=>{!tt.isTransitioning&&ct&&(et(ct.state),nt({isTransitioning:!0,flushSync:!1,currentLocation:ct.currentLocation,nextLocation:ct.nextLocation}),rt(void 0))},[tt.isTransitioning,ct]),reactExports.useEffect(()=>{},[]);let mt=reactExports.useMemo(()=>({createHref:o.createHref,encodeLocation:o.encodeLocation,go:yt=>o.navigate(yt),push:(yt,bt,gt)=>o.navigate(yt,{state:bt,preventScrollReset:gt==null?void 0:gt.preventScrollReset}),replace:(yt,bt,gt)=>o.navigate(yt,{replace:!0,state:bt,preventScrollReset:gt==null?void 0:gt.preventScrollReset})}),[o]),ft=o.basename||"/",ht=reactExports.useMemo(()=>({router:o,navigator:mt,static:!1,basename:ft}),[o,mt,ft]);return reactExports.createElement(reactExports.Fragment,null,reactExports.createElement(DataRouterContext.Provider,{value:ht},reactExports.createElement(DataRouterStateContext.Provider,{value:$},reactExports.createElement(FetchersContext.Provider,{value:ut.current},reactExports.createElement(ViewTransitionContext.Provider,{value:tt},reactExports.createElement(Router,{basename:ft,location:$.location,navigationType:$.historyAction,navigator:mt,future:{v7_relativeSplatPath:o.future.v7_relativeSplatPath}},$.initialized||o.future.v7_partialHydration?reactExports.createElement(DataRoutes,{routes:o.routes,future:o.future,state:$}):i))))),null)}function DataRoutes(a){let{routes:i,future:o,state:s}=a;return useRoutesImpl(i,void 0,s,o)}const isBrowser$2=typeof window<"u"&&typeof window.document<"u"&&typeof window.document.createElement<"u",ABSOLUTE_URL_REGEX=/^(?:[a-z][a-z0-9+.-]*:|\/\/)/i,Link$2=reactExports.forwardRef(function(i,o){let{onClick:s,relative:$,reloadDocument:j,replace:_e,state:et,target:tt,to:nt,preventScrollReset:at,unstable_viewTransition:it}=i,st=_objectWithoutPropertiesLoose$1(i,_excluded$1X),{basename:lt}=reactExports.useContext(NavigationContext),ct,rt=!1;if(typeof nt=="string"&&ABSOLUTE_URL_REGEX.test(nt)&&(ct=nt,isBrowser$2))try{let pt=new URL(window.location.href),mt=nt.startsWith("//")?new URL(pt.protocol+nt):new URL(nt),ft=stripBasename(mt.pathname,lt);mt.origin===pt.origin&&ft!=null?nt=ft+mt.search+mt.hash:rt=!0}catch{}let ut=useHref(nt,{relative:$}),ot=useLinkClickHandler(nt,{replace:_e,state:et,target:tt,preventScrollReset:at,relative:$,unstable_viewTransition:it});function dt(pt){s&&s(pt),pt.defaultPrevented||ot(pt)}return reactExports.createElement("a",_extends$2({},st,{href:ct||ut,onClick:rt||j?s:dt,ref:o,target:tt}))});var DataRouterHook;(function(a){a.UseScrollRestoration="useScrollRestoration",a.UseSubmit="useSubmit",a.UseSubmitFetcher="useSubmitFetcher",a.UseFetcher="useFetcher",a.useViewTransitionState="useViewTransitionState"})(DataRouterHook||(DataRouterHook={}));var DataRouterStateHook;(function(a){a.UseFetcher="useFetcher",a.UseFetchers="useFetchers",a.UseScrollRestoration="useScrollRestoration"})(DataRouterStateHook||(DataRouterStateHook={}));function useLinkClickHandler(a,i){let{target:o,replace:s,state:$,preventScrollReset:j,relative:_e,unstable_viewTransition:et}=i===void 0?{}:i,tt=useNavigate(),nt=useLocation(),at=useResolvedPath(a,{relative:_e});return reactExports.useCallback(it=>{if(shouldProcessLinkClick(it,o)){it.preventDefault();let st=s!==void 0?s:createPath(nt)===createPath(at);tt(a,{replace:st,state:$,preventScrollReset:j,relative:_e,unstable_viewTransition:et})}},[nt,tt,at,s,$,o,a,j,_e,et])}function formatMuiErrorMessage$1(a){let i="https://mui.com/production-error/?code="+a;for(let o=1;o<arguments.length;o+=1)i+="&args[]="+encodeURIComponent(arguments[o]);return"Minified MUI error #"+a+"; visit "+i+" for the full message."}const formatMuiErrorMessage=Object.freeze(Object.defineProperty({__proto__:null,default:formatMuiErrorMessage$1},Symbol.toStringTag,{value:"Module"})),THEME_ID="$$material";function _extends(){return _extends=Object.assign?Object.assign.bind():function(a){for(var i=1;i<arguments.length;i++){var o=arguments[i];for(var s in o)Object.prototype.hasOwnProperty.call(o,s)&&(a[s]=o[s])}return a},_extends.apply(this,arguments)}const _extends$1=Object.freeze(Object.defineProperty({__proto__:null,default:_extends},Symbol.toStringTag,{value:"Module"}));function _objectWithoutPropertiesLoose(a,i){if(a==null)return{};var o={},s=Object.keys(a),$,j;for(j=0;j<s.length;j++)$=s[j],!(i.indexOf($)>=0)&&(o[$]=a[$]);return o}function memoize$1(a){var i=Object.create(null);return function(o){return i[o]===void 0&&(i[o]=a(o)),i[o]}}var reactPropsRegex=/^((children|dangerouslySetInnerHTML|key|ref|autoFocus|defaultValue|defaultChecked|innerHTML|suppressContentEditableWarning|suppressHydrationWarning|valueLink|abbr|accept|acceptCharset|accessKey|action|allow|allowUserMedia|allowPaymentRequest|allowFullScreen|allowTransparency|alt|async|autoComplete|autoPlay|capture|cellPadding|cellSpacing|challenge|charSet|checked|cite|classID|className|cols|colSpan|content|contentEditable|contextMenu|controls|controlsList|coords|crossOrigin|data|dateTime|decoding|default|defer|dir|disabled|disablePictureInPicture|disableRemotePlayback|download|draggable|encType|enterKeyHint|form|formAction|formEncType|formMethod|formNoValidate|formTarget|frameBorder|headers|height|hidden|high|href|hrefLang|htmlFor|httpEquiv|id|inputMode|integrity|is|keyParams|keyType|kind|label|lang|list|loading|loop|low|marginHeight|marginWidth|max|maxLength|media|mediaGroup|method|min|minLength|multiple|muted|name|nonce|noValidate|open|optimum|pattern|placeholder|playsInline|poster|preload|profile|radioGroup|readOnly|referrerPolicy|rel|required|reversed|role|rows|rowSpan|sandbox|scope|scoped|scrolling|seamless|selected|shape|size|sizes|slot|span|spellCheck|src|srcDoc|srcLang|srcSet|start|step|style|summary|tabIndex|target|title|translate|type|useMap|value|width|wmode|wrap|about|datatype|inlist|prefix|property|resource|typeof|vocab|autoCapitalize|autoCorrect|autoSave|color|incremental|fallback|inert|itemProp|itemScope|itemType|itemID|itemRef|on|option|results|security|unselectable|accentHeight|accumulate|additive|alignmentBaseline|allowReorder|alphabetic|amplitude|arabicForm|ascent|attributeName|attributeType|autoReverse|azimuth|baseFrequency|baselineShift|baseProfile|bbox|begin|bias|by|calcMode|capHeight|clip|clipPathUnits|clipPath|clipRule|colorInterpolation|colorInterpolationFilters|colorProfile|colorRendering|contentScriptType|contentStyleType|cursor|cx|cy|d|decelerate|descent|diffuseConstant|direction|display|divisor|dominantBaseline|dur|dx|dy|edgeMode|elevation|enableBackground|end|exponent|externalResourcesRequired|fill|fillOpacity|fillRule|filter|filterRes|filterUnits|floodColor|floodOpacity|focusable|fontFamily|fontSize|fontSizeAdjust|fontStretch|fontStyle|fontVariant|fontWeight|format|from|fr|fx|fy|g1|g2|glyphName|glyphOrientationHorizontal|glyphOrientationVertical|glyphRef|gradientTransform|gradientUnits|hanging|horizAdvX|horizOriginX|ideographic|imageRendering|in|in2|intercept|k|k1|k2|k3|k4|kernelMatrix|kernelUnitLength|kerning|keyPoints|keySplines|keyTimes|lengthAdjust|letterSpacing|lightingColor|limitingConeAngle|local|markerEnd|markerMid|markerStart|markerHeight|markerUnits|markerWidth|mask|maskContentUnits|maskUnits|mathematical|mode|numOctaves|offset|opacity|operator|order|orient|orientation|origin|overflow|overlinePosition|overlineThickness|panose1|paintOrder|pathLength|patternContentUnits|patternTransform|patternUnits|pointerEvents|points|pointsAtX|pointsAtY|pointsAtZ|preserveAlpha|preserveAspectRatio|primitiveUnits|r|radius|refX|refY|renderingIntent|repeatCount|repeatDur|requiredExtensions|requiredFeatures|restart|result|rotate|rx|ry|scale|seed|shapeRendering|slope|spacing|specularConstant|specularExponent|speed|spreadMethod|startOffset|stdDeviation|stemh|stemv|stitchTiles|stopColor|stopOpacity|strikethroughPosition|strikethroughThickness|string|stroke|strokeDasharray|strokeDashoffset|strokeLinecap|strokeLinejoin|strokeMiterlimit|strokeOpacity|strokeWidth|surfaceScale|systemLanguage|tableValues|targetX|targetY|textAnchor|textDecoration|textRendering|textLength|to|transform|u1|u2|underlinePosition|underlineThickness|unicode|unicodeBidi|unicodeRange|unitsPerEm|vAlphabetic|vHanging|vIdeographic|vMathematical|values|vectorEffect|version|vertAdvY|vertOriginX|vertOriginY|viewBox|viewTarget|visibility|widths|wordSpacing|writingMode|x|xHeight|x1|x2|xChannelSelector|xlinkActuate|xlinkArcrole|xlinkHref|xlinkRole|xlinkShow|xlinkTitle|xlinkType|xmlBase|xmlns|xmlnsXlink|xmlLang|xmlSpace|y|y1|y2|yChannelSelector|z|zoomAndPan|for|class|autofocus)|(([Dd][Aa][Tt][Aa]|[Aa][Rr][Ii][Aa]|x)-.*))$/,isPropValid=memoize$1(function(a){return reactPropsRegex.test(a)||a.charCodeAt(0)===111&&a.charCodeAt(1)===110&&a.charCodeAt(2)<91});function sheetForTag(a){if(a.sheet)return a.sheet;for(var i=0;i<document.styleSheets.length;i++)if(document.styleSheets[i].ownerNode===a)return document.styleSheets[i]}function createStyleElement(a){var i=document.createElement("style");return i.setAttribute("data-emotion",a.key),a.nonce!==void 0&&i.setAttribute("nonce",a.nonce),i.appendChild(document.createTextNode("")),i.setAttribute("data-s",""),i}var StyleSheet=function(){function a(o){var s=this;this._insertTag=function($){var j;s.tags.length===0?s.insertionPoint?j=s.insertionPoint.nextSibling:s.prepend?j=s.container.firstChild:j=s.before:j=s.tags[s.tags.length-1].nextSibling,s.container.insertBefore($,j),s.tags.push($)},this.isSpeedy=o.speedy===void 0?!0:o.speedy,this.tags=[],this.ctr=0,this.nonce=o.nonce,this.key=o.key,this.container=o.container,this.prepend=o.prepend,this.insertionPoint=o.insertionPoint,this.before=null}var i=a.prototype;return i.hydrate=function(s){s.forEach(this._insertTag)},i.insert=function(s){this.ctr%(this.isSpeedy?65e3:1)===0&&this._insertTag(createStyleElement(this));var $=this.tags[this.tags.length-1];if(this.isSpeedy){var j=sheetForTag($);try{j.insertRule(s,j.cssRules.length)}catch{}}else $.appendChild(document.createTextNode(s));this.ctr++},i.flush=function(){this.tags.forEach(function(s){return s.parentNode&&s.parentNode.removeChild(s)}),this.tags=[],this.ctr=0},a}(),MS="-ms-",MOZ="-moz-",WEBKIT="-webkit-",COMMENT="comm",RULESET="rule",DECLARATION="decl",IMPORT="@import",KEYFRAMES="@keyframes",LAYER="@layer",abs=Math.abs,from=String.fromCharCode,assign=Object.assign;function hash$2(a,i){return charat(a,0)^45?(((i<<2^charat(a,0))<<2^charat(a,1))<<2^charat(a,2))<<2^charat(a,3):0}function trim$1(a){return a.trim()}function match(a,i){return(a=i.exec(a))?a[0]:a}function replace(a,i,o){return a.replace(i,o)}function indexof(a,i){return a.indexOf(i)}function charat(a,i){return a.charCodeAt(i)|0}function substr(a,i,o){return a.slice(i,o)}function strlen(a){return a.length}function sizeof(a){return a.length}function append(a,i){return i.push(a),a}function combine(a,i){return a.map(i).join("")}var line=1,column=1,length=0,position=0,character=0,characters="";function node(a,i,o,s,$,j,_e){return{value:a,root:i,parent:o,type:s,props:$,children:j,line,column,length:_e,return:""}}function copy(a,i){return assign(node("",null,null,"",null,null,0),a,{length:-a.length},i)}function char(){return character}function prev(){return character=position>0?charat(characters,--position):0,column--,character===10&&(column=1,line--),character}function next(){return character=position<length?charat(characters,position++):0,column++,character===10&&(column=1,line++),character}function peek(){return charat(characters,position)}function caret(){return position}function slice$1(a,i){return substr(characters,a,i)}function token(a){switch(a){case 0:case 9:case 10:case 13:case 32:return 5;case 33:case 43:case 44:case 47:case 62:case 64:case 126:case 59:case 123:case 125:return 4;case 58:return 3;case 34:case 39:case 40:case 91:return 2;case 41:case 93:return 1}return 0}function alloc(a){return line=column=1,length=strlen(characters=a),position=0,[]}function dealloc(a){return characters="",a}function delimit(a){return trim$1(slice$1(position-1,delimiter(a===91?a+2:a===40?a+1:a)))}function whitespace(a){for(;(character=peek())&&character<33;)next();return token(a)>2||token(character)>3?"":" "}function escaping(a,i){for(;--i&&next()&&!(character<48||character>102||character>57&&character<65||character>70&&character<97););return slice$1(a,caret()+(i<6&&peek()==32&&next()==32))}function delimiter(a){for(;next();)switch(character){case a:return position;case 34:case 39:a!==34&&a!==39&&delimiter(character);break;case 40:a===41&&delimiter(a);break;case 92:next();break}return position}function commenter(a,i){for(;next()&&a+character!==57;)if(a+character===84&&peek()===47)break;return"/*"+slice$1(i,position-1)+"*"+from(a===47?a:next())}function identifier(a){for(;!token(peek());)next();return slice$1(a,position)}function compile(a){return dealloc(parse("",null,null,null,[""],a=alloc(a),0,[0],a))}function parse(a,i,o,s,$,j,_e,et,tt){for(var nt=0,at=0,it=_e,st=0,lt=0,ct=0,rt=1,ut=1,ot=1,dt=0,pt="",mt=$,ft=j,ht=s,yt=pt;ut;)switch(ct=dt,dt=next()){case 40:if(ct!=108&&charat(yt,it-1)==58){indexof(yt+=replace(delimit(dt),"&","&\f"),"&\f")!=-1&&(ot=-1);break}case 34:case 39:case 91:yt+=delimit(dt);break;case 9:case 10:case 13:case 32:yt+=whitespace(ct);break;case 92:yt+=escaping(caret()-1,7);continue;case 47:switch(peek()){case 42:case 47:append(comment(commenter(next(),caret()),i,o),tt);break;default:yt+="/"}break;case 123*rt:et[nt++]=strlen(yt)*ot;case 125*rt:case 59:case 0:switch(dt){case 0:case 125:ut=0;case 59+at:ot==-1&&(yt=replace(yt,/\f/g,"")),lt>0&&strlen(yt)-it&&append(lt>32?declaration(yt+";",s,o,it-1):declaration(replace(yt," ","")+";",s,o,it-2),tt);break;case 59:yt+=";";default:if(append(ht=ruleset(yt,i,o,nt,at,$,et,pt,mt=[],ft=[],it),j),dt===123)if(at===0)parse(yt,i,ht,ht,mt,j,it,et,ft);else switch(st===99&&charat(yt,3)===110?100:st){case 100:case 108:case 109:case 115:parse(a,ht,ht,s&&append(ruleset(a,ht,ht,0,0,$,et,pt,$,mt=[],it),ft),$,ft,it,et,s?mt:ft);break;default:parse(yt,ht,ht,ht,[""],ft,0,et,ft)}}nt=at=lt=0,rt=ot=1,pt=yt="",it=_e;break;case 58:it=1+strlen(yt),lt=ct;default:if(rt<1){if(dt==123)--rt;else if(dt==125&&rt++==0&&prev()==125)continue}switch(yt+=from(dt),dt*rt){case 38:ot=at>0?1:(yt+="\f",-1);break;case 44:et[nt++]=(strlen(yt)-1)*ot,ot=1;break;case 64:peek()===45&&(yt+=delimit(next())),st=peek(),at=it=strlen(pt=yt+=identifier(caret())),dt++;break;case 45:ct===45&&strlen(yt)==2&&(rt=0)}}return j}function ruleset(a,i,o,s,$,j,_e,et,tt,nt,at){for(var it=$-1,st=$===0?j:[""],lt=sizeof(st),ct=0,rt=0,ut=0;ct<s;++ct)for(var ot=0,dt=substr(a,it+1,it=abs(rt=_e[ct])),pt=a;ot<lt;++ot)(pt=trim$1(rt>0?st[ot]+" "+dt:replace(dt,/&\f/g,st[ot])))&&(tt[ut++]=pt);return node(a,i,o,$===0?RULESET:et,tt,nt,at)}function comment(a,i,o){return node(a,i,o,COMMENT,from(char()),substr(a,2,-2),0)}function declaration(a,i,o,s){return node(a,i,o,DECLARATION,substr(a,0,s),substr(a,s+1,-1),s)}function serialize(a,i){for(var o="",s=sizeof(a),$=0;$<s;$++)o+=i(a[$],$,a,i)||"";return o}function stringify(a,i,o,s){switch(a.type){case LAYER:if(a.children.length)break;case IMPORT:case DECLARATION:return a.return=a.return||a.value;case COMMENT:return"";case KEYFRAMES:return a.return=a.value+"{"+serialize(a.children,s)+"}";case RULESET:a.value=a.props.join(",")}return strlen(o=serialize(a.children,s))?a.return=a.value+"{"+o+"}":""}function middleware(a){var i=sizeof(a);return function(o,s,$,j){for(var _e="",et=0;et<i;et++)_e+=a[et](o,s,$,j)||"";return _e}}function rulesheet(a){return function(i){i.root||(i=i.return)&&a(i)}}var identifierWithPointTracking=function(i,o,s){for(var $=0,j=0;$=j,j=peek(),$===38&&j===12&&(o[s]=1),!token(j);)next();return slice$1(i,position)},toRules=function(i,o){var s=-1,$=44;do switch(token($)){case 0:$===38&&peek()===12&&(o[s]=1),i[s]+=identifierWithPointTracking(position-1,o,s);break;case 2:i[s]+=delimit($);break;case 4:if($===44){i[++s]=peek()===58?"&\f":"",o[s]=i[s].length;break}default:i[s]+=from($)}while($=next());return i},getRules=function(i,o){return dealloc(toRules(alloc(i),o))},fixedElements=new WeakMap,compat=function(i){if(!(i.type!=="rule"||!i.parent||i.length<1)){for(var o=i.value,s=i.parent,$=i.column===s.column&&i.line===s.line;s.type!=="rule";)if(s=s.parent,!s)return;if(!(i.props.length===1&&o.charCodeAt(0)!==58&&!fixedElements.get(s))&&!$){fixedElements.set(i,!0);for(var j=[],_e=getRules(o,j),et=s.props,tt=0,nt=0;tt<_e.length;tt++)for(var at=0;at<et.length;at++,nt++)i.props[nt]=j[tt]?_e[tt].replace(/&\f/g,et[at]):et[at]+" "+_e[tt]}}},removeLabel=function(i){if(i.type==="decl"){var o=i.value;o.charCodeAt(0)===108&&o.charCodeAt(2)===98&&(i.return="",i.value="")}};function prefix$1(a,i){switch(hash$2(a,i)){case 5103:return WEBKIT+"print-"+a+a;case 5737:case 4201:case 3177:case 3433:case 1641:case 4457:case 2921:case 5572:case 6356:case 5844:case 3191:case 6645:case 3005:case 6391:case 5879:case 5623:case 6135:case 4599:case 4855:case 4215:case 6389:case 5109:case 5365:case 5621:case 3829:return WEBKIT+a+a;case 5349:case 4246:case 4810:case 6968:case 2756:return WEBKIT+a+MOZ+a+MS+a+a;case 6828:case 4268:return WEBKIT+a+MS+a+a;case 6165:return WEBKIT+a+MS+"flex-"+a+a;case 5187:return WEBKIT+a+replace(a,/(\w+).+(:[^]+)/,WEBKIT+"box-$1$2"+MS+"flex-$1$2")+a;case 5443:return WEBKIT+a+MS+"flex-item-"+replace(a,/flex-|-self/,"")+a;case 4675:return WEBKIT+a+MS+"flex-line-pack"+replace(a,/align-content|flex-|-self/,"")+a;case 5548:return WEBKIT+a+MS+replace(a,"shrink","negative")+a;case 5292:return WEBKIT+a+MS+replace(a,"basis","preferred-size")+a;case 6060:return WEBKIT+"box-"+replace(a,"-grow","")+WEBKIT+a+MS+replace(a,"grow","positive")+a;case 4554:return WEBKIT+replace(a,/([^-])(transform)/g,"$1"+WEBKIT+"$2")+a;case 6187:return replace(replace(replace(a,/(zoom-|grab)/,WEBKIT+"$1"),/(image-set)/,WEBKIT+"$1"),a,"")+a;case 5495:case 3959:return replace(a,/(image-set\([^]*)/,WEBKIT+"$1$`$1");case 4968:return replace(replace(a,/(.+:)(flex-)?(.*)/,WEBKIT+"box-pack:$3"+MS+"flex-pack:$3"),/s.+-b[^;]+/,"justify")+WEBKIT+a+a;case 4095:case 3583:case 4068:case 2532:return replace(a,/(.+)-inline(.+)/,WEBKIT+"$1$2")+a;case 8116:case 7059:case 5753:case 5535:case 5445:case 5701:case 4933:case 4677:case 5533:case 5789:case 5021:case 4765:if(strlen(a)-1-i>6)switch(charat(a,i+1)){case 109:if(charat(a,i+4)!==45)break;case 102:return replace(a,/(.+:)(.+)-([^]+)/,"$1"+WEBKIT+"$2-$3$1"+MOZ+(charat(a,i+3)==108?"$3":"$2-$3"))+a;case 115:return~indexof(a,"stretch")?prefix$1(replace(a,"stretch","fill-available"),i)+a:a}break;case 4949:if(charat(a,i+1)!==115)break;case 6444:switch(charat(a,strlen(a)-3-(~indexof(a,"!important")&&10))){case 107:return replace(a,":",":"+WEBKIT)+a;case 101:return replace(a,/(.+:)([^;!]+)(;|!.+)?/,"$1"+WEBKIT+(charat(a,14)===45?"inline-":"")+"box$3$1"+WEBKIT+"$2$3$1"+MS+"$2box$3")+a}break;case 5936:switch(charat(a,i+11)){case 114:return WEBKIT+a+MS+replace(a,/[svh]\w+-[tblr]{2}/,"tb")+a;case 108:return WEBKIT+a+MS+replace(a,/[svh]\w+-[tblr]{2}/,"tb-rl")+a;case 45:return WEBKIT+a+MS+replace(a,/[svh]\w+-[tblr]{2}/,"lr")+a}return WEBKIT+a+MS+a+a}return a}var prefixer=function(i,o,s,$){if(i.length>-1&&!i.return)switch(i.type){case DECLARATION:i.return=prefix$1(i.value,i.length);break;case KEYFRAMES:return serialize([copy(i,{value:replace(i.value,"@","@"+WEBKIT)})],$);case RULESET:if(i.length)return combine(i.props,function(j){switch(match(j,/(::plac\w+|:read-\w+)/)){case":read-only":case":read-write":return serialize([copy(i,{props:[replace(j,/:(read-\w+)/,":"+MOZ+"$1")]})],$);case"::placeholder":return serialize([copy(i,{props:[replace(j,/:(plac\w+)/,":"+WEBKIT+"input-$1")]}),copy(i,{props:[replace(j,/:(plac\w+)/,":"+MOZ+"$1")]}),copy(i,{props:[replace(j,/:(plac\w+)/,MS+"input-$1")]})],$)}return""})}},defaultStylisPlugins=[prefixer],createCache=function(i){var o=i.key;if(o==="css"){var s=document.querySelectorAll("style[data-emotion]:not([data-s])");Array.prototype.forEach.call(s,function(rt){var ut=rt.getAttribute("data-emotion");ut.indexOf(" ")!==-1&&(document.head.appendChild(rt),rt.setAttribute("data-s",""))})}var $=i.stylisPlugins||defaultStylisPlugins,j={},_e,et=[];_e=i.container||document.head,Array.prototype.forEach.call(document.querySelectorAll('style[data-emotion^="'+o+' "]'),function(rt){for(var ut=rt.getAttribute("data-emotion").split(" "),ot=1;ot<ut.length;ot++)j[ut[ot]]=!0;et.push(rt)});var tt,nt=[compat,removeLabel];{var at,it=[stringify,rulesheet(function(rt){at.insert(rt)})],st=middleware(nt.concat($,it)),lt=function(ut){return serialize(compile(ut),st)};tt=function(ut,ot,dt,pt){at=dt,lt(ut?ut+"{"+ot.styles+"}":ot.styles),pt&&(ct.inserted[ot.name]=!0)}}var ct={key:o,sheet:new StyleSheet({key:o,container:_e,nonce:i.nonce,speedy:i.speedy,prepend:i.prepend,insertionPoint:i.insertionPoint}),nonce:i.nonce,inserted:j,registered:{},insert:tt};return ct.sheet.hydrate(et),ct},reactIs$2={exports:{}},reactIs_production_min$1={};/** @license React v16.13.1
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var b$1=typeof Symbol=="function"&&Symbol.for,c$1=b$1?Symbol.for("react.element"):60103,d$1=b$1?Symbol.for("react.portal"):60106,e$1=b$1?Symbol.for("react.fragment"):60107,f$1=b$1?Symbol.for("react.strict_mode"):60108,g$1=b$1?Symbol.for("react.profiler"):60114,h$1=b$1?Symbol.for("react.provider"):60109,k$1=b$1?Symbol.for("react.context"):60110,l$1=b$1?Symbol.for("react.async_mode"):60111,m$1=b$1?Symbol.for("react.concurrent_mode"):60111,n$1=b$1?Symbol.for("react.forward_ref"):60112,p$1=b$1?Symbol.for("react.suspense"):60113,q$1=b$1?Symbol.for("react.suspense_list"):60120,r$1=b$1?Symbol.for("react.memo"):60115,t$1=b$1?Symbol.for("react.lazy"):60116,v$1=b$1?Symbol.for("react.block"):60121,w=b$1?Symbol.for("react.fundamental"):60117,x=b$1?Symbol.for("react.responder"):60118,y=b$1?Symbol.for("react.scope"):60119;function z(a){if(typeof a=="object"&&a!==null){var i=a.$$typeof;switch(i){case c$1:switch(a=a.type,a){case l$1:case m$1:case e$1:case g$1:case f$1:case p$1:return a;default:switch(a=a&&a.$$typeof,a){case k$1:case n$1:case t$1:case r$1:case h$1:return a;default:return i}}case d$1:return i}}}function A(a){return z(a)===m$1}reactIs_production_min$1.AsyncMode=l$1;reactIs_production_min$1.ConcurrentMode=m$1;reactIs_production_min$1.ContextConsumer=k$1;reactIs_production_min$1.ContextProvider=h$1;reactIs_production_min$1.Element=c$1;reactIs_production_min$1.ForwardRef=n$1;reactIs_production_min$1.Fragment=e$1;reactIs_production_min$1.Lazy=t$1;reactIs_production_min$1.Memo=r$1;reactIs_production_min$1.Portal=d$1;reactIs_production_min$1.Profiler=g$1;reactIs_production_min$1.StrictMode=f$1;reactIs_production_min$1.Suspense=p$1;reactIs_production_min$1.isAsyncMode=function(a){return A(a)||z(a)===l$1};reactIs_production_min$1.isConcurrentMode=A;reactIs_production_min$1.isContextConsumer=function(a){return z(a)===k$1};reactIs_production_min$1.isContextProvider=function(a){return z(a)===h$1};reactIs_production_min$1.isElement=function(a){return typeof a=="object"&&a!==null&&a.$$typeof===c$1};reactIs_production_min$1.isForwardRef=function(a){return z(a)===n$1};reactIs_production_min$1.isFragment=function(a){return z(a)===e$1};reactIs_production_min$1.isLazy=function(a){return z(a)===t$1};reactIs_production_min$1.isMemo=function(a){return z(a)===r$1};reactIs_production_min$1.isPortal=function(a){return z(a)===d$1};reactIs_production_min$1.isProfiler=function(a){return z(a)===g$1};reactIs_production_min$1.isStrictMode=function(a){return z(a)===f$1};reactIs_production_min$1.isSuspense=function(a){return z(a)===p$1};reactIs_production_min$1.isValidElementType=function(a){return typeof a=="string"||typeof a=="function"||a===e$1||a===m$1||a===g$1||a===f$1||a===p$1||a===q$1||typeof a=="object"&&a!==null&&(a.$$typeof===t$1||a.$$typeof===r$1||a.$$typeof===h$1||a.$$typeof===k$1||a.$$typeof===n$1||a.$$typeof===w||a.$$typeof===x||a.$$typeof===y||a.$$typeof===v$1)};reactIs_production_min$1.typeOf=z;reactIs$2.exports=reactIs_production_min$1;var reactIsExports$1=reactIs$2.exports,reactIs$1=reactIsExports$1,FORWARD_REF_STATICS={$$typeof:!0,render:!0,defaultProps:!0,displayName:!0,propTypes:!0},MEMO_STATICS={$$typeof:!0,compare:!0,defaultProps:!0,displayName:!0,propTypes:!0,type:!0},TYPE_STATICS={};TYPE_STATICS[reactIs$1.ForwardRef]=FORWARD_REF_STATICS;TYPE_STATICS[reactIs$1.Memo]=MEMO_STATICS;var isBrowser$1=!0;function getRegisteredStyles(a,i,o){var s="";return o.split(" ").forEach(function($){a[$]!==void 0?i.push(a[$]+";"):s+=$+" "}),s}var registerStyles=function(i,o,s){var $=i.key+"-"+o.name;(s===!1||isBrowser$1===!1)&&i.registered[$]===void 0&&(i.registered[$]=o.styles)},insertStyles=function(i,o,s){registerStyles(i,o,s);var $=i.key+"-"+o.name;if(i.inserted[o.name]===void 0){var j=o;do i.insert(o===j?"."+$:"",j,i.sheet,!0),j=j.next;while(j!==void 0)}};function murmur2(a){for(var i=0,o,s=0,$=a.length;$>=4;++s,$-=4)o=a.charCodeAt(s)&255|(a.charCodeAt(++s)&255)<<8|(a.charCodeAt(++s)&255)<<16|(a.charCodeAt(++s)&255)<<24,o=(o&65535)*1540483477+((o>>>16)*59797<<16),o^=o>>>24,i=(o&65535)*1540483477+((o>>>16)*59797<<16)^(i&65535)*1540483477+((i>>>16)*59797<<16);switch($){case 3:i^=(a.charCodeAt(s+2)&255)<<16;case 2:i^=(a.charCodeAt(s+1)&255)<<8;case 1:i^=a.charCodeAt(s)&255,i=(i&65535)*1540483477+((i>>>16)*59797<<16)}return i^=i>>>13,i=(i&65535)*1540483477+((i>>>16)*59797<<16),((i^i>>>15)>>>0).toString(36)}var unitlessKeys={animationIterationCount:1,aspectRatio:1,borderImageOutset:1,borderImageSlice:1,borderImageWidth:1,boxFlex:1,boxFlexGroup:1,boxOrdinalGroup:1,columnCount:1,columns:1,flex:1,flexGrow:1,flexPositive:1,flexShrink:1,flexNegative:1,flexOrder:1,gridRow:1,gridRowEnd:1,gridRowSpan:1,gridRowStart:1,gridColumn:1,gridColumnEnd:1,gridColumnSpan:1,gridColumnStart:1,msGridRow:1,msGridRowSpan:1,msGridColumn:1,msGridColumnSpan:1,fontWeight:1,lineHeight:1,opacity:1,order:1,orphans:1,tabSize:1,widows:1,zIndex:1,zoom:1,WebkitLineClamp:1,fillOpacity:1,floodOpacity:1,stopOpacity:1,strokeDasharray:1,strokeDashoffset:1,strokeMiterlimit:1,strokeOpacity:1,strokeWidth:1},hyphenateRegex=/[A-Z]|^ms/g,animationRegex=/_EMO_([^_]+?)_([^]*?)_EMO_/g,isCustomProperty=function(i){return i.charCodeAt(1)===45},isProcessableValue=function(i){return i!=null&&typeof i!="boolean"},processStyleName=memoize$1(function(a){return isCustomProperty(a)?a:a.replace(hyphenateRegex,"-$&").toLowerCase()}),processStyleValue=function(i,o){switch(i){case"animation":case"animationName":if(typeof o=="string")return o.replace(animationRegex,function(s,$,j){return cursor={name:$,styles:j,next:cursor},$})}return unitlessKeys[i]!==1&&!isCustomProperty(i)&&typeof o=="number"&&o!==0?o+"px":o},noComponentSelectorMessage="Component selectors can only be used in conjunction with @emotion/babel-plugin, the swc Emotion plugin, or another Emotion-aware compiler transform.";function handleInterpolation(a,i,o){if(o==null)return"";if(o.__emotion_styles!==void 0)return o;switch(typeof o){case"boolean":return"";case"object":{if(o.anim===1)return cursor={name:o.name,styles:o.styles,next:cursor},o.name;if(o.styles!==void 0){var s=o.next;if(s!==void 0)for(;s!==void 0;)cursor={name:s.name,styles:s.styles,next:cursor},s=s.next;var $=o.styles+";";return $}return createStringFromObject(a,i,o)}case"function":{if(a!==void 0){var j=cursor,_e=o(a);return cursor=j,handleInterpolation(a,i,_e)}break}}if(i==null)return o;var et=i[o];return et!==void 0?et:o}function createStringFromObject(a,i,o){var s="";if(Array.isArray(o))for(var $=0;$<o.length;$++)s+=handleInterpolation(a,i,o[$])+";";else for(var j in o){var _e=o[j];if(typeof _e!="object")i!=null&&i[_e]!==void 0?s+=j+"{"+i[_e]+"}":isProcessableValue(_e)&&(s+=processStyleName(j)+":"+processStyleValue(j,_e)+";");else if(Array.isArray(_e)&&typeof _e[0]=="string"&&(i==null||i[_e[0]]===void 0))for(var et=0;et<_e.length;et++)isProcessableValue(_e[et])&&(s+=processStyleName(j)+":"+processStyleValue(j,_e[et])+";");else{var tt=handleInterpolation(a,i,_e);switch(j){case"animation":case"animationName":{s+=processStyleName(j)+":"+tt+";";break}default:s+=j+"{"+tt+"}"}}}return s}var labelPattern=/label:\s*([^\s;\n{]+)\s*(;|$)/g,cursor,serializeStyles=function(i,o,s){if(i.length===1&&typeof i[0]=="object"&&i[0]!==null&&i[0].styles!==void 0)return i[0];var $=!0,j="";cursor=void 0;var _e=i[0];_e==null||_e.raw===void 0?($=!1,j+=handleInterpolation(s,o,_e)):j+=_e[0];for(var et=1;et<i.length;et++)j+=handleInterpolation(s,o,i[et]),$&&(j+=_e[et]);labelPattern.lastIndex=0;for(var tt="",nt;(nt=labelPattern.exec(j))!==null;)tt+="-"+nt[1];var at=murmur2(j)+tt;return{name:at,styles:j,next:cursor}},syncFallback=function(i){return i()},useInsertionEffect=React$2.useInsertionEffect?React$2.useInsertionEffect:!1,useInsertionEffectAlwaysWithSyncFallback=useInsertionEffect||syncFallback,useInsertionEffectWithLayoutFallback=useInsertionEffect||reactExports.useLayoutEffect,EmotionCacheContext=reactExports.createContext(typeof HTMLElement<"u"?createCache({key:"css"}):null),CacheProvider=EmotionCacheContext.Provider,withEmotionCache=function(i){return reactExports.forwardRef(function(o,s){var $=reactExports.useContext(EmotionCacheContext);return i(o,$,s)})},ThemeContext$2=reactExports.createContext({}),Global=withEmotionCache(function(a,i){var o=a.styles,s=serializeStyles([o],void 0,reactExports.useContext(ThemeContext$2)),$=reactExports.useRef();return useInsertionEffectWithLayoutFallback(function(){var j=i.key+"-global",_e=new i.sheet.constructor({key:j,nonce:i.sheet.nonce,container:i.sheet.container,speedy:i.sheet.isSpeedy}),et=!1,tt=document.querySelector('style[data-emotion="'+j+" "+s.name+'"]');return i.sheet.tags.length&&(_e.before=i.sheet.tags[0]),tt!==null&&(et=!0,tt.setAttribute("data-emotion",j),_e.hydrate([tt])),$.current=[_e,et],function(){_e.flush()}},[i]),useInsertionEffectWithLayoutFallback(function(){var j=$.current,_e=j[0],et=j[1];if(et){j[1]=!1;return}if(s.next!==void 0&&insertStyles(i,s.next,!0),_e.tags.length){var tt=_e.tags[_e.tags.length-1].nextElementSibling;_e.before=tt,_e.flush()}i.insert("",s,_e,!1)},[i,s.name]),null});function css(){for(var a=arguments.length,i=new Array(a),o=0;o<a;o++)i[o]=arguments[o];return serializeStyles(i)}var keyframes=function(){var i=css.apply(void 0,arguments),o="animation-"+i.name;return{name:o,styles:"@keyframes "+o+"{"+i.styles+"}",anim:1,toString:function(){return"_EMO_"+this.name+"_"+this.styles+"_EMO_"}}},testOmitPropsOnStringTag=isPropValid,testOmitPropsOnComponent=function(i){return i!=="theme"},getDefaultShouldForwardProp=function(i){return typeof i=="string"&&i.charCodeAt(0)>96?testOmitPropsOnStringTag:testOmitPropsOnComponent},composeShouldForwardProps=function(i,o,s){var $;if(o){var j=o.shouldForwardProp;$=i.__emotion_forwardProp&&j?function(_e){return i.__emotion_forwardProp(_e)&&j(_e)}:j}return typeof $!="function"&&s&&($=i.__emotion_forwardProp),$},Insertion=function(i){var o=i.cache,s=i.serialized,$=i.isStringTag;return registerStyles(o,s,$),useInsertionEffectAlwaysWithSyncFallback(function(){return insertStyles(o,s,$)}),null},createStyled$3=function a(i,o){var s=i.__emotion_real===i,$=s&&i.__emotion_base||i,j,_e;o!==void 0&&(j=o.label,_e=o.target);var et=composeShouldForwardProps(i,o,s),tt=et||getDefaultShouldForwardProp($),nt=!tt("as");return function(){var at=arguments,it=s&&i.__emotion_styles!==void 0?i.__emotion_styles.slice(0):[];if(j!==void 0&&it.push("label:"+j+";"),at[0]==null||at[0].raw===void 0)it.push.apply(it,at);else{it.push(at[0][0]);for(var st=at.length,lt=1;lt<st;lt++)it.push(at[lt],at[0][lt])}var ct=withEmotionCache(function(rt,ut,ot){var dt=nt&&rt.as||$,pt="",mt=[],ft=rt;if(rt.theme==null){ft={};for(var ht in rt)ft[ht]=rt[ht];ft.theme=reactExports.useContext(ThemeContext$2)}typeof rt.className=="string"?pt=getRegisteredStyles(ut.registered,mt,rt.className):rt.className!=null&&(pt=rt.className+" ");var yt=serializeStyles(it.concat(mt),ut.registered,ft);pt+=ut.key+"-"+yt.name,_e!==void 0&&(pt+=" "+_e);var bt=nt&&et===void 0?getDefaultShouldForwardProp(dt):tt,gt={};for(var xt in rt)nt&&xt==="as"||bt(xt)&&(gt[xt]=rt[xt]);return gt.className=pt,gt.ref=ot,reactExports.createElement(reactExports.Fragment,null,reactExports.createElement(Insertion,{cache:ut,serialized:yt,isStringTag:typeof dt=="string"}),reactExports.createElement(dt,gt))});return ct.displayName=j!==void 0?j:"Styled("+(typeof $=="string"?$:$.displayName||$.name||"Component")+")",ct.defaultProps=i.defaultProps,ct.__emotion_real=ct,ct.__emotion_base=$,ct.__emotion_styles=it,ct.__emotion_forwardProp=et,Object.defineProperty(ct,"toString",{value:function(){return"."+_e}}),ct.withComponent=function(rt,ut){return a(rt,_extends({},o,ut,{shouldForwardProp:composeShouldForwardProps(ct,ut,!0)})).apply(void 0,it)},ct}},tags=["a","abbr","address","area","article","aside","audio","b","base","bdi","bdo","big","blockquote","body","br","button","canvas","caption","cite","code","col","colgroup","data","datalist","dd","del","details","dfn","dialog","div","dl","dt","em","embed","fieldset","figcaption","figure","footer","form","h1","h2","h3","h4","h5","h6","head","header","hgroup","hr","html","i","iframe","img","input","ins","kbd","keygen","label","legend","li","link","main","map","mark","marquee","menu","menuitem","meta","meter","nav","noscript","object","ol","optgroup","option","output","p","param","picture","pre","progress","q","rp","rt","ruby","s","samp","script","section","select","small","source","span","strong","style","sub","summary","sup","table","tbody","td","textarea","tfoot","th","thead","time","title","tr","track","u","ul","var","video","wbr","circle","clipPath","defs","ellipse","foreignObject","g","image","line","linearGradient","mask","path","pattern","polygon","polyline","radialGradient","rect","stop","svg","text","tspan"],newStyled=createStyled$3.bind();tags.forEach(function(a){newStyled[a]=newStyled(a)});var propTypes={exports:{}},ReactPropTypesSecret$1="SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED",ReactPropTypesSecret_1=ReactPropTypesSecret$1,ReactPropTypesSecret=ReactPropTypesSecret_1;function emptyFunction(){}function emptyFunctionWithReset(){}emptyFunctionWithReset.resetWarningCache=emptyFunction;var factoryWithThrowingShims=function(){function a(s,$,j,_e,et,tt){if(tt!==ReactPropTypesSecret){var nt=new Error("Calling PropTypes validators directly is not supported by the `prop-types` package. Use PropTypes.checkPropTypes() to call them. Read more at http://fb.me/use-check-prop-types");throw nt.name="Invariant Violation",nt}}a.isRequired=a;function i(){return a}var o={array:a,bigint:a,bool:a,func:a,number:a,object:a,string:a,symbol:a,any:a,arrayOf:i,element:a,elementType:a,instanceOf:i,node:a,objectOf:i,oneOf:i,oneOfType:i,shape:i,exact:i,checkPropTypes:emptyFunctionWithReset,resetWarningCache:emptyFunction};return o.PropTypes=o,o};propTypes.exports=factoryWithThrowingShims();var propTypesExports=propTypes.exports;const PropTypes=getDefaultExportFromCjs(propTypesExports);let cache;typeof document=="object"&&(cache=createCache({key:"css",prepend:!0}));function StyledEngineProvider(a){const{injectFirst:i,children:o}=a;return i&&cache?jsxRuntimeExports.jsx(CacheProvider,{value:cache,children:o}):o}function isEmpty$4(a){return a==null||Object.keys(a).length===0}function GlobalStyles$2(a){const{styles:i,defaultTheme:o={}}=a,s=typeof i=="function"?$=>i(isEmpty$4($)?o:$):i;return jsxRuntimeExports.jsx(Global,{styles:s})}function styled$2(a,i){return newStyled(a,i)}const internal_processStyles=(a,i)=>{Array.isArray(a.__emotion_styles)&&(a.__emotion_styles=i(a.__emotion_styles))},styledEngine=Object.freeze(Object.defineProperty({__proto__:null,GlobalStyles:GlobalStyles$2,StyledEngineProvider,ThemeContext:ThemeContext$2,css,default:styled$2,internal_processStyles,keyframes},Symbol.toStringTag,{value:"Module"}));function isPlainObject$1(a){if(typeof a!="object"||a===null)return!1;const i=Object.getPrototypeOf(a);return(i===null||i===Object.prototype||Object.getPrototypeOf(i)===null)&&!(Symbol.toStringTag in a)&&!(Symbol.iterator in a)}function deepClone(a){if(!isPlainObject$1(a))return a;const i={};return Object.keys(a).forEach(o=>{i[o]=deepClone(a[o])}),i}function deepmerge$1(a,i,o={clone:!0}){const s=o.clone?_extends({},a):a;return isPlainObject$1(a)&&isPlainObject$1(i)&&Object.keys(i).forEach($=>{$!=="__proto__"&&(isPlainObject$1(i[$])&&$ in a&&isPlainObject$1(a[$])?s[$]=deepmerge$1(a[$],i[$],o):o.clone?s[$]=isPlainObject$1(i[$])?deepClone(i[$]):i[$]:s[$]=i[$])}),s}const deepmerge=Object.freeze(Object.defineProperty({__proto__:null,default:deepmerge$1,isPlainObject:isPlainObject$1},Symbol.toStringTag,{value:"Module"})),_excluded$1W=["values","unit","step"],sortBreakpointsValues=a=>{const i=Object.keys(a).map(o=>({key:o,val:a[o]}))||[];return i.sort((o,s)=>o.val-s.val),i.reduce((o,s)=>_extends({},o,{[s.key]:s.val}),{})};function createBreakpoints(a){const{values:i={xs:0,sm:600,md:900,lg:1200,xl:1536},unit:o="px",step:s=5}=a,$=_objectWithoutPropertiesLoose(a,_excluded$1W),j=sortBreakpointsValues(i),_e=Object.keys(j);function et(st){return`@media (min-width:${typeof i[st]=="number"?i[st]:st}${o})`}function tt(st){return`@media (max-width:${(typeof i[st]=="number"?i[st]:st)-s/100}${o})`}function nt(st,lt){const ct=_e.indexOf(lt);return`@media (min-width:${typeof i[st]=="number"?i[st]:st}${o}) and (max-width:${(ct!==-1&&typeof i[_e[ct]]=="number"?i[_e[ct]]:lt)-s/100}${o})`}function at(st){return _e.indexOf(st)+1<_e.length?nt(st,_e[_e.indexOf(st)+1]):et(st)}function it(st){const lt=_e.indexOf(st);return lt===0?et(_e[1]):lt===_e.length-1?tt(_e[lt]):nt(st,_e[_e.indexOf(st)+1]).replace("@media","@media not all and")}return _extends({keys:_e,values:j,up:et,down:tt,between:nt,only:at,not:it,unit:o},$)}const shape={borderRadius:4},shape$1=shape;function merge$1(a,i){return i?deepmerge$1(a,i,{clone:!1}):a}const values$1={xs:0,sm:600,md:900,lg:1200,xl:1536},defaultBreakpoints={keys:["xs","sm","md","lg","xl"],up:a=>`@media (min-width:${values$1[a]}px)`};function handleBreakpoints(a,i,o){const s=a.theme||{};if(Array.isArray(i)){const j=s.breakpoints||defaultBreakpoints;return i.reduce((_e,et,tt)=>(_e[j.up(j.keys[tt])]=o(i[tt]),_e),{})}if(typeof i=="object"){const j=s.breakpoints||defaultBreakpoints;return Object.keys(i).reduce((_e,et)=>{if(Object.keys(j.values||values$1).indexOf(et)!==-1){const tt=j.up(et);_e[tt]=o(i[et],et)}else{const tt=et;_e[tt]=i[tt]}return _e},{})}return o(i)}function createEmptyBreakpointObject(a={}){var i;return((i=a.keys)==null?void 0:i.reduce((s,$)=>{const j=a.up($);return s[j]={},s},{}))||{}}function removeUnusedBreakpoints(a,i){return a.reduce((o,s)=>{const $=o[s];return(!$||Object.keys($).length===0)&&delete o[s],o},i)}function mergeBreakpointsInOrder(a,...i){const o=createEmptyBreakpointObject(a),s=[o,...i].reduce(($,j)=>deepmerge$1($,j),{});return removeUnusedBreakpoints(Object.keys(o),s)}function computeBreakpointsBase(a,i){if(typeof a!="object")return{};const o={},s=Object.keys(i);return Array.isArray(a)?s.forEach(($,j)=>{j<a.length&&(o[$]=!0)}):s.forEach($=>{a[$]!=null&&(o[$]=!0)}),o}function resolveBreakpointValues({values:a,breakpoints:i,base:o}){const s=o||computeBreakpointsBase(a,i),$=Object.keys(s);if($.length===0)return a;let j;return $.reduce((_e,et,tt)=>(Array.isArray(a)?(_e[et]=a[tt]!=null?a[tt]:a[j],j=tt):typeof a=="object"?(_e[et]=a[et]!=null?a[et]:a[j],j=et):_e[et]=a,_e),{})}function capitalize$2(a){if(typeof a!="string")throw new Error(formatMuiErrorMessage$1(7));return a.charAt(0).toUpperCase()+a.slice(1)}const capitalize$1=Object.freeze(Object.defineProperty({__proto__:null,default:capitalize$2},Symbol.toStringTag,{value:"Module"}));function getPath(a,i,o=!0){if(!i||typeof i!="string")return null;if(a&&a.vars&&o){const s=`vars.${i}`.split(".").reduce(($,j)=>$&&$[j]?$[j]:null,a);if(s!=null)return s}return i.split(".").reduce((s,$)=>s&&s[$]!=null?s[$]:null,a)}function getStyleValue$1(a,i,o,s=o){let $;return typeof a=="function"?$=a(o):Array.isArray(a)?$=a[o]||s:$=getPath(a,o)||s,i&&($=i($,s,a)),$}function style$3(a){const{prop:i,cssProperty:o=a.prop,themeKey:s,transform:$}=a,j=_e=>{if(_e[i]==null)return null;const et=_e[i],tt=_e.theme,nt=getPath(tt,s)||{};return handleBreakpoints(_e,et,it=>{let st=getStyleValue$1(nt,$,it);return it===st&&typeof it=="string"&&(st=getStyleValue$1(nt,$,`${i}${it==="default"?"":capitalize$2(it)}`,it)),o===!1?st:{[o]:st}})};return j.propTypes={},j.filterProps=[i],j}function memoize(a){const i={};return o=>(i[o]===void 0&&(i[o]=a(o)),i[o])}const properties={m:"margin",p:"padding"},directions={t:"Top",r:"Right",b:"Bottom",l:"Left",x:["Left","Right"],y:["Top","Bottom"]},aliases={marginX:"mx",marginY:"my",paddingX:"px",paddingY:"py"},getCssProperties=memoize(a=>{if(a.length>2)if(aliases[a])a=aliases[a];else return[a];const[i,o]=a.split(""),s=properties[i],$=directions[o]||"";return Array.isArray($)?$.map(j=>s+j):[s+$]}),marginKeys=["m","mt","mr","mb","ml","mx","my","margin","marginTop","marginRight","marginBottom","marginLeft","marginX","marginY","marginInline","marginInlineStart","marginInlineEnd","marginBlock","marginBlockStart","marginBlockEnd"],paddingKeys=["p","pt","pr","pb","pl","px","py","padding","paddingTop","paddingRight","paddingBottom","paddingLeft","paddingX","paddingY","paddingInline","paddingInlineStart","paddingInlineEnd","paddingBlock","paddingBlockStart","paddingBlockEnd"];[...marginKeys,...paddingKeys];function createUnaryUnit(a,i,o,s){var $;const j=($=getPath(a,i,!1))!=null?$:o;return typeof j=="number"?_e=>typeof _e=="string"?_e:j*_e:Array.isArray(j)?_e=>typeof _e=="string"?_e:j[_e]:typeof j=="function"?j:()=>{}}function createUnarySpacing(a){return createUnaryUnit(a,"spacing",8)}function getValue(a,i){if(typeof i=="string"||i==null)return i;const o=Math.abs(i),s=a(o);return i>=0?s:typeof s=="number"?-s:`-${s}`}function getStyleFromPropValue(a,i){return o=>a.reduce((s,$)=>(s[$]=getValue(i,o),s),{})}function resolveCssProperty(a,i,o,s){if(i.indexOf(o)===-1)return null;const $=getCssProperties(o),j=getStyleFromPropValue($,s),_e=a[o];return handleBreakpoints(a,_e,j)}function style$2(a,i){const o=createUnarySpacing(a.theme);return Object.keys(a).map(s=>resolveCssProperty(a,i,s,o)).reduce(merge$1,{})}function margin(a){return style$2(a,marginKeys)}margin.propTypes={};margin.filterProps=marginKeys;function padding$1(a){return style$2(a,paddingKeys)}padding$1.propTypes={};padding$1.filterProps=paddingKeys;function createSpacing(a=8){if(a.mui)return a;const i=createUnarySpacing({spacing:a}),o=(...s)=>(s.length===0?[1]:s).map(j=>{const _e=i(j);return typeof _e=="number"?`${_e}px`:_e}).join(" ");return o.mui=!0,o}function compose(...a){const i=a.reduce((s,$)=>($.filterProps.forEach(j=>{s[j]=$}),s),{}),o=s=>Object.keys(s).reduce(($,j)=>i[j]?merge$1($,i[j](s)):$,{});return o.propTypes={},o.filterProps=a.reduce((s,$)=>s.concat($.filterProps),[]),o}function borderTransform(a){return typeof a!="number"?a:`${a}px solid`}function createBorderStyle(a,i){return style$3({prop:a,themeKey:"borders",transform:i})}const border=createBorderStyle("border",borderTransform),borderTop=createBorderStyle("borderTop",borderTransform),borderRight=createBorderStyle("borderRight",borderTransform),borderBottom=createBorderStyle("borderBottom",borderTransform),borderLeft=createBorderStyle("borderLeft",borderTransform),borderColor$1=createBorderStyle("borderColor"),borderTopColor=createBorderStyle("borderTopColor"),borderRightColor=createBorderStyle("borderRightColor"),borderBottomColor=createBorderStyle("borderBottomColor"),borderLeftColor=createBorderStyle("borderLeftColor"),outline=createBorderStyle("outline",borderTransform),outlineColor=createBorderStyle("outlineColor"),borderRadius=a=>{if(a.borderRadius!==void 0&&a.borderRadius!==null){const i=createUnaryUnit(a.theme,"shape.borderRadius",4),o=s=>({borderRadius:getValue(i,s)});return handleBreakpoints(a,a.borderRadius,o)}return null};borderRadius.propTypes={};borderRadius.filterProps=["borderRadius"];compose(border,borderTop,borderRight,borderBottom,borderLeft,borderColor$1,borderTopColor,borderRightColor,borderBottomColor,borderLeftColor,borderRadius,outline,outlineColor);const gap=a=>{if(a.gap!==void 0&&a.gap!==null){const i=createUnaryUnit(a.theme,"spacing",8),o=s=>({gap:getValue(i,s)});return handleBreakpoints(a,a.gap,o)}return null};gap.propTypes={};gap.filterProps=["gap"];const columnGap=a=>{if(a.columnGap!==void 0&&a.columnGap!==null){const i=createUnaryUnit(a.theme,"spacing",8),o=s=>({columnGap:getValue(i,s)});return handleBreakpoints(a,a.columnGap,o)}return null};columnGap.propTypes={};columnGap.filterProps=["columnGap"];const rowGap=a=>{if(a.rowGap!==void 0&&a.rowGap!==null){const i=createUnaryUnit(a.theme,"spacing",8),o=s=>({rowGap:getValue(i,s)});return handleBreakpoints(a,a.rowGap,o)}return null};rowGap.propTypes={};rowGap.filterProps=["rowGap"];const gridColumn=style$3({prop:"gridColumn"}),gridRow=style$3({prop:"gridRow"}),gridAutoFlow=style$3({prop:"gridAutoFlow"}),gridAutoColumns=style$3({prop:"gridAutoColumns"}),gridAutoRows=style$3({prop:"gridAutoRows"}),gridTemplateColumns=style$3({prop:"gridTemplateColumns"}),gridTemplateRows=style$3({prop:"gridTemplateRows"}),gridTemplateAreas=style$3({prop:"gridTemplateAreas"}),gridArea=style$3({prop:"gridArea"});compose(gap,columnGap,rowGap,gridColumn,gridRow,gridAutoFlow,gridAutoColumns,gridAutoRows,gridTemplateColumns,gridTemplateRows,gridTemplateAreas,gridArea);function paletteTransform(a,i){return i==="grey"?i:a}const color=style$3({prop:"color",themeKey:"palette",transform:paletteTransform}),bgcolor=style$3({prop:"bgcolor",cssProperty:"backgroundColor",themeKey:"palette",transform:paletteTransform}),backgroundColor=style$3({prop:"backgroundColor",themeKey:"palette",transform:paletteTransform});compose(color,bgcolor,backgroundColor);function sizingTransform(a){return a<=1&&a!==0?`${a*100}%`:a}const width=style$3({prop:"width",transform:sizingTransform}),maxWidth=a=>{if(a.maxWidth!==void 0&&a.maxWidth!==null){const i=o=>{var s,$;const j=((s=a.theme)==null||(s=s.breakpoints)==null||(s=s.values)==null?void 0:s[o])||values$1[o];return j?(($=a.theme)==null||($=$.breakpoints)==null?void 0:$.unit)!=="px"?{maxWidth:`${j}${a.theme.breakpoints.unit}`}:{maxWidth:j}:{maxWidth:sizingTransform(o)}};return handleBreakpoints(a,a.maxWidth,i)}return null};maxWidth.filterProps=["maxWidth"];const minWidth=style$3({prop:"minWidth",transform:sizingTransform}),height$1=style$3({prop:"height",transform:sizingTransform}),maxHeight=style$3({prop:"maxHeight",transform:sizingTransform}),minHeight=style$3({prop:"minHeight",transform:sizingTransform});style$3({prop:"size",cssProperty:"width",transform:sizingTransform});style$3({prop:"size",cssProperty:"height",transform:sizingTransform});const boxSizing=style$3({prop:"boxSizing"});compose(width,maxWidth,minWidth,height$1,maxHeight,minHeight,boxSizing);const defaultSxConfig={border:{themeKey:"borders",transform:borderTransform},borderTop:{themeKey:"borders",transform:borderTransform},borderRight:{themeKey:"borders",transform:borderTransform},borderBottom:{themeKey:"borders",transform:borderTransform},borderLeft:{themeKey:"borders",transform:borderTransform},borderColor:{themeKey:"palette"},borderTopColor:{themeKey:"palette"},borderRightColor:{themeKey:"palette"},borderBottomColor:{themeKey:"palette"},borderLeftColor:{themeKey:"palette"},outline:{themeKey:"borders",transform:borderTransform},outlineColor:{themeKey:"palette"},borderRadius:{themeKey:"shape.borderRadius",style:borderRadius},color:{themeKey:"palette",transform:paletteTransform},bgcolor:{themeKey:"palette",cssProperty:"backgroundColor",transform:paletteTransform},backgroundColor:{themeKey:"palette",transform:paletteTransform},p:{style:padding$1},pt:{style:padding$1},pr:{style:padding$1},pb:{style:padding$1},pl:{style:padding$1},px:{style:padding$1},py:{style:padding$1},padding:{style:padding$1},paddingTop:{style:padding$1},paddingRight:{style:padding$1},paddingBottom:{style:padding$1},paddingLeft:{style:padding$1},paddingX:{style:padding$1},paddingY:{style:padding$1},paddingInline:{style:padding$1},paddingInlineStart:{style:padding$1},paddingInlineEnd:{style:padding$1},paddingBlock:{style:padding$1},paddingBlockStart:{style:padding$1},paddingBlockEnd:{style:padding$1},m:{style:margin},mt:{style:margin},mr:{style:margin},mb:{style:margin},ml:{style:margin},mx:{style:margin},my:{style:margin},margin:{style:margin},marginTop:{style:margin},marginRight:{style:margin},marginBottom:{style:margin},marginLeft:{style:margin},marginX:{style:margin},marginY:{style:margin},marginInline:{style:margin},marginInlineStart:{style:margin},marginInlineEnd:{style:margin},marginBlock:{style:margin},marginBlockStart:{style:margin},marginBlockEnd:{style:margin},displayPrint:{cssProperty:!1,transform:a=>({"@media print":{display:a}})},display:{},overflow:{},textOverflow:{},visibility:{},whiteSpace:{},flexBasis:{},flexDirection:{},flexWrap:{},justifyContent:{},alignItems:{},alignContent:{},order:{},flex:{},flexGrow:{},flexShrink:{},alignSelf:{},justifyItems:{},justifySelf:{},gap:{style:gap},rowGap:{style:rowGap},columnGap:{style:columnGap},gridColumn:{},gridRow:{},gridAutoFlow:{},gridAutoColumns:{},gridAutoRows:{},gridTemplateColumns:{},gridTemplateRows:{},gridTemplateAreas:{},gridArea:{},position:{},zIndex:{themeKey:"zIndex"},top:{},right:{},bottom:{},left:{},boxShadow:{themeKey:"shadows"},width:{transform:sizingTransform},maxWidth:{style:maxWidth},minWidth:{transform:sizingTransform},height:{transform:sizingTransform},maxHeight:{transform:sizingTransform},minHeight:{transform:sizingTransform},boxSizing:{},fontFamily:{themeKey:"typography"},fontSize:{themeKey:"typography"},fontStyle:{themeKey:"typography"},fontWeight:{themeKey:"typography"},letterSpacing:{},textTransform:{},lineHeight:{},textAlign:{},typography:{cssProperty:!1,themeKey:"typography"}},defaultSxConfig$1=defaultSxConfig;function objectsHaveSameKeys(...a){const i=a.reduce((s,$)=>s.concat(Object.keys($)),[]),o=new Set(i);return a.every(s=>o.size===Object.keys(s).length)}function callIfFn(a,i){return typeof a=="function"?a(i):a}function unstable_createStyleFunctionSx(){function a(o,s,$,j){const _e={[o]:s,theme:$},et=j[o];if(!et)return{[o]:s};const{cssProperty:tt=o,themeKey:nt,transform:at,style:it}=et;if(s==null)return null;if(nt==="typography"&&s==="inherit")return{[o]:s};const st=getPath($,nt)||{};return it?it(_e):handleBreakpoints(_e,s,ct=>{let rt=getStyleValue$1(st,at,ct);return ct===rt&&typeof ct=="string"&&(rt=getStyleValue$1(st,at,`${o}${ct==="default"?"":capitalize$2(ct)}`,ct)),tt===!1?rt:{[tt]:rt}})}function i(o){var s;const{sx:$,theme:j={}}=o||{};if(!$)return null;const _e=(s=j.unstable_sxConfig)!=null?s:defaultSxConfig$1;function et(tt){let nt=tt;if(typeof tt=="function")nt=tt(j);else if(typeof tt!="object")return tt;if(!nt)return null;const at=createEmptyBreakpointObject(j.breakpoints),it=Object.keys(at);let st=at;return Object.keys(nt).forEach(lt=>{const ct=callIfFn(nt[lt],j);if(ct!=null)if(typeof ct=="object")if(_e[lt])st=merge$1(st,a(lt,ct,j,_e));else{const rt=handleBreakpoints({theme:j},ct,ut=>({[lt]:ut}));objectsHaveSameKeys(rt,ct)?st[lt]=i({sx:ct,theme:j}):st=merge$1(st,rt)}else st=merge$1(st,a(lt,ct,j,_e))}),removeUnusedBreakpoints(it,st)}return Array.isArray($)?$.map(et):et($)}return i}const styleFunctionSx$1=unstable_createStyleFunctionSx();styleFunctionSx$1.filterProps=["sx"];function applyStyles$2(a,i){const o=this;return o.vars&&typeof o.getColorSchemeSelector=="function"?{[o.getColorSchemeSelector(a).replace(/(\[[^\]]+\])/,"*:where($1)")]:i}:o.palette.mode===a?i:{}}const _excluded$1V=["breakpoints","palette","spacing","shape"];function createTheme$2(a={},...i){const{breakpoints:o={},palette:s={},spacing:$,shape:j={}}=a,_e=_objectWithoutPropertiesLoose(a,_excluded$1V),et=createBreakpoints(o),tt=createSpacing($);let nt=deepmerge$1({breakpoints:et,direction:"ltr",components:{},palette:_extends({mode:"light"},s),spacing:tt,shape:_extends({},shape$1,j)},_e);return nt.applyStyles=applyStyles$2,nt=i.reduce((at,it)=>deepmerge$1(at,it),nt),nt.unstable_sxConfig=_extends({},defaultSxConfig$1,_e==null?void 0:_e.unstable_sxConfig),nt.unstable_sx=function(it){return styleFunctionSx$1({sx:it,theme:this})},nt}const createTheme$1=Object.freeze(Object.defineProperty({__proto__:null,default:createTheme$2,private_createBreakpoints:createBreakpoints,unstable_applyStyles:applyStyles$2},Symbol.toStringTag,{value:"Module"}));function isObjectEmpty$1(a){return Object.keys(a).length===0}function useTheme$4(a=null){const i=reactExports.useContext(ThemeContext$2);return!i||isObjectEmpty$1(i)?a:i}const systemDefaultTheme$2=createTheme$2();function useTheme$3(a=systemDefaultTheme$2){return useTheme$4(a)}function GlobalStyles$1({styles:a,themeId:i,defaultTheme:o={}}){const s=useTheme$3(o),$=typeof a=="function"?a(i&&s[i]||s):a;return jsxRuntimeExports.jsx(GlobalStyles$2,{styles:$})}const _excluded$1U=["sx"],splitProps=a=>{var i,o;const s={systemProps:{},otherProps:{}},$=(i=a==null||(o=a.theme)==null?void 0:o.unstable_sxConfig)!=null?i:defaultSxConfig$1;return Object.keys(a).forEach(j=>{$[j]?s.systemProps[j]=a[j]:s.otherProps[j]=a[j]}),s};function extendSxProp(a){const{sx:i}=a,o=_objectWithoutPropertiesLoose(a,_excluded$1U),{systemProps:s,otherProps:$}=splitProps(o);let j;return Array.isArray(i)?j=[s,...i]:typeof i=="function"?j=(..._e)=>{const et=i(..._e);return isPlainObject$1(et)?_extends({},s,et):s}:j=_extends({},s,i),_extends({},$,{sx:j})}const styleFunctionSx=Object.freeze(Object.defineProperty({__proto__:null,default:styleFunctionSx$1,extendSxProp,unstable_createStyleFunctionSx,unstable_defaultSxConfig:defaultSxConfig$1},Symbol.toStringTag,{value:"Module"})),defaultGenerator=a=>a,createClassNameGenerator=()=>{let a=defaultGenerator;return{configure(i){a=i},generate(i){return a(i)},reset(){a=defaultGenerator}}},ClassNameGenerator=createClassNameGenerator(),ClassNameGenerator$1=ClassNameGenerator;function r(a){var i,o,s="";if(typeof a=="string"||typeof a=="number")s+=a;else if(typeof a=="object")if(Array.isArray(a)){var $=a.length;for(i=0;i<$;i++)a[i]&&(o=r(a[i]))&&(s&&(s+=" "),s+=o)}else for(o in a)a[o]&&(s&&(s+=" "),s+=o);return s}function clsx(){for(var a,i,o=0,s="",$=arguments.length;o<$;o++)(a=arguments[o])&&(i=r(a))&&(s&&(s+=" "),s+=i);return s}const _excluded$1T=["className","component"];function createBox(a={}){const{themeId:i,defaultTheme:o,defaultClassName:s="MuiBox-root",generateClassName:$}=a,j=styled$2("div",{shouldForwardProp:et=>et!=="theme"&&et!=="sx"&&et!=="as"})(styleFunctionSx$1);return reactExports.forwardRef(function(tt,nt){const at=useTheme$3(o),it=extendSxProp(tt),{className:st,component:lt="div"}=it,ct=_objectWithoutPropertiesLoose(it,_excluded$1T);return jsxRuntimeExports.jsx(j,_extends({as:lt,ref:nt,className:clsx(st,$?$(s):s),theme:i&&at[i]||at},ct))})}const globalStateClasses={active:"active",checked:"checked",completed:"completed",disabled:"disabled",error:"error",expanded:"expanded",focused:"focused",focusVisible:"focusVisible",open:"open",readOnly:"readOnly",required:"required",selected:"selected"};function generateUtilityClass$1(a,i,o="Mui"){const s=globalStateClasses[i];return s?`${o}-${s}`:`${ClassNameGenerator$1.generate(a)}-${i}`}function generateUtilityClasses$1(a,i,o="Mui"){const s={};return i.forEach($=>{s[$]=generateUtilityClass$1(a,$,o)}),s}var reactIs={exports:{}},reactIs_production_min={};/**
 * @license React
 * react-is.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var b=Symbol.for("react.element"),c=Symbol.for("react.portal"),d=Symbol.for("react.fragment"),e=Symbol.for("react.strict_mode"),f=Symbol.for("react.profiler"),g=Symbol.for("react.provider"),h=Symbol.for("react.context"),k=Symbol.for("react.server_context"),l=Symbol.for("react.forward_ref"),m=Symbol.for("react.suspense"),n=Symbol.for("react.suspense_list"),p=Symbol.for("react.memo"),q=Symbol.for("react.lazy"),t=Symbol.for("react.offscreen"),u;u=Symbol.for("react.module.reference");function v(a){if(typeof a=="object"&&a!==null){var i=a.$$typeof;switch(i){case b:switch(a=a.type,a){case d:case f:case e:case m:case n:return a;default:switch(a=a&&a.$$typeof,a){case k:case h:case l:case q:case p:case g:return a;default:return i}}case c:return i}}}reactIs_production_min.ContextConsumer=h;reactIs_production_min.ContextProvider=g;reactIs_production_min.Element=b;reactIs_production_min.ForwardRef=l;reactIs_production_min.Fragment=d;reactIs_production_min.Lazy=q;reactIs_production_min.Memo=p;reactIs_production_min.Portal=c;reactIs_production_min.Profiler=f;reactIs_production_min.StrictMode=e;reactIs_production_min.Suspense=m;reactIs_production_min.SuspenseList=n;reactIs_production_min.isAsyncMode=function(){return!1};reactIs_production_min.isConcurrentMode=function(){return!1};reactIs_production_min.isContextConsumer=function(a){return v(a)===h};reactIs_production_min.isContextProvider=function(a){return v(a)===g};reactIs_production_min.isElement=function(a){return typeof a=="object"&&a!==null&&a.$$typeof===b};reactIs_production_min.isForwardRef=function(a){return v(a)===l};reactIs_production_min.isFragment=function(a){return v(a)===d};reactIs_production_min.isLazy=function(a){return v(a)===q};reactIs_production_min.isMemo=function(a){return v(a)===p};reactIs_production_min.isPortal=function(a){return v(a)===c};reactIs_production_min.isProfiler=function(a){return v(a)===f};reactIs_production_min.isStrictMode=function(a){return v(a)===e};reactIs_production_min.isSuspense=function(a){return v(a)===m};reactIs_production_min.isSuspenseList=function(a){return v(a)===n};reactIs_production_min.isValidElementType=function(a){return typeof a=="string"||typeof a=="function"||a===d||a===f||a===e||a===m||a===n||a===t||typeof a=="object"&&a!==null&&(a.$$typeof===q||a.$$typeof===p||a.$$typeof===g||a.$$typeof===h||a.$$typeof===l||a.$$typeof===u||a.getModuleId!==void 0)};reactIs_production_min.typeOf=v;reactIs.exports=reactIs_production_min;var reactIsExports=reactIs.exports;const fnNameMatchRegex=/^\s*function(?:\s|\s*\/\*.*\*\/\s*)+([^(\s/]*)\s*/;function getFunctionName(a){const i=`${a}`.match(fnNameMatchRegex);return i&&i[1]||""}function getFunctionComponentName(a,i=""){return a.displayName||a.name||getFunctionName(a)||i}function getWrappedName(a,i,o){const s=getFunctionComponentName(i);return a.displayName||(s!==""?`${o}(${s})`:o)}function getDisplayName$1(a){if(a!=null){if(typeof a=="string")return a;if(typeof a=="function")return getFunctionComponentName(a,"Component");if(typeof a=="object")switch(a.$$typeof){case reactIsExports.ForwardRef:return getWrappedName(a,a.render,"ForwardRef");case reactIsExports.Memo:return getWrappedName(a,a.type,"memo");default:return}}}const getDisplayName=Object.freeze(Object.defineProperty({__proto__:null,default:getDisplayName$1,getFunctionName},Symbol.toStringTag,{value:"Module"})),_excluded$1S=["ownerState"],_excluded2$g=["variants"],_excluded3$6=["name","slot","skipVariantsResolver","skipSx","overridesResolver"];function isEmpty$3(a){return Object.keys(a).length===0}function isStringTag$1(a){return typeof a=="string"&&a.charCodeAt(0)>96}function shouldForwardProp$1(a){return a!=="ownerState"&&a!=="theme"&&a!=="sx"&&a!=="as"}const systemDefaultTheme$1=createTheme$2(),lowercaseFirstLetter$1=a=>a&&a.charAt(0).toLowerCase()+a.slice(1);function resolveTheme$1({defaultTheme:a,theme:i,themeId:o}){return isEmpty$3(i)?a:i[o]||i}function defaultOverridesResolver$1(a){return a?(i,o)=>o[a]:null}function processStyleArg$1(a,i){let{ownerState:o}=i,s=_objectWithoutPropertiesLoose(i,_excluded$1S);const $=typeof a=="function"?a(_extends({ownerState:o},s)):a;if(Array.isArray($))return $.flatMap(j=>processStyleArg$1(j,_extends({ownerState:o},s)));if($&&typeof $=="object"&&Array.isArray($.variants)){const{variants:j=[]}=$;let et=_objectWithoutPropertiesLoose($,_excluded2$g);return j.forEach(tt=>{let nt=!0;typeof tt.props=="function"?nt=tt.props(_extends({ownerState:o},s,o)):Object.keys(tt.props).forEach(at=>{(o==null?void 0:o[at])!==tt.props[at]&&s[at]!==tt.props[at]&&(nt=!1)}),nt&&(Array.isArray(et)||(et=[et]),et.push(typeof tt.style=="function"?tt.style(_extends({ownerState:o},s,o)):tt.style))}),et}return $}function createStyled$2(a={}){const{themeId:i,defaultTheme:o=systemDefaultTheme$1,rootShouldForwardProp:s=shouldForwardProp$1,slotShouldForwardProp:$=shouldForwardProp$1}=a,j=_e=>styleFunctionSx$1(_extends({},_e,{theme:resolveTheme$1(_extends({},_e,{defaultTheme:o,themeId:i}))}));return j.__mui_systemSx=!0,(_e,et={})=>{internal_processStyles(_e,ft=>ft.filter(ht=>!(ht!=null&&ht.__mui_systemSx)));const{name:tt,slot:nt,skipVariantsResolver:at,skipSx:it,overridesResolver:st=defaultOverridesResolver$1(lowercaseFirstLetter$1(nt))}=et,lt=_objectWithoutPropertiesLoose(et,_excluded3$6),ct=at!==void 0?at:nt&&nt!=="Root"&&nt!=="root"||!1,rt=it||!1;let ut,ot=shouldForwardProp$1;nt==="Root"||nt==="root"?ot=s:nt?ot=$:isStringTag$1(_e)&&(ot=void 0);const dt=styled$2(_e,_extends({shouldForwardProp:ot,label:ut},lt)),pt=ft=>typeof ft=="function"&&ft.__emotion_real!==ft||isPlainObject$1(ft)?ht=>processStyleArg$1(ft,_extends({},ht,{theme:resolveTheme$1({theme:ht.theme,defaultTheme:o,themeId:i})})):ft,mt=(ft,...ht)=>{let yt=pt(ft);const bt=ht?ht.map(pt):[];tt&&st&&bt.push(vt=>{const Lt=resolveTheme$1(_extends({},vt,{defaultTheme:o,themeId:i}));if(!Lt.components||!Lt.components[tt]||!Lt.components[tt].styleOverrides)return null;const $t=Lt.components[tt].styleOverrides,Tt={};return Object.entries($t).forEach(([Et,Dt])=>{Tt[Et]=processStyleArg$1(Dt,_extends({},vt,{theme:Lt}))}),st(vt,Tt)}),tt&&!ct&&bt.push(vt=>{var Lt;const $t=resolveTheme$1(_extends({},vt,{defaultTheme:o,themeId:i})),Tt=$t==null||(Lt=$t.components)==null||(Lt=Lt[tt])==null?void 0:Lt.variants;return processStyleArg$1({variants:Tt},_extends({},vt,{theme:$t}))}),rt||bt.push(j);const gt=bt.length-ht.length;if(Array.isArray(ft)&&gt>0){const vt=new Array(gt).fill("");yt=[...ft,...vt],yt.raw=[...ft.raw,...vt]}const xt=dt(yt,...bt);return _e.muiName&&(xt.muiName=_e.muiName),xt};return dt.withConfig&&(mt.withConfig=dt.withConfig),mt}}const styled$1=createStyled$2();function resolveProps(a,i){const o=_extends({},i);return Object.keys(a).forEach(s=>{if(s.toString().match(/^(components|slots)$/))o[s]=_extends({},a[s],o[s]);else if(s.toString().match(/^(componentsProps|slotProps)$/)){const $=a[s]||{},j=i[s];o[s]={},!j||!Object.keys(j)?o[s]=$:!$||!Object.keys($)?o[s]=j:(o[s]=_extends({},j),Object.keys($).forEach(_e=>{o[s][_e]=resolveProps($[_e],j[_e])}))}else o[s]===void 0&&(o[s]=a[s])}),o}function getThemeProps(a){const{theme:i,name:o,props:s}=a;return!i||!i.components||!i.components[o]||!i.components[o].defaultProps?s:resolveProps(i.components[o].defaultProps,s)}function useThemeProps$7({props:a,name:i,defaultTheme:o,themeId:s}){let $=useTheme$3(o);return s&&($=$[s]||$),getThemeProps({theme:$,name:i,props:a})}const useEnhancedEffect=typeof window<"u"?reactExports.useLayoutEffect:reactExports.useEffect;function useMediaQueryOld(a,i,o,s,$){const[j,_e]=reactExports.useState(()=>$&&o?o(a).matches:s?s(a).matches:i);return useEnhancedEffect(()=>{let et=!0;if(!o)return;const tt=o(a),nt=()=>{et&&_e(tt.matches)};return nt(),tt.addListener(nt),()=>{et=!1,tt.removeListener(nt)}},[a,o]),j}const maybeReactUseSyncExternalStore=reactExports.useSyncExternalStore;function useMediaQueryNew(a,i,o,s,$){const j=reactExports.useCallback(()=>i,[i]),_e=reactExports.useMemo(()=>{if($&&o)return()=>o(a).matches;if(s!==null){const{matches:at}=s(a);return()=>at}return j},[j,a,s,$,o]),[et,tt]=reactExports.useMemo(()=>{if(o===null)return[j,()=>()=>{}];const at=o(a);return[()=>at.matches,it=>(at.addListener(it),()=>{at.removeListener(it)})]},[j,o,a]);return maybeReactUseSyncExternalStore(tt,et,_e)}function useMediaQuery(a,i={}){const o=useTheme$4(),s=typeof window<"u"&&typeof window.matchMedia<"u",{defaultMatches:$=!1,matchMedia:j=s?window.matchMedia:null,ssrMatchMedia:_e=null,noSsr:et=!1}=getThemeProps({name:"MuiUseMediaQuery",props:i,theme:o});let tt=typeof a=="function"?a(o):a;return tt=tt.replace(/^@media( ?)/m,""),(maybeReactUseSyncExternalStore!==void 0?useMediaQueryNew:useMediaQueryOld)(tt,$,j,_e,et)}function clamp$1(a,i=Number.MIN_SAFE_INTEGER,o=Number.MAX_SAFE_INTEGER){return Math.max(i,Math.min(a,o))}const clamp=Object.freeze(Object.defineProperty({__proto__:null,default:clamp$1},Symbol.toStringTag,{value:"Module"}));function clampWrapper$1(a,i=0,o=1){return clamp$1(a,i,o)}function hexToRgb$1(a){a=a.slice(1);const i=new RegExp(`.{1,${a.length>=6?2:1}}`,"g");let o=a.match(i);return o&&o[0].length===1&&(o=o.map(s=>s+s)),o?`rgb${o.length===4?"a":""}(${o.map((s,$)=>$<3?parseInt(s,16):Math.round(parseInt(s,16)/255*1e3)/1e3).join(", ")})`:""}function decomposeColor$1(a){if(a.type)return a;if(a.charAt(0)==="#")return decomposeColor$1(hexToRgb$1(a));const i=a.indexOf("("),o=a.substring(0,i);if(["rgb","rgba","hsl","hsla","color"].indexOf(o)===-1)throw new Error(formatMuiErrorMessage$1(9,a));let s=a.substring(i+1,a.length-1),$;if(o==="color"){if(s=s.split(" "),$=s.shift(),s.length===4&&s[3].charAt(0)==="/"&&(s[3]=s[3].slice(1)),["srgb","display-p3","a98-rgb","prophoto-rgb","rec-2020"].indexOf($)===-1)throw new Error(formatMuiErrorMessage$1(10,$))}else s=s.split(",");return s=s.map(j=>parseFloat(j)),{type:o,values:s,colorSpace:$}}function recomposeColor$1(a){const{type:i,colorSpace:o}=a;let{values:s}=a;return i.indexOf("rgb")!==-1?s=s.map(($,j)=>j<3?parseInt($,10):$):i.indexOf("hsl")!==-1&&(s[1]=`${s[1]}%`,s[2]=`${s[2]}%`),i.indexOf("color")!==-1?s=`${o} ${s.join(" ")}`:s=`${s.join(", ")}`,`${i}(${s})`}function alpha$1(a,i){return a=decomposeColor$1(a),i=clampWrapper$1(i),(a.type==="rgb"||a.type==="hsl")&&(a.type+="a"),a.type==="color"?a.values[3]=`/${i}`:a.values[3]=i,recomposeColor$1(a)}const refType=PropTypes.oneOfType([PropTypes.func,PropTypes.object]),refType$1=refType;function createChainedFunction(...a){return a.reduce((i,o)=>o==null?i:function(...$){i.apply(this,$),o.apply(this,$)},()=>{})}function debounce$1(a,i=166){let o;function s(...$){const j=()=>{a.apply(this,$)};clearTimeout(o),o=setTimeout(j,i)}return s.clear=()=>{clearTimeout(o)},s}function deprecatedPropType(a,i){return()=>null}function isMuiElement(a,i){var o,s;return reactExports.isValidElement(a)&&i.indexOf((o=a.type.muiName)!=null?o:(s=a.type)==null||(s=s._payload)==null||(s=s.value)==null?void 0:s.muiName)!==-1}function ownerDocument(a){return a&&a.ownerDocument||document}function ownerWindow(a){return ownerDocument(a).defaultView||window}function requirePropFactory(a,i){return()=>null}function setRef(a,i){typeof a=="function"?a(i):a&&(a.current=i)}let globalId=0;function useGlobalId(a){const[i,o]=reactExports.useState(a),s=a||i;return reactExports.useEffect(()=>{i==null&&(globalId+=1,o(`mui-${globalId}`))},[i]),s}const maybeReactUseId=React$2.useId;function useId(a){if(maybeReactUseId!==void 0){const i=maybeReactUseId();return a??i}return useGlobalId(a)}function unsupportedProp(a,i,o,s,$){return null}function useControlled({controlled:a,default:i,name:o,state:s="value"}){const{current:$}=reactExports.useRef(a!==void 0),[j,_e]=reactExports.useState(i),et=$?a:j,tt=reactExports.useCallback(nt=>{$||_e(nt)},[]);return[et,tt]}function useEventCallback(a){const i=reactExports.useRef(a);return useEnhancedEffect(()=>{i.current=a}),reactExports.useRef((...o)=>(0,i.current)(...o)).current}function useForkRef(...a){return reactExports.useMemo(()=>a.every(i=>i==null)?null:i=>{a.forEach(o=>{setRef(o,i)})},a)}const UNINITIALIZED={};function useLazyRef(a,i){const o=reactExports.useRef(UNINITIALIZED);return o.current===UNINITIALIZED&&(o.current=a(i)),o}const EMPTY=[];function useOnMount(a){reactExports.useEffect(a,EMPTY)}class Timeout{constructor(){this.currentId=null,this.clear=()=>{this.currentId!==null&&(clearTimeout(this.currentId),this.currentId=null)},this.disposeEffect=()=>this.clear}static create(){return new Timeout}start(i,o){this.clear(),this.currentId=setTimeout(()=>{this.currentId=null,o()},i)}}function useTimeout(){const a=useLazyRef(Timeout.create).current;return useOnMount(a.disposeEffect),a}let hadKeyboardEvent=!0,hadFocusVisibleRecently=!1;const hadFocusVisibleRecentlyTimeout=new Timeout,inputTypesWhitelist={text:!0,search:!0,url:!0,tel:!0,email:!0,password:!0,number:!0,date:!0,month:!0,week:!0,time:!0,datetime:!0,"datetime-local":!0};function focusTriggersKeyboardModality(a){const{type:i,tagName:o}=a;return!!(o==="INPUT"&&inputTypesWhitelist[i]&&!a.readOnly||o==="TEXTAREA"&&!a.readOnly||a.isContentEditable)}function handleKeyDown(a){a.metaKey||a.altKey||a.ctrlKey||(hadKeyboardEvent=!0)}function handlePointerDown(){hadKeyboardEvent=!1}function handleVisibilityChange(){this.visibilityState==="hidden"&&hadFocusVisibleRecently&&(hadKeyboardEvent=!0)}function prepare(a){a.addEventListener("keydown",handleKeyDown,!0),a.addEventListener("mousedown",handlePointerDown,!0),a.addEventListener("pointerdown",handlePointerDown,!0),a.addEventListener("touchstart",handlePointerDown,!0),a.addEventListener("visibilitychange",handleVisibilityChange,!0)}function isFocusVisible(a){const{target:i}=a;try{return i.matches(":focus-visible")}catch{}return hadKeyboardEvent||focusTriggersKeyboardModality(i)}function useIsFocusVisible(){const a=reactExports.useCallback($=>{$!=null&&prepare($.ownerDocument)},[]),i=reactExports.useRef(!1);function o(){return i.current?(hadFocusVisibleRecently=!0,hadFocusVisibleRecentlyTimeout.start(100,()=>{hadFocusVisibleRecently=!1}),i.current=!1,!0):!1}function s($){return isFocusVisible($)?(i.current=!0,!0):!1}return{isFocusVisibleRef:i,onFocus:s,onBlur:o,ref:a}}function getScrollbarSize(a){const i=a.documentElement.clientWidth;return Math.abs(window.innerWidth-i)}const usePreviousProps=a=>{const i=reactExports.useRef({});return reactExports.useEffect(()=>{i.current=a}),i.current};function getValidReactChildren(a){return reactExports.Children.toArray(a).filter(i=>reactExports.isValidElement(i))}function composeClasses(a,i,o=void 0){const s={};return Object.keys(a).forEach($=>{s[$]=a[$].reduce((j,_e)=>{if(_e){const et=i(_e);et!==""&&j.push(et),o&&o[_e]&&j.push(o[_e])}return j},[]).join(" ")}),s}const ThemeContext=reactExports.createContext(null),ThemeContext$1=ThemeContext;function useTheme$2(){return reactExports.useContext(ThemeContext$1)}const hasSymbol=typeof Symbol=="function"&&Symbol.for,nested=hasSymbol?Symbol.for("mui.nested"):"__THEME_NESTED__";function mergeOuterLocalTheme(a,i){return typeof i=="function"?i(a):_extends({},a,i)}function ThemeProvider$2(a){const{children:i,theme:o}=a,s=useTheme$2(),$=reactExports.useMemo(()=>{const j=s===null?o:mergeOuterLocalTheme(s,o);return j!=null&&(j[nested]=s!==null),j},[o,s]);return jsxRuntimeExports.jsx(ThemeContext$1.Provider,{value:$,children:i})}const _excluded$1R=["value"],RtlContext=reactExports.createContext();function RtlProvider(a){let{value:i}=a,o=_objectWithoutPropertiesLoose(a,_excluded$1R);return jsxRuntimeExports.jsx(RtlContext.Provider,_extends({value:i??!0},o))}const useRtl=()=>{const a=reactExports.useContext(RtlContext);return a??!1},EMPTY_THEME={};function useThemeScoping(a,i,o,s=!1){return reactExports.useMemo(()=>{const $=a&&i[a]||i;if(typeof o=="function"){const j=o($),_e=a?_extends({},i,{[a]:j}):j;return s?()=>_e:_e}return a?_extends({},i,{[a]:o}):_extends({},i,o)},[a,i,o,s])}function ThemeProvider$1(a){const{children:i,theme:o,themeId:s}=a,$=useTheme$4(EMPTY_THEME),j=useTheme$2()||EMPTY_THEME,_e=useThemeScoping(s,$,o),et=useThemeScoping(s,j,o,!0),tt=_e.direction==="rtl";return jsxRuntimeExports.jsx(ThemeProvider$2,{theme:et,children:jsxRuntimeExports.jsx(ThemeContext$2.Provider,{value:_e,children:jsxRuntimeExports.jsx(RtlProvider,{value:tt,children:i})})})}const _excluded$1Q=["component","direction","spacing","divider","children","className","useFlexGap"],defaultTheme$3=createTheme$2(),defaultCreateStyledComponent=styled$1("div",{name:"MuiStack",slot:"Root",overridesResolver:(a,i)=>i.root});function useThemePropsDefault(a){return useThemeProps$7({props:a,name:"MuiStack",defaultTheme:defaultTheme$3})}function joinChildren(a,i){const o=reactExports.Children.toArray(a).filter(Boolean);return o.reduce((s,$,j)=>(s.push($),j<o.length-1&&s.push(reactExports.cloneElement(i,{key:`separator-${j}`})),s),[])}const getSideFromDirection=a=>({row:"Left","row-reverse":"Right",column:"Top","column-reverse":"Bottom"})[a],style$1=({ownerState:a,theme:i})=>{let o=_extends({display:"flex",flexDirection:"column"},handleBreakpoints({theme:i},resolveBreakpointValues({values:a.direction,breakpoints:i.breakpoints.values}),s=>({flexDirection:s})));if(a.spacing){const s=createUnarySpacing(i),$=Object.keys(i.breakpoints.values).reduce((tt,nt)=>((typeof a.spacing=="object"&&a.spacing[nt]!=null||typeof a.direction=="object"&&a.direction[nt]!=null)&&(tt[nt]=!0),tt),{}),j=resolveBreakpointValues({values:a.direction,base:$}),_e=resolveBreakpointValues({values:a.spacing,base:$});typeof j=="object"&&Object.keys(j).forEach((tt,nt,at)=>{if(!j[tt]){const st=nt>0?j[at[nt-1]]:"column";j[tt]=st}}),o=deepmerge$1(o,handleBreakpoints({theme:i},_e,(tt,nt)=>a.useFlexGap?{gap:getValue(s,tt)}:{"& > :not(style):not(style)":{margin:0},"& > :not(style) ~ :not(style)":{[`margin${getSideFromDirection(nt?j[nt]:a.direction)}`]:getValue(s,tt)}}))}return o=mergeBreakpointsInOrder(i.breakpoints,o),o};function createStack(a={}){const{createStyledComponent:i=defaultCreateStyledComponent,useThemeProps:o=useThemePropsDefault,componentName:s="MuiStack"}=a,$=()=>composeClasses({root:["root"]},tt=>generateUtilityClass$1(s,tt),{}),j=i(style$1);return reactExports.forwardRef(function(tt,nt){const at=o(tt),it=extendSxProp(at),{component:st="div",direction:lt="column",spacing:ct=0,divider:rt,children:ut,className:ot,useFlexGap:dt=!1}=it,pt=_objectWithoutPropertiesLoose(it,_excluded$1Q),mt={direction:lt,spacing:ct,useFlexGap:dt},ft=$();return jsxRuntimeExports.jsx(j,_extends({as:st,ownerState:mt,ref:nt,className:clsx(ft.root,ot)},pt,{children:rt?joinChildren(ut,rt):ut}))})}function createMixins(a,i){return _extends({toolbar:{minHeight:56,[a.up("xs")]:{"@media (orientation: landscape)":{minHeight:48}},[a.up("sm")]:{minHeight:64}}},i)}var colorManipulator={},interopRequireDefault={exports:{}};(function(a){function i(o){return o&&o.__esModule?o:{default:o}}a.exports=i,a.exports.__esModule=!0,a.exports.default=a.exports})(interopRequireDefault);var interopRequireDefaultExports=interopRequireDefault.exports;const require$$1$2=getAugmentedNamespace(formatMuiErrorMessage),require$$2=getAugmentedNamespace(clamp);var _interopRequireDefault$v=interopRequireDefaultExports;Object.defineProperty(colorManipulator,"__esModule",{value:!0});var alpha_1=colorManipulator.alpha=alpha;colorManipulator.blend=blend;colorManipulator.colorChannel=void 0;var darken_1=colorManipulator.darken=darken;colorManipulator.decomposeColor=decomposeColor;var emphasize_1=colorManipulator.emphasize=emphasize,getContrastRatio_1=colorManipulator.getContrastRatio=getContrastRatio;colorManipulator.getLuminance=getLuminance;colorManipulator.hexToRgb=hexToRgb;colorManipulator.hslToRgb=hslToRgb;var lighten_1=colorManipulator.lighten=lighten;colorManipulator.private_safeAlpha=private_safeAlpha;colorManipulator.private_safeColorChannel=void 0;colorManipulator.private_safeDarken=private_safeDarken;colorManipulator.private_safeEmphasize=private_safeEmphasize;colorManipulator.private_safeLighten=private_safeLighten;colorManipulator.recomposeColor=recomposeColor;colorManipulator.rgbToHex=rgbToHex;var _formatMuiErrorMessage2=_interopRequireDefault$v(require$$1$2),_clamp=_interopRequireDefault$v(require$$2);function clampWrapper(a,i=0,o=1){return(0,_clamp.default)(a,i,o)}function hexToRgb(a){a=a.slice(1);const i=new RegExp(`.{1,${a.length>=6?2:1}}`,"g");let o=a.match(i);return o&&o[0].length===1&&(o=o.map(s=>s+s)),o?`rgb${o.length===4?"a":""}(${o.map((s,$)=>$<3?parseInt(s,16):Math.round(parseInt(s,16)/255*1e3)/1e3).join(", ")})`:""}function intToHex(a){const i=a.toString(16);return i.length===1?`0${i}`:i}function decomposeColor(a){if(a.type)return a;if(a.charAt(0)==="#")return decomposeColor(hexToRgb(a));const i=a.indexOf("("),o=a.substring(0,i);if(["rgb","rgba","hsl","hsla","color"].indexOf(o)===-1)throw new Error((0,_formatMuiErrorMessage2.default)(9,a));let s=a.substring(i+1,a.length-1),$;if(o==="color"){if(s=s.split(" "),$=s.shift(),s.length===4&&s[3].charAt(0)==="/"&&(s[3]=s[3].slice(1)),["srgb","display-p3","a98-rgb","prophoto-rgb","rec-2020"].indexOf($)===-1)throw new Error((0,_formatMuiErrorMessage2.default)(10,$))}else s=s.split(",");return s=s.map(j=>parseFloat(j)),{type:o,values:s,colorSpace:$}}const colorChannel=a=>{const i=decomposeColor(a);return i.values.slice(0,3).map((o,s)=>i.type.indexOf("hsl")!==-1&&s!==0?`${o}%`:o).join(" ")};colorManipulator.colorChannel=colorChannel;const private_safeColorChannel=(a,i)=>{try{return colorChannel(a)}catch{return a}};colorManipulator.private_safeColorChannel=private_safeColorChannel;function recomposeColor(a){const{type:i,colorSpace:o}=a;let{values:s}=a;return i.indexOf("rgb")!==-1?s=s.map(($,j)=>j<3?parseInt($,10):$):i.indexOf("hsl")!==-1&&(s[1]=`${s[1]}%`,s[2]=`${s[2]}%`),i.indexOf("color")!==-1?s=`${o} ${s.join(" ")}`:s=`${s.join(", ")}`,`${i}(${s})`}function rgbToHex(a){if(a.indexOf("#")===0)return a;const{values:i}=decomposeColor(a);return`#${i.map((o,s)=>intToHex(s===3?Math.round(255*o):o)).join("")}`}function hslToRgb(a){a=decomposeColor(a);const{values:i}=a,o=i[0],s=i[1]/100,$=i[2]/100,j=s*Math.min($,1-$),_e=(nt,at=(nt+o/30)%12)=>$-j*Math.max(Math.min(at-3,9-at,1),-1);let et="rgb";const tt=[Math.round(_e(0)*255),Math.round(_e(8)*255),Math.round(_e(4)*255)];return a.type==="hsla"&&(et+="a",tt.push(i[3])),recomposeColor({type:et,values:tt})}function getLuminance(a){a=decomposeColor(a);let i=a.type==="hsl"||a.type==="hsla"?decomposeColor(hslToRgb(a)).values:a.values;return i=i.map(o=>(a.type!=="color"&&(o/=255),o<=.03928?o/12.92:((o+.055)/1.055)**2.4)),Number((.2126*i[0]+.7152*i[1]+.0722*i[2]).toFixed(3))}function getContrastRatio(a,i){const o=getLuminance(a),s=getLuminance(i);return(Math.max(o,s)+.05)/(Math.min(o,s)+.05)}function alpha(a,i){return a=decomposeColor(a),i=clampWrapper(i),(a.type==="rgb"||a.type==="hsl")&&(a.type+="a"),a.type==="color"?a.values[3]=`/${i}`:a.values[3]=i,recomposeColor(a)}function private_safeAlpha(a,i,o){try{return alpha(a,i)}catch{return a}}function darken(a,i){if(a=decomposeColor(a),i=clampWrapper(i),a.type.indexOf("hsl")!==-1)a.values[2]*=1-i;else if(a.type.indexOf("rgb")!==-1||a.type.indexOf("color")!==-1)for(let o=0;o<3;o+=1)a.values[o]*=1-i;return recomposeColor(a)}function private_safeDarken(a,i,o){try{return darken(a,i)}catch{return a}}function lighten(a,i){if(a=decomposeColor(a),i=clampWrapper(i),a.type.indexOf("hsl")!==-1)a.values[2]+=(100-a.values[2])*i;else if(a.type.indexOf("rgb")!==-1)for(let o=0;o<3;o+=1)a.values[o]+=(255-a.values[o])*i;else if(a.type.indexOf("color")!==-1)for(let o=0;o<3;o+=1)a.values[o]+=(1-a.values[o])*i;return recomposeColor(a)}function private_safeLighten(a,i,o){try{return lighten(a,i)}catch{return a}}function emphasize(a,i=.15){return getLuminance(a)>.5?darken(a,i):lighten(a,i)}function private_safeEmphasize(a,i,o){try{return emphasize(a,i)}catch{return a}}function blend(a,i,o,s=1){const $=(tt,nt)=>Math.round((tt**(1/s)*(1-o)+nt**(1/s)*o)**s),j=decomposeColor(a),_e=decomposeColor(i),et=[$(j.values[0],_e.values[0]),$(j.values[1],_e.values[1]),$(j.values[2],_e.values[2])];return recomposeColor({type:"rgb",values:et})}const common={black:"#000",white:"#fff"},common$1=common,grey={50:"#fafafa",100:"#f5f5f5",200:"#eeeeee",300:"#e0e0e0",400:"#bdbdbd",500:"#9e9e9e",600:"#757575",700:"#616161",800:"#424242",900:"#212121",A100:"#f5f5f5",A200:"#eeeeee",A400:"#bdbdbd",A700:"#616161"},purple={50:"#f3e5f5",100:"#e1bee7",200:"#ce93d8",300:"#ba68c8",400:"#ab47bc",500:"#9c27b0",600:"#8e24aa",700:"#7b1fa2",800:"#6a1b9a",900:"#4a148c",A100:"#ea80fc",A200:"#e040fb",A400:"#d500f9",A700:"#aa00ff"},purple$1=purple,red={50:"#ffebee",100:"#ffcdd2",200:"#ef9a9a",300:"#e57373",400:"#ef5350",500:"#f44336",600:"#e53935",700:"#d32f2f",800:"#c62828",900:"#b71c1c",A100:"#ff8a80",A200:"#ff5252",A400:"#ff1744",A700:"#d50000"},orange={50:"#fff3e0",100:"#ffe0b2",200:"#ffcc80",300:"#ffb74d",400:"#ffa726",500:"#ff9800",600:"#fb8c00",700:"#f57c00",800:"#ef6c00",900:"#e65100",A100:"#ffd180",A200:"#ffab40",A400:"#ff9100",A700:"#ff6d00"},orange$1=orange,blue={50:"#e3f2fd",100:"#bbdefb",200:"#90caf9",300:"#64b5f6",400:"#42a5f5",500:"#2196f3",600:"#1e88e5",700:"#1976d2",800:"#1565c0",900:"#0d47a1",A100:"#82b1ff",A200:"#448aff",A400:"#2979ff",A700:"#2962ff"},blue$1=blue,lightBlue={50:"#e1f5fe",100:"#b3e5fc",200:"#81d4fa",300:"#4fc3f7",400:"#29b6f6",500:"#03a9f4",600:"#039be5",700:"#0288d1",800:"#0277bd",900:"#01579b",A100:"#80d8ff",A200:"#40c4ff",A400:"#00b0ff",A700:"#0091ea"},lightBlue$1=lightBlue,green={50:"#e8f5e9",100:"#c8e6c9",200:"#a5d6a7",300:"#81c784",400:"#66bb6a",500:"#4caf50",600:"#43a047",700:"#388e3c",800:"#2e7d32",900:"#1b5e20",A100:"#b9f6ca",A200:"#69f0ae",A400:"#00e676",A700:"#00c853"},_excluded$1P=["mode","contrastThreshold","tonalOffset"],light={text:{primary:"rgba(0, 0, 0, 0.87)",secondary:"rgba(0, 0, 0, 0.6)",disabled:"rgba(0, 0, 0, 0.38)"},divider:"rgba(0, 0, 0, 0.12)",background:{paper:common$1.white,default:common$1.white},action:{active:"rgba(0, 0, 0, 0.54)",hover:"rgba(0, 0, 0, 0.04)",hoverOpacity:.04,selected:"rgba(0, 0, 0, 0.08)",selectedOpacity:.08,disabled:"rgba(0, 0, 0, 0.26)",disabledBackground:"rgba(0, 0, 0, 0.12)",disabledOpacity:.38,focus:"rgba(0, 0, 0, 0.12)",focusOpacity:.12,activatedOpacity:.12}},dark={text:{primary:common$1.white,secondary:"rgba(255, 255, 255, 0.7)",disabled:"rgba(255, 255, 255, 0.5)",icon:"rgba(255, 255, 255, 0.5)"},divider:"rgba(255, 255, 255, 0.12)",background:{paper:"#121212",default:"#121212"},action:{active:common$1.white,hover:"rgba(255, 255, 255, 0.08)",hoverOpacity:.08,selected:"rgba(255, 255, 255, 0.16)",selectedOpacity:.16,disabled:"rgba(255, 255, 255, 0.3)",disabledBackground:"rgba(255, 255, 255, 0.12)",disabledOpacity:.38,focus:"rgba(255, 255, 255, 0.12)",focusOpacity:.12,activatedOpacity:.24}};function addLightOrDark(a,i,o,s){const $=s.light||s,j=s.dark||s*1.5;a[i]||(a.hasOwnProperty(o)?a[i]=a[o]:i==="light"?a.light=lighten_1(a.main,$):i==="dark"&&(a.dark=darken_1(a.main,j)))}function getDefaultPrimary(a="light"){return a==="dark"?{main:blue$1[200],light:blue$1[50],dark:blue$1[400]}:{main:blue$1[700],light:blue$1[400],dark:blue$1[800]}}function getDefaultSecondary(a="light"){return a==="dark"?{main:purple$1[200],light:purple$1[50],dark:purple$1[400]}:{main:purple$1[500],light:purple$1[300],dark:purple$1[700]}}function getDefaultError(a="light"){return a==="dark"?{main:red[500],light:red[300],dark:red[700]}:{main:red[700],light:red[400],dark:red[800]}}function getDefaultInfo(a="light"){return a==="dark"?{main:lightBlue$1[400],light:lightBlue$1[300],dark:lightBlue$1[700]}:{main:lightBlue$1[700],light:lightBlue$1[500],dark:lightBlue$1[900]}}function getDefaultSuccess(a="light"){return a==="dark"?{main:green[400],light:green[300],dark:green[700]}:{main:green[800],light:green[500],dark:green[900]}}function getDefaultWarning(a="light"){return a==="dark"?{main:orange$1[400],light:orange$1[300],dark:orange$1[700]}:{main:"#ed6c02",light:orange$1[500],dark:orange$1[900]}}function createPalette(a){const{mode:i="light",contrastThreshold:o=3,tonalOffset:s=.2}=a,$=_objectWithoutPropertiesLoose(a,_excluded$1P),j=a.primary||getDefaultPrimary(i),_e=a.secondary||getDefaultSecondary(i),et=a.error||getDefaultError(i),tt=a.info||getDefaultInfo(i),nt=a.success||getDefaultSuccess(i),at=a.warning||getDefaultWarning(i);function it(rt){return getContrastRatio_1(rt,dark.text.primary)>=o?dark.text.primary:light.text.primary}const st=({color:rt,name:ut,mainShade:ot=500,lightShade:dt=300,darkShade:pt=700})=>{if(rt=_extends({},rt),!rt.main&&rt[ot]&&(rt.main=rt[ot]),!rt.hasOwnProperty("main"))throw new Error(formatMuiErrorMessage$1(11,ut?` (${ut})`:"",ot));if(typeof rt.main!="string")throw new Error(formatMuiErrorMessage$1(12,ut?` (${ut})`:"",JSON.stringify(rt.main)));return addLightOrDark(rt,"light",dt,s),addLightOrDark(rt,"dark",pt,s),rt.contrastText||(rt.contrastText=it(rt.main)),rt},lt={dark,light};return deepmerge$1(_extends({common:_extends({},common$1),mode:i,primary:st({color:j,name:"primary"}),secondary:st({color:_e,name:"secondary",mainShade:"A400",lightShade:"A200",darkShade:"A700"}),error:st({color:et,name:"error"}),warning:st({color:at,name:"warning"}),info:st({color:tt,name:"info"}),success:st({color:nt,name:"success"}),grey,contrastThreshold:o,getContrastText:it,augmentColor:st,tonalOffset:s},lt[i]),$)}const _excluded$1O=["fontFamily","fontSize","fontWeightLight","fontWeightRegular","fontWeightMedium","fontWeightBold","htmlFontSize","allVariants","pxToRem"];function round$2(a){return Math.round(a*1e5)/1e5}const caseAllCaps={textTransform:"uppercase"},defaultFontFamily='"Roboto", "Helvetica", "Arial", sans-serif';function createTypography(a,i){const o=typeof i=="function"?i(a):i,{fontFamily:s=defaultFontFamily,fontSize:$=14,fontWeightLight:j=300,fontWeightRegular:_e=400,fontWeightMedium:et=500,fontWeightBold:tt=700,htmlFontSize:nt=16,allVariants:at,pxToRem:it}=o,st=_objectWithoutPropertiesLoose(o,_excluded$1O),lt=$/14,ct=it||(ot=>`${ot/nt*lt}rem`),rt=(ot,dt,pt,mt,ft)=>_extends({fontFamily:s,fontWeight:ot,fontSize:ct(dt),lineHeight:pt},s===defaultFontFamily?{letterSpacing:`${round$2(mt/dt)}em`}:{},ft,at),ut={h1:rt(j,96,1.167,-1.5),h2:rt(j,60,1.2,-.5),h3:rt(_e,48,1.167,0),h4:rt(_e,34,1.235,.25),h5:rt(_e,24,1.334,0),h6:rt(et,20,1.6,.15),subtitle1:rt(_e,16,1.75,.15),subtitle2:rt(et,14,1.57,.1),body1:rt(_e,16,1.5,.15),body2:rt(_e,14,1.43,.15),button:rt(et,14,1.75,.4,caseAllCaps),caption:rt(_e,12,1.66,.4),overline:rt(_e,12,2.66,1,caseAllCaps),inherit:{fontFamily:"inherit",fontWeight:"inherit",fontSize:"inherit",lineHeight:"inherit",letterSpacing:"inherit"}};return deepmerge$1(_extends({htmlFontSize:nt,pxToRem:ct,fontFamily:s,fontSize:$,fontWeightLight:j,fontWeightRegular:_e,fontWeightMedium:et,fontWeightBold:tt},ut),st,{clone:!1})}const shadowKeyUmbraOpacity=.2,shadowKeyPenumbraOpacity=.14,shadowAmbientShadowOpacity=.12;function createShadow(...a){return[`${a[0]}px ${a[1]}px ${a[2]}px ${a[3]}px rgba(0,0,0,${shadowKeyUmbraOpacity})`,`${a[4]}px ${a[5]}px ${a[6]}px ${a[7]}px rgba(0,0,0,${shadowKeyPenumbraOpacity})`,`${a[8]}px ${a[9]}px ${a[10]}px ${a[11]}px rgba(0,0,0,${shadowAmbientShadowOpacity})`].join(",")}const shadows=["none",createShadow(0,2,1,-1,0,1,1,0,0,1,3,0),createShadow(0,3,1,-2,0,2,2,0,0,1,5,0),createShadow(0,3,3,-2,0,3,4,0,0,1,8,0),createShadow(0,2,4,-1,0,4,5,0,0,1,10,0),createShadow(0,3,5,-1,0,5,8,0,0,1,14,0),createShadow(0,3,5,-1,0,6,10,0,0,1,18,0),createShadow(0,4,5,-2,0,7,10,1,0,2,16,1),createShadow(0,5,5,-3,0,8,10,1,0,3,14,2),createShadow(0,5,6,-3,0,9,12,1,0,3,16,2),createShadow(0,6,6,-3,0,10,14,1,0,4,18,3),createShadow(0,6,7,-4,0,11,15,1,0,4,20,3),createShadow(0,7,8,-4,0,12,17,2,0,5,22,4),createShadow(0,7,8,-4,0,13,19,2,0,5,24,4),createShadow(0,7,9,-4,0,14,21,2,0,5,26,4),createShadow(0,8,9,-5,0,15,22,2,0,6,28,5),createShadow(0,8,10,-5,0,16,24,2,0,6,30,5),createShadow(0,8,11,-5,0,17,26,2,0,6,32,5),createShadow(0,9,11,-5,0,18,28,2,0,7,34,6),createShadow(0,9,12,-6,0,19,29,2,0,7,36,6),createShadow(0,10,13,-6,0,20,31,3,0,8,38,7),createShadow(0,10,13,-6,0,21,33,3,0,8,40,7),createShadow(0,10,14,-6,0,22,35,3,0,8,42,7),createShadow(0,11,14,-7,0,23,36,3,0,9,44,8),createShadow(0,11,15,-7,0,24,38,3,0,9,46,8)],_excluded$1N=["duration","easing","delay"],easing={easeInOut:"cubic-bezier(0.4, 0, 0.2, 1)",easeOut:"cubic-bezier(0.0, 0, 0.2, 1)",easeIn:"cubic-bezier(0.4, 0, 1, 1)",sharp:"cubic-bezier(0.4, 0, 0.6, 1)"},duration={shortest:150,shorter:200,short:250,standard:300,complex:375,enteringScreen:225,leavingScreen:195};function formatMs(a){return`${Math.round(a)}ms`}function getAutoHeightDuration(a){if(!a)return 0;const i=a/36;return Math.round((4+15*i**.25+i/5)*10)}function createTransitions(a){const i=_extends({},easing,a.easing),o=_extends({},duration,a.duration);return _extends({getAutoHeightDuration,create:($=["all"],j={})=>{const{duration:_e=o.standard,easing:et=i.easeInOut,delay:tt=0}=j;return _objectWithoutPropertiesLoose(j,_excluded$1N),(Array.isArray($)?$:[$]).map(nt=>`${nt} ${typeof _e=="string"?_e:formatMs(_e)} ${et} ${typeof tt=="string"?tt:formatMs(tt)}`).join(",")}},a,{easing:i,duration:o})}const zIndex={mobileStepper:1e3,fab:1050,speedDial:1050,appBar:1100,drawer:1200,modal:1300,snackbar:1400,tooltip:1500},zIndex$1=zIndex,_excluded$1M=["breakpoints","mixins","spacing","palette","transitions","typography","shape"];function createTheme(a={},...i){const{mixins:o={},palette:s={},transitions:$={},typography:j={}}=a,_e=_objectWithoutPropertiesLoose(a,_excluded$1M);if(a.vars)throw new Error(formatMuiErrorMessage$1(18));const et=createPalette(s),tt=createTheme$2(a);let nt=deepmerge$1(tt,{mixins:createMixins(tt.breakpoints,o),palette:et,shadows:shadows.slice(),typography:createTypography(et,j),transitions:createTransitions($),zIndex:_extends({},zIndex$1)});return nt=deepmerge$1(nt,_e),nt=i.reduce((at,it)=>deepmerge$1(at,it),nt),nt.unstable_sxConfig=_extends({},defaultSxConfig$1,_e==null?void 0:_e.unstable_sxConfig),nt.unstable_sx=function(it){return styleFunctionSx$1({sx:it,theme:this})},nt}function getUnit(a){return String(a).match(/[\d.\-+]*\s*(.*)/)[1]||""}function toUnitless(a){return parseFloat(a)}const defaultTheme$1=createTheme(),defaultTheme$2=defaultTheme$1;function useTheme$1(){const a=useTheme$3(defaultTheme$2);return a[THEME_ID]||a}function useThemeProps$6({props:a,name:i}){return useThemeProps$7({props:a,name:i,defaultTheme:defaultTheme$2,themeId:THEME_ID})}var createStyled$1={};const require$$1$1=getAugmentedNamespace(_extends$1);var objectWithoutPropertiesLoose={exports:{}},hasRequiredObjectWithoutPropertiesLoose;function requireObjectWithoutPropertiesLoose(){return hasRequiredObjectWithoutPropertiesLoose||(hasRequiredObjectWithoutPropertiesLoose=1,function(a){function i(o,s){if(o==null)return{};var $={},j=Object.keys(o),_e,et;for(et=0;et<j.length;et++)_e=j[et],!(s.indexOf(_e)>=0)&&($[_e]=o[_e]);return $}a.exports=i,a.exports.__esModule=!0,a.exports.default=a.exports}(objectWithoutPropertiesLoose)),objectWithoutPropertiesLoose.exports}const require$$1=getAugmentedNamespace(styledEngine),require$$4=getAugmentedNamespace(deepmerge),require$$5$1=getAugmentedNamespace(capitalize$1),require$$6=getAugmentedNamespace(getDisplayName),require$$7=getAugmentedNamespace(createTheme$1),require$$8=getAugmentedNamespace(styleFunctionSx);var _interopRequireDefault$u=interopRequireDefaultExports;Object.defineProperty(createStyled$1,"__esModule",{value:!0});var _default=createStyled$1.default=createStyled;createStyled$1.shouldForwardProp=shouldForwardProp;createStyled$1.systemDefaultTheme=void 0;var _extends2=_interopRequireDefault$u(require$$1$1),_objectWithoutPropertiesLoose2=_interopRequireDefault$u(requireObjectWithoutPropertiesLoose()),_styledEngine$1=_interopRequireWildcard$1(require$$1),_deepmerge=require$$4;_interopRequireDefault$u(require$$5$1);_interopRequireDefault$u(require$$6);var _createTheme=_interopRequireDefault$u(require$$7),_styleFunctionSx=_interopRequireDefault$u(require$$8);const _excluded$1L=["ownerState"],_excluded2$f=["variants"],_excluded3$5=["name","slot","skipVariantsResolver","skipSx","overridesResolver"];function _getRequireWildcardCache$1(a){if(typeof WeakMap!="function")return null;var i=new WeakMap,o=new WeakMap;return(_getRequireWildcardCache$1=function(s){return s?o:i})(a)}function _interopRequireWildcard$1(a,i){if(!i&&a&&a.__esModule)return a;if(a===null||typeof a!="object"&&typeof a!="function")return{default:a};var o=_getRequireWildcardCache$1(i);if(o&&o.has(a))return o.get(a);var s={__proto__:null},$=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var j in a)if(j!=="default"&&Object.prototype.hasOwnProperty.call(a,j)){var _e=$?Object.getOwnPropertyDescriptor(a,j):null;_e&&(_e.get||_e.set)?Object.defineProperty(s,j,_e):s[j]=a[j]}return s.default=a,o&&o.set(a,s),s}function isEmpty$2(a){return Object.keys(a).length===0}function isStringTag(a){return typeof a=="string"&&a.charCodeAt(0)>96}function shouldForwardProp(a){return a!=="ownerState"&&a!=="theme"&&a!=="sx"&&a!=="as"}const systemDefaultTheme=createStyled$1.systemDefaultTheme=(0,_createTheme.default)(),lowercaseFirstLetter=a=>a&&a.charAt(0).toLowerCase()+a.slice(1);function resolveTheme({defaultTheme:a,theme:i,themeId:o}){return isEmpty$2(i)?a:i[o]||i}function defaultOverridesResolver(a){return a?(i,o)=>o[a]:null}function processStyleArg(a,i){let{ownerState:o}=i,s=(0,_objectWithoutPropertiesLoose2.default)(i,_excluded$1L);const $=typeof a=="function"?a((0,_extends2.default)({ownerState:o},s)):a;if(Array.isArray($))return $.flatMap(j=>processStyleArg(j,(0,_extends2.default)({ownerState:o},s)));if($&&typeof $=="object"&&Array.isArray($.variants)){const{variants:j=[]}=$;let et=(0,_objectWithoutPropertiesLoose2.default)($,_excluded2$f);return j.forEach(tt=>{let nt=!0;typeof tt.props=="function"?nt=tt.props((0,_extends2.default)({ownerState:o},s,o)):Object.keys(tt.props).forEach(at=>{(o==null?void 0:o[at])!==tt.props[at]&&s[at]!==tt.props[at]&&(nt=!1)}),nt&&(Array.isArray(et)||(et=[et]),et.push(typeof tt.style=="function"?tt.style((0,_extends2.default)({ownerState:o},s,o)):tt.style))}),et}return $}function createStyled(a={}){const{themeId:i,defaultTheme:o=systemDefaultTheme,rootShouldForwardProp:s=shouldForwardProp,slotShouldForwardProp:$=shouldForwardProp}=a,j=_e=>(0,_styleFunctionSx.default)((0,_extends2.default)({},_e,{theme:resolveTheme((0,_extends2.default)({},_e,{defaultTheme:o,themeId:i}))}));return j.__mui_systemSx=!0,(_e,et={})=>{(0,_styledEngine$1.internal_processStyles)(_e,ft=>ft.filter(ht=>!(ht!=null&&ht.__mui_systemSx)));const{name:tt,slot:nt,skipVariantsResolver:at,skipSx:it,overridesResolver:st=defaultOverridesResolver(lowercaseFirstLetter(nt))}=et,lt=(0,_objectWithoutPropertiesLoose2.default)(et,_excluded3$5),ct=at!==void 0?at:nt&&nt!=="Root"&&nt!=="root"||!1,rt=it||!1;let ut,ot=shouldForwardProp;nt==="Root"||nt==="root"?ot=s:nt?ot=$:isStringTag(_e)&&(ot=void 0);const dt=(0,_styledEngine$1.default)(_e,(0,_extends2.default)({shouldForwardProp:ot,label:ut},lt)),pt=ft=>typeof ft=="function"&&ft.__emotion_real!==ft||(0,_deepmerge.isPlainObject)(ft)?ht=>processStyleArg(ft,(0,_extends2.default)({},ht,{theme:resolveTheme({theme:ht.theme,defaultTheme:o,themeId:i})})):ft,mt=(ft,...ht)=>{let yt=pt(ft);const bt=ht?ht.map(pt):[];tt&&st&&bt.push(vt=>{const Lt=resolveTheme((0,_extends2.default)({},vt,{defaultTheme:o,themeId:i}));if(!Lt.components||!Lt.components[tt]||!Lt.components[tt].styleOverrides)return null;const $t=Lt.components[tt].styleOverrides,Tt={};return Object.entries($t).forEach(([Et,Dt])=>{Tt[Et]=processStyleArg(Dt,(0,_extends2.default)({},vt,{theme:Lt}))}),st(vt,Tt)}),tt&&!ct&&bt.push(vt=>{var Lt;const $t=resolveTheme((0,_extends2.default)({},vt,{defaultTheme:o,themeId:i})),Tt=$t==null||(Lt=$t.components)==null||(Lt=Lt[tt])==null?void 0:Lt.variants;return processStyleArg({variants:Tt},(0,_extends2.default)({},vt,{theme:$t}))}),rt||bt.push(j);const gt=bt.length-ht.length;if(Array.isArray(ft)&&gt>0){const vt=new Array(gt).fill("");yt=[...ft,...vt],yt.raw=[...ft.raw,...vt]}const xt=dt(yt,...bt);return _e.muiName&&(xt.muiName=_e.muiName),xt};return dt.withConfig&&(mt.withConfig=dt.withConfig),mt}}function slotShouldForwardProp(a){return a!=="ownerState"&&a!=="theme"&&a!=="sx"&&a!=="as"}const rootShouldForwardProp$1=a=>slotShouldForwardProp(a)&&a!=="classes",rootShouldForwardProp$2=rootShouldForwardProp$1,styled=_default({themeId:THEME_ID,defaultTheme:defaultTheme$2,rootShouldForwardProp:rootShouldForwardProp$2}),_excluded$1K=["theme"];function ThemeProvider(a){let{theme:i}=a,o=_objectWithoutPropertiesLoose(a,_excluded$1K);const s=i[THEME_ID];return jsxRuntimeExports.jsx(ThemeProvider$1,_extends({},o,{themeId:s?THEME_ID:void 0,theme:s||i}))}const getOverlayAlpha=a=>{let i;return a<1?i=5.11916*a**2:i=4.5*Math.log(a+1)+2,(i/100).toFixed(2)};var define_import_meta_env_default$2={BASE_URL:"/",MODE:"production",DEV:!1,PROD:!0,SSR:!1};let keyCount=0;function atom(a,i){const o=`atom${++keyCount}`,s={toString:()=>o};return typeof a=="function"?s.read=a:(s.init=a,s.read=defaultRead,s.write=defaultWrite),i&&(s.write=i),s}function defaultRead(a){return a(this)}function defaultWrite(a,i,o){return i(this,typeof o=="function"?o(a(this)):o)}const isSelfAtom=(a,i)=>a.unstable_is?a.unstable_is(i):i===a,hasInitialValue=a=>"init"in a,isActuallyWritableAtom=a=>!!a.write,cancelPromiseMap=new WeakMap,registerCancelPromise=(a,i)=>{cancelPromiseMap.set(a,i),a.catch(()=>{}).finally(()=>cancelPromiseMap.delete(a))},cancelPromise=(a,i)=>{const o=cancelPromiseMap.get(a);o&&(cancelPromiseMap.delete(a),o(i))},resolvePromise=(a,i)=>{a.status="fulfilled",a.value=i},rejectPromise=(a,i)=>{a.status="rejected",a.reason=i},isPromiseLike$2=a=>typeof(a==null?void 0:a.then)=="function",isEqualAtomValue=(a,i)=>!!a&&"v"in a&&"v"in i&&Object.is(a.v,i.v),isEqualAtomError=(a,i)=>!!a&&"e"in a&&"e"in i&&Object.is(a.e,i.e),hasPromiseAtomValue=a=>!!a&&"v"in a&&a.v instanceof Promise,isEqualPromiseAtomValue=(a,i)=>"v"in a&&"v"in i&&a.v.orig&&a.v.orig===i.v.orig,returnAtomValue=a=>{if("e"in a)throw a.e;return a.v},createStore=()=>{const a=new WeakMap,i=new WeakMap,o=[],s=new WeakMap;let $,j;(define_import_meta_env_default$2?"production":void 0)!=="production"&&($=new Set,j=new Set);const _e=xt=>a.get(xt),et=(xt,vt)=>{vt.d.forEach((Lt,$t)=>{var Tt;if(!s.has($t)){const Et=_e($t);(Tt=o[o.length-1])==null||Tt.add($t),s.set($t,[Et,new Set]),Et&&et($t,Et)}s.get($t)[1].add(xt)})},tt=(xt,vt)=>{var Lt;(define_import_meta_env_default$2?"production":void 0)!=="production"&&Object.freeze(vt);const $t=_e(xt);if(a.set(xt,vt),s.has(xt)||((Lt=o[o.length-1])==null||Lt.add(xt),s.set(xt,[$t,new Set]),et(xt,vt)),hasPromiseAtomValue($t)){const Tt="v"in vt?vt.v instanceof Promise?vt.v:Promise.resolve(vt.v):Promise.reject(vt.e);$t.v!==Tt&&cancelPromise($t.v,Tt)}},nt=(xt,vt,Lt,$t)=>{const Tt=new Map($t?vt.d:null);let Et=!1;Lt.forEach((Dt,It)=>{!Dt&&isSelfAtom(xt,It)&&(Dt=vt),Dt?(Tt.set(It,Dt),vt.d.get(It)!==Dt&&(Et=!0)):(define_import_meta_env_default$2?"production":void 0)!=="production"&&console.warn("[Bug] atom state not found")}),(Et||vt.d.size!==Tt.size)&&(vt.d=Tt)},at=(xt,vt,Lt,$t)=>{const Tt=_e(xt),Et={d:(Tt==null?void 0:Tt.d)||new Map,v:vt};if(Lt&&nt(xt,Et,Lt,$t),isEqualAtomValue(Tt,Et)&&Tt.d===Et.d)return Tt;if(hasPromiseAtomValue(Tt)&&hasPromiseAtomValue(Et)&&isEqualPromiseAtomValue(Tt,Et)){if(Tt.d===Et.d)return Tt;Et.v=Tt.v}return tt(xt,Et),Et},it=(xt,vt,Lt,$t)=>{if(isPromiseLike$2(vt)){let Tt;const Et=()=>{const It=_e(xt);if(!hasPromiseAtomValue(It)||It.v!==Dt)return;const Ct=at(xt,Dt,Lt);i.has(xt)&&It.d!==Ct.d&&yt(xt,Ct,It.d)},Dt=new Promise((It,Ct)=>{let jt=!1;vt.then(Zt=>{jt||(jt=!0,resolvePromise(Dt,Zt),It(Zt),Et())},Zt=>{jt||(jt=!0,rejectPromise(Dt,Zt),Ct(Zt),Et())}),Tt=Zt=>{jt||(jt=!0,Zt.then(Xt=>resolvePromise(Dt,Xt),Xt=>rejectPromise(Dt,Xt)),It(Zt))}});return Dt.orig=vt,Dt.status="pending",registerCancelPromise(Dt,It=>{It&&Tt(It),$t==null||$t()}),at(xt,Dt,Lt,!0)}return at(xt,vt,Lt)},st=(xt,vt,Lt)=>{const $t=_e(xt),Tt={d:($t==null?void 0:$t.d)||new Map,e:vt};return Lt&&nt(xt,Tt,Lt),isEqualAtomError($t,Tt)&&$t.d===Tt.d?$t:(tt(xt,Tt),Tt)},lt=(xt,vt)=>{const Lt=_e(xt);if(!vt&&Lt&&(i.has(xt)||Array.from(Lt.d).every(([jt,Zt])=>{if(jt===xt)return!0;const Xt=lt(jt);return Xt===Zt||isEqualAtomValue(Xt,Zt)})))return Lt;const $t=new Map;let Tt=!0;const Et=jt=>{if(isSelfAtom(xt,jt)){const Xt=_e(jt);if(Xt)return $t.set(jt,Xt),returnAtomValue(Xt);if(hasInitialValue(jt))return $t.set(jt,void 0),jt.init;throw new Error("no atom init")}const Zt=lt(jt);return $t.set(jt,Zt),returnAtomValue(Zt)};let Dt,It;const Ct={get signal(){return Dt||(Dt=new AbortController),Dt.signal},get setSelf(){return(define_import_meta_env_default$2?"production":void 0)!=="production"&&!isActuallyWritableAtom(xt)&&console.warn("setSelf function cannot be used with read-only atom"),!It&&isActuallyWritableAtom(xt)&&(It=(...jt)=>{if((define_import_meta_env_default$2?"production":void 0)!=="production"&&Tt&&console.warn("setSelf function cannot be called in sync"),!Tt)return mt(xt,...jt)}),It}};try{const jt=xt.read(Et,Ct);return it(xt,jt,$t,()=>Dt==null?void 0:Dt.abort())}catch(jt){return st(xt,jt,$t)}finally{Tt=!1}},ct=xt=>returnAtomValue(lt(xt)),rt=xt=>{let vt=i.get(xt);return vt||(vt=ft(xt)),vt},ut=(xt,vt)=>!vt.l.size&&(!vt.t.size||vt.t.size===1&&vt.t.has(xt)),ot=xt=>{const vt=i.get(xt);vt&&ut(xt,vt)&&ht(xt)},dt=xt=>{const vt=Dt=>{var It,Ct;const jt=new Set((It=i.get(Dt))==null?void 0:It.t);return(Ct=s.get(Dt))==null||Ct[1].forEach(Zt=>{jt.add(Zt)}),jt},Lt=new Array,$t=new Set,Tt=Dt=>{if(!$t.has(Dt)){$t.add(Dt);for(const It of vt(Dt))Dt!==It&&Tt(It);Lt.push(Dt)}};Tt(xt);const Et=new Set([xt]);for(let Dt=Lt.length-1;Dt>=0;--Dt){const It=Lt[Dt],Ct=_e(It);if(!Ct)continue;let jt=!1;for(const Zt of Ct.d.keys())if(Zt!==It&&Et.has(Zt)){jt=!0;break}if(jt){const Zt=lt(It,!0);isEqualAtomValue(Ct,Zt)||Et.add(It)}}},pt=(xt,...vt)=>{const Lt=Et=>returnAtomValue(lt(Et)),$t=(Et,...Dt)=>{const It=o.length>0;It||o.push(new Set([Et]));let Ct;if(isSelfAtom(xt,Et)){if(!hasInitialValue(Et))throw new Error("atom not writable");const jt=_e(Et),Zt=it(Et,Dt[0]);isEqualAtomValue(jt,Zt)||dt(Et)}else Ct=pt(Et,...Dt);if(!It){const jt=bt(o.pop());(define_import_meta_env_default$2?"production":void 0)!=="production"&&$.forEach(Zt=>Zt({type:"async-write",flushed:jt}))}return Ct};return xt.write(Lt,$t,...vt)},mt=(xt,...vt)=>{o.push(new Set([xt]));const Lt=pt(xt,...vt),$t=bt(o.pop());return(define_import_meta_env_default$2?"production":void 0)!=="production"&&$.forEach(Tt=>Tt({type:"write",flushed:$t})),Lt},ft=(xt,vt,Lt)=>{var $t;const Tt=Lt||[];($t=_e(xt))==null||$t.d.forEach((Dt,It)=>{const Ct=i.get(It);Ct?Ct.t.add(xt):It!==xt&&ft(It,xt,Tt)}),lt(xt);const Et={t:new Set(vt&&[vt]),l:new Set};if(i.set(xt,Et),(define_import_meta_env_default$2?"production":void 0)!=="production"&&j.add(xt),isActuallyWritableAtom(xt)&&xt.onMount){const{onMount:Dt}=xt;Tt.push(()=>{const It=Dt((...Ct)=>mt(xt,...Ct));It&&(Et.u=It)})}return Lt||Tt.forEach(Dt=>Dt()),Et},ht=xt=>{var vt;const Lt=(vt=i.get(xt))==null?void 0:vt.u;Lt&&Lt(),i.delete(xt),(define_import_meta_env_default$2?"production":void 0)!=="production"&&j.delete(xt);const $t=_e(xt);$t?(hasPromiseAtomValue($t)&&cancelPromise($t.v),$t.d.forEach((Tt,Et)=>{if(Et!==xt){const Dt=i.get(Et);Dt&&(Dt.t.delete(xt),ut(Et,Dt)&&ht(Et))}})):(define_import_meta_env_default$2?"production":void 0)!=="production"&&console.warn("[Bug] could not find atom state to unmount",xt)},yt=(xt,vt,Lt)=>{const $t=new Set(vt.d.keys()),Tt=new Set;Lt==null||Lt.forEach((Et,Dt)=>{if($t.has(Dt)){$t.delete(Dt);return}Tt.add(Dt);const It=i.get(Dt);It&&It.t.delete(xt)}),$t.forEach(Et=>{const Dt=i.get(Et);Dt?Dt.t.add(xt):i.has(xt)&&ft(Et,xt)}),Tt.forEach(Et=>{const Dt=i.get(Et);Dt&&ut(Et,Dt)&&ht(Et)})},bt=xt=>{let vt;(define_import_meta_env_default$2?"production":void 0)!=="production"&&(vt=new Set);const Lt=[],$t=Tt=>{var Et;if(!s.has(Tt))return;const[Dt,It]=s.get(Tt);s.delete(Tt),Lt.push([Tt,Dt]),It.forEach($t),(Et=_e(Tt))==null||Et.d.forEach((Ct,jt)=>$t(jt))};if(xt.forEach($t),Lt.forEach(([Tt,Et])=>{const Dt=_e(Tt);if(!Dt){(define_import_meta_env_default$2?"production":void 0)!=="production"&&console.warn("[Bug] no atom state to flush");return}if(Dt!==Et){const It=i.get(Tt);It&&Dt.d!==(Et==null?void 0:Et.d)&&yt(Tt,Dt,Et==null?void 0:Et.d),It&&!(!hasPromiseAtomValue(Et)&&(isEqualAtomValue(Et,Dt)||isEqualAtomError(Et,Dt)))&&(It.l.forEach(Ct=>Ct()),(define_import_meta_env_default$2?"production":void 0)!=="production"&&vt.add(Tt))}}),(define_import_meta_env_default$2?"production":void 0)!=="production")return vt},gt=(xt,vt)=>{const Lt=rt(xt),$t=bt([xt]),Tt=Lt.l;return Tt.add(vt),(define_import_meta_env_default$2?"production":void 0)!=="production"&&$.forEach(Et=>Et({type:"sub",flushed:$t})),()=>{Tt.delete(vt),ot(xt),(define_import_meta_env_default$2?"production":void 0)!=="production"&&$.forEach(Et=>Et({type:"unsub"}))}};return(define_import_meta_env_default$2?"production":void 0)!=="production"?{get:ct,set:mt,sub:gt,dev_subscribe_store:(xt,vt)=>{if(vt!==2)throw new Error("The current StoreListener revision is 2.");return $.add(xt),()=>{$.delete(xt)}},dev_get_mounted_atoms:()=>j.values(),dev_get_atom_state:xt=>a.get(xt),dev_get_mounted:xt=>i.get(xt),dev_restore_atoms:xt=>{o.push(new Set);for(const[Lt,$t]of xt)hasInitialValue(Lt)&&(it(Lt,$t),dt(Lt));const vt=bt(o.pop());$.forEach(Lt=>Lt({type:"restore",flushed:vt}))}}:{get:ct,set:mt,sub:gt}};let defaultStore;(define_import_meta_env_default$2?"production":void 0)!=="production"&&(typeof globalThis.__NUMBER_OF_JOTAI_INSTANCES__=="number"?++globalThis.__NUMBER_OF_JOTAI_INSTANCES__:globalThis.__NUMBER_OF_JOTAI_INSTANCES__=1);const getDefaultStore=()=>(defaultStore||((define_import_meta_env_default$2?"production":void 0)!=="production"&&globalThis.__NUMBER_OF_JOTAI_INSTANCES__!==1&&console.warn("Detected multiple Jotai instances. It may cause unexpected behavior with the default store. https://github.com/pmndrs/jotai/discussions/2044"),defaultStore=createStore()),defaultStore);var define_import_meta_env_default$1={BASE_URL:"/",MODE:"production",DEV:!1,PROD:!0,SSR:!1};const StoreContext=reactExports.createContext(void 0),useStore=a=>{const i=reactExports.useContext(StoreContext);return(a==null?void 0:a.store)||i||getDefaultStore()},Provider=({children:a,store:i})=>{const o=reactExports.useRef();return!i&&!o.current&&(o.current=createStore()),reactExports.createElement(StoreContext.Provider,{value:i||o.current},a)},isPromiseLike$1=a=>typeof(a==null?void 0:a.then)=="function",use=React$1.use||(a=>{if(a.status==="pending")throw a;if(a.status==="fulfilled")return a.value;throw a.status==="rejected"?a.reason:(a.status="pending",a.then(i=>{a.status="fulfilled",a.value=i},i=>{a.status="rejected",a.reason=i}),a)});function useAtomValue(a,i){const o=useStore(i),[[s,$,j],_e]=reactExports.useReducer(nt=>{const at=o.get(a);return Object.is(nt[0],at)&&nt[1]===o&&nt[2]===a?nt:[at,o,a]},void 0,()=>[o.get(a),o,a]);let et=s;($!==o||j!==a)&&(_e(),et=o.get(a));const tt=i==null?void 0:i.delay;return reactExports.useEffect(()=>{const nt=o.sub(a,()=>{if(typeof tt=="number"){setTimeout(_e,tt);return}_e()});return _e(),nt},[o,a,tt]),reactExports.useDebugValue(et),isPromiseLike$1(et)?use(et):et}function useSetAtom(a,i){const o=useStore(i);return reactExports.useCallback((...$)=>{if((define_import_meta_env_default$1?"production":void 0)!=="production"&&!("write"in a))throw new Error("not writable atom");return o.set(a,...$)},[o,a])}function useAtom(a,i){return[useAtomValue(a,i),useSetAtom(a,i)]}const deepPurple={50:"#ede7f6",100:"#d1c4e9",200:"#b39ddb",300:"#9575cd",400:"#7e57c2",500:"#673ab7",600:"#5e35b1",700:"#512da8",800:"#4527a0",900:"#311b92",A100:"#b388ff",A200:"#7c4dff",A400:"#651fff",A700:"#6200ea"},indigo={50:"#e8eaf6",100:"#c5cae9",200:"#9fa8da",300:"#7986cb",400:"#5c6bc0",500:"#3f51b5",600:"#3949ab",700:"#303f9f",800:"#283593",900:"#1a237e",A100:"#8c9eff",A200:"#536dfe",A400:"#3d5afe",A700:"#304ffe"},amber={50:"#fff8e1",100:"#ffecb3",200:"#ffe082",300:"#ffd54f",400:"#ffca28",500:"#ffc107",600:"#ffb300",700:"#ffa000",800:"#ff8f00",900:"#ff6f00",A100:"#ffe57f",A200:"#ffd740",A400:"#ffc400",A700:"#ffab00"},scrollBar={track:"#2b2b2b",thumb:"#6b6b6b",active:"#959595"};function darkScrollbar(a=scrollBar){return{scrollbarColor:`${a.thumb} ${a.track}`,"&::-webkit-scrollbar, & *::-webkit-scrollbar":{backgroundColor:a.track},"&::-webkit-scrollbar-thumb, & *::-webkit-scrollbar-thumb":{borderRadius:8,backgroundColor:a.thumb,minHeight:24,border:`3px solid ${a.track}`},"&::-webkit-scrollbar-thumb:focus, & *::-webkit-scrollbar-thumb:focus":{backgroundColor:a.active},"&::-webkit-scrollbar-thumb:active, & *::-webkit-scrollbar-thumb:active":{backgroundColor:a.active},"&::-webkit-scrollbar-thumb:hover, & *::-webkit-scrollbar-thumb:hover":{backgroundColor:a.active},"&::-webkit-scrollbar-corner, & *::-webkit-scrollbar-corner":{backgroundColor:a.track}}}const colors=createTheme({components:{MuiCssBaseline:{styleOverrides:a=>({body:darkScrollbar()})}},palette:{mode:"dark",success:{main:green[600]},primary:{main:"#4a39ab",light:"#5a48a7",dark:"#3b278e"},secondary:{main:indigo[700],light:indigo[400]},background:{default:"#171717",paper:grey[900]},warning:{main:amber[700]},amber:{main:amber[900]},red:{main:red[600]},mom:{main:deepPurple[900]}}});function getSvgIconUtilityClass(a){return generateUtilityClass$1("MuiSvgIcon",a)}generateUtilityClasses$1("MuiSvgIcon",["root","colorPrimary","colorSecondary","colorAction","colorError","colorDisabled","fontSizeInherit","fontSizeSmall","fontSizeMedium","fontSizeLarge"]);const _excluded$1J=["children","className","color","component","fontSize","htmlColor","inheritViewBox","titleAccess","viewBox"],useUtilityClasses$1r=a=>{const{color:i,fontSize:o,classes:s}=a,$={root:["root",i!=="inherit"&&`color${capitalize$2(i)}`,`fontSize${capitalize$2(o)}`]};return composeClasses($,getSvgIconUtilityClass,s)},SvgIconRoot=styled("svg",{name:"MuiSvgIcon",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.color!=="inherit"&&i[`color${capitalize$2(o.color)}`],i[`fontSize${capitalize$2(o.fontSize)}`]]}})(({theme:a,ownerState:i})=>{var o,s,$,j,_e,et,tt,nt,at,it,st,lt,ct;return{userSelect:"none",width:"1em",height:"1em",display:"inline-block",fill:i.hasSvgAsChild?void 0:"currentColor",flexShrink:0,transition:(o=a.transitions)==null||(s=o.create)==null?void 0:s.call(o,"fill",{duration:($=a.transitions)==null||($=$.duration)==null?void 0:$.shorter}),fontSize:{inherit:"inherit",small:((j=a.typography)==null||(_e=j.pxToRem)==null?void 0:_e.call(j,20))||"1.25rem",medium:((et=a.typography)==null||(tt=et.pxToRem)==null?void 0:tt.call(et,24))||"1.5rem",large:((nt=a.typography)==null||(at=nt.pxToRem)==null?void 0:at.call(nt,35))||"2.1875rem"}[i.fontSize],color:(it=(st=(a.vars||a).palette)==null||(st=st[i.color])==null?void 0:st.main)!=null?it:{action:(lt=(a.vars||a).palette)==null||(lt=lt.action)==null?void 0:lt.active,disabled:(ct=(a.vars||a).palette)==null||(ct=ct.action)==null?void 0:ct.disabled,inherit:void 0}[i.color]}}),SvgIcon=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiSvgIcon"}),{children:$,className:j,color:_e="inherit",component:et="svg",fontSize:tt="medium",htmlColor:nt,inheritViewBox:at=!1,titleAccess:it,viewBox:st="0 0 24 24"}=s,lt=_objectWithoutPropertiesLoose(s,_excluded$1J),ct=reactExports.isValidElement($)&&$.type==="svg",rt=_extends({},s,{color:_e,component:et,fontSize:tt,instanceFontSize:i.fontSize,inheritViewBox:at,viewBox:st,hasSvgAsChild:ct}),ut={};at||(ut.viewBox=st);const ot=useUtilityClasses$1r(rt);return jsxRuntimeExports.jsxs(SvgIconRoot,_extends({as:et,className:clsx(ot.root,j),focusable:"false",color:nt,"aria-hidden":it?void 0:!0,role:it?"img":void 0,ref:o},ut,lt,ct&&$.props,{ownerState:rt,children:[ct?$.props.children:$,it?jsxRuntimeExports.jsx("title",{children:it}):null]}))});SvgIcon.muiName="SvgIcon";function createSvgIcon$1(a,i){function o(s,$){return jsxRuntimeExports.jsx(SvgIcon,_extends({"data-testid":`${i}Icon`,ref:$},s,{children:a}))}return o.muiName=SvgIcon.muiName,reactExports.memo(reactExports.forwardRef(o))}const unstable_ClassNameGenerator={configure:a=>{ClassNameGenerator$1.configure(a)}},utils$2=Object.freeze(Object.defineProperty({__proto__:null,capitalize:capitalize$2,createChainedFunction,createSvgIcon:createSvgIcon$1,debounce:debounce$1,deprecatedPropType,isMuiElement,ownerDocument,ownerWindow,requirePropFactory,setRef,unstable_ClassNameGenerator,unstable_useEnhancedEffect:useEnhancedEffect,unstable_useId:useId,unsupportedProp,useControlled,useEventCallback,useForkRef,useIsFocusVisible},Symbol.toStringTag,{value:"Module"}));function createUseThemeProps(a){return useThemeProps$6}function _setPrototypeOf(a,i){return _setPrototypeOf=Object.setPrototypeOf?Object.setPrototypeOf.bind():function(s,$){return s.__proto__=$,s},_setPrototypeOf(a,i)}function _inheritsLoose(a,i){a.prototype=Object.create(i.prototype),a.prototype.constructor=a,_setPrototypeOf(a,i)}function hasClass(a,i){return a.classList?!!i&&a.classList.contains(i):(" "+(a.className.baseVal||a.className)+" ").indexOf(" "+i+" ")!==-1}function addClass(a,i){a.classList?a.classList.add(i):hasClass(a,i)||(typeof a.className=="string"?a.className=a.className+" "+i:a.setAttribute("class",(a.className&&a.className.baseVal||"")+" "+i))}function replaceClassName(a,i){return a.replace(new RegExp("(^|\\s)"+i+"(?:\\s|$)","g"),"$1").replace(/\s+/g," ").replace(/^\s*|\s*$/g,"")}function removeClass$1(a,i){a.classList?a.classList.remove(i):typeof a.className=="string"?a.className=replaceClassName(a.className,i):a.setAttribute("class",replaceClassName(a.className&&a.className.baseVal||"",i))}const config$1={disabled:!1},TransitionGroupContext=React$1.createContext(null);var forceReflow=function(i){return i.scrollTop},UNMOUNTED="unmounted",EXITED="exited",ENTERING="entering",ENTERED="entered",EXITING="exiting",Transition=function(a){_inheritsLoose(i,a);function i(s,$){var j;j=a.call(this,s,$)||this;var _e=$,et=_e&&!_e.isMounting?s.enter:s.appear,tt;return j.appearStatus=null,s.in?et?(tt=EXITED,j.appearStatus=ENTERING):tt=ENTERED:s.unmountOnExit||s.mountOnEnter?tt=UNMOUNTED:tt=EXITED,j.state={status:tt},j.nextCallback=null,j}i.getDerivedStateFromProps=function($,j){var _e=$.in;return _e&&j.status===UNMOUNTED?{status:EXITED}:null};var o=i.prototype;return o.componentDidMount=function(){this.updateStatus(!0,this.appearStatus)},o.componentDidUpdate=function($){var j=null;if($!==this.props){var _e=this.state.status;this.props.in?_e!==ENTERING&&_e!==ENTERED&&(j=ENTERING):(_e===ENTERING||_e===ENTERED)&&(j=EXITING)}this.updateStatus(!1,j)},o.componentWillUnmount=function(){this.cancelNextCallback()},o.getTimeouts=function(){var $=this.props.timeout,j,_e,et;return j=_e=et=$,$!=null&&typeof $!="number"&&(j=$.exit,_e=$.enter,et=$.appear!==void 0?$.appear:_e),{exit:j,enter:_e,appear:et}},o.updateStatus=function($,j){if($===void 0&&($=!1),j!==null)if(this.cancelNextCallback(),j===ENTERING){if(this.props.unmountOnExit||this.props.mountOnEnter){var _e=this.props.nodeRef?this.props.nodeRef.current:ReactDOM.findDOMNode(this);_e&&forceReflow(_e)}this.performEnter($)}else this.performExit();else this.props.unmountOnExit&&this.state.status===EXITED&&this.setState({status:UNMOUNTED})},o.performEnter=function($){var j=this,_e=this.props.enter,et=this.context?this.context.isMounting:$,tt=this.props.nodeRef?[et]:[ReactDOM.findDOMNode(this),et],nt=tt[0],at=tt[1],it=this.getTimeouts(),st=et?it.appear:it.enter;if(!$&&!_e||config$1.disabled){this.safeSetState({status:ENTERED},function(){j.props.onEntered(nt)});return}this.props.onEnter(nt,at),this.safeSetState({status:ENTERING},function(){j.props.onEntering(nt,at),j.onTransitionEnd(st,function(){j.safeSetState({status:ENTERED},function(){j.props.onEntered(nt,at)})})})},o.performExit=function(){var $=this,j=this.props.exit,_e=this.getTimeouts(),et=this.props.nodeRef?void 0:ReactDOM.findDOMNode(this);if(!j||config$1.disabled){this.safeSetState({status:EXITED},function(){$.props.onExited(et)});return}this.props.onExit(et),this.safeSetState({status:EXITING},function(){$.props.onExiting(et),$.onTransitionEnd(_e.exit,function(){$.safeSetState({status:EXITED},function(){$.props.onExited(et)})})})},o.cancelNextCallback=function(){this.nextCallback!==null&&(this.nextCallback.cancel(),this.nextCallback=null)},o.safeSetState=function($,j){j=this.setNextCallback(j),this.setState($,j)},o.setNextCallback=function($){var j=this,_e=!0;return this.nextCallback=function(et){_e&&(_e=!1,j.nextCallback=null,$(et))},this.nextCallback.cancel=function(){_e=!1},this.nextCallback},o.onTransitionEnd=function($,j){this.setNextCallback(j);var _e=this.props.nodeRef?this.props.nodeRef.current:ReactDOM.findDOMNode(this),et=$==null&&!this.props.addEndListener;if(!_e||et){setTimeout(this.nextCallback,0);return}if(this.props.addEndListener){var tt=this.props.nodeRef?[this.nextCallback]:[_e,this.nextCallback],nt=tt[0],at=tt[1];this.props.addEndListener(nt,at)}$!=null&&setTimeout(this.nextCallback,$)},o.render=function(){var $=this.state.status;if($===UNMOUNTED)return null;var j=this.props,_e=j.children;j.in,j.mountOnEnter,j.unmountOnExit,j.appear,j.enter,j.exit,j.timeout,j.addEndListener,j.onEnter,j.onEntering,j.onEntered,j.onExit,j.onExiting,j.onExited,j.nodeRef;var et=_objectWithoutPropertiesLoose(j,["children","in","mountOnEnter","unmountOnExit","appear","enter","exit","timeout","addEndListener","onEnter","onEntering","onEntered","onExit","onExiting","onExited","nodeRef"]);return React$1.createElement(TransitionGroupContext.Provider,{value:null},typeof _e=="function"?_e($,et):React$1.cloneElement(React$1.Children.only(_e),et))},i}(React$1.Component);Transition.contextType=TransitionGroupContext;Transition.propTypes={};function noop$2(){}Transition.defaultProps={in:!1,mountOnEnter:!1,unmountOnExit:!1,appear:!1,enter:!0,exit:!0,onEnter:noop$2,onEntering:noop$2,onEntered:noop$2,onExit:noop$2,onExiting:noop$2,onExited:noop$2};Transition.UNMOUNTED=UNMOUNTED;Transition.EXITED=EXITED;Transition.ENTERING=ENTERING;Transition.ENTERED=ENTERED;Transition.EXITING=EXITING;const Transition$1=Transition;var _addClass=function(i,o){return i&&o&&o.split(" ").forEach(function(s){return addClass(i,s)})},removeClass=function(i,o){return i&&o&&o.split(" ").forEach(function(s){return removeClass$1(i,s)})},CSSTransition=function(a){_inheritsLoose(i,a);function i(){for(var s,$=arguments.length,j=new Array($),_e=0;_e<$;_e++)j[_e]=arguments[_e];return s=a.call.apply(a,[this].concat(j))||this,s.appliedClasses={appear:{},enter:{},exit:{}},s.onEnter=function(et,tt){var nt=s.resolveArguments(et,tt),at=nt[0],it=nt[1];s.removeClasses(at,"exit"),s.addClass(at,it?"appear":"enter","base"),s.props.onEnter&&s.props.onEnter(et,tt)},s.onEntering=function(et,tt){var nt=s.resolveArguments(et,tt),at=nt[0],it=nt[1],st=it?"appear":"enter";s.addClass(at,st,"active"),s.props.onEntering&&s.props.onEntering(et,tt)},s.onEntered=function(et,tt){var nt=s.resolveArguments(et,tt),at=nt[0],it=nt[1],st=it?"appear":"enter";s.removeClasses(at,st),s.addClass(at,st,"done"),s.props.onEntered&&s.props.onEntered(et,tt)},s.onExit=function(et){var tt=s.resolveArguments(et),nt=tt[0];s.removeClasses(nt,"appear"),s.removeClasses(nt,"enter"),s.addClass(nt,"exit","base"),s.props.onExit&&s.props.onExit(et)},s.onExiting=function(et){var tt=s.resolveArguments(et),nt=tt[0];s.addClass(nt,"exit","active"),s.props.onExiting&&s.props.onExiting(et)},s.onExited=function(et){var tt=s.resolveArguments(et),nt=tt[0];s.removeClasses(nt,"exit"),s.addClass(nt,"exit","done"),s.props.onExited&&s.props.onExited(et)},s.resolveArguments=function(et,tt){return s.props.nodeRef?[s.props.nodeRef.current,et]:[et,tt]},s.getClassNames=function(et){var tt=s.props.classNames,nt=typeof tt=="string",at=nt&&tt?tt+"-":"",it=nt?""+at+et:tt[et],st=nt?it+"-active":tt[et+"Active"],lt=nt?it+"-done":tt[et+"Done"];return{baseClassName:it,activeClassName:st,doneClassName:lt}},s}var o=i.prototype;return o.addClass=function($,j,_e){var et=this.getClassNames(j)[_e+"ClassName"],tt=this.getClassNames("enter"),nt=tt.doneClassName;j==="appear"&&_e==="done"&&nt&&(et+=" "+nt),_e==="active"&&$&&forceReflow($),et&&(this.appliedClasses[j][_e]=et,_addClass($,et))},o.removeClasses=function($,j){var _e=this.appliedClasses[j],et=_e.base,tt=_e.active,nt=_e.done;this.appliedClasses[j]={},et&&removeClass($,et),tt&&removeClass($,tt),nt&&removeClass($,nt)},o.render=function(){var $=this.props;$.classNames;var j=_objectWithoutPropertiesLoose($,["classNames"]);return React$1.createElement(Transition$1,_extends({},j,{onEnter:this.onEnter,onEntered:this.onEntered,onEntering:this.onEntering,onExit:this.onExit,onExiting:this.onExiting,onExited:this.onExited}))},i}(React$1.Component);CSSTransition.defaultProps={classNames:""};CSSTransition.propTypes={};const CSSTransition$1=CSSTransition;function _assertThisInitialized(a){if(a===void 0)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return a}function getChildMapping(a,i){var o=function(j){return i&&reactExports.isValidElement(j)?i(j):j},s=Object.create(null);return a&&reactExports.Children.map(a,function($){return $}).forEach(function($){s[$.key]=o($)}),s}function mergeChildMappings(a,i){a=a||{},i=i||{};function o(at){return at in i?i[at]:a[at]}var s=Object.create(null),$=[];for(var j in a)j in i?$.length&&(s[j]=$,$=[]):$.push(j);var _e,et={};for(var tt in i){if(s[tt])for(_e=0;_e<s[tt].length;_e++){var nt=s[tt][_e];et[s[tt][_e]]=o(nt)}et[tt]=o(tt)}for(_e=0;_e<$.length;_e++)et[$[_e]]=o($[_e]);return et}function getProp(a,i,o){return o[i]!=null?o[i]:a.props[i]}function getInitialChildMapping(a,i){return getChildMapping(a.children,function(o){return reactExports.cloneElement(o,{onExited:i.bind(null,o),in:!0,appear:getProp(o,"appear",a),enter:getProp(o,"enter",a),exit:getProp(o,"exit",a)})})}function getNextChildMapping(a,i,o){var s=getChildMapping(a.children),$=mergeChildMappings(i,s);return Object.keys($).forEach(function(j){var _e=$[j];if(reactExports.isValidElement(_e)){var et=j in i,tt=j in s,nt=i[j],at=reactExports.isValidElement(nt)&&!nt.props.in;tt&&(!et||at)?$[j]=reactExports.cloneElement(_e,{onExited:o.bind(null,_e),in:!0,exit:getProp(_e,"exit",a),enter:getProp(_e,"enter",a)}):!tt&&et&&!at?$[j]=reactExports.cloneElement(_e,{in:!1}):tt&&et&&reactExports.isValidElement(nt)&&($[j]=reactExports.cloneElement(_e,{onExited:o.bind(null,_e),in:nt.props.in,exit:getProp(_e,"exit",a),enter:getProp(_e,"enter",a)}))}}),$}var values=Object.values||function(a){return Object.keys(a).map(function(i){return a[i]})},defaultProps={component:"div",childFactory:function(i){return i}},TransitionGroup=function(a){_inheritsLoose(i,a);function i(s,$){var j;j=a.call(this,s,$)||this;var _e=j.handleExited.bind(_assertThisInitialized(j));return j.state={contextValue:{isMounting:!0},handleExited:_e,firstRender:!0},j}var o=i.prototype;return o.componentDidMount=function(){this.mounted=!0,this.setState({contextValue:{isMounting:!1}})},o.componentWillUnmount=function(){this.mounted=!1},i.getDerivedStateFromProps=function($,j){var _e=j.children,et=j.handleExited,tt=j.firstRender;return{children:tt?getInitialChildMapping($,et):getNextChildMapping($,_e,et),firstRender:!1}},o.handleExited=function($,j){var _e=getChildMapping(this.props.children);$.key in _e||($.props.onExited&&$.props.onExited(j),this.mounted&&this.setState(function(et){var tt=_extends({},et.children);return delete tt[$.key],{children:tt}}))},o.render=function(){var $=this.props,j=$.component,_e=$.childFactory,et=_objectWithoutPropertiesLoose($,["component","childFactory"]),tt=this.state.contextValue,nt=values(this.state.children).map(_e);return delete et.appear,delete et.enter,delete et.exit,j===null?React$1.createElement(TransitionGroupContext.Provider,{value:tt},nt):React$1.createElement(TransitionGroupContext.Provider,{value:tt},React$1.createElement(j,et,nt))},i}(React$1.Component);TransitionGroup.propTypes={};TransitionGroup.defaultProps=defaultProps;const TransitionGroup$1=TransitionGroup,reflow=a=>a.scrollTop;function getTransitionProps(a,i){var o,s;const{timeout:$,easing:j,style:_e={}}=a;return{duration:(o=_e.transitionDuration)!=null?o:typeof $=="number"?$:$[i.mode]||0,easing:(s=_e.transitionTimingFunction)!=null?s:typeof j=="object"?j[i.mode]:j,delay:_e.transitionDelay}}function getCollapseUtilityClass(a){return generateUtilityClass$1("MuiCollapse",a)}generateUtilityClasses$1("MuiCollapse",["root","horizontal","vertical","entered","hidden","wrapper","wrapperInner"]);const _excluded$1I=["addEndListener","children","className","collapsedSize","component","easing","in","onEnter","onEntered","onEntering","onExit","onExited","onExiting","orientation","style","timeout","TransitionComponent"],useUtilityClasses$1q=a=>{const{orientation:i,classes:o}=a,s={root:["root",`${i}`],entered:["entered"],hidden:["hidden"],wrapper:["wrapper",`${i}`],wrapperInner:["wrapperInner",`${i}`]};return composeClasses(s,getCollapseUtilityClass,o)},CollapseRoot=styled("div",{name:"MuiCollapse",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.orientation],o.state==="entered"&&i.entered,o.state==="exited"&&!o.in&&o.collapsedSize==="0px"&&i.hidden]}})(({theme:a,ownerState:i})=>_extends({height:0,overflow:"hidden",transition:a.transitions.create("height")},i.orientation==="horizontal"&&{height:"auto",width:0,transition:a.transitions.create("width")},i.state==="entered"&&_extends({height:"auto",overflow:"visible"},i.orientation==="horizontal"&&{width:"auto"}),i.state==="exited"&&!i.in&&i.collapsedSize==="0px"&&{visibility:"hidden"})),CollapseWrapper=styled("div",{name:"MuiCollapse",slot:"Wrapper",overridesResolver:(a,i)=>i.wrapper})(({ownerState:a})=>_extends({display:"flex",width:"100%"},a.orientation==="horizontal"&&{width:"auto",height:"100%"})),CollapseWrapperInner=styled("div",{name:"MuiCollapse",slot:"WrapperInner",overridesResolver:(a,i)=>i.wrapperInner})(({ownerState:a})=>_extends({width:"100%"},a.orientation==="horizontal"&&{width:"auto",height:"100%"})),Collapse=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiCollapse"}),{addEndListener:$,children:j,className:_e,collapsedSize:et="0px",component:tt,easing:nt,in:at,onEnter:it,onEntered:st,onEntering:lt,onExit:ct,onExited:rt,onExiting:ut,orientation:ot="vertical",style:dt,timeout:pt=duration.standard,TransitionComponent:mt=Transition$1}=s,ft=_objectWithoutPropertiesLoose(s,_excluded$1I),ht=_extends({},s,{orientation:ot,collapsedSize:et}),yt=useUtilityClasses$1q(ht),bt=useTheme$1(),gt=useTimeout(),xt=reactExports.useRef(null),vt=reactExports.useRef(),Lt=typeof et=="number"?`${et}px`:et,$t=ot==="horizontal",Tt=$t?"width":"height",Et=reactExports.useRef(null),Dt=useForkRef(o,Et),It=At=>Pt=>{if(At){const Mt=Et.current;Pt===void 0?At(Mt):At(Mt,Pt)}},Ct=()=>xt.current?xt.current[$t?"clientWidth":"clientHeight"]:0,jt=It((At,Pt)=>{xt.current&&$t&&(xt.current.style.position="absolute"),At.style[Tt]=Lt,it&&it(At,Pt)}),Zt=It((At,Pt)=>{const Mt=Ct();xt.current&&$t&&(xt.current.style.position="");const{duration:Ot,easing:Bt}=getTransitionProps({style:dt,timeout:pt,easing:nt},{mode:"enter"});if(pt==="auto"){const zt=bt.transitions.getAutoHeightDuration(Mt);At.style.transitionDuration=`${zt}ms`,vt.current=zt}else At.style.transitionDuration=typeof Ot=="string"?Ot:`${Ot}ms`;At.style[Tt]=`${Mt}px`,At.style.transitionTimingFunction=Bt,lt&&lt(At,Pt)}),Xt=It((At,Pt)=>{At.style[Tt]="auto",st&&st(At,Pt)}),sn=It(At=>{At.style[Tt]=`${Ct()}px`,ct&&ct(At)}),Ft=It(rt),wt=It(At=>{const Pt=Ct(),{duration:Mt,easing:Ot}=getTransitionProps({style:dt,timeout:pt,easing:nt},{mode:"exit"});if(pt==="auto"){const Bt=bt.transitions.getAutoHeightDuration(Pt);At.style.transitionDuration=`${Bt}ms`,vt.current=Bt}else At.style.transitionDuration=typeof Mt=="string"?Mt:`${Mt}ms`;At.style[Tt]=Lt,At.style.transitionTimingFunction=Ot,ut&&ut(At)}),kt=At=>{pt==="auto"&&gt.start(vt.current||0,At),$&&$(Et.current,At)};return jsxRuntimeExports.jsx(mt,_extends({in:at,onEnter:jt,onEntered:Xt,onEntering:Zt,onExit:sn,onExited:Ft,onExiting:wt,addEndListener:kt,nodeRef:Et,timeout:pt==="auto"?null:pt},ft,{children:(At,Pt)=>jsxRuntimeExports.jsx(CollapseRoot,_extends({as:tt,className:clsx(yt.root,_e,{entered:yt.entered,exited:!at&&Lt==="0px"&&yt.hidden}[At]),style:_extends({[$t?"minWidth":"minHeight"]:Lt},dt),ref:Dt},Pt,{ownerState:_extends({},ht,{state:At}),children:jsxRuntimeExports.jsx(CollapseWrapper,{ownerState:_extends({},ht,{state:At}),className:yt.wrapper,ref:xt,children:jsxRuntimeExports.jsx(CollapseWrapperInner,{ownerState:_extends({},ht,{state:At}),className:yt.wrapperInner,children:j})})}))}))});Collapse.muiSupportAuto=!0;const Collapse$1=Collapse;function getPaperUtilityClass(a){return generateUtilityClass$1("MuiPaper",a)}generateUtilityClasses$1("MuiPaper",["root","rounded","outlined","elevation","elevation0","elevation1","elevation2","elevation3","elevation4","elevation5","elevation6","elevation7","elevation8","elevation9","elevation10","elevation11","elevation12","elevation13","elevation14","elevation15","elevation16","elevation17","elevation18","elevation19","elevation20","elevation21","elevation22","elevation23","elevation24"]);const _excluded$1H=["className","component","elevation","square","variant"],useUtilityClasses$1p=a=>{const{square:i,elevation:o,variant:s,classes:$}=a,j={root:["root",s,!i&&"rounded",s==="elevation"&&`elevation${o}`]};return composeClasses(j,getPaperUtilityClass,$)},PaperRoot=styled("div",{name:"MuiPaper",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],!o.square&&i.rounded,o.variant==="elevation"&&i[`elevation${o.elevation}`]]}})(({theme:a,ownerState:i})=>{var o;return _extends({backgroundColor:(a.vars||a).palette.background.paper,color:(a.vars||a).palette.text.primary,transition:a.transitions.create("box-shadow")},!i.square&&{borderRadius:a.shape.borderRadius},i.variant==="outlined"&&{border:`1px solid ${(a.vars||a).palette.divider}`},i.variant==="elevation"&&_extends({boxShadow:(a.vars||a).shadows[i.elevation]},!a.vars&&a.palette.mode==="dark"&&{backgroundImage:`linear-gradient(${alpha_1("#fff",getOverlayAlpha(i.elevation))}, ${alpha_1("#fff",getOverlayAlpha(i.elevation))})`},a.vars&&{backgroundImage:(o=a.vars.overlays)==null?void 0:o[i.elevation]}))}),Paper=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiPaper"}),{className:$,component:j="div",elevation:_e=1,square:et=!1,variant:tt="elevation"}=s,nt=_objectWithoutPropertiesLoose(s,_excluded$1H),at=_extends({},s,{component:j,elevation:_e,square:et,variant:tt}),it=useUtilityClasses$1p(at);return jsxRuntimeExports.jsx(PaperRoot,_extends({as:j,ownerState:at,className:clsx(it.root,$),ref:o},nt))}),Paper$1=Paper,AccordionContext=reactExports.createContext({});function isHostComponent(a){return typeof a=="string"}function appendOwnerState(a,i,o){return a===void 0||isHostComponent(a)?i:_extends({},i,{ownerState:_extends({},i.ownerState,o)})}const defaultContextValue={disableDefaultClasses:!1},ClassNameConfiguratorContext=reactExports.createContext(defaultContextValue);function useClassNamesOverride(a){const{disableDefaultClasses:i}=reactExports.useContext(ClassNameConfiguratorContext);return o=>i?"":a(o)}function extractEventHandlers(a,i=[]){if(a===void 0)return{};const o={};return Object.keys(a).filter(s=>s.match(/^on[A-Z]/)&&typeof a[s]=="function"&&!i.includes(s)).forEach(s=>{o[s]=a[s]}),o}function resolveComponentProps(a,i,o){return typeof a=="function"?a(i,o):a}function omitEventHandlers(a){if(a===void 0)return{};const i={};return Object.keys(a).filter(o=>!(o.match(/^on[A-Z]/)&&typeof a[o]=="function")).forEach(o=>{i[o]=a[o]}),i}function mergeSlotProps(a){const{getSlotProps:i,additionalProps:o,externalSlotProps:s,externalForwardedProps:$,className:j}=a;if(!i){const lt=clsx(o==null?void 0:o.className,j,$==null?void 0:$.className,s==null?void 0:s.className),ct=_extends({},o==null?void 0:o.style,$==null?void 0:$.style,s==null?void 0:s.style),rt=_extends({},o,$,s);return lt.length>0&&(rt.className=lt),Object.keys(ct).length>0&&(rt.style=ct),{props:rt,internalRef:void 0}}const _e=extractEventHandlers(_extends({},$,s)),et=omitEventHandlers(s),tt=omitEventHandlers($),nt=i(_e),at=clsx(nt==null?void 0:nt.className,o==null?void 0:o.className,j,$==null?void 0:$.className,s==null?void 0:s.className),it=_extends({},nt==null?void 0:nt.style,o==null?void 0:o.style,$==null?void 0:$.style,s==null?void 0:s.style),st=_extends({},nt,o,tt,et);return at.length>0&&(st.className=at),Object.keys(it).length>0&&(st.style=it),{props:st,internalRef:nt.ref}}const _excluded$1G=["elementType","externalSlotProps","ownerState","skipResolvingSlotProps"];function useSlotProps(a){var i;const{elementType:o,externalSlotProps:s,ownerState:$,skipResolvingSlotProps:j=!1}=a,_e=_objectWithoutPropertiesLoose(a,_excluded$1G),et=j?{}:resolveComponentProps(s,$),{props:tt,internalRef:nt}=mergeSlotProps(_extends({},_e,{externalSlotProps:et})),at=useForkRef(nt,et==null?void 0:et.ref,(i=a.additionalProps)==null?void 0:i.ref);return appendOwnerState(o,_extends({},tt,{ref:at}),$)}const _excluded$1F=["className","elementType","ownerState","externalForwardedProps","getSlotOwnerState","internalForwardedProps"],_excluded2$e=["component","slots","slotProps"],_excluded3$4=["component"];function useSlot(a,i){const{className:o,elementType:s,ownerState:$,externalForwardedProps:j,getSlotOwnerState:_e,internalForwardedProps:et}=i,tt=_objectWithoutPropertiesLoose(i,_excluded$1F),{component:nt,slots:at={[a]:void 0},slotProps:it={[a]:void 0}}=j,st=_objectWithoutPropertiesLoose(j,_excluded2$e),lt=at[a]||s,ct=resolveComponentProps(it[a],$),rt=mergeSlotProps(_extends({className:o},tt,{externalForwardedProps:a==="root"?st:void 0,externalSlotProps:ct})),{props:{component:ut},internalRef:ot}=rt,dt=_objectWithoutPropertiesLoose(rt.props,_excluded3$4),pt=useForkRef(ot,ct==null?void 0:ct.ref,i.ref),mt=_e?_e(dt):{},ft=_extends({},$,mt),ht=a==="root"?ut||nt:ut,yt=appendOwnerState(lt,_extends({},a==="root"&&!nt&&!at[a]&&et,a!=="root"&&!at[a]&&et,dt,ht&&{as:ht},{ref:pt}),ft);return Object.keys(mt).forEach(bt=>{delete yt[bt]}),[lt,yt]}function getAccordionUtilityClass(a){return generateUtilityClass$1("MuiAccordion",a)}const accordionClasses=generateUtilityClasses$1("MuiAccordion",["root","rounded","expanded","disabled","gutters","region"]),accordionClasses$1=accordionClasses,_excluded$1E=["children","className","defaultExpanded","disabled","disableGutters","expanded","onChange","square","slots","slotProps","TransitionComponent","TransitionProps"],useThemeProps$5=createUseThemeProps(),useUtilityClasses$1o=a=>{const{classes:i,square:o,expanded:s,disabled:$,disableGutters:j}=a;return composeClasses({root:["root",!o&&"rounded",s&&"expanded",$&&"disabled",!j&&"gutters"],region:["region"]},getAccordionUtilityClass,i)},AccordionRoot=styled(Paper$1,{name:"MuiAccordion",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`& .${accordionClasses$1.region}`]:i.region},i.root,!o.square&&i.rounded,!o.disableGutters&&i.gutters]}})(({theme:a})=>{const i={duration:a.transitions.duration.shortest};return{position:"relative",transition:a.transitions.create(["margin"],i),overflowAnchor:"none","&::before":{position:"absolute",left:0,top:-1,right:0,height:1,content:'""',opacity:1,backgroundColor:(a.vars||a).palette.divider,transition:a.transitions.create(["opacity","background-color"],i)},"&:first-of-type":{"&::before":{display:"none"}},[`&.${accordionClasses$1.expanded}`]:{"&::before":{opacity:0},"&:first-of-type":{marginTop:0},"&:last-of-type":{marginBottom:0},"& + &":{"&::before":{display:"none"}}},[`&.${accordionClasses$1.disabled}`]:{backgroundColor:(a.vars||a).palette.action.disabledBackground}}},({theme:a})=>({variants:[{props:i=>!i.square,style:{borderRadius:0,"&:first-of-type":{borderTopLeftRadius:(a.vars||a).shape.borderRadius,borderTopRightRadius:(a.vars||a).shape.borderRadius},"&:last-of-type":{borderBottomLeftRadius:(a.vars||a).shape.borderRadius,borderBottomRightRadius:(a.vars||a).shape.borderRadius,"@supports (-ms-ime-align: auto)":{borderBottomLeftRadius:0,borderBottomRightRadius:0}}}},{props:i=>!i.disableGutters,style:{[`&.${accordionClasses$1.expanded}`]:{margin:"16px 0"}}}]})),Accordion=reactExports.forwardRef(function(i,o){const s=useThemeProps$5({props:i,name:"MuiAccordion"}),{children:$,className:j,defaultExpanded:_e=!1,disabled:et=!1,disableGutters:tt=!1,expanded:nt,onChange:at,square:it=!1,slots:st={},slotProps:lt={},TransitionComponent:ct,TransitionProps:rt}=s,ut=_objectWithoutPropertiesLoose(s,_excluded$1E),[ot,dt]=useControlled({controlled:nt,default:_e,name:"Accordion",state:"expanded"}),pt=reactExports.useCallback($t=>{dt(!ot),at&&at($t,!ot)},[ot,at,dt]),[mt,...ft]=reactExports.Children.toArray($),ht=reactExports.useMemo(()=>({expanded:ot,disabled:et,disableGutters:tt,toggle:pt}),[ot,et,tt,pt]),yt=_extends({},s,{square:it,disabled:et,disableGutters:tt,expanded:ot}),bt=useUtilityClasses$1o(yt),gt=_extends({transition:ct},st),xt=_extends({transition:rt},lt),[vt,Lt]=useSlot("transition",{elementType:Collapse$1,externalForwardedProps:{slots:gt,slotProps:xt},ownerState:yt});return jsxRuntimeExports.jsxs(AccordionRoot,_extends({className:clsx(bt.root,j),ref:o,ownerState:yt,square:it},ut,{children:[jsxRuntimeExports.jsx(AccordionContext.Provider,{value:ht,children:mt}),jsxRuntimeExports.jsx(vt,_extends({in:ot,timeout:"auto"},Lt,{children:jsxRuntimeExports.jsx("div",{"aria-labelledby":mt.props.id,id:mt.props["aria-controls"],role:"region",className:bt.region,children:ft})}))]}))}),Accordion$1=Accordion;function getAccordionDetailsUtilityClass(a){return generateUtilityClass$1("MuiAccordionDetails",a)}generateUtilityClasses$1("MuiAccordionDetails",["root"]);const _excluded$1D=["className"],useThemeProps$4=createUseThemeProps(),useUtilityClasses$1n=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getAccordionDetailsUtilityClass,i)},AccordionDetailsRoot=styled("div",{name:"MuiAccordionDetails",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a})=>({padding:a.spacing(1,2,2)})),AccordionDetails=reactExports.forwardRef(function(i,o){const s=useThemeProps$4({props:i,name:"MuiAccordionDetails"}),{className:$}=s,j=_objectWithoutPropertiesLoose(s,_excluded$1D),_e=s,et=useUtilityClasses$1n(_e);return jsxRuntimeExports.jsx(AccordionDetailsRoot,_extends({className:clsx(et.root,$),ref:o,ownerState:_e},j))}),AccordionDetails$1=AccordionDetails;function Ripple(a){const{className:i,classes:o,pulsate:s=!1,rippleX:$,rippleY:j,rippleSize:_e,in:et,onExited:tt,timeout:nt}=a,[at,it]=reactExports.useState(!1),st=clsx(i,o.ripple,o.rippleVisible,s&&o.ripplePulsate),lt={width:_e,height:_e,top:-(_e/2)+j,left:-(_e/2)+$},ct=clsx(o.child,at&&o.childLeaving,s&&o.childPulsate);return!et&&!at&&it(!0),reactExports.useEffect(()=>{if(!et&&tt!=null){const rt=setTimeout(tt,nt);return()=>{clearTimeout(rt)}}},[tt,et,nt]),jsxRuntimeExports.jsx("span",{className:st,style:lt,children:jsxRuntimeExports.jsx("span",{className:ct})})}const touchRippleClasses=generateUtilityClasses$1("MuiTouchRipple",["root","ripple","rippleVisible","ripplePulsate","child","childLeaving","childPulsate"]),_excluded$1C=["center","classes","className"];let _$3=a=>a,_t$3,_t2$3,_t3$3,_t4$3;const DURATION=550,DELAY_RIPPLE=80,enterKeyframe=keyframes(_t$3||(_t$3=_$3`
  0% {
    transform: scale(0);
    opacity: 0.1;
  }

  100% {
    transform: scale(1);
    opacity: 0.3;
  }
`)),exitKeyframe=keyframes(_t2$3||(_t2$3=_$3`
  0% {
    opacity: 1;
  }

  100% {
    opacity: 0;
  }
`)),pulsateKeyframe=keyframes(_t3$3||(_t3$3=_$3`
  0% {
    transform: scale(1);
  }

  50% {
    transform: scale(0.92);
  }

  100% {
    transform: scale(1);
  }
`)),TouchRippleRoot=styled("span",{name:"MuiTouchRipple",slot:"Root"})({overflow:"hidden",pointerEvents:"none",position:"absolute",zIndex:0,top:0,right:0,bottom:0,left:0,borderRadius:"inherit"}),TouchRippleRipple=styled(Ripple,{name:"MuiTouchRipple",slot:"Ripple"})(_t4$3||(_t4$3=_$3`
  opacity: 0;
  position: absolute;

  &.${0} {
    opacity: 0.3;
    transform: scale(1);
    animation-name: ${0};
    animation-duration: ${0}ms;
    animation-timing-function: ${0};
  }

  &.${0} {
    animation-duration: ${0}ms;
  }

  & .${0} {
    opacity: 1;
    display: block;
    width: 100%;
    height: 100%;
    border-radius: 50%;
    background-color: currentColor;
  }

  & .${0} {
    opacity: 0;
    animation-name: ${0};
    animation-duration: ${0}ms;
    animation-timing-function: ${0};
  }

  & .${0} {
    position: absolute;
    /* @noflip */
    left: 0px;
    top: 0;
    animation-name: ${0};
    animation-duration: 2500ms;
    animation-timing-function: ${0};
    animation-iteration-count: infinite;
    animation-delay: 200ms;
  }
`),touchRippleClasses.rippleVisible,enterKeyframe,DURATION,({theme:a})=>a.transitions.easing.easeInOut,touchRippleClasses.ripplePulsate,({theme:a})=>a.transitions.duration.shorter,touchRippleClasses.child,touchRippleClasses.childLeaving,exitKeyframe,DURATION,({theme:a})=>a.transitions.easing.easeInOut,touchRippleClasses.childPulsate,pulsateKeyframe,({theme:a})=>a.transitions.easing.easeInOut),TouchRipple=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTouchRipple"}),{center:$=!1,classes:j={},className:_e}=s,et=_objectWithoutPropertiesLoose(s,_excluded$1C),[tt,nt]=reactExports.useState([]),at=reactExports.useRef(0),it=reactExports.useRef(null);reactExports.useEffect(()=>{it.current&&(it.current(),it.current=null)},[tt]);const st=reactExports.useRef(!1),lt=useTimeout(),ct=reactExports.useRef(null),rt=reactExports.useRef(null),ut=reactExports.useCallback(mt=>{const{pulsate:ft,rippleX:ht,rippleY:yt,rippleSize:bt,cb:gt}=mt;nt(xt=>[...xt,jsxRuntimeExports.jsx(TouchRippleRipple,{classes:{ripple:clsx(j.ripple,touchRippleClasses.ripple),rippleVisible:clsx(j.rippleVisible,touchRippleClasses.rippleVisible),ripplePulsate:clsx(j.ripplePulsate,touchRippleClasses.ripplePulsate),child:clsx(j.child,touchRippleClasses.child),childLeaving:clsx(j.childLeaving,touchRippleClasses.childLeaving),childPulsate:clsx(j.childPulsate,touchRippleClasses.childPulsate)},timeout:DURATION,pulsate:ft,rippleX:ht,rippleY:yt,rippleSize:bt},at.current)]),at.current+=1,it.current=gt},[j]),ot=reactExports.useCallback((mt={},ft={},ht=()=>{})=>{const{pulsate:yt=!1,center:bt=$||ft.pulsate,fakeElement:gt=!1}=ft;if((mt==null?void 0:mt.type)==="mousedown"&&st.current){st.current=!1;return}(mt==null?void 0:mt.type)==="touchstart"&&(st.current=!0);const xt=gt?null:rt.current,vt=xt?xt.getBoundingClientRect():{width:0,height:0,left:0,top:0};let Lt,$t,Tt;if(bt||mt===void 0||mt.clientX===0&&mt.clientY===0||!mt.clientX&&!mt.touches)Lt=Math.round(vt.width/2),$t=Math.round(vt.height/2);else{const{clientX:Et,clientY:Dt}=mt.touches&&mt.touches.length>0?mt.touches[0]:mt;Lt=Math.round(Et-vt.left),$t=Math.round(Dt-vt.top)}if(bt)Tt=Math.sqrt((2*vt.width**2+vt.height**2)/3),Tt%2===0&&(Tt+=1);else{const Et=Math.max(Math.abs((xt?xt.clientWidth:0)-Lt),Lt)*2+2,Dt=Math.max(Math.abs((xt?xt.clientHeight:0)-$t),$t)*2+2;Tt=Math.sqrt(Et**2+Dt**2)}mt!=null&&mt.touches?ct.current===null&&(ct.current=()=>{ut({pulsate:yt,rippleX:Lt,rippleY:$t,rippleSize:Tt,cb:ht})},lt.start(DELAY_RIPPLE,()=>{ct.current&&(ct.current(),ct.current=null)})):ut({pulsate:yt,rippleX:Lt,rippleY:$t,rippleSize:Tt,cb:ht})},[$,ut,lt]),dt=reactExports.useCallback(()=>{ot({},{pulsate:!0})},[ot]),pt=reactExports.useCallback((mt,ft)=>{if(lt.clear(),(mt==null?void 0:mt.type)==="touchend"&&ct.current){ct.current(),ct.current=null,lt.start(0,()=>{pt(mt,ft)});return}ct.current=null,nt(ht=>ht.length>0?ht.slice(1):ht),it.current=ft},[lt]);return reactExports.useImperativeHandle(o,()=>({pulsate:dt,start:ot,stop:pt}),[dt,ot,pt]),jsxRuntimeExports.jsx(TouchRippleRoot,_extends({className:clsx(touchRippleClasses.root,j.root,_e),ref:rt},et,{children:jsxRuntimeExports.jsx(TransitionGroup$1,{component:null,exit:!0,children:tt})}))}),TouchRipple$1=TouchRipple;function getButtonBaseUtilityClass(a){return generateUtilityClass$1("MuiButtonBase",a)}const buttonBaseClasses=generateUtilityClasses$1("MuiButtonBase",["root","disabled","focusVisible"]),_excluded$1B=["action","centerRipple","children","className","component","disabled","disableRipple","disableTouchRipple","focusRipple","focusVisibleClassName","LinkComponent","onBlur","onClick","onContextMenu","onDragLeave","onFocus","onFocusVisible","onKeyDown","onKeyUp","onMouseDown","onMouseLeave","onMouseUp","onTouchEnd","onTouchMove","onTouchStart","tabIndex","TouchRippleProps","touchRippleRef","type"],useUtilityClasses$1m=a=>{const{disabled:i,focusVisible:o,focusVisibleClassName:s,classes:$}=a,_e=composeClasses({root:["root",i&&"disabled",o&&"focusVisible"]},getButtonBaseUtilityClass,$);return o&&s&&(_e.root+=` ${s}`),_e},ButtonBaseRoot=styled("button",{name:"MuiButtonBase",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"inline-flex",alignItems:"center",justifyContent:"center",position:"relative",boxSizing:"border-box",WebkitTapHighlightColor:"transparent",backgroundColor:"transparent",outline:0,border:0,margin:0,borderRadius:0,padding:0,cursor:"pointer",userSelect:"none",verticalAlign:"middle",MozAppearance:"none",WebkitAppearance:"none",textDecoration:"none",color:"inherit","&::-moz-focus-inner":{borderStyle:"none"},[`&.${buttonBaseClasses.disabled}`]:{pointerEvents:"none",cursor:"default"},"@media print":{colorAdjust:"exact"}}),ButtonBase=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiButtonBase"}),{action:$,centerRipple:j=!1,children:_e,className:et,component:tt="button",disabled:nt=!1,disableRipple:at=!1,disableTouchRipple:it=!1,focusRipple:st=!1,LinkComponent:lt="a",onBlur:ct,onClick:rt,onContextMenu:ut,onDragLeave:ot,onFocus:dt,onFocusVisible:pt,onKeyDown:mt,onKeyUp:ft,onMouseDown:ht,onMouseLeave:yt,onMouseUp:bt,onTouchEnd:gt,onTouchMove:xt,onTouchStart:vt,tabIndex:Lt=0,TouchRippleProps:$t,touchRippleRef:Tt,type:Et}=s,Dt=_objectWithoutPropertiesLoose(s,_excluded$1B),It=reactExports.useRef(null),Ct=reactExports.useRef(null),jt=useForkRef(Ct,Tt),{isFocusVisibleRef:Zt,onFocus:Xt,onBlur:sn,ref:Ft}=useIsFocusVisible(),[wt,kt]=reactExports.useState(!1);nt&&wt&&kt(!1),reactExports.useImperativeHandle($,()=>({focusVisible:()=>{kt(!0),It.current.focus()}}),[]);const[At,Pt]=reactExports.useState(!1);reactExports.useEffect(()=>{Pt(!0)},[]);const Mt=At&&!at&&!nt;reactExports.useEffect(()=>{wt&&st&&!at&&At&&Ct.current.pulsate()},[at,st,wt,At]);function Ot(En,Bn,Hn=it){return useEventCallback(Wn=>(Bn&&Bn(Wn),!Hn&&Ct.current&&Ct.current[En](Wn),!0))}const Bt=Ot("start",ht),zt=Ot("stop",ut),Gt=Ot("stop",ot),Wt=Ot("stop",bt),qt=Ot("stop",En=>{wt&&En.preventDefault(),yt&&yt(En)}),tn=Ot("start",vt),ln=Ot("stop",gt),gn=Ot("stop",xt),yn=Ot("stop",En=>{sn(En),Zt.current===!1&&kt(!1),ct&&ct(En)},!1),Pn=useEventCallback(En=>{It.current||(It.current=En.currentTarget),Xt(En),Zt.current===!0&&(kt(!0),pt&&pt(En)),dt&&dt(En)}),cn=()=>{const En=It.current;return tt&&tt!=="button"&&!(En.tagName==="A"&&En.href)},xn=reactExports.useRef(!1),hn=useEventCallback(En=>{st&&!xn.current&&wt&&Ct.current&&En.key===" "&&(xn.current=!0,Ct.current.stop(En,()=>{Ct.current.start(En)})),En.target===En.currentTarget&&cn()&&En.key===" "&&En.preventDefault(),mt&&mt(En),En.target===En.currentTarget&&cn()&&En.key==="Enter"&&!nt&&(En.preventDefault(),rt&&rt(En))}),en=useEventCallback(En=>{st&&En.key===" "&&Ct.current&&wt&&!En.defaultPrevented&&(xn.current=!1,Ct.current.stop(En,()=>{Ct.current.pulsate(En)})),ft&&ft(En),rt&&En.target===En.currentTarget&&cn()&&En.key===" "&&!En.defaultPrevented&&rt(En)});let Jt=tt;Jt==="button"&&(Dt.href||Dt.to)&&(Jt=lt);const vn={};Jt==="button"?(vn.type=Et===void 0?"button":Et,vn.disabled=nt):(!Dt.href&&!Dt.to&&(vn.role="button"),nt&&(vn["aria-disabled"]=nt));const $n=useForkRef(o,Ft,It),Mn=_extends({},s,{centerRipple:j,component:tt,disabled:nt,disableRipple:at,disableTouchRipple:it,focusRipple:st,tabIndex:Lt,focusVisible:wt}),On=useUtilityClasses$1m(Mn);return jsxRuntimeExports.jsxs(ButtonBaseRoot,_extends({as:Jt,className:clsx(On.root,et),ownerState:Mn,onBlur:yn,onClick:rt,onContextMenu:zt,onFocus:Pn,onKeyDown:hn,onKeyUp:en,onMouseDown:Bt,onMouseLeave:qt,onMouseUp:Wt,onDragLeave:Gt,onTouchEnd:ln,onTouchMove:gn,onTouchStart:tn,ref:$n,tabIndex:nt?-1:Lt,type:Et},vn,Dt,{children:[_e,Mt?jsxRuntimeExports.jsx(TouchRipple$1,_extends({ref:jt,center:j},$t)):null]}))}),ButtonBase$1=ButtonBase;function getAccordionSummaryUtilityClass(a){return generateUtilityClass$1("MuiAccordionSummary",a)}const accordionSummaryClasses=generateUtilityClasses$1("MuiAccordionSummary",["root","expanded","focusVisible","disabled","gutters","contentGutters","content","expandIconWrapper"]),accordionSummaryClasses$1=accordionSummaryClasses,_excluded$1A=["children","className","expandIcon","focusVisibleClassName","onClick"],useThemeProps$3=createUseThemeProps(),useUtilityClasses$1l=a=>{const{classes:i,expanded:o,disabled:s,disableGutters:$}=a;return composeClasses({root:["root",o&&"expanded",s&&"disabled",!$&&"gutters"],focusVisible:["focusVisible"],content:["content",o&&"expanded",!$&&"contentGutters"],expandIconWrapper:["expandIconWrapper",o&&"expanded"]},getAccordionSummaryUtilityClass,i)},AccordionSummaryRoot=styled(ButtonBase$1,{name:"MuiAccordionSummary",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a})=>{const i={duration:a.transitions.duration.shortest};return{display:"flex",minHeight:48,padding:a.spacing(0,2),transition:a.transitions.create(["min-height","background-color"],i),[`&.${accordionSummaryClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},[`&.${accordionSummaryClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity},[`&:hover:not(.${accordionSummaryClasses$1.disabled})`]:{cursor:"pointer"},variants:[{props:o=>!o.disableGutters,style:{[`&.${accordionSummaryClasses$1.expanded}`]:{minHeight:64}}}]}}),AccordionSummaryContent=styled("div",{name:"MuiAccordionSummary",slot:"Content",overridesResolver:(a,i)=>i.content})(({theme:a})=>({display:"flex",flexGrow:1,margin:"12px 0",variants:[{props:i=>!i.disableGutters,style:{transition:a.transitions.create(["margin"],{duration:a.transitions.duration.shortest}),[`&.${accordionSummaryClasses$1.expanded}`]:{margin:"20px 0"}}}]})),AccordionSummaryExpandIconWrapper=styled("div",{name:"MuiAccordionSummary",slot:"ExpandIconWrapper",overridesResolver:(a,i)=>i.expandIconWrapper})(({theme:a})=>({display:"flex",color:(a.vars||a).palette.action.active,transform:"rotate(0deg)",transition:a.transitions.create("transform",{duration:a.transitions.duration.shortest}),[`&.${accordionSummaryClasses$1.expanded}`]:{transform:"rotate(180deg)"}})),AccordionSummary=reactExports.forwardRef(function(i,o){const s=useThemeProps$3({props:i,name:"MuiAccordionSummary"}),{children:$,className:j,expandIcon:_e,focusVisibleClassName:et,onClick:tt}=s,nt=_objectWithoutPropertiesLoose(s,_excluded$1A),{disabled:at=!1,disableGutters:it,expanded:st,toggle:lt}=reactExports.useContext(AccordionContext),ct=ot=>{lt&&lt(ot),tt&&tt(ot)},rt=_extends({},s,{expanded:st,disabled:at,disableGutters:it}),ut=useUtilityClasses$1l(rt);return jsxRuntimeExports.jsxs(AccordionSummaryRoot,_extends({focusRipple:!1,disableRipple:!0,disabled:at,component:"div","aria-expanded":st,className:clsx(ut.root,j),focusVisibleClassName:clsx(ut.focusVisible,et),onClick:ct,ref:o,ownerState:rt},nt,{children:[jsxRuntimeExports.jsx(AccordionSummaryContent,{className:ut.content,ownerState:rt,children:$}),_e&&jsxRuntimeExports.jsx(AccordionSummaryExpandIconWrapper,{className:ut.expandIconWrapper,ownerState:rt,children:_e})]}))}),AccordionSummary$1=AccordionSummary;function getAlertUtilityClass(a){return generateUtilityClass$1("MuiAlert",a)}const alertClasses=generateUtilityClasses$1("MuiAlert",["root","action","icon","message","filled","colorSuccess","colorInfo","colorWarning","colorError","filledSuccess","filledInfo","filledWarning","filledError","outlined","outlinedSuccess","outlinedInfo","outlinedWarning","outlinedError","standard","standardSuccess","standardInfo","standardWarning","standardError"]),alertClasses$1=alertClasses;function getIconButtonUtilityClass(a){return generateUtilityClass$1("MuiIconButton",a)}const iconButtonClasses=generateUtilityClasses$1("MuiIconButton",["root","disabled","colorInherit","colorPrimary","colorSecondary","colorError","colorInfo","colorSuccess","colorWarning","edgeStart","edgeEnd","sizeSmall","sizeMedium","sizeLarge"]),_excluded$1z=["edge","children","className","color","disabled","disableFocusRipple","size"],useUtilityClasses$1k=a=>{const{classes:i,disabled:o,color:s,edge:$,size:j}=a,_e={root:["root",o&&"disabled",s!=="default"&&`color${capitalize$2(s)}`,$&&`edge${capitalize$2($)}`,`size${capitalize$2(j)}`]};return composeClasses(_e,getIconButtonUtilityClass,i)},IconButtonRoot=styled(ButtonBase$1,{name:"MuiIconButton",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.color!=="default"&&i[`color${capitalize$2(o.color)}`],o.edge&&i[`edge${capitalize$2(o.edge)}`],i[`size${capitalize$2(o.size)}`]]}})(({theme:a,ownerState:i})=>_extends({textAlign:"center",flex:"0 0 auto",fontSize:a.typography.pxToRem(24),padding:8,borderRadius:"50%",overflow:"visible",color:(a.vars||a).palette.action.active,transition:a.transitions.create("background-color",{duration:a.transitions.duration.shortest})},!i.disableRipple&&{"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.activeChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(a.palette.action.active,a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}}},i.edge==="start"&&{marginLeft:i.size==="small"?-3:-12},i.edge==="end"&&{marginRight:i.size==="small"?-3:-12}),({theme:a,ownerState:i})=>{var o;const s=(o=(a.vars||a).palette)==null?void 0:o[i.color];return _extends({},i.color==="inherit"&&{color:"inherit"},i.color!=="inherit"&&i.color!=="default"&&_extends({color:s==null?void 0:s.main},!i.disableRipple&&{"&:hover":_extends({},s&&{backgroundColor:a.vars?`rgba(${s.mainChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(s.main,a.palette.action.hoverOpacity)},{"@media (hover: none)":{backgroundColor:"transparent"}})}),i.size==="small"&&{padding:5,fontSize:a.typography.pxToRem(18)},i.size==="large"&&{padding:12,fontSize:a.typography.pxToRem(28)},{[`&.${iconButtonClasses.disabled}`]:{backgroundColor:"transparent",color:(a.vars||a).palette.action.disabled}})}),IconButton=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiIconButton"}),{edge:$=!1,children:j,className:_e,color:et="default",disabled:tt=!1,disableFocusRipple:nt=!1,size:at="medium"}=s,it=_objectWithoutPropertiesLoose(s,_excluded$1z),st=_extends({},s,{edge:$,color:et,disabled:tt,disableFocusRipple:nt,size:at}),lt=useUtilityClasses$1k(st);return jsxRuntimeExports.jsx(IconButtonRoot,_extends({className:clsx(lt.root,_e),centerRipple:!0,focusRipple:!nt,disabled:tt,ref:o},it,{ownerState:st,children:j}))}),IconButton$1=IconButton,SuccessOutlinedIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M20,12A8,8 0 0,1 12,20A8,8 0 0,1 4,12A8,8 0 0,1 12,4C12.76,4 13.5,4.11 14.2, 4.31L15.77,2.74C14.61,2.26 13.34,2 12,2A10,10 0 0,0 2,12A10,10 0 0,0 12,22A10,10 0 0, 0 22,12M7.91,10.08L6.5,11.5L11,16L21,6L19.59,4.58L11,13.17L7.91,10.08Z"}),"SuccessOutlined"),ReportProblemOutlinedIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M12 5.99L19.53 19H4.47L12 5.99M12 2L1 21h22L12 2zm1 14h-2v2h2v-2zm0-6h-2v4h2v-4z"}),"ReportProblemOutlined"),ErrorOutlineIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M11 15h2v2h-2zm0-8h2v6h-2zm.99-5C6.47 2 2 6.48 2 12s4.47 10 9.99 10C17.52 22 22 17.52 22 12S17.52 2 11.99 2zM12 20c-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8-3.58 8-8 8z"}),"ErrorOutline"),InfoOutlinedIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M11,9H13V7H11M12,20C7.59,20 4,16.41 4,12C4,7.59 7.59,4 12,4C16.41,4 20,7.59 20, 12C20,16.41 16.41,20 12,20M12,2A10,10 0 0,0 2,12A10,10 0 0,0 12,22A10,10 0 0,0 22,12A10, 10 0 0,0 12,2M11,17H13V11H11V17Z"}),"InfoOutlined"),ClearIcon$1=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"}),"Close"),_excluded$1y=["action","children","className","closeText","color","components","componentsProps","icon","iconMapping","onClose","role","severity","slotProps","slots","variant"],useThemeProps$2=createUseThemeProps(),useUtilityClasses$1j=a=>{const{variant:i,color:o,severity:s,classes:$}=a,j={root:["root",`color${capitalize$2(o||s)}`,`${i}${capitalize$2(o||s)}`,`${i}`],icon:["icon"],message:["message"],action:["action"]};return composeClasses(j,getAlertUtilityClass,$)},AlertRoot=styled(Paper$1,{name:"MuiAlert",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],i[`${o.variant}${capitalize$2(o.color||o.severity)}`]]}})(({theme:a})=>{const i=a.palette.mode==="light"?darken_1:lighten_1,o=a.palette.mode==="light"?lighten_1:darken_1;return _extends({},a.typography.body2,{backgroundColor:"transparent",display:"flex",padding:"6px 16px",variants:[...Object.entries(a.palette).filter(([,s])=>s.main&&s.light).map(([s])=>({props:{colorSeverity:s,variant:"standard"},style:{color:a.vars?a.vars.palette.Alert[`${s}Color`]:i(a.palette[s].light,.6),backgroundColor:a.vars?a.vars.palette.Alert[`${s}StandardBg`]:o(a.palette[s].light,.9),[`& .${alertClasses$1.icon}`]:a.vars?{color:a.vars.palette.Alert[`${s}IconColor`]}:{color:a.palette[s].main}}})),...Object.entries(a.palette).filter(([,s])=>s.main&&s.light).map(([s])=>({props:{colorSeverity:s,variant:"outlined"},style:{color:a.vars?a.vars.palette.Alert[`${s}Color`]:i(a.palette[s].light,.6),border:`1px solid ${(a.vars||a).palette[s].light}`,[`& .${alertClasses$1.icon}`]:a.vars?{color:a.vars.palette.Alert[`${s}IconColor`]}:{color:a.palette[s].main}}})),...Object.entries(a.palette).filter(([,s])=>s.main&&s.dark).map(([s])=>({props:{colorSeverity:s,variant:"filled"},style:_extends({fontWeight:a.typography.fontWeightMedium},a.vars?{color:a.vars.palette.Alert[`${s}FilledColor`],backgroundColor:a.vars.palette.Alert[`${s}FilledBg`]}:{backgroundColor:a.palette.mode==="dark"?a.palette[s].dark:a.palette[s].main,color:a.palette.getContrastText(a.palette[s].main)})}))]})}),AlertIcon=styled("div",{name:"MuiAlert",slot:"Icon",overridesResolver:(a,i)=>i.icon})({marginRight:12,padding:"7px 0",display:"flex",fontSize:22,opacity:.9}),AlertMessage=styled("div",{name:"MuiAlert",slot:"Message",overridesResolver:(a,i)=>i.message})({padding:"8px 0",minWidth:0,overflow:"auto"}),AlertAction=styled("div",{name:"MuiAlert",slot:"Action",overridesResolver:(a,i)=>i.action})({display:"flex",alignItems:"flex-start",padding:"4px 0 0 16px",marginLeft:"auto",marginRight:-8}),defaultIconMapping={success:jsxRuntimeExports.jsx(SuccessOutlinedIcon,{fontSize:"inherit"}),warning:jsxRuntimeExports.jsx(ReportProblemOutlinedIcon,{fontSize:"inherit"}),error:jsxRuntimeExports.jsx(ErrorOutlineIcon,{fontSize:"inherit"}),info:jsxRuntimeExports.jsx(InfoOutlinedIcon,{fontSize:"inherit"})},Alert=reactExports.forwardRef(function(i,o){const s=useThemeProps$2({props:i,name:"MuiAlert"}),{action:$,children:j,className:_e,closeText:et="Close",color:tt,components:nt={},componentsProps:at={},icon:it,iconMapping:st=defaultIconMapping,onClose:lt,role:ct="alert",severity:rt="success",slotProps:ut={},slots:ot={},variant:dt="standard"}=s,pt=_objectWithoutPropertiesLoose(s,_excluded$1y),mt=_extends({},s,{color:tt,severity:rt,variant:dt,colorSeverity:tt||rt}),ft=useUtilityClasses$1j(mt),ht={slots:_extends({closeButton:nt.CloseButton,closeIcon:nt.CloseIcon},ot),slotProps:_extends({},at,ut)},[yt,bt]=useSlot("closeButton",{elementType:IconButton$1,externalForwardedProps:ht,ownerState:mt}),[gt,xt]=useSlot("closeIcon",{elementType:ClearIcon$1,externalForwardedProps:ht,ownerState:mt});return jsxRuntimeExports.jsxs(AlertRoot,_extends({role:ct,elevation:0,ownerState:mt,className:clsx(ft.root,_e),ref:o},pt,{children:[it!==!1?jsxRuntimeExports.jsx(AlertIcon,{ownerState:mt,className:ft.icon,children:it||st[rt]||defaultIconMapping[rt]}):null,jsxRuntimeExports.jsx(AlertMessage,{ownerState:mt,className:ft.message,children:j}),$!=null?jsxRuntimeExports.jsx(AlertAction,{ownerState:mt,className:ft.action,children:$}):null,$==null&&lt?jsxRuntimeExports.jsx(AlertAction,{ownerState:mt,className:ft.action,children:jsxRuntimeExports.jsx(yt,_extends({size:"small","aria-label":et,title:et,color:"inherit",onClick:lt},bt,{children:jsxRuntimeExports.jsx(gt,_extends({fontSize:"small"},xt))}))}):null]}))}),Alert$1=Alert;function getTypographyUtilityClass(a){return generateUtilityClass$1("MuiTypography",a)}generateUtilityClasses$1("MuiTypography",["root","h1","h2","h3","h4","h5","h6","subtitle1","subtitle2","body1","body2","inherit","button","caption","overline","alignLeft","alignRight","alignCenter","alignJustify","noWrap","gutterBottom","paragraph"]);const _excluded$1x=["align","className","component","gutterBottom","noWrap","paragraph","variant","variantMapping"],useUtilityClasses$1i=a=>{const{align:i,gutterBottom:o,noWrap:s,paragraph:$,variant:j,classes:_e}=a,et={root:["root",j,a.align!=="inherit"&&`align${capitalize$2(i)}`,o&&"gutterBottom",s&&"noWrap",$&&"paragraph"]};return composeClasses(et,getTypographyUtilityClass,_e)},TypographyRoot=styled("span",{name:"MuiTypography",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.variant&&i[o.variant],o.align!=="inherit"&&i[`align${capitalize$2(o.align)}`],o.noWrap&&i.noWrap,o.gutterBottom&&i.gutterBottom,o.paragraph&&i.paragraph]}})(({theme:a,ownerState:i})=>_extends({margin:0},i.variant==="inherit"&&{font:"inherit"},i.variant!=="inherit"&&a.typography[i.variant],i.align!=="inherit"&&{textAlign:i.align},i.noWrap&&{overflow:"hidden",textOverflow:"ellipsis",whiteSpace:"nowrap"},i.gutterBottom&&{marginBottom:"0.35em"},i.paragraph&&{marginBottom:16})),defaultVariantMapping={h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",subtitle1:"h6",subtitle2:"h6",body1:"p",body2:"p",inherit:"p"},colorTransformations$1={primary:"primary.main",textPrimary:"text.primary",secondary:"secondary.main",textSecondary:"text.secondary",error:"error.main"},transformDeprecatedColors$1=a=>colorTransformations$1[a]||a,Typography=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTypography"}),$=transformDeprecatedColors$1(s.color),j=extendSxProp(_extends({},s,{color:$})),{align:_e="inherit",className:et,component:tt,gutterBottom:nt=!1,noWrap:at=!1,paragraph:it=!1,variant:st="body1",variantMapping:lt=defaultVariantMapping}=j,ct=_objectWithoutPropertiesLoose(j,_excluded$1x),rt=_extends({},j,{align:_e,color:$,className:et,component:tt,gutterBottom:nt,noWrap:at,paragraph:it,variant:st,variantMapping:lt}),ut=tt||(it?"p":lt[st]||defaultVariantMapping[st])||"span",ot=useUtilityClasses$1i(rt);return jsxRuntimeExports.jsx(TypographyRoot,_extends({as:ut,ref:o,ownerState:rt,className:clsx(ot.root,et)},ct))}),Typography$1=Typography;function getAppBarUtilityClass(a){return generateUtilityClass$1("MuiAppBar",a)}generateUtilityClasses$1("MuiAppBar",["root","positionFixed","positionAbsolute","positionSticky","positionStatic","positionRelative","colorDefault","colorPrimary","colorSecondary","colorInherit","colorTransparent","colorError","colorInfo","colorSuccess","colorWarning"]);const _excluded$1w=["className","color","enableColorOnDark","position"],useUtilityClasses$1h=a=>{const{color:i,position:o,classes:s}=a,$={root:["root",`color${capitalize$2(i)}`,`position${capitalize$2(o)}`]};return composeClasses($,getAppBarUtilityClass,s)},joinVars=(a,i)=>a?`${a==null?void 0:a.replace(")","")}, ${i})`:i,AppBarRoot=styled(Paper$1,{name:"MuiAppBar",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[`position${capitalize$2(o.position)}`],i[`color${capitalize$2(o.color)}`]]}})(({theme:a,ownerState:i})=>{const o=a.palette.mode==="light"?a.palette.grey[100]:a.palette.grey[900];return _extends({display:"flex",flexDirection:"column",width:"100%",boxSizing:"border-box",flexShrink:0},i.position==="fixed"&&{position:"fixed",zIndex:(a.vars||a).zIndex.appBar,top:0,left:"auto",right:0,"@media print":{position:"absolute"}},i.position==="absolute"&&{position:"absolute",zIndex:(a.vars||a).zIndex.appBar,top:0,left:"auto",right:0},i.position==="sticky"&&{position:"sticky",zIndex:(a.vars||a).zIndex.appBar,top:0,left:"auto",right:0},i.position==="static"&&{position:"static"},i.position==="relative"&&{position:"relative"},!a.vars&&_extends({},i.color==="default"&&{backgroundColor:o,color:a.palette.getContrastText(o)},i.color&&i.color!=="default"&&i.color!=="inherit"&&i.color!=="transparent"&&{backgroundColor:a.palette[i.color].main,color:a.palette[i.color].contrastText},i.color==="inherit"&&{color:"inherit"},a.palette.mode==="dark"&&!i.enableColorOnDark&&{backgroundColor:null,color:null},i.color==="transparent"&&_extends({backgroundColor:"transparent",color:"inherit"},a.palette.mode==="dark"&&{backgroundImage:"none"})),a.vars&&_extends({},i.color==="default"&&{"--AppBar-background":i.enableColorOnDark?a.vars.palette.AppBar.defaultBg:joinVars(a.vars.palette.AppBar.darkBg,a.vars.palette.AppBar.defaultBg),"--AppBar-color":i.enableColorOnDark?a.vars.palette.text.primary:joinVars(a.vars.palette.AppBar.darkColor,a.vars.palette.text.primary)},i.color&&!i.color.match(/^(default|inherit|transparent)$/)&&{"--AppBar-background":i.enableColorOnDark?a.vars.palette[i.color].main:joinVars(a.vars.palette.AppBar.darkBg,a.vars.palette[i.color].main),"--AppBar-color":i.enableColorOnDark?a.vars.palette[i.color].contrastText:joinVars(a.vars.palette.AppBar.darkColor,a.vars.palette[i.color].contrastText)},{backgroundColor:"var(--AppBar-background)",color:i.color==="inherit"?"inherit":"var(--AppBar-color)"},i.color==="transparent"&&{backgroundImage:"none",backgroundColor:"transparent",color:"inherit"}))}),AppBar=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiAppBar"}),{className:$,color:j="primary",enableColorOnDark:_e=!1,position:et="fixed"}=s,tt=_objectWithoutPropertiesLoose(s,_excluded$1w),nt=_extends({},s,{color:j,position:et,enableColorOnDark:_e}),at=useUtilityClasses$1h(nt);return jsxRuntimeExports.jsx(AppBarRoot,_extends({square:!0,component:"header",ownerState:nt,elevation:4,className:clsx(at.root,$,et==="fixed"&&"mui-fixed"),ref:o},tt))}),AppBar$1=AppBar;function useBadge(a){const{badgeContent:i,invisible:o=!1,max:s=99,showZero:$=!1}=a,j=usePreviousProps({badgeContent:i,max:s});let _e=o;o===!1&&i===0&&!$&&(_e=!0);const{badgeContent:et,max:tt=s}=_e?j:a,nt=et&&Number(et)>tt?`${tt}+`:et;return{badgeContent:et,invisible:_e,max:tt,displayValue:nt}}const GLOBAL_CLASS_PREFIX="base";function buildStateClass(a){return`${GLOBAL_CLASS_PREFIX}--${a}`}function buildSlotClass(a,i){return`${GLOBAL_CLASS_PREFIX}-${a}-${i}`}function generateUtilityClass(a,i){const o=globalStateClasses[i];return o?buildStateClass(o):buildSlotClass(a,i)}function generateUtilityClasses(a,i){const o={};return i.forEach(s=>{o[s]=generateUtilityClass(a,s)}),o}function mapEventPropToEvent(a){return a.substring(2).toLowerCase()}function clickedRootScrollbar$1(a,i){return i.documentElement.clientWidth<a.clientX||i.documentElement.clientHeight<a.clientY}function ClickAwayListener(a){const{children:i,disableReactTree:o=!1,mouseEvent:s="onClick",onClickAway:$,touchEvent:j="onTouchEnd"}=a,_e=reactExports.useRef(!1),et=reactExports.useRef(null),tt=reactExports.useRef(!1),nt=reactExports.useRef(!1);reactExports.useEffect(()=>(setTimeout(()=>{tt.current=!0},0),()=>{tt.current=!1}),[]);const at=useForkRef(i.ref,et),it=useEventCallback(ct=>{const rt=nt.current;nt.current=!1;const ut=ownerDocument(et.current);if(!tt.current||!et.current||"clientX"in ct&&clickedRootScrollbar$1(ct,ut))return;if(_e.current){_e.current=!1;return}let ot;ct.composedPath?ot=ct.composedPath().indexOf(et.current)>-1:ot=!ut.documentElement.contains(ct.target)||et.current.contains(ct.target),!ot&&(o||!rt)&&$(ct)}),st=ct=>rt=>{nt.current=!0;const ut=i.props[ct];ut&&ut(rt)},lt={ref:at};return j!==!1&&(lt[j]=st(j)),reactExports.useEffect(()=>{if(j!==!1){const ct=mapEventPropToEvent(j),rt=ownerDocument(et.current),ut=()=>{_e.current=!0};return rt.addEventListener(ct,it),rt.addEventListener("touchmove",ut),()=>{rt.removeEventListener(ct,it),rt.removeEventListener("touchmove",ut)}}},[it,j]),s!==!1&&(lt[s]=st(s)),reactExports.useEffect(()=>{if(s!==!1){const ct=mapEventPropToEvent(s),rt=ownerDocument(et.current);return rt.addEventListener(ct,it),()=>{rt.removeEventListener(ct,it)}}},[it,s]),jsxRuntimeExports.jsx(reactExports.Fragment,{children:reactExports.cloneElement(i,lt)})}const candidatesSelector=["input","select","textarea","a[href]","button","[tabindex]","audio[controls]","video[controls]",'[contenteditable]:not([contenteditable="false"])'].join(",");function getTabIndex(a){const i=parseInt(a.getAttribute("tabindex")||"",10);return Number.isNaN(i)?a.contentEditable==="true"||(a.nodeName==="AUDIO"||a.nodeName==="VIDEO"||a.nodeName==="DETAILS")&&a.getAttribute("tabindex")===null?0:a.tabIndex:i}function isNonTabbableRadio(a){if(a.tagName!=="INPUT"||a.type!=="radio"||!a.name)return!1;const i=s=>a.ownerDocument.querySelector(`input[type="radio"]${s}`);let o=i(`[name="${a.name}"]:checked`);return o||(o=i(`[name="${a.name}"]`)),o!==a}function isNodeMatchingSelectorFocusable(a){return!(a.disabled||a.tagName==="INPUT"&&a.type==="hidden"||isNonTabbableRadio(a))}function defaultGetTabbable(a){const i=[],o=[];return Array.from(a.querySelectorAll(candidatesSelector)).forEach((s,$)=>{const j=getTabIndex(s);j===-1||!isNodeMatchingSelectorFocusable(s)||(j===0?i.push(s):o.push({documentOrder:$,tabIndex:j,node:s}))}),o.sort((s,$)=>s.tabIndex===$.tabIndex?s.documentOrder-$.documentOrder:s.tabIndex-$.tabIndex).map(s=>s.node).concat(i)}function defaultIsEnabled(){return!0}function FocusTrap(a){const{children:i,disableAutoFocus:o=!1,disableEnforceFocus:s=!1,disableRestoreFocus:$=!1,getTabbable:j=defaultGetTabbable,isEnabled:_e=defaultIsEnabled,open:et}=a,tt=reactExports.useRef(!1),nt=reactExports.useRef(null),at=reactExports.useRef(null),it=reactExports.useRef(null),st=reactExports.useRef(null),lt=reactExports.useRef(!1),ct=reactExports.useRef(null),rt=useForkRef(i.ref,ct),ut=reactExports.useRef(null);reactExports.useEffect(()=>{!et||!ct.current||(lt.current=!o)},[o,et]),reactExports.useEffect(()=>{if(!et||!ct.current)return;const pt=ownerDocument(ct.current);return ct.current.contains(pt.activeElement)||(ct.current.hasAttribute("tabIndex")||ct.current.setAttribute("tabIndex","-1"),lt.current&&ct.current.focus()),()=>{$||(it.current&&it.current.focus&&(tt.current=!0,it.current.focus()),it.current=null)}},[et]),reactExports.useEffect(()=>{if(!et||!ct.current)return;const pt=ownerDocument(ct.current),mt=yt=>{ut.current=yt,!(s||!_e()||yt.key!=="Tab")&&pt.activeElement===ct.current&&yt.shiftKey&&(tt.current=!0,at.current&&at.current.focus())},ft=()=>{const yt=ct.current;if(yt===null)return;if(!pt.hasFocus()||!_e()||tt.current){tt.current=!1;return}if(yt.contains(pt.activeElement)||s&&pt.activeElement!==nt.current&&pt.activeElement!==at.current)return;if(pt.activeElement!==st.current)st.current=null;else if(st.current!==null)return;if(!lt.current)return;let bt=[];if((pt.activeElement===nt.current||pt.activeElement===at.current)&&(bt=j(ct.current)),bt.length>0){var gt,xt;const vt=!!((gt=ut.current)!=null&&gt.shiftKey&&((xt=ut.current)==null?void 0:xt.key)==="Tab"),Lt=bt[0],$t=bt[bt.length-1];typeof Lt!="string"&&typeof $t!="string"&&(vt?$t.focus():Lt.focus())}else yt.focus()};pt.addEventListener("focusin",ft),pt.addEventListener("keydown",mt,!0);const ht=setInterval(()=>{pt.activeElement&&pt.activeElement.tagName==="BODY"&&ft()},50);return()=>{clearInterval(ht),pt.removeEventListener("focusin",ft),pt.removeEventListener("keydown",mt,!0)}},[o,s,$,_e,et,j]);const ot=pt=>{it.current===null&&(it.current=pt.relatedTarget),lt.current=!0,st.current=pt.target;const mt=i.props.onFocus;mt&&mt(pt)},dt=pt=>{it.current===null&&(it.current=pt.relatedTarget),lt.current=!0};return jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx("div",{tabIndex:et?0:-1,onFocus:dt,ref:nt,"data-testid":"sentinelStart"}),reactExports.cloneElement(i,{ref:rt,onFocus:ot}),jsxRuntimeExports.jsx("div",{tabIndex:et?0:-1,onFocus:dt,ref:at,"data-testid":"sentinelEnd"})]})}function getContainer$1(a){return typeof a=="function"?a():a}const Portal=reactExports.forwardRef(function(i,o){const{children:s,container:$,disablePortal:j=!1}=i,[_e,et]=reactExports.useState(null),tt=useForkRef(reactExports.isValidElement(s)?s.ref:null,o);if(useEnhancedEffect(()=>{j||et(getContainer$1($)||document.body)},[$,j]),useEnhancedEffect(()=>{if(_e&&!j)return setRef(o,_e),()=>{setRef(o,null)}},[o,_e,j]),j){if(reactExports.isValidElement(s)){const nt={ref:tt};return reactExports.cloneElement(s,nt)}return jsxRuntimeExports.jsx(reactExports.Fragment,{children:s})}return jsxRuntimeExports.jsx(reactExports.Fragment,{children:_e&&reactDomExports.createPortal(s,_e)})});function isOverflowing(a){const i=ownerDocument(a);return i.body===a?ownerWindow(a).innerWidth>i.documentElement.clientWidth:a.scrollHeight>a.clientHeight}function ariaHidden(a,i){i?a.setAttribute("aria-hidden","true"):a.removeAttribute("aria-hidden")}function getPaddingRight(a){return parseInt(ownerWindow(a).getComputedStyle(a).paddingRight,10)||0}function isAriaHiddenForbiddenOnElement(a){const o=["TEMPLATE","SCRIPT","STYLE","LINK","MAP","META","NOSCRIPT","PICTURE","COL","COLGROUP","PARAM","SLOT","SOURCE","TRACK"].indexOf(a.tagName)!==-1,s=a.tagName==="INPUT"&&a.getAttribute("type")==="hidden";return o||s}function ariaHiddenSiblings(a,i,o,s,$){const j=[i,o,...s];[].forEach.call(a.children,_e=>{const et=j.indexOf(_e)===-1,tt=!isAriaHiddenForbiddenOnElement(_e);et&&tt&&ariaHidden(_e,$)})}function findIndexOf(a,i){let o=-1;return a.some((s,$)=>i(s)?(o=$,!0):!1),o}function handleContainer(a,i){const o=[],s=a.container;if(!i.disableScrollLock){if(isOverflowing(s)){const _e=getScrollbarSize(ownerDocument(s));o.push({value:s.style.paddingRight,property:"padding-right",el:s}),s.style.paddingRight=`${getPaddingRight(s)+_e}px`;const et=ownerDocument(s).querySelectorAll(".mui-fixed");[].forEach.call(et,tt=>{o.push({value:tt.style.paddingRight,property:"padding-right",el:tt}),tt.style.paddingRight=`${getPaddingRight(tt)+_e}px`})}let j;if(s.parentNode instanceof DocumentFragment)j=ownerDocument(s).body;else{const _e=s.parentElement,et=ownerWindow(s);j=(_e==null?void 0:_e.nodeName)==="HTML"&&et.getComputedStyle(_e).overflowY==="scroll"?_e:s}o.push({value:j.style.overflow,property:"overflow",el:j},{value:j.style.overflowX,property:"overflow-x",el:j},{value:j.style.overflowY,property:"overflow-y",el:j}),j.style.overflow="hidden"}return()=>{o.forEach(({value:j,el:_e,property:et})=>{j?_e.style.setProperty(et,j):_e.style.removeProperty(et)})}}function getHiddenSiblings(a){const i=[];return[].forEach.call(a.children,o=>{o.getAttribute("aria-hidden")==="true"&&i.push(o)}),i}class ModalManager{constructor(){this.containers=void 0,this.modals=void 0,this.modals=[],this.containers=[]}add(i,o){let s=this.modals.indexOf(i);if(s!==-1)return s;s=this.modals.length,this.modals.push(i),i.modalRef&&ariaHidden(i.modalRef,!1);const $=getHiddenSiblings(o);ariaHiddenSiblings(o,i.mount,i.modalRef,$,!0);const j=findIndexOf(this.containers,_e=>_e.container===o);return j!==-1?(this.containers[j].modals.push(i),s):(this.containers.push({modals:[i],container:o,restore:null,hiddenSiblings:$}),s)}mount(i,o){const s=findIndexOf(this.containers,j=>j.modals.indexOf(i)!==-1),$=this.containers[s];$.restore||($.restore=handleContainer($,o))}remove(i,o=!0){const s=this.modals.indexOf(i);if(s===-1)return s;const $=findIndexOf(this.containers,_e=>_e.modals.indexOf(i)!==-1),j=this.containers[$];if(j.modals.splice(j.modals.indexOf(i),1),this.modals.splice(s,1),j.modals.length===0)j.restore&&j.restore(),i.modalRef&&ariaHidden(i.modalRef,o),ariaHiddenSiblings(j.container,i.mount,i.modalRef,j.hiddenSiblings,!1),this.containers.splice($,1);else{const _e=j.modals[j.modals.length-1];_e.modalRef&&ariaHidden(_e.modalRef,!1)}return s}isTopModal(i){return this.modals.length>0&&this.modals[this.modals.length-1]===i}}function getContainer(a){return typeof a=="function"?a():a}function getHasTransition(a){return a?a.props.hasOwnProperty("in"):!1}const defaultManager=new ModalManager;function useModal(a){const{container:i,disableEscapeKeyDown:o=!1,disableScrollLock:s=!1,manager:$=defaultManager,closeAfterTransition:j=!1,onTransitionEnter:_e,onTransitionExited:et,children:tt,onClose:nt,open:at,rootRef:it}=a,st=reactExports.useRef({}),lt=reactExports.useRef(null),ct=reactExports.useRef(null),rt=useForkRef(ct,it),[ut,ot]=reactExports.useState(!at),dt=getHasTransition(tt);let pt=!0;(a["aria-hidden"]==="false"||a["aria-hidden"]===!1)&&(pt=!1);const mt=()=>ownerDocument(lt.current),ft=()=>(st.current.modalRef=ct.current,st.current.mount=lt.current,st.current),ht=()=>{$.mount(ft(),{disableScrollLock:s}),ct.current&&(ct.current.scrollTop=0)},yt=useEventCallback(()=>{const Dt=getContainer(i)||mt().body;$.add(ft(),Dt),ct.current&&ht()}),bt=reactExports.useCallback(()=>$.isTopModal(ft()),[$]),gt=useEventCallback(Dt=>{lt.current=Dt,Dt&&(at&&bt()?ht():ct.current&&ariaHidden(ct.current,pt))}),xt=reactExports.useCallback(()=>{$.remove(ft(),pt)},[pt,$]);reactExports.useEffect(()=>()=>{xt()},[xt]),reactExports.useEffect(()=>{at?yt():(!dt||!j)&&xt()},[at,xt,dt,j,yt]);const vt=Dt=>It=>{var Ct;(Ct=Dt.onKeyDown)==null||Ct.call(Dt,It),!(It.key!=="Escape"||It.which===229||!bt())&&(o||(It.stopPropagation(),nt&&nt(It,"escapeKeyDown")))},Lt=Dt=>It=>{var Ct;(Ct=Dt.onClick)==null||Ct.call(Dt,It),It.target===It.currentTarget&&nt&&nt(It,"backdropClick")};return{getRootProps:(Dt={})=>{const It=extractEventHandlers(a);delete It.onTransitionEnter,delete It.onTransitionExited;const Ct=_extends({},It,Dt);return _extends({role:"presentation"},Ct,{onKeyDown:vt(Ct),ref:rt})},getBackdropProps:(Dt={})=>{const It=Dt;return _extends({"aria-hidden":!0},It,{onClick:Lt(It),open:at})},getTransitionProps:()=>{const Dt=()=>{ot(!1),_e&&_e()},It=()=>{ot(!0),et&&et(),j&&xt()};return{onEnter:createChainedFunction(Dt,tt==null?void 0:tt.props.onEnter),onExited:createChainedFunction(It,tt==null?void 0:tt.props.onExited)}},rootRef:rt,portalRef:gt,isTopModal:bt,exited:ut,hasTransition:dt}}var top="top",bottom="bottom",right="right",left="left",auto="auto",basePlacements=[top,bottom,right,left],start="start",end="end",clippingParents="clippingParents",viewport="viewport",popper="popper",reference="reference",variationPlacements=basePlacements.reduce(function(a,i){return a.concat([i+"-"+start,i+"-"+end])},[]),placements=[].concat(basePlacements,[auto]).reduce(function(a,i){return a.concat([i,i+"-"+start,i+"-"+end])},[]),beforeRead="beforeRead",read="read",afterRead="afterRead",beforeMain="beforeMain",main="main",afterMain="afterMain",beforeWrite="beforeWrite",write="write",afterWrite="afterWrite",modifierPhases=[beforeRead,read,afterRead,beforeMain,main,afterMain,beforeWrite,write,afterWrite];function getNodeName(a){return a?(a.nodeName||"").toLowerCase():null}function getWindow(a){if(a==null)return window;if(a.toString()!=="[object Window]"){var i=a.ownerDocument;return i&&i.defaultView||window}return a}function isElement(a){var i=getWindow(a).Element;return a instanceof i||a instanceof Element}function isHTMLElement$1(a){var i=getWindow(a).HTMLElement;return a instanceof i||a instanceof HTMLElement}function isShadowRoot(a){if(typeof ShadowRoot>"u")return!1;var i=getWindow(a).ShadowRoot;return a instanceof i||a instanceof ShadowRoot}function applyStyles(a){var i=a.state;Object.keys(i.elements).forEach(function(o){var s=i.styles[o]||{},$=i.attributes[o]||{},j=i.elements[o];!isHTMLElement$1(j)||!getNodeName(j)||(Object.assign(j.style,s),Object.keys($).forEach(function(_e){var et=$[_e];et===!1?j.removeAttribute(_e):j.setAttribute(_e,et===!0?"":et)}))})}function effect$2(a){var i=a.state,o={popper:{position:i.options.strategy,left:"0",top:"0",margin:"0"},arrow:{position:"absolute"},reference:{}};return Object.assign(i.elements.popper.style,o.popper),i.styles=o,i.elements.arrow&&Object.assign(i.elements.arrow.style,o.arrow),function(){Object.keys(i.elements).forEach(function(s){var $=i.elements[s],j=i.attributes[s]||{},_e=Object.keys(i.styles.hasOwnProperty(s)?i.styles[s]:o[s]),et=_e.reduce(function(tt,nt){return tt[nt]="",tt},{});!isHTMLElement$1($)||!getNodeName($)||(Object.assign($.style,et),Object.keys(j).forEach(function(tt){$.removeAttribute(tt)}))})}}const applyStyles$1={name:"applyStyles",enabled:!0,phase:"write",fn:applyStyles,effect:effect$2,requires:["computeStyles"]};function getBasePlacement(a){return a.split("-")[0]}var max=Math.max,min=Math.min,round$1=Math.round;function getUAString(){var a=navigator.userAgentData;return a!=null&&a.brands&&Array.isArray(a.brands)?a.brands.map(function(i){return i.brand+"/"+i.version}).join(" "):navigator.userAgent}function isLayoutViewport(){return!/^((?!chrome|android).)*safari/i.test(getUAString())}function getBoundingClientRect(a,i,o){i===void 0&&(i=!1),o===void 0&&(o=!1);var s=a.getBoundingClientRect(),$=1,j=1;i&&isHTMLElement$1(a)&&($=a.offsetWidth>0&&round$1(s.width)/a.offsetWidth||1,j=a.offsetHeight>0&&round$1(s.height)/a.offsetHeight||1);var _e=isElement(a)?getWindow(a):window,et=_e.visualViewport,tt=!isLayoutViewport()&&o,nt=(s.left+(tt&&et?et.offsetLeft:0))/$,at=(s.top+(tt&&et?et.offsetTop:0))/j,it=s.width/$,st=s.height/j;return{width:it,height:st,top:at,right:nt+it,bottom:at+st,left:nt,x:nt,y:at}}function getLayoutRect(a){var i=getBoundingClientRect(a),o=a.offsetWidth,s=a.offsetHeight;return Math.abs(i.width-o)<=1&&(o=i.width),Math.abs(i.height-s)<=1&&(s=i.height),{x:a.offsetLeft,y:a.offsetTop,width:o,height:s}}function contains(a,i){var o=i.getRootNode&&i.getRootNode();if(a.contains(i))return!0;if(o&&isShadowRoot(o)){var s=i;do{if(s&&a.isSameNode(s))return!0;s=s.parentNode||s.host}while(s)}return!1}function getComputedStyle$1(a){return getWindow(a).getComputedStyle(a)}function isTableElement(a){return["table","td","th"].indexOf(getNodeName(a))>=0}function getDocumentElement(a){return((isElement(a)?a.ownerDocument:a.document)||window.document).documentElement}function getParentNode(a){return getNodeName(a)==="html"?a:a.assignedSlot||a.parentNode||(isShadowRoot(a)?a.host:null)||getDocumentElement(a)}function getTrueOffsetParent(a){return!isHTMLElement$1(a)||getComputedStyle$1(a).position==="fixed"?null:a.offsetParent}function getContainingBlock(a){var i=/firefox/i.test(getUAString()),o=/Trident/i.test(getUAString());if(o&&isHTMLElement$1(a)){var s=getComputedStyle$1(a);if(s.position==="fixed")return null}var $=getParentNode(a);for(isShadowRoot($)&&($=$.host);isHTMLElement$1($)&&["html","body"].indexOf(getNodeName($))<0;){var j=getComputedStyle$1($);if(j.transform!=="none"||j.perspective!=="none"||j.contain==="paint"||["transform","perspective"].indexOf(j.willChange)!==-1||i&&j.willChange==="filter"||i&&j.filter&&j.filter!=="none")return $;$=$.parentNode}return null}function getOffsetParent(a){for(var i=getWindow(a),o=getTrueOffsetParent(a);o&&isTableElement(o)&&getComputedStyle$1(o).position==="static";)o=getTrueOffsetParent(o);return o&&(getNodeName(o)==="html"||getNodeName(o)==="body"&&getComputedStyle$1(o).position==="static")?i:o||getContainingBlock(a)||i}function getMainAxisFromPlacement(a){return["top","bottom"].indexOf(a)>=0?"x":"y"}function within(a,i,o){return max(a,min(i,o))}function withinMaxClamp(a,i,o){var s=within(a,i,o);return s>o?o:s}function getFreshSideObject(){return{top:0,right:0,bottom:0,left:0}}function mergePaddingObject(a){return Object.assign({},getFreshSideObject(),a)}function expandToHashMap(a,i){return i.reduce(function(o,s){return o[s]=a,o},{})}var toPaddingObject=function(i,o){return i=typeof i=="function"?i(Object.assign({},o.rects,{placement:o.placement})):i,mergePaddingObject(typeof i!="number"?i:expandToHashMap(i,basePlacements))};function arrow(a){var i,o=a.state,s=a.name,$=a.options,j=o.elements.arrow,_e=o.modifiersData.popperOffsets,et=getBasePlacement(o.placement),tt=getMainAxisFromPlacement(et),nt=[left,right].indexOf(et)>=0,at=nt?"height":"width";if(!(!j||!_e)){var it=toPaddingObject($.padding,o),st=getLayoutRect(j),lt=tt==="y"?top:left,ct=tt==="y"?bottom:right,rt=o.rects.reference[at]+o.rects.reference[tt]-_e[tt]-o.rects.popper[at],ut=_e[tt]-o.rects.reference[tt],ot=getOffsetParent(j),dt=ot?tt==="y"?ot.clientHeight||0:ot.clientWidth||0:0,pt=rt/2-ut/2,mt=it[lt],ft=dt-st[at]-it[ct],ht=dt/2-st[at]/2+pt,yt=within(mt,ht,ft),bt=tt;o.modifiersData[s]=(i={},i[bt]=yt,i.centerOffset=yt-ht,i)}}function effect$1(a){var i=a.state,o=a.options,s=o.element,$=s===void 0?"[data-popper-arrow]":s;$!=null&&(typeof $=="string"&&($=i.elements.popper.querySelector($),!$)||contains(i.elements.popper,$)&&(i.elements.arrow=$))}const arrow$1={name:"arrow",enabled:!0,phase:"main",fn:arrow,effect:effect$1,requires:["popperOffsets"],requiresIfExists:["preventOverflow"]};function getVariation(a){return a.split("-")[1]}var unsetSides={top:"auto",right:"auto",bottom:"auto",left:"auto"};function roundOffsetsByDPR(a,i){var o=a.x,s=a.y,$=i.devicePixelRatio||1;return{x:round$1(o*$)/$||0,y:round$1(s*$)/$||0}}function mapToStyles(a){var i,o=a.popper,s=a.popperRect,$=a.placement,j=a.variation,_e=a.offsets,et=a.position,tt=a.gpuAcceleration,nt=a.adaptive,at=a.roundOffsets,it=a.isFixed,st=_e.x,lt=st===void 0?0:st,ct=_e.y,rt=ct===void 0?0:ct,ut=typeof at=="function"?at({x:lt,y:rt}):{x:lt,y:rt};lt=ut.x,rt=ut.y;var ot=_e.hasOwnProperty("x"),dt=_e.hasOwnProperty("y"),pt=left,mt=top,ft=window;if(nt){var ht=getOffsetParent(o),yt="clientHeight",bt="clientWidth";if(ht===getWindow(o)&&(ht=getDocumentElement(o),getComputedStyle$1(ht).position!=="static"&&et==="absolute"&&(yt="scrollHeight",bt="scrollWidth")),ht=ht,$===top||($===left||$===right)&&j===end){mt=bottom;var gt=it&&ht===ft&&ft.visualViewport?ft.visualViewport.height:ht[yt];rt-=gt-s.height,rt*=tt?1:-1}if($===left||($===top||$===bottom)&&j===end){pt=right;var xt=it&&ht===ft&&ft.visualViewport?ft.visualViewport.width:ht[bt];lt-=xt-s.width,lt*=tt?1:-1}}var vt=Object.assign({position:et},nt&&unsetSides),Lt=at===!0?roundOffsetsByDPR({x:lt,y:rt},getWindow(o)):{x:lt,y:rt};if(lt=Lt.x,rt=Lt.y,tt){var $t;return Object.assign({},vt,($t={},$t[mt]=dt?"0":"",$t[pt]=ot?"0":"",$t.transform=(ft.devicePixelRatio||1)<=1?"translate("+lt+"px, "+rt+"px)":"translate3d("+lt+"px, "+rt+"px, 0)",$t))}return Object.assign({},vt,(i={},i[mt]=dt?rt+"px":"",i[pt]=ot?lt+"px":"",i.transform="",i))}function computeStyles(a){var i=a.state,o=a.options,s=o.gpuAcceleration,$=s===void 0?!0:s,j=o.adaptive,_e=j===void 0?!0:j,et=o.roundOffsets,tt=et===void 0?!0:et,nt={placement:getBasePlacement(i.placement),variation:getVariation(i.placement),popper:i.elements.popper,popperRect:i.rects.popper,gpuAcceleration:$,isFixed:i.options.strategy==="fixed"};i.modifiersData.popperOffsets!=null&&(i.styles.popper=Object.assign({},i.styles.popper,mapToStyles(Object.assign({},nt,{offsets:i.modifiersData.popperOffsets,position:i.options.strategy,adaptive:_e,roundOffsets:tt})))),i.modifiersData.arrow!=null&&(i.styles.arrow=Object.assign({},i.styles.arrow,mapToStyles(Object.assign({},nt,{offsets:i.modifiersData.arrow,position:"absolute",adaptive:!1,roundOffsets:tt})))),i.attributes.popper=Object.assign({},i.attributes.popper,{"data-popper-placement":i.placement})}const computeStyles$1={name:"computeStyles",enabled:!0,phase:"beforeWrite",fn:computeStyles,data:{}};var passive={passive:!0};function effect(a){var i=a.state,o=a.instance,s=a.options,$=s.scroll,j=$===void 0?!0:$,_e=s.resize,et=_e===void 0?!0:_e,tt=getWindow(i.elements.popper),nt=[].concat(i.scrollParents.reference,i.scrollParents.popper);return j&&nt.forEach(function(at){at.addEventListener("scroll",o.update,passive)}),et&&tt.addEventListener("resize",o.update,passive),function(){j&&nt.forEach(function(at){at.removeEventListener("scroll",o.update,passive)}),et&&tt.removeEventListener("resize",o.update,passive)}}const eventListeners={name:"eventListeners",enabled:!0,phase:"write",fn:function(){},effect,data:{}};var hash$1={left:"right",right:"left",bottom:"top",top:"bottom"};function getOppositePlacement(a){return a.replace(/left|right|bottom|top/g,function(i){return hash$1[i]})}var hash={start:"end",end:"start"};function getOppositeVariationPlacement(a){return a.replace(/start|end/g,function(i){return hash[i]})}function getWindowScroll(a){var i=getWindow(a),o=i.pageXOffset,s=i.pageYOffset;return{scrollLeft:o,scrollTop:s}}function getWindowScrollBarX(a){return getBoundingClientRect(getDocumentElement(a)).left+getWindowScroll(a).scrollLeft}function getViewportRect(a,i){var o=getWindow(a),s=getDocumentElement(a),$=o.visualViewport,j=s.clientWidth,_e=s.clientHeight,et=0,tt=0;if($){j=$.width,_e=$.height;var nt=isLayoutViewport();(nt||!nt&&i==="fixed")&&(et=$.offsetLeft,tt=$.offsetTop)}return{width:j,height:_e,x:et+getWindowScrollBarX(a),y:tt}}function getDocumentRect(a){var i,o=getDocumentElement(a),s=getWindowScroll(a),$=(i=a.ownerDocument)==null?void 0:i.body,j=max(o.scrollWidth,o.clientWidth,$?$.scrollWidth:0,$?$.clientWidth:0),_e=max(o.scrollHeight,o.clientHeight,$?$.scrollHeight:0,$?$.clientHeight:0),et=-s.scrollLeft+getWindowScrollBarX(a),tt=-s.scrollTop;return getComputedStyle$1($||o).direction==="rtl"&&(et+=max(o.clientWidth,$?$.clientWidth:0)-j),{width:j,height:_e,x:et,y:tt}}function isScrollParent(a){var i=getComputedStyle$1(a),o=i.overflow,s=i.overflowX,$=i.overflowY;return/auto|scroll|overlay|hidden/.test(o+$+s)}function getScrollParent(a){return["html","body","#document"].indexOf(getNodeName(a))>=0?a.ownerDocument.body:isHTMLElement$1(a)&&isScrollParent(a)?a:getScrollParent(getParentNode(a))}function listScrollParents(a,i){var o;i===void 0&&(i=[]);var s=getScrollParent(a),$=s===((o=a.ownerDocument)==null?void 0:o.body),j=getWindow(s),_e=$?[j].concat(j.visualViewport||[],isScrollParent(s)?s:[]):s,et=i.concat(_e);return $?et:et.concat(listScrollParents(getParentNode(_e)))}function rectToClientRect(a){return Object.assign({},a,{left:a.x,top:a.y,right:a.x+a.width,bottom:a.y+a.height})}function getInnerBoundingClientRect(a,i){var o=getBoundingClientRect(a,!1,i==="fixed");return o.top=o.top+a.clientTop,o.left=o.left+a.clientLeft,o.bottom=o.top+a.clientHeight,o.right=o.left+a.clientWidth,o.width=a.clientWidth,o.height=a.clientHeight,o.x=o.left,o.y=o.top,o}function getClientRectFromMixedType(a,i,o){return i===viewport?rectToClientRect(getViewportRect(a,o)):isElement(i)?getInnerBoundingClientRect(i,o):rectToClientRect(getDocumentRect(getDocumentElement(a)))}function getClippingParents(a){var i=listScrollParents(getParentNode(a)),o=["absolute","fixed"].indexOf(getComputedStyle$1(a).position)>=0,s=o&&isHTMLElement$1(a)?getOffsetParent(a):a;return isElement(s)?i.filter(function($){return isElement($)&&contains($,s)&&getNodeName($)!=="body"}):[]}function getClippingRect(a,i,o,s){var $=i==="clippingParents"?getClippingParents(a):[].concat(i),j=[].concat($,[o]),_e=j[0],et=j.reduce(function(tt,nt){var at=getClientRectFromMixedType(a,nt,s);return tt.top=max(at.top,tt.top),tt.right=min(at.right,tt.right),tt.bottom=min(at.bottom,tt.bottom),tt.left=max(at.left,tt.left),tt},getClientRectFromMixedType(a,_e,s));return et.width=et.right-et.left,et.height=et.bottom-et.top,et.x=et.left,et.y=et.top,et}function computeOffsets(a){var i=a.reference,o=a.element,s=a.placement,$=s?getBasePlacement(s):null,j=s?getVariation(s):null,_e=i.x+i.width/2-o.width/2,et=i.y+i.height/2-o.height/2,tt;switch($){case top:tt={x:_e,y:i.y-o.height};break;case bottom:tt={x:_e,y:i.y+i.height};break;case right:tt={x:i.x+i.width,y:et};break;case left:tt={x:i.x-o.width,y:et};break;default:tt={x:i.x,y:i.y}}var nt=$?getMainAxisFromPlacement($):null;if(nt!=null){var at=nt==="y"?"height":"width";switch(j){case start:tt[nt]=tt[nt]-(i[at]/2-o[at]/2);break;case end:tt[nt]=tt[nt]+(i[at]/2-o[at]/2);break}}return tt}function detectOverflow(a,i){i===void 0&&(i={});var o=i,s=o.placement,$=s===void 0?a.placement:s,j=o.strategy,_e=j===void 0?a.strategy:j,et=o.boundary,tt=et===void 0?clippingParents:et,nt=o.rootBoundary,at=nt===void 0?viewport:nt,it=o.elementContext,st=it===void 0?popper:it,lt=o.altBoundary,ct=lt===void 0?!1:lt,rt=o.padding,ut=rt===void 0?0:rt,ot=mergePaddingObject(typeof ut!="number"?ut:expandToHashMap(ut,basePlacements)),dt=st===popper?reference:popper,pt=a.rects.popper,mt=a.elements[ct?dt:st],ft=getClippingRect(isElement(mt)?mt:mt.contextElement||getDocumentElement(a.elements.popper),tt,at,_e),ht=getBoundingClientRect(a.elements.reference),yt=computeOffsets({reference:ht,element:pt,strategy:"absolute",placement:$}),bt=rectToClientRect(Object.assign({},pt,yt)),gt=st===popper?bt:ht,xt={top:ft.top-gt.top+ot.top,bottom:gt.bottom-ft.bottom+ot.bottom,left:ft.left-gt.left+ot.left,right:gt.right-ft.right+ot.right},vt=a.modifiersData.offset;if(st===popper&&vt){var Lt=vt[$];Object.keys(xt).forEach(function($t){var Tt=[right,bottom].indexOf($t)>=0?1:-1,Et=[top,bottom].indexOf($t)>=0?"y":"x";xt[$t]+=Lt[Et]*Tt})}return xt}function computeAutoPlacement(a,i){i===void 0&&(i={});var o=i,s=o.placement,$=o.boundary,j=o.rootBoundary,_e=o.padding,et=o.flipVariations,tt=o.allowedAutoPlacements,nt=tt===void 0?placements:tt,at=getVariation(s),it=at?et?variationPlacements:variationPlacements.filter(function(ct){return getVariation(ct)===at}):basePlacements,st=it.filter(function(ct){return nt.indexOf(ct)>=0});st.length===0&&(st=it);var lt=st.reduce(function(ct,rt){return ct[rt]=detectOverflow(a,{placement:rt,boundary:$,rootBoundary:j,padding:_e})[getBasePlacement(rt)],ct},{});return Object.keys(lt).sort(function(ct,rt){return lt[ct]-lt[rt]})}function getExpandedFallbackPlacements(a){if(getBasePlacement(a)===auto)return[];var i=getOppositePlacement(a);return[getOppositeVariationPlacement(a),i,getOppositeVariationPlacement(i)]}function flip(a){var i=a.state,o=a.options,s=a.name;if(!i.modifiersData[s]._skip){for(var $=o.mainAxis,j=$===void 0?!0:$,_e=o.altAxis,et=_e===void 0?!0:_e,tt=o.fallbackPlacements,nt=o.padding,at=o.boundary,it=o.rootBoundary,st=o.altBoundary,lt=o.flipVariations,ct=lt===void 0?!0:lt,rt=o.allowedAutoPlacements,ut=i.options.placement,ot=getBasePlacement(ut),dt=ot===ut,pt=tt||(dt||!ct?[getOppositePlacement(ut)]:getExpandedFallbackPlacements(ut)),mt=[ut].concat(pt).reduce(function(wt,kt){return wt.concat(getBasePlacement(kt)===auto?computeAutoPlacement(i,{placement:kt,boundary:at,rootBoundary:it,padding:nt,flipVariations:ct,allowedAutoPlacements:rt}):kt)},[]),ft=i.rects.reference,ht=i.rects.popper,yt=new Map,bt=!0,gt=mt[0],xt=0;xt<mt.length;xt++){var vt=mt[xt],Lt=getBasePlacement(vt),$t=getVariation(vt)===start,Tt=[top,bottom].indexOf(Lt)>=0,Et=Tt?"width":"height",Dt=detectOverflow(i,{placement:vt,boundary:at,rootBoundary:it,altBoundary:st,padding:nt}),It=Tt?$t?right:left:$t?bottom:top;ft[Et]>ht[Et]&&(It=getOppositePlacement(It));var Ct=getOppositePlacement(It),jt=[];if(j&&jt.push(Dt[Lt]<=0),et&&jt.push(Dt[It]<=0,Dt[Ct]<=0),jt.every(function(wt){return wt})){gt=vt,bt=!1;break}yt.set(vt,jt)}if(bt)for(var Zt=ct?3:1,Xt=function(kt){var At=mt.find(function(Pt){var Mt=yt.get(Pt);if(Mt)return Mt.slice(0,kt).every(function(Ot){return Ot})});if(At)return gt=At,"break"},sn=Zt;sn>0;sn--){var Ft=Xt(sn);if(Ft==="break")break}i.placement!==gt&&(i.modifiersData[s]._skip=!0,i.placement=gt,i.reset=!0)}}const flip$1={name:"flip",enabled:!0,phase:"main",fn:flip,requiresIfExists:["offset"],data:{_skip:!1}};function getSideOffsets(a,i,o){return o===void 0&&(o={x:0,y:0}),{top:a.top-i.height-o.y,right:a.right-i.width+o.x,bottom:a.bottom-i.height+o.y,left:a.left-i.width-o.x}}function isAnySideFullyClipped(a){return[top,right,bottom,left].some(function(i){return a[i]>=0})}function hide(a){var i=a.state,o=a.name,s=i.rects.reference,$=i.rects.popper,j=i.modifiersData.preventOverflow,_e=detectOverflow(i,{elementContext:"reference"}),et=detectOverflow(i,{altBoundary:!0}),tt=getSideOffsets(_e,s),nt=getSideOffsets(et,$,j),at=isAnySideFullyClipped(tt),it=isAnySideFullyClipped(nt);i.modifiersData[o]={referenceClippingOffsets:tt,popperEscapeOffsets:nt,isReferenceHidden:at,hasPopperEscaped:it},i.attributes.popper=Object.assign({},i.attributes.popper,{"data-popper-reference-hidden":at,"data-popper-escaped":it})}const hide$1={name:"hide",enabled:!0,phase:"main",requiresIfExists:["preventOverflow"],fn:hide};function distanceAndSkiddingToXY(a,i,o){var s=getBasePlacement(a),$=[left,top].indexOf(s)>=0?-1:1,j=typeof o=="function"?o(Object.assign({},i,{placement:a})):o,_e=j[0],et=j[1];return _e=_e||0,et=(et||0)*$,[left,right].indexOf(s)>=0?{x:et,y:_e}:{x:_e,y:et}}function offset(a){var i=a.state,o=a.options,s=a.name,$=o.offset,j=$===void 0?[0,0]:$,_e=placements.reduce(function(at,it){return at[it]=distanceAndSkiddingToXY(it,i.rects,j),at},{}),et=_e[i.placement],tt=et.x,nt=et.y;i.modifiersData.popperOffsets!=null&&(i.modifiersData.popperOffsets.x+=tt,i.modifiersData.popperOffsets.y+=nt),i.modifiersData[s]=_e}const offset$1={name:"offset",enabled:!0,phase:"main",requires:["popperOffsets"],fn:offset};function popperOffsets(a){var i=a.state,o=a.name;i.modifiersData[o]=computeOffsets({reference:i.rects.reference,element:i.rects.popper,strategy:"absolute",placement:i.placement})}const popperOffsets$1={name:"popperOffsets",enabled:!0,phase:"read",fn:popperOffsets,data:{}};function getAltAxis(a){return a==="x"?"y":"x"}function preventOverflow(a){var i=a.state,o=a.options,s=a.name,$=o.mainAxis,j=$===void 0?!0:$,_e=o.altAxis,et=_e===void 0?!1:_e,tt=o.boundary,nt=o.rootBoundary,at=o.altBoundary,it=o.padding,st=o.tether,lt=st===void 0?!0:st,ct=o.tetherOffset,rt=ct===void 0?0:ct,ut=detectOverflow(i,{boundary:tt,rootBoundary:nt,padding:it,altBoundary:at}),ot=getBasePlacement(i.placement),dt=getVariation(i.placement),pt=!dt,mt=getMainAxisFromPlacement(ot),ft=getAltAxis(mt),ht=i.modifiersData.popperOffsets,yt=i.rects.reference,bt=i.rects.popper,gt=typeof rt=="function"?rt(Object.assign({},i.rects,{placement:i.placement})):rt,xt=typeof gt=="number"?{mainAxis:gt,altAxis:gt}:Object.assign({mainAxis:0,altAxis:0},gt),vt=i.modifiersData.offset?i.modifiersData.offset[i.placement]:null,Lt={x:0,y:0};if(ht){if(j){var $t,Tt=mt==="y"?top:left,Et=mt==="y"?bottom:right,Dt=mt==="y"?"height":"width",It=ht[mt],Ct=It+ut[Tt],jt=It-ut[Et],Zt=lt?-bt[Dt]/2:0,Xt=dt===start?yt[Dt]:bt[Dt],sn=dt===start?-bt[Dt]:-yt[Dt],Ft=i.elements.arrow,wt=lt&&Ft?getLayoutRect(Ft):{width:0,height:0},kt=i.modifiersData["arrow#persistent"]?i.modifiersData["arrow#persistent"].padding:getFreshSideObject(),At=kt[Tt],Pt=kt[Et],Mt=within(0,yt[Dt],wt[Dt]),Ot=pt?yt[Dt]/2-Zt-Mt-At-xt.mainAxis:Xt-Mt-At-xt.mainAxis,Bt=pt?-yt[Dt]/2+Zt+Mt+Pt+xt.mainAxis:sn+Mt+Pt+xt.mainAxis,zt=i.elements.arrow&&getOffsetParent(i.elements.arrow),Gt=zt?mt==="y"?zt.clientTop||0:zt.clientLeft||0:0,Wt=($t=vt==null?void 0:vt[mt])!=null?$t:0,qt=It+Ot-Wt-Gt,tn=It+Bt-Wt,ln=within(lt?min(Ct,qt):Ct,It,lt?max(jt,tn):jt);ht[mt]=ln,Lt[mt]=ln-It}if(et){var gn,yn=mt==="x"?top:left,Pn=mt==="x"?bottom:right,cn=ht[ft],xn=ft==="y"?"height":"width",hn=cn+ut[yn],en=cn-ut[Pn],Jt=[top,left].indexOf(ot)!==-1,vn=(gn=vt==null?void 0:vt[ft])!=null?gn:0,$n=Jt?hn:cn-yt[xn]-bt[xn]-vn+xt.altAxis,Mn=Jt?cn+yt[xn]+bt[xn]-vn-xt.altAxis:en,On=lt&&Jt?withinMaxClamp($n,cn,Mn):within(lt?$n:hn,cn,lt?Mn:en);ht[ft]=On,Lt[ft]=On-cn}i.modifiersData[s]=Lt}}const preventOverflow$1={name:"preventOverflow",enabled:!0,phase:"main",fn:preventOverflow,requiresIfExists:["offset"]};function getHTMLElementScroll(a){return{scrollLeft:a.scrollLeft,scrollTop:a.scrollTop}}function getNodeScroll(a){return a===getWindow(a)||!isHTMLElement$1(a)?getWindowScroll(a):getHTMLElementScroll(a)}function isElementScaled(a){var i=a.getBoundingClientRect(),o=round$1(i.width)/a.offsetWidth||1,s=round$1(i.height)/a.offsetHeight||1;return o!==1||s!==1}function getCompositeRect(a,i,o){o===void 0&&(o=!1);var s=isHTMLElement$1(i),$=isHTMLElement$1(i)&&isElementScaled(i),j=getDocumentElement(i),_e=getBoundingClientRect(a,$,o),et={scrollLeft:0,scrollTop:0},tt={x:0,y:0};return(s||!s&&!o)&&((getNodeName(i)!=="body"||isScrollParent(j))&&(et=getNodeScroll(i)),isHTMLElement$1(i)?(tt=getBoundingClientRect(i,!0),tt.x+=i.clientLeft,tt.y+=i.clientTop):j&&(tt.x=getWindowScrollBarX(j))),{x:_e.left+et.scrollLeft-tt.x,y:_e.top+et.scrollTop-tt.y,width:_e.width,height:_e.height}}function order(a){var i=new Map,o=new Set,s=[];a.forEach(function(j){i.set(j.name,j)});function $(j){o.add(j.name);var _e=[].concat(j.requires||[],j.requiresIfExists||[]);_e.forEach(function(et){if(!o.has(et)){var tt=i.get(et);tt&&$(tt)}}),s.push(j)}return a.forEach(function(j){o.has(j.name)||$(j)}),s}function orderModifiers(a){var i=order(a);return modifierPhases.reduce(function(o,s){return o.concat(i.filter(function($){return $.phase===s}))},[])}function debounce(a){var i;return function(){return i||(i=new Promise(function(o){Promise.resolve().then(function(){i=void 0,o(a())})})),i}}function mergeByName(a){var i=a.reduce(function(o,s){var $=o[s.name];return o[s.name]=$?Object.assign({},$,s,{options:Object.assign({},$.options,s.options),data:Object.assign({},$.data,s.data)}):s,o},{});return Object.keys(i).map(function(o){return i[o]})}var DEFAULT_OPTIONS={placement:"bottom",modifiers:[],strategy:"absolute"};function areValidElements(){for(var a=arguments.length,i=new Array(a),o=0;o<a;o++)i[o]=arguments[o];return!i.some(function(s){return!(s&&typeof s.getBoundingClientRect=="function")})}function popperGenerator(a){a===void 0&&(a={});var i=a,o=i.defaultModifiers,s=o===void 0?[]:o,$=i.defaultOptions,j=$===void 0?DEFAULT_OPTIONS:$;return function(et,tt,nt){nt===void 0&&(nt=j);var at={placement:"bottom",orderedModifiers:[],options:Object.assign({},DEFAULT_OPTIONS,j),modifiersData:{},elements:{reference:et,popper:tt},attributes:{},styles:{}},it=[],st=!1,lt={state:at,setOptions:function(ot){var dt=typeof ot=="function"?ot(at.options):ot;rt(),at.options=Object.assign({},j,at.options,dt),at.scrollParents={reference:isElement(et)?listScrollParents(et):et.contextElement?listScrollParents(et.contextElement):[],popper:listScrollParents(tt)};var pt=orderModifiers(mergeByName([].concat(s,at.options.modifiers)));return at.orderedModifiers=pt.filter(function(mt){return mt.enabled}),ct(),lt.update()},forceUpdate:function(){if(!st){var ot=at.elements,dt=ot.reference,pt=ot.popper;if(areValidElements(dt,pt)){at.rects={reference:getCompositeRect(dt,getOffsetParent(pt),at.options.strategy==="fixed"),popper:getLayoutRect(pt)},at.reset=!1,at.placement=at.options.placement,at.orderedModifiers.forEach(function(xt){return at.modifiersData[xt.name]=Object.assign({},xt.data)});for(var mt=0;mt<at.orderedModifiers.length;mt++){if(at.reset===!0){at.reset=!1,mt=-1;continue}var ft=at.orderedModifiers[mt],ht=ft.fn,yt=ft.options,bt=yt===void 0?{}:yt,gt=ft.name;typeof ht=="function"&&(at=ht({state:at,options:bt,name:gt,instance:lt})||at)}}}},update:debounce(function(){return new Promise(function(ut){lt.forceUpdate(),ut(at)})}),destroy:function(){rt(),st=!0}};if(!areValidElements(et,tt))return lt;lt.setOptions(nt).then(function(ut){!st&&nt.onFirstUpdate&&nt.onFirstUpdate(ut)});function ct(){at.orderedModifiers.forEach(function(ut){var ot=ut.name,dt=ut.options,pt=dt===void 0?{}:dt,mt=ut.effect;if(typeof mt=="function"){var ft=mt({state:at,name:ot,instance:lt,options:pt}),ht=function(){};it.push(ft||ht)}})}function rt(){it.forEach(function(ut){return ut()}),it=[]}return lt}}var defaultModifiers=[eventListeners,popperOffsets$1,computeStyles$1,applyStyles$1,offset$1,flip$1,preventOverflow$1,arrow$1,hide$1],createPopper=popperGenerator({defaultModifiers});const COMPONENT_NAME="Popper";function getPopperUtilityClass(a){return generateUtilityClass(COMPONENT_NAME,a)}generateUtilityClasses(COMPONENT_NAME,["root"]);const _excluded$1v=["anchorEl","children","direction","disablePortal","modifiers","open","placement","popperOptions","popperRef","slotProps","slots","TransitionProps","ownerState"],_excluded2$d=["anchorEl","children","container","direction","disablePortal","keepMounted","modifiers","open","placement","popperOptions","popperRef","style","transition","slotProps","slots"];function flipPlacement(a,i){if(i==="ltr")return a;switch(a){case"bottom-end":return"bottom-start";case"bottom-start":return"bottom-end";case"top-end":return"top-start";case"top-start":return"top-end";default:return a}}function resolveAnchorEl$1(a){return typeof a=="function"?a():a}function isHTMLElement(a){return a.nodeType!==void 0}const useUtilityClasses$1g=()=>composeClasses({root:["root"]},useClassNamesOverride(getPopperUtilityClass)),defaultPopperOptions={},PopperTooltip=reactExports.forwardRef(function(i,o){var s;const{anchorEl:$,children:j,direction:_e,disablePortal:et,modifiers:tt,open:nt,placement:at,popperOptions:it,popperRef:st,slotProps:lt={},slots:ct={},TransitionProps:rt}=i,ut=_objectWithoutPropertiesLoose(i,_excluded$1v),ot=reactExports.useRef(null),dt=useForkRef(ot,o),pt=reactExports.useRef(null),mt=useForkRef(pt,st),ft=reactExports.useRef(mt);useEnhancedEffect(()=>{ft.current=mt},[mt]),reactExports.useImperativeHandle(st,()=>pt.current,[]);const ht=flipPlacement(at,_e),[yt,bt]=reactExports.useState(ht),[gt,xt]=reactExports.useState(resolveAnchorEl$1($));reactExports.useEffect(()=>{pt.current&&pt.current.forceUpdate()}),reactExports.useEffect(()=>{$&&xt(resolveAnchorEl$1($))},[$]),useEnhancedEffect(()=>{if(!gt||!nt)return;const Et=Ct=>{bt(Ct.placement)};let Dt=[{name:"preventOverflow",options:{altBoundary:et}},{name:"flip",options:{altBoundary:et}},{name:"onUpdate",enabled:!0,phase:"afterWrite",fn:({state:Ct})=>{Et(Ct)}}];tt!=null&&(Dt=Dt.concat(tt)),it&&it.modifiers!=null&&(Dt=Dt.concat(it.modifiers));const It=createPopper(gt,ot.current,_extends({placement:ht},it,{modifiers:Dt}));return ft.current(It),()=>{It.destroy(),ft.current(null)}},[gt,et,tt,nt,it,ht]);const vt={placement:yt};rt!==null&&(vt.TransitionProps=rt);const Lt=useUtilityClasses$1g(),$t=(s=ct.root)!=null?s:"div",Tt=useSlotProps({elementType:$t,externalSlotProps:lt.root,externalForwardedProps:ut,additionalProps:{role:"tooltip",ref:dt},ownerState:i,className:Lt.root});return jsxRuntimeExports.jsx($t,_extends({},Tt,{children:typeof j=="function"?j(vt):j}))}),Popper$1=reactExports.forwardRef(function(i,o){const{anchorEl:s,children:$,container:j,direction:_e="ltr",disablePortal:et=!1,keepMounted:tt=!1,modifiers:nt,open:at,placement:it="bottom",popperOptions:st=defaultPopperOptions,popperRef:lt,style:ct,transition:rt=!1,slotProps:ut={},slots:ot={}}=i,dt=_objectWithoutPropertiesLoose(i,_excluded2$d),[pt,mt]=reactExports.useState(!0),ft=()=>{mt(!1)},ht=()=>{mt(!0)};if(!tt&&!at&&(!rt||pt))return null;let yt;if(j)yt=j;else if(s){const xt=resolveAnchorEl$1(s);yt=xt&&isHTMLElement(xt)?ownerDocument(xt).body:ownerDocument(null).body}const bt=!at&&tt&&(!rt||pt)?"none":void 0,gt=rt?{in:at,onEnter:ft,onExited:ht}:void 0;return jsxRuntimeExports.jsx(Portal,{disablePortal:et,container:yt,children:jsxRuntimeExports.jsx(PopperTooltip,_extends({anchorEl:s,direction:_e,disablePortal:et,modifiers:nt,ref:o,open:rt?!pt:at,placement:it,popperOptions:st,popperRef:lt,slotProps:ut,slots:ot},dt,{style:_extends({position:"fixed",top:0,left:0,display:bt},ct),TransitionProps:gt,children:$}))})});function useSnackbar(a={}){const{autoHideDuration:i=null,disableWindowBlurListener:o=!1,onClose:s,open:$,resumeHideDuration:j}=a,_e=useTimeout();reactExports.useEffect(()=>{if(!$)return;function ot(dt){dt.defaultPrevented||(dt.key==="Escape"||dt.key==="Esc")&&(s==null||s(dt,"escapeKeyDown"))}return document.addEventListener("keydown",ot),()=>{document.removeEventListener("keydown",ot)}},[$,s]);const et=useEventCallback((ot,dt)=>{s==null||s(ot,dt)}),tt=useEventCallback(ot=>{!s||ot==null||_e.start(ot,()=>{et(null,"timeout")})});reactExports.useEffect(()=>($&&tt(i),_e.clear),[$,i,tt,_e]);const nt=ot=>{s==null||s(ot,"clickaway")},at=_e.clear,it=reactExports.useCallback(()=>{i!=null&&tt(j??i*.5)},[i,j,tt]),st=ot=>dt=>{const pt=ot.onBlur;pt==null||pt(dt),it()},lt=ot=>dt=>{const pt=ot.onFocus;pt==null||pt(dt),at()},ct=ot=>dt=>{const pt=ot.onMouseEnter;pt==null||pt(dt),at()},rt=ot=>dt=>{const pt=ot.onMouseLeave;pt==null||pt(dt),it()};return reactExports.useEffect(()=>{if(!o&&$)return window.addEventListener("focus",it),window.addEventListener("blur",at),()=>{window.removeEventListener("focus",it),window.removeEventListener("blur",at)}},[o,$,it,at]),{getRootProps:(ot={})=>{const dt=_extends({},extractEventHandlers(a),extractEventHandlers(ot));return _extends({role:"presentation"},ot,dt,{onBlur:st(dt),onFocus:lt(dt),onMouseEnter:ct(dt),onMouseLeave:rt(dt)})},onClickAway:nt}}const _excluded$1u=["onChange","maxRows","minRows","style","value"];function getStyleValue(a){return parseInt(a,10)||0}const styles$2={shadow:{visibility:"hidden",position:"absolute",overflow:"hidden",height:0,top:0,left:0,transform:"translateZ(0)"}};function isEmpty$1(a){return a==null||Object.keys(a).length===0||a.outerHeightStyle===0&&!a.overflowing}const TextareaAutosize=reactExports.forwardRef(function(i,o){const{onChange:s,maxRows:$,minRows:j=1,style:_e,value:et}=i,tt=_objectWithoutPropertiesLoose(i,_excluded$1u),{current:nt}=reactExports.useRef(et!=null),at=reactExports.useRef(null),it=useForkRef(o,at),st=reactExports.useRef(null),lt=reactExports.useCallback(()=>{const ut=at.current,dt=ownerWindow(ut).getComputedStyle(ut);if(dt.width==="0px")return{outerHeightStyle:0,overflowing:!1};const pt=st.current;pt.style.width=dt.width,pt.value=ut.value||i.placeholder||"x",pt.value.slice(-1)===`
`&&(pt.value+=" ");const mt=dt.boxSizing,ft=getStyleValue(dt.paddingBottom)+getStyleValue(dt.paddingTop),ht=getStyleValue(dt.borderBottomWidth)+getStyleValue(dt.borderTopWidth),yt=pt.scrollHeight;pt.value="x";const bt=pt.scrollHeight;let gt=yt;j&&(gt=Math.max(Number(j)*bt,gt)),$&&(gt=Math.min(Number($)*bt,gt)),gt=Math.max(gt,bt);const xt=gt+(mt==="border-box"?ft+ht:0),vt=Math.abs(gt-yt)<=1;return{outerHeightStyle:xt,overflowing:vt}},[$,j,i.placeholder]),ct=reactExports.useCallback(()=>{const ut=lt();if(isEmpty$1(ut))return;const ot=at.current;ot.style.height=`${ut.outerHeightStyle}px`,ot.style.overflow=ut.overflowing?"hidden":""},[lt]);useEnhancedEffect(()=>{const ut=()=>{ct()};let ot;const dt=debounce$1(ut),pt=at.current,mt=ownerWindow(pt);mt.addEventListener("resize",dt);let ft;return typeof ResizeObserver<"u"&&(ft=new ResizeObserver(ut),ft.observe(pt)),()=>{dt.clear(),cancelAnimationFrame(ot),mt.removeEventListener("resize",dt),ft&&ft.disconnect()}},[lt,ct]),useEnhancedEffect(()=>{ct()});const rt=ut=>{nt||ct(),s&&s(ut)};return jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx("textarea",_extends({value:et,onChange:rt,ref:it,rows:j,style:_e},tt)),jsxRuntimeExports.jsx("textarea",{"aria-hidden":!0,className:i.className,readOnly:!0,ref:st,tabIndex:-1,style:_extends({},styles$2.shadow,_e,{paddingTop:0,paddingBottom:0})})]})});function stripDiacritics(a){return typeof a.normalize<"u"?a.normalize("NFD").replace(/[\u0300-\u036f]/g,""):a}function createFilterOptions(a={}){const{ignoreAccents:i=!0,ignoreCase:o=!0,limit:s,matchFrom:$="any",stringify:j,trim:_e=!1}=a;return(et,{inputValue:tt,getOptionLabel:nt})=>{let at=_e?tt.trim():tt;o&&(at=at.toLowerCase()),i&&(at=stripDiacritics(at));const it=at?et.filter(st=>{let lt=(j||nt)(st);return o&&(lt=lt.toLowerCase()),i&&(lt=stripDiacritics(lt)),$==="start"?lt.indexOf(at)===0:lt.indexOf(at)>-1}):et;return typeof s=="number"?it.slice(0,s):it}}function findIndex(a,i){for(let o=0;o<a.length;o+=1)if(i(a[o]))return o;return-1}const defaultFilterOptions=createFilterOptions(),pageSize=5,defaultIsActiveElementInListbox=a=>{var i;return a.current!==null&&((i=a.current.parentElement)==null?void 0:i.contains(document.activeElement))};function useAutocomplete(a){const{unstable_isActiveElementInListbox:i=defaultIsActiveElementInListbox,unstable_classNamePrefix:o="Mui",autoComplete:s=!1,autoHighlight:$=!1,autoSelect:j=!1,blurOnSelect:_e=!1,clearOnBlur:et=!a.freeSolo,clearOnEscape:tt=!1,componentName:nt="useAutocomplete",defaultValue:at=a.multiple?[]:null,disableClearable:it=!1,disableCloseOnSelect:st=!1,disabled:lt,disabledItemsFocusable:ct=!1,disableListWrap:rt=!1,filterOptions:ut=defaultFilterOptions,filterSelectedOptions:ot=!1,freeSolo:dt=!1,getOptionDisabled:pt,getOptionKey:mt,getOptionLabel:ft=on=>{var mn;return(mn=on.label)!=null?mn:on},groupBy:ht,handleHomeEndKeys:yt=!a.freeSolo,id:bt,includeInputInList:gt=!1,inputValue:xt,isOptionEqualToValue:vt=(on,mn)=>on===mn,multiple:Lt=!1,onChange:$t,onClose:Tt,onHighlightChange:Et,onInputChange:Dt,onOpen:It,open:Ct,openOnFocus:jt=!1,options:Zt,readOnly:Xt=!1,selectOnFocus:sn=!a.freeSolo,value:Ft}=a,wt=useId(bt);let kt=ft;kt=on=>{const mn=ft(on);return typeof mn!="string"?String(mn):mn};const At=reactExports.useRef(!1),Pt=reactExports.useRef(!0),Mt=reactExports.useRef(null),Ot=reactExports.useRef(null),[Bt,zt]=reactExports.useState(null),[Gt,Wt]=reactExports.useState(-1),qt=$?0:-1,tn=reactExports.useRef(qt),[ln,gn]=useControlled({controlled:Ft,default:at,name:nt}),[yn,Pn]=useControlled({controlled:xt,default:"",name:nt,state:"inputValue"}),[cn,xn]=reactExports.useState(!1),hn=reactExports.useCallback((on,mn)=>{if(!(Lt?ln.length<mn.length:mn!==null)&&!et)return;let An;if(Lt)An="";else if(mn==null)An="";else{const Fn=kt(mn);An=typeof Fn=="string"?Fn:""}yn!==An&&(Pn(An),Dt&&Dt(on,An,"reset"))},[kt,yn,Lt,Dt,Pn,et,ln]),[en,Jt]=useControlled({controlled:Ct,default:!1,name:nt,state:"open"}),[vn,$n]=reactExports.useState(!0),Mn=!Lt&&ln!=null&&yn===kt(ln),On=en&&!Xt,En=On?ut(Zt.filter(on=>!(ot&&(Lt?ln:[ln]).some(mn=>mn!==null&&vt(on,mn)))),{inputValue:Mn&&vn?"":yn,getOptionLabel:kt}):[],Bn=usePreviousProps({filteredOptions:En,value:ln,inputValue:yn});reactExports.useEffect(()=>{const on=ln!==Bn.value;cn&&!on||dt&&!on||hn(null,ln)},[ln,hn,cn,Bn.value,dt]);const Hn=en&&En.length>0&&!Xt,Wn=useEventCallback(on=>{on===-1?Mt.current.focus():Bt.querySelector(`[data-tag-index="${on}"]`).focus()});reactExports.useEffect(()=>{Lt&&Gt>ln.length-1&&(Wt(-1),Wn(-1))},[ln,Lt,Gt,Wn]);function _n(on,mn){if(!Ot.current||on<0||on>=En.length)return-1;let Sn=on;for(;;){const An=Ot.current.querySelector(`[data-option-index="${Sn}"]`),Fn=ct?!1:!An||An.disabled||An.getAttribute("aria-disabled")==="true";if(An&&An.hasAttribute("tabindex")&&!Fn)return Sn;if(mn==="next"?Sn=(Sn+1)%En.length:Sn=(Sn-1+En.length)%En.length,Sn===on)return-1}}const Zn=useEventCallback(({event:on,index:mn,reason:Sn="auto"})=>{if(tn.current=mn,mn===-1?Mt.current.removeAttribute("aria-activedescendant"):Mt.current.setAttribute("aria-activedescendant",`${wt}-option-${mn}`),Et&&Et(on,mn===-1?null:En[mn],Sn),!Ot.current)return;const An=Ot.current.querySelector(`[role="option"].${o}-focused`);An&&(An.classList.remove(`${o}-focused`),An.classList.remove(`${o}-focusVisible`));let Fn=Ot.current;if(Ot.current.getAttribute("role")!=="listbox"&&(Fn=Ot.current.parentElement.querySelector('[role="listbox"]')),!Fn)return;if(mn===-1){Fn.scrollTop=0;return}const jn=Ot.current.querySelector(`[data-option-index="${mn}"]`);if(jn&&(jn.classList.add(`${o}-focused`),Sn==="keyboard"&&jn.classList.add(`${o}-focusVisible`),Fn.scrollHeight>Fn.clientHeight&&Sn!=="mouse"&&Sn!=="touch")){const Vn=jn,Kn=Fn.clientHeight+Fn.scrollTop,Gn=Vn.offsetTop+Vn.offsetHeight;Gn>Kn?Fn.scrollTop=Gn-Fn.clientHeight:Vn.offsetTop-Vn.offsetHeight*(ht?1.3:0)<Fn.scrollTop&&(Fn.scrollTop=Vn.offsetTop-Vn.offsetHeight*(ht?1.3:0))}}),bn=useEventCallback(({event:on,diff:mn,direction:Sn="next",reason:An="auto"})=>{if(!On)return;const jn=_n((()=>{const Vn=En.length-1;if(mn==="reset")return qt;if(mn==="start")return 0;if(mn==="end")return Vn;const Kn=tn.current+mn;return Kn<0?Kn===-1&&gt?-1:rt&&tn.current!==-1||Math.abs(mn)>1?0:Vn:Kn>Vn?Kn===Vn+1&&gt?-1:rt||Math.abs(mn)>1?Vn:0:Kn})(),Sn);if(Zn({index:jn,reason:An,event:on}),s&&mn!=="reset")if(jn===-1)Mt.current.value=yn;else{const Vn=kt(En[jn]);Mt.current.value=Vn,Vn.toLowerCase().indexOf(yn.toLowerCase())===0&&yn.length>0&&Mt.current.setSelectionRange(yn.length,Vn.length)}}),dn=()=>{const on=(mn,Sn)=>{const An=mn?kt(mn):"",Fn=Sn?kt(Sn):"";return An===Fn};if(tn.current!==-1&&Bn.filteredOptions&&Bn.filteredOptions.length!==En.length&&Bn.inputValue===yn&&(Lt?ln.length===Bn.value.length&&Bn.value.every((mn,Sn)=>kt(ln[Sn])===kt(mn)):on(Bn.value,ln))){const mn=Bn.filteredOptions[tn.current];if(mn)return findIndex(En,Sn=>kt(Sn)===kt(mn))}return-1},an=reactExports.useCallback(()=>{if(!On)return;const on=dn();if(on!==-1){tn.current=on;return}const mn=Lt?ln[0]:ln;if(En.length===0||mn==null){bn({diff:"reset"});return}if(Ot.current){if(mn!=null){const Sn=En[tn.current];if(Lt&&Sn&&findIndex(ln,Fn=>vt(Sn,Fn))!==-1)return;const An=findIndex(En,Fn=>vt(Fn,mn));An===-1?bn({diff:"reset"}):Zn({index:An});return}if(tn.current>=En.length-1){Zn({index:En.length-1});return}Zn({index:tn.current})}},[En.length,Lt?!1:ln,ot,bn,Zn,On,yn,Lt]),In=useEventCallback(on=>{setRef(Ot,on),on&&an()});reactExports.useEffect(()=>{an()},[an]);const Dn=on=>{en||(Jt(!0),$n(!0),It&&It(on))},Xn=(on,mn)=>{en&&(Jt(!1),Tt&&Tt(on,mn))},Yn=(on,mn,Sn,An)=>{if(Lt){if(ln.length===mn.length&&ln.every((Fn,jn)=>Fn===mn[jn]))return}else if(ln===mn)return;$t&&$t(on,mn,Sn,An),gn(mn)},pn=reactExports.useRef(!1),Ht=(on,mn,Sn="selectOption",An="options")=>{let Fn=Sn,jn=mn;if(Lt){jn=Array.isArray(ln)?ln.slice():[];const Vn=findIndex(jn,Kn=>vt(mn,Kn));Vn===-1?jn.push(mn):An!=="freeSolo"&&(jn.splice(Vn,1),Fn="removeOption")}hn(on,jn),Yn(on,jn,Fn,{option:mn}),!st&&(!on||!on.ctrlKey&&!on.metaKey)&&Xn(on,Fn),(_e===!0||_e==="touch"&&pn.current||_e==="mouse"&&!pn.current)&&Mt.current.blur()};function Kt(on,mn){if(on===-1)return-1;let Sn=on;for(;;){if(mn==="next"&&Sn===ln.length||mn==="previous"&&Sn===-1)return-1;const An=Bt.querySelector(`[data-tag-index="${Sn}"]`);if(!An||!An.hasAttribute("tabindex")||An.disabled||An.getAttribute("aria-disabled")==="true")Sn+=mn==="next"?1:-1;else return Sn}}const nn=(on,mn)=>{if(!Lt)return;yn===""&&Xn(on,"toggleInput");let Sn=Gt;Gt===-1?yn===""&&mn==="previous"&&(Sn=ln.length-1):(Sn+=mn==="next"?1:-1,Sn<0&&(Sn=0),Sn===ln.length&&(Sn=-1)),Sn=Kt(Sn,mn),Wt(Sn),Wn(Sn)},kn=on=>{At.current=!0,Pn(""),Dt&&Dt(on,"","clear"),Yn(on,Lt?[]:null,"clear")},Rn=on=>mn=>{if(on.onKeyDown&&on.onKeyDown(mn),!mn.defaultMuiPrevented&&(Gt!==-1&&["ArrowLeft","ArrowRight"].indexOf(mn.key)===-1&&(Wt(-1),Wn(-1)),mn.which!==229))switch(mn.key){case"Home":On&&yt&&(mn.preventDefault(),bn({diff:"start",direction:"next",reason:"keyboard",event:mn}));break;case"End":On&&yt&&(mn.preventDefault(),bn({diff:"end",direction:"previous",reason:"keyboard",event:mn}));break;case"PageUp":mn.preventDefault(),bn({diff:-pageSize,direction:"previous",reason:"keyboard",event:mn}),Dn(mn);break;case"PageDown":mn.preventDefault(),bn({diff:pageSize,direction:"next",reason:"keyboard",event:mn}),Dn(mn);break;case"ArrowDown":mn.preventDefault(),bn({diff:1,direction:"next",reason:"keyboard",event:mn}),Dn(mn);break;case"ArrowUp":mn.preventDefault(),bn({diff:-1,direction:"previous",reason:"keyboard",event:mn}),Dn(mn);break;case"ArrowLeft":nn(mn,"previous");break;case"ArrowRight":nn(mn,"next");break;case"Enter":if(tn.current!==-1&&On){const Sn=En[tn.current],An=pt?pt(Sn):!1;if(mn.preventDefault(),An)return;Ht(mn,Sn,"selectOption"),s&&Mt.current.setSelectionRange(Mt.current.value.length,Mt.current.value.length)}else dt&&yn!==""&&Mn===!1&&(Lt&&mn.preventDefault(),Ht(mn,yn,"createOption","freeSolo"));break;case"Escape":On?(mn.preventDefault(),mn.stopPropagation(),Xn(mn,"escape")):tt&&(yn!==""||Lt&&ln.length>0)&&(mn.preventDefault(),mn.stopPropagation(),kn(mn));break;case"Backspace":if(Lt&&!Xt&&yn===""&&ln.length>0){const Sn=Gt===-1?ln.length-1:Gt,An=ln.slice();An.splice(Sn,1),Yn(mn,An,"removeOption",{option:ln[Sn]})}break;case"Delete":if(Lt&&!Xt&&yn===""&&ln.length>0&&Gt!==-1){const Sn=Gt,An=ln.slice();An.splice(Sn,1),Yn(mn,An,"removeOption",{option:ln[Sn]})}break}},Un=on=>{xn(!0),jt&&!At.current&&Dn(on)},Tn=on=>{if(i(Ot)){Mt.current.focus();return}xn(!1),Pt.current=!0,At.current=!1,j&&tn.current!==-1&&On?Ht(on,En[tn.current],"blur"):j&&dt&&yn!==""?Ht(on,yn,"blur","freeSolo"):et&&hn(on,ln),Xn(on,"blur")},Nn=on=>{const mn=on.target.value;yn!==mn&&(Pn(mn),$n(!1),Dt&&Dt(on,mn,"input")),mn===""?!it&&!Lt&&Yn(on,null,"clear"):Dn(on)},Cn=on=>{const mn=Number(on.currentTarget.getAttribute("data-option-index"));tn.current!==mn&&Zn({event:on,index:mn,reason:"mouse"})},Ut=on=>{Zn({event:on,index:Number(on.currentTarget.getAttribute("data-option-index")),reason:"touch"}),pn.current=!0},Rt=on=>{const mn=Number(on.currentTarget.getAttribute("data-option-index"));Ht(on,En[mn],"selectOption"),pn.current=!1},Nt=on=>mn=>{const Sn=ln.slice();Sn.splice(on,1),Yn(mn,Sn,"removeOption",{option:ln[on]})},Vt=on=>{en?Xn(on,"toggleInput"):Dn(on)},Qt=on=>{on.currentTarget.contains(on.target)&&on.target.getAttribute("id")!==wt&&on.preventDefault()},rn=on=>{on.currentTarget.contains(on.target)&&(Mt.current.focus(),sn&&Pt.current&&Mt.current.selectionEnd-Mt.current.selectionStart===0&&Mt.current.select(),Pt.current=!1)},fn=on=>{!lt&&(yn===""||!en)&&Vt(on)};let Ln=dt&&yn.length>0;Ln=Ln||(Lt?ln.length>0:ln!==null);let zn=En;return ht&&(zn=En.reduce((on,mn,Sn)=>{const An=ht(mn);return on.length>0&&on[on.length-1].group===An?on[on.length-1].options.push(mn):on.push({key:Sn,index:Sn,group:An,options:[mn]}),on},[])),lt&&cn&&Tn(),{getRootProps:(on={})=>_extends({"aria-owns":Hn?`${wt}-listbox`:null},on,{onKeyDown:Rn(on),onMouseDown:Qt,onClick:rn}),getInputLabelProps:()=>({id:`${wt}-label`,htmlFor:wt}),getInputProps:()=>({id:wt,value:yn,onBlur:Tn,onFocus:Un,onChange:Nn,onMouseDown:fn,"aria-activedescendant":On?"":null,"aria-autocomplete":s?"both":"list","aria-controls":Hn?`${wt}-listbox`:void 0,"aria-expanded":Hn,autoComplete:"off",ref:Mt,autoCapitalize:"none",spellCheck:"false",role:"combobox",disabled:lt}),getClearProps:()=>({tabIndex:-1,type:"button",onClick:kn}),getPopupIndicatorProps:()=>({tabIndex:-1,type:"button",onClick:Vt}),getTagProps:({index:on})=>_extends({key:on,"data-tag-index":on,tabIndex:-1},!Xt&&{onDelete:Nt(on)}),getListboxProps:()=>({role:"listbox",id:`${wt}-listbox`,"aria-labelledby":`${wt}-label`,ref:In,onMouseDown:on=>{on.preventDefault()}}),getOptionProps:({index:on,option:mn})=>{var Sn;const An=(Lt?ln:[ln]).some(jn=>jn!=null&&vt(mn,jn)),Fn=pt?pt(mn):!1;return{key:(Sn=mt==null?void 0:mt(mn))!=null?Sn:kt(mn),tabIndex:-1,role:"option",id:`${wt}-option-${on}`,onMouseMove:Cn,onClick:Rt,onTouchStart:Ut,"data-option-index":on,"aria-disabled":Fn,"aria-selected":An}},id:wt,inputValue:yn,value:ln,dirty:Ln,expanded:On&&Bt,popupOpen:On,focused:cn||Gt!==-1,anchorEl:Bt,setAnchorEl:zt,focusedTag:Gt,groupedOptions:zn}}var useThemeWithoutDefault={};Object.defineProperty(useThemeWithoutDefault,"__esModule",{value:!0});var default_1$u=useThemeWithoutDefault.default=void 0,React=_interopRequireWildcard(reactExports),_styledEngine=require$$1;function _getRequireWildcardCache(a){if(typeof WeakMap!="function")return null;var i=new WeakMap,o=new WeakMap;return(_getRequireWildcardCache=function(s){return s?o:i})(a)}function _interopRequireWildcard(a,i){if(!i&&a&&a.__esModule)return a;if(a===null||typeof a!="object"&&typeof a!="function")return{default:a};var o=_getRequireWildcardCache(i);if(o&&o.has(a))return o.get(a);var s={__proto__:null},$=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var j in a)if(j!=="default"&&Object.prototype.hasOwnProperty.call(a,j)){var _e=$?Object.getOwnPropertyDescriptor(a,j):null;_e&&(_e.get||_e.set)?Object.defineProperty(s,j,_e):s[j]=a[j]}return s.default=a,o&&o.set(a,s),s}function isObjectEmpty(a){return Object.keys(a).length===0}function useTheme(a=null){const i=React.useContext(_styledEngine.ThemeContext);return!i||isObjectEmpty(i)?a:i}default_1$u=useThemeWithoutDefault.default=useTheme;const _excluded$1t=["anchorEl","component","components","componentsProps","container","disablePortal","keepMounted","modifiers","open","placement","popperOptions","popperRef","transition","slots","slotProps"],PopperRoot=styled(Popper$1,{name:"MuiPopper",slot:"Root",overridesResolver:(a,i)=>i.root})({}),Popper=reactExports.forwardRef(function(i,o){var s;const $=default_1$u(),j=useThemeProps$6({props:i,name:"MuiPopper"}),{anchorEl:_e,component:et,components:tt,componentsProps:nt,container:at,disablePortal:it,keepMounted:st,modifiers:lt,open:ct,placement:rt,popperOptions:ut,popperRef:ot,transition:dt,slots:pt,slotProps:mt}=j,ft=_objectWithoutPropertiesLoose(j,_excluded$1t),ht=(s=pt==null?void 0:pt.root)!=null?s:tt==null?void 0:tt.Root,yt=_extends({anchorEl:_e,container:at,disablePortal:it,keepMounted:st,modifiers:lt,open:ct,placement:rt,popperOptions:ut,popperRef:ot,transition:dt},ft);return jsxRuntimeExports.jsx(PopperRoot,_extends({as:et,direction:$==null?void 0:$.direction,slots:{root:ht},slotProps:mt??nt},yt,{ref:o}))}),MuiPopper=Popper;function getListSubheaderUtilityClass(a){return generateUtilityClass$1("MuiListSubheader",a)}generateUtilityClasses$1("MuiListSubheader",["root","colorPrimary","colorInherit","gutters","inset","sticky"]);const _excluded$1s=["className","color","component","disableGutters","disableSticky","inset"],useUtilityClasses$1f=a=>{const{classes:i,color:o,disableGutters:s,inset:$,disableSticky:j}=a,_e={root:["root",o!=="default"&&`color${capitalize$2(o)}`,!s&&"gutters",$&&"inset",!j&&"sticky"]};return composeClasses(_e,getListSubheaderUtilityClass,i)},ListSubheaderRoot=styled("li",{name:"MuiListSubheader",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.color!=="default"&&i[`color${capitalize$2(o.color)}`],!o.disableGutters&&i.gutters,o.inset&&i.inset,!o.disableSticky&&i.sticky]}})(({theme:a,ownerState:i})=>_extends({boxSizing:"border-box",lineHeight:"48px",listStyle:"none",color:(a.vars||a).palette.text.secondary,fontFamily:a.typography.fontFamily,fontWeight:a.typography.fontWeightMedium,fontSize:a.typography.pxToRem(14)},i.color==="primary"&&{color:(a.vars||a).palette.primary.main},i.color==="inherit"&&{color:"inherit"},!i.disableGutters&&{paddingLeft:16,paddingRight:16},i.inset&&{paddingLeft:72},!i.disableSticky&&{position:"sticky",top:0,zIndex:1,backgroundColor:(a.vars||a).palette.background.paper})),ListSubheader=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiListSubheader"}),{className:$,color:j="default",component:_e="li",disableGutters:et=!1,disableSticky:tt=!1,inset:nt=!1}=s,at=_objectWithoutPropertiesLoose(s,_excluded$1s),it=_extends({},s,{color:j,component:_e,disableGutters:et,disableSticky:tt,inset:nt}),st=useUtilityClasses$1f(it);return jsxRuntimeExports.jsx(ListSubheaderRoot,_extends({as:_e,className:clsx(st.root,$),ref:o,ownerState:it},at))});ListSubheader.muiSkipListHighlight=!0;const ListSubheader$1=ListSubheader,CancelIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M12 2C6.47 2 2 6.47 2 12s4.47 10 10 10 10-4.47 10-10S17.53 2 12 2zm5 13.59L15.59 17 12 13.41 8.41 17 7 15.59 10.59 12 7 8.41 8.41 7 12 10.59 15.59 7 17 8.41 13.41 12 17 15.59z"}),"Cancel");function getChipUtilityClass(a){return generateUtilityClass$1("MuiChip",a)}const chipClasses=generateUtilityClasses$1("MuiChip",["root","sizeSmall","sizeMedium","colorError","colorInfo","colorPrimary","colorSecondary","colorSuccess","colorWarning","disabled","clickable","clickableColorPrimary","clickableColorSecondary","deletable","deletableColorPrimary","deletableColorSecondary","outlined","filled","outlinedPrimary","outlinedSecondary","filledPrimary","filledSecondary","avatar","avatarSmall","avatarMedium","avatarColorPrimary","avatarColorSecondary","icon","iconSmall","iconMedium","iconColorPrimary","iconColorSecondary","label","labelSmall","labelMedium","deleteIcon","deleteIconSmall","deleteIconMedium","deleteIconColorPrimary","deleteIconColorSecondary","deleteIconOutlinedColorPrimary","deleteIconOutlinedColorSecondary","deleteIconFilledColorPrimary","deleteIconFilledColorSecondary","focusVisible"]),chipClasses$1=chipClasses,_excluded$1r=["avatar","className","clickable","color","component","deleteIcon","disabled","icon","label","onClick","onDelete","onKeyDown","onKeyUp","size","variant","tabIndex","skipFocusWhenDisabled"],useUtilityClasses$1e=a=>{const{classes:i,disabled:o,size:s,color:$,iconColor:j,onDelete:_e,clickable:et,variant:tt}=a,nt={root:["root",tt,o&&"disabled",`size${capitalize$2(s)}`,`color${capitalize$2($)}`,et&&"clickable",et&&`clickableColor${capitalize$2($)}`,_e&&"deletable",_e&&`deletableColor${capitalize$2($)}`,`${tt}${capitalize$2($)}`],label:["label",`label${capitalize$2(s)}`],avatar:["avatar",`avatar${capitalize$2(s)}`,`avatarColor${capitalize$2($)}`],icon:["icon",`icon${capitalize$2(s)}`,`iconColor${capitalize$2(j)}`],deleteIcon:["deleteIcon",`deleteIcon${capitalize$2(s)}`,`deleteIconColor${capitalize$2($)}`,`deleteIcon${capitalize$2(tt)}Color${capitalize$2($)}`]};return composeClasses(nt,getChipUtilityClass,i)},ChipRoot=styled("div",{name:"MuiChip",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a,{color:s,iconColor:$,clickable:j,onDelete:_e,size:et,variant:tt}=o;return[{[`& .${chipClasses$1.avatar}`]:i.avatar},{[`& .${chipClasses$1.avatar}`]:i[`avatar${capitalize$2(et)}`]},{[`& .${chipClasses$1.avatar}`]:i[`avatarColor${capitalize$2(s)}`]},{[`& .${chipClasses$1.icon}`]:i.icon},{[`& .${chipClasses$1.icon}`]:i[`icon${capitalize$2(et)}`]},{[`& .${chipClasses$1.icon}`]:i[`iconColor${capitalize$2($)}`]},{[`& .${chipClasses$1.deleteIcon}`]:i.deleteIcon},{[`& .${chipClasses$1.deleteIcon}`]:i[`deleteIcon${capitalize$2(et)}`]},{[`& .${chipClasses$1.deleteIcon}`]:i[`deleteIconColor${capitalize$2(s)}`]},{[`& .${chipClasses$1.deleteIcon}`]:i[`deleteIcon${capitalize$2(tt)}Color${capitalize$2(s)}`]},i.root,i[`size${capitalize$2(et)}`],i[`color${capitalize$2(s)}`],j&&i.clickable,j&&s!=="default"&&i[`clickableColor${capitalize$2(s)})`],_e&&i.deletable,_e&&s!=="default"&&i[`deletableColor${capitalize$2(s)}`],i[tt],i[`${tt}${capitalize$2(s)}`]]}})(({theme:a,ownerState:i})=>{const o=a.palette.mode==="light"?a.palette.grey[700]:a.palette.grey[300];return _extends({maxWidth:"100%",fontFamily:a.typography.fontFamily,fontSize:a.typography.pxToRem(13),display:"inline-flex",alignItems:"center",justifyContent:"center",height:32,color:(a.vars||a).palette.text.primary,backgroundColor:(a.vars||a).palette.action.selected,borderRadius:32/2,whiteSpace:"nowrap",transition:a.transitions.create(["background-color","box-shadow"]),cursor:"unset",outline:0,textDecoration:"none",border:0,padding:0,verticalAlign:"middle",boxSizing:"border-box",[`&.${chipClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity,pointerEvents:"none"},[`& .${chipClasses$1.avatar}`]:{marginLeft:5,marginRight:-6,width:24,height:24,color:a.vars?a.vars.palette.Chip.defaultAvatarColor:o,fontSize:a.typography.pxToRem(12)},[`& .${chipClasses$1.avatarColorPrimary}`]:{color:(a.vars||a).palette.primary.contrastText,backgroundColor:(a.vars||a).palette.primary.dark},[`& .${chipClasses$1.avatarColorSecondary}`]:{color:(a.vars||a).palette.secondary.contrastText,backgroundColor:(a.vars||a).palette.secondary.dark},[`& .${chipClasses$1.avatarSmall}`]:{marginLeft:4,marginRight:-4,width:18,height:18,fontSize:a.typography.pxToRem(10)},[`& .${chipClasses$1.icon}`]:_extends({marginLeft:5,marginRight:-6},i.size==="small"&&{fontSize:18,marginLeft:4,marginRight:-4},i.iconColor===i.color&&_extends({color:a.vars?a.vars.palette.Chip.defaultIconColor:o},i.color!=="default"&&{color:"inherit"})),[`& .${chipClasses$1.deleteIcon}`]:_extends({WebkitTapHighlightColor:"transparent",color:a.vars?`rgba(${a.vars.palette.text.primaryChannel} / 0.26)`:alpha_1(a.palette.text.primary,.26),fontSize:22,cursor:"pointer",margin:"0 5px 0 -6px","&:hover":{color:a.vars?`rgba(${a.vars.palette.text.primaryChannel} / 0.4)`:alpha_1(a.palette.text.primary,.4)}},i.size==="small"&&{fontSize:16,marginRight:4,marginLeft:-4},i.color!=="default"&&{color:a.vars?`rgba(${a.vars.palette[i.color].contrastTextChannel} / 0.7)`:alpha_1(a.palette[i.color].contrastText,.7),"&:hover, &:active":{color:(a.vars||a).palette[i.color].contrastText}})},i.size==="small"&&{height:24},i.color!=="default"&&{backgroundColor:(a.vars||a).palette[i.color].main,color:(a.vars||a).palette[i.color].contrastText},i.onDelete&&{[`&.${chipClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.action.selectedChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.action.selected,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)}},i.onDelete&&i.color!=="default"&&{[`&.${chipClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette[i.color].dark}})},({theme:a,ownerState:i})=>_extends({},i.clickable&&{userSelect:"none",WebkitTapHighlightColor:"transparent",cursor:"pointer","&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.selectedChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.action.selected,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity)},[`&.${chipClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.action.selectedChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.action.selected,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)},"&:active":{boxShadow:(a.vars||a).shadows[1]}},i.clickable&&i.color!=="default"&&{[`&:hover, &.${chipClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette[i.color].dark}}),({theme:a,ownerState:i})=>_extends({},i.variant==="outlined"&&{backgroundColor:"transparent",border:a.vars?`1px solid ${a.vars.palette.Chip.defaultBorder}`:`1px solid ${a.palette.mode==="light"?a.palette.grey[400]:a.palette.grey[700]}`,[`&.${chipClasses$1.clickable}:hover`]:{backgroundColor:(a.vars||a).palette.action.hover},[`&.${chipClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},[`& .${chipClasses$1.avatar}`]:{marginLeft:4},[`& .${chipClasses$1.avatarSmall}`]:{marginLeft:2},[`& .${chipClasses$1.icon}`]:{marginLeft:4},[`& .${chipClasses$1.iconSmall}`]:{marginLeft:2},[`& .${chipClasses$1.deleteIcon}`]:{marginRight:5},[`& .${chipClasses$1.deleteIconSmall}`]:{marginRight:3}},i.variant==="outlined"&&i.color!=="default"&&{color:(a.vars||a).palette[i.color].main,border:`1px solid ${a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / 0.7)`:alpha_1(a.palette[i.color].main,.7)}`,[`&.${chipClasses$1.clickable}:hover`]:{backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(a.palette[i.color].main,a.palette.action.hoverOpacity)},[`&.${chipClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / ${a.vars.palette.action.focusOpacity})`:alpha_1(a.palette[i.color].main,a.palette.action.focusOpacity)},[`& .${chipClasses$1.deleteIcon}`]:{color:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / 0.7)`:alpha_1(a.palette[i.color].main,.7),"&:hover, &:active":{color:(a.vars||a).palette[i.color].main}}})),ChipLabel=styled("span",{name:"MuiChip",slot:"Label",overridesResolver:(a,i)=>{const{ownerState:o}=a,{size:s}=o;return[i.label,i[`label${capitalize$2(s)}`]]}})(({ownerState:a})=>_extends({overflow:"hidden",textOverflow:"ellipsis",paddingLeft:12,paddingRight:12,whiteSpace:"nowrap"},a.variant==="outlined"&&{paddingLeft:11,paddingRight:11},a.size==="small"&&{paddingLeft:8,paddingRight:8},a.size==="small"&&a.variant==="outlined"&&{paddingLeft:7,paddingRight:7}));function isDeleteKeyboardEvent(a){return a.key==="Backspace"||a.key==="Delete"}const Chip=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiChip"}),{avatar:$,className:j,clickable:_e,color:et="default",component:tt,deleteIcon:nt,disabled:at=!1,icon:it,label:st,onClick:lt,onDelete:ct,onKeyDown:rt,onKeyUp:ut,size:ot="medium",variant:dt="filled",tabIndex:pt,skipFocusWhenDisabled:mt=!1}=s,ft=_objectWithoutPropertiesLoose(s,_excluded$1r),ht=reactExports.useRef(null),yt=useForkRef(ht,o),bt=jt=>{jt.stopPropagation(),ct&&ct(jt)},gt=jt=>{jt.currentTarget===jt.target&&isDeleteKeyboardEvent(jt)&&jt.preventDefault(),rt&&rt(jt)},xt=jt=>{jt.currentTarget===jt.target&&(ct&&isDeleteKeyboardEvent(jt)?ct(jt):jt.key==="Escape"&&ht.current&&ht.current.blur()),ut&&ut(jt)},vt=_e!==!1&&lt?!0:_e,Lt=vt||ct?ButtonBase$1:tt||"div",$t=_extends({},s,{component:Lt,disabled:at,size:ot,color:et,iconColor:reactExports.isValidElement(it)&&it.props.color||et,onDelete:!!ct,clickable:vt,variant:dt}),Tt=useUtilityClasses$1e($t),Et=Lt===ButtonBase$1?_extends({component:tt||"div",focusVisibleClassName:Tt.focusVisible},ct&&{disableRipple:!0}):{};let Dt=null;ct&&(Dt=nt&&reactExports.isValidElement(nt)?reactExports.cloneElement(nt,{className:clsx(nt.props.className,Tt.deleteIcon),onClick:bt}):jsxRuntimeExports.jsx(CancelIcon,{className:clsx(Tt.deleteIcon),onClick:bt}));let It=null;$&&reactExports.isValidElement($)&&(It=reactExports.cloneElement($,{className:clsx(Tt.avatar,$.props.className)}));let Ct=null;return it&&reactExports.isValidElement(it)&&(Ct=reactExports.cloneElement(it,{className:clsx(Tt.icon,it.props.className)})),jsxRuntimeExports.jsxs(ChipRoot,_extends({as:Lt,className:clsx(Tt.root,j),disabled:vt&&at?!0:void 0,onClick:lt,onKeyDown:gt,onKeyUp:xt,ref:yt,tabIndex:mt&&at?-1:pt,ownerState:$t},Et,ft,{children:[It||Ct,jsxRuntimeExports.jsx(ChipLabel,{className:clsx(Tt.label),ownerState:$t,children:st}),Dt]}))}),Chip$1=Chip;function formControlState({props:a,states:i,muiFormControl:o}){return i.reduce((s,$)=>(s[$]=a[$],o&&typeof a[$]>"u"&&(s[$]=o[$]),s),{})}const FormControlContext=reactExports.createContext(void 0),FormControlContext$1=FormControlContext;function useFormControl(){return reactExports.useContext(FormControlContext$1)}function GlobalStyles(a){return jsxRuntimeExports.jsx(GlobalStyles$1,_extends({},a,{defaultTheme:defaultTheme$2,themeId:THEME_ID}))}function hasValue(a){return a!=null&&!(Array.isArray(a)&&a.length===0)}function isFilled(a,i=!1){return a&&(hasValue(a.value)&&a.value!==""||i&&hasValue(a.defaultValue)&&a.defaultValue!=="")}function isAdornedStart(a){return a.startAdornment}function getInputBaseUtilityClass(a){return generateUtilityClass$1("MuiInputBase",a)}const inputBaseClasses=generateUtilityClasses$1("MuiInputBase",["root","formControl","focused","disabled","adornedStart","adornedEnd","error","sizeSmall","multiline","colorSecondary","fullWidth","hiddenLabel","readOnly","input","inputSizeSmall","inputMultiline","inputTypeSearch","inputAdornedStart","inputAdornedEnd","inputHiddenLabel"]),inputBaseClasses$1=inputBaseClasses,_excluded$1q=["aria-describedby","autoComplete","autoFocus","className","color","components","componentsProps","defaultValue","disabled","disableInjectingGlobalStyles","endAdornment","error","fullWidth","id","inputComponent","inputProps","inputRef","margin","maxRows","minRows","multiline","name","onBlur","onChange","onClick","onFocus","onKeyDown","onKeyUp","placeholder","readOnly","renderSuffix","rows","size","slotProps","slots","startAdornment","type","value"],rootOverridesResolver=(a,i)=>{const{ownerState:o}=a;return[i.root,o.formControl&&i.formControl,o.startAdornment&&i.adornedStart,o.endAdornment&&i.adornedEnd,o.error&&i.error,o.size==="small"&&i.sizeSmall,o.multiline&&i.multiline,o.color&&i[`color${capitalize$2(o.color)}`],o.fullWidth&&i.fullWidth,o.hiddenLabel&&i.hiddenLabel]},inputOverridesResolver=(a,i)=>{const{ownerState:o}=a;return[i.input,o.size==="small"&&i.inputSizeSmall,o.multiline&&i.inputMultiline,o.type==="search"&&i.inputTypeSearch,o.startAdornment&&i.inputAdornedStart,o.endAdornment&&i.inputAdornedEnd,o.hiddenLabel&&i.inputHiddenLabel]},useUtilityClasses$1d=a=>{const{classes:i,color:o,disabled:s,error:$,endAdornment:j,focused:_e,formControl:et,fullWidth:tt,hiddenLabel:nt,multiline:at,readOnly:it,size:st,startAdornment:lt,type:ct}=a,rt={root:["root",`color${capitalize$2(o)}`,s&&"disabled",$&&"error",tt&&"fullWidth",_e&&"focused",et&&"formControl",st&&st!=="medium"&&`size${capitalize$2(st)}`,at&&"multiline",lt&&"adornedStart",j&&"adornedEnd",nt&&"hiddenLabel",it&&"readOnly"],input:["input",s&&"disabled",ct==="search"&&"inputTypeSearch",at&&"inputMultiline",st==="small"&&"inputSizeSmall",nt&&"inputHiddenLabel",lt&&"inputAdornedStart",j&&"inputAdornedEnd",it&&"readOnly"]};return composeClasses(rt,getInputBaseUtilityClass,i)},InputBaseRoot=styled("div",{name:"MuiInputBase",slot:"Root",overridesResolver:rootOverridesResolver})(({theme:a,ownerState:i})=>_extends({},a.typography.body1,{color:(a.vars||a).palette.text.primary,lineHeight:"1.4375em",boxSizing:"border-box",position:"relative",cursor:"text",display:"inline-flex",alignItems:"center",[`&.${inputBaseClasses$1.disabled}`]:{color:(a.vars||a).palette.text.disabled,cursor:"default"}},i.multiline&&_extends({padding:"4px 0 5px"},i.size==="small"&&{paddingTop:1}),i.fullWidth&&{width:"100%"})),InputBaseComponent=styled("input",{name:"MuiInputBase",slot:"Input",overridesResolver:inputOverridesResolver})(({theme:a,ownerState:i})=>{const o=a.palette.mode==="light",s=_extends({color:"currentColor"},a.vars?{opacity:a.vars.opacity.inputPlaceholder}:{opacity:o?.42:.5},{transition:a.transitions.create("opacity",{duration:a.transitions.duration.shorter})}),$={opacity:"0 !important"},j=a.vars?{opacity:a.vars.opacity.inputPlaceholder}:{opacity:o?.42:.5};return _extends({font:"inherit",letterSpacing:"inherit",color:"currentColor",padding:"4px 0 5px",border:0,boxSizing:"content-box",background:"none",height:"1.4375em",margin:0,WebkitTapHighlightColor:"transparent",display:"block",minWidth:0,width:"100%",animationName:"mui-auto-fill-cancel",animationDuration:"10ms","&::-webkit-input-placeholder":s,"&::-moz-placeholder":s,"&:-ms-input-placeholder":s,"&::-ms-input-placeholder":s,"&:focus":{outline:0},"&:invalid":{boxShadow:"none"},"&::-webkit-search-decoration":{WebkitAppearance:"none"},[`label[data-shrink=false] + .${inputBaseClasses$1.formControl} &`]:{"&::-webkit-input-placeholder":$,"&::-moz-placeholder":$,"&:-ms-input-placeholder":$,"&::-ms-input-placeholder":$,"&:focus::-webkit-input-placeholder":j,"&:focus::-moz-placeholder":j,"&:focus:-ms-input-placeholder":j,"&:focus::-ms-input-placeholder":j},[`&.${inputBaseClasses$1.disabled}`]:{opacity:1,WebkitTextFillColor:(a.vars||a).palette.text.disabled},"&:-webkit-autofill":{animationDuration:"5000s",animationName:"mui-auto-fill"}},i.size==="small"&&{paddingTop:1},i.multiline&&{height:"auto",resize:"none",padding:0,paddingTop:0},i.type==="search"&&{MozAppearance:"textfield"})}),inputGlobalStyles=jsxRuntimeExports.jsx(GlobalStyles,{styles:{"@keyframes mui-auto-fill":{from:{display:"block"}},"@keyframes mui-auto-fill-cancel":{from:{display:"block"}}}}),InputBase=reactExports.forwardRef(function(i,o){var s;const $=useThemeProps$6({props:i,name:"MuiInputBase"}),{"aria-describedby":j,autoComplete:_e,autoFocus:et,className:tt,components:nt={},componentsProps:at={},defaultValue:it,disabled:st,disableInjectingGlobalStyles:lt,endAdornment:ct,fullWidth:rt=!1,id:ut,inputComponent:ot="input",inputProps:dt={},inputRef:pt,maxRows:mt,minRows:ft,multiline:ht=!1,name:yt,onBlur:bt,onChange:gt,onClick:xt,onFocus:vt,onKeyDown:Lt,onKeyUp:$t,placeholder:Tt,readOnly:Et,renderSuffix:Dt,rows:It,slotProps:Ct={},slots:jt={},startAdornment:Zt,type:Xt="text",value:sn}=$,Ft=_objectWithoutPropertiesLoose($,_excluded$1q),wt=dt.value!=null?dt.value:sn,{current:kt}=reactExports.useRef(wt!=null),At=reactExports.useRef(),Pt=reactExports.useCallback(On=>{},[]),Mt=useForkRef(At,pt,dt.ref,Pt),[Ot,Bt]=reactExports.useState(!1),zt=useFormControl(),Gt=formControlState({props:$,muiFormControl:zt,states:["color","disabled","error","hiddenLabel","size","required","filled"]});Gt.focused=zt?zt.focused:Ot,reactExports.useEffect(()=>{!zt&&st&&Ot&&(Bt(!1),bt&&bt())},[zt,st,Ot,bt]);const Wt=zt&&zt.onFilled,qt=zt&&zt.onEmpty,tn=reactExports.useCallback(On=>{isFilled(On)?Wt&&Wt():qt&&qt()},[Wt,qt]);useEnhancedEffect(()=>{kt&&tn({value:wt})},[wt,tn,kt]);const ln=On=>{if(Gt.disabled){On.stopPropagation();return}vt&&vt(On),dt.onFocus&&dt.onFocus(On),zt&&zt.onFocus?zt.onFocus(On):Bt(!0)},gn=On=>{bt&&bt(On),dt.onBlur&&dt.onBlur(On),zt&&zt.onBlur?zt.onBlur(On):Bt(!1)},yn=(On,...En)=>{if(!kt){const Bn=On.target||At.current;if(Bn==null)throw new Error(formatMuiErrorMessage$1(1));tn({value:Bn.value})}dt.onChange&&dt.onChange(On,...En),gt&&gt(On,...En)};reactExports.useEffect(()=>{tn(At.current)},[]);const Pn=On=>{At.current&&On.currentTarget===On.target&&At.current.focus(),xt&&xt(On)};let cn=ot,xn=dt;ht&&cn==="input"&&(It?xn=_extends({type:void 0,minRows:It,maxRows:It},xn):xn=_extends({type:void 0,maxRows:mt,minRows:ft},xn),cn=TextareaAutosize);const hn=On=>{tn(On.animationName==="mui-auto-fill-cancel"?At.current:{value:"x"})};reactExports.useEffect(()=>{zt&&zt.setAdornedStart(!!Zt)},[zt,Zt]);const en=_extends({},$,{color:Gt.color||"primary",disabled:Gt.disabled,endAdornment:ct,error:Gt.error,focused:Gt.focused,formControl:zt,fullWidth:rt,hiddenLabel:Gt.hiddenLabel,multiline:ht,size:Gt.size,startAdornment:Zt,type:Xt}),Jt=useUtilityClasses$1d(en),vn=jt.root||nt.Root||InputBaseRoot,$n=Ct.root||at.root||{},Mn=jt.input||nt.Input||InputBaseComponent;return xn=_extends({},xn,(s=Ct.input)!=null?s:at.input),jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[!lt&&inputGlobalStyles,jsxRuntimeExports.jsxs(vn,_extends({},$n,!isHostComponent(vn)&&{ownerState:_extends({},en,$n.ownerState)},{ref:o,onClick:Pn},Ft,{className:clsx(Jt.root,$n.className,tt,Et&&"MuiInputBase-readOnly"),children:[Zt,jsxRuntimeExports.jsx(FormControlContext$1.Provider,{value:null,children:jsxRuntimeExports.jsx(Mn,_extends({ownerState:en,"aria-invalid":Gt.error,"aria-describedby":j,autoComplete:_e,autoFocus:et,defaultValue:it,disabled:Gt.disabled,id:ut,onAnimationStart:hn,name:yt,placeholder:Tt,readOnly:Et,required:Gt.required,rows:It,value:wt,onKeyDown:Lt,onKeyUp:$t,type:Xt},xn,!isHostComponent(Mn)&&{as:cn,ownerState:_extends({},en,xn.ownerState)},{ref:Mt,className:clsx(Jt.input,xn.className,Et&&"MuiInputBase-readOnly"),onBlur:gn,onChange:yn,onFocus:ln}))}),ct,Dt?Dt(_extends({},Gt,{startAdornment:Zt})):null]}))]})}),InputBase$1=InputBase;function getInputUtilityClass(a){return generateUtilityClass$1("MuiInput",a)}const inputClasses=_extends({},inputBaseClasses$1,generateUtilityClasses$1("MuiInput",["root","underline","input"])),inputClasses$1=inputClasses;function getOutlinedInputUtilityClass(a){return generateUtilityClass$1("MuiOutlinedInput",a)}const outlinedInputClasses=_extends({},inputBaseClasses$1,generateUtilityClasses$1("MuiOutlinedInput",["root","notchedOutline","input"])),outlinedInputClasses$1=outlinedInputClasses;function getFilledInputUtilityClass(a){return generateUtilityClass$1("MuiFilledInput",a)}const filledInputClasses=_extends({},inputBaseClasses$1,generateUtilityClasses$1("MuiFilledInput",["root","underline","input"])),filledInputClasses$1=filledInputClasses,ArrowDropDownIcon$1=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M7 10l5 5 5-5z"}),"ArrowDropDown");function getAutocompleteUtilityClass(a){return generateUtilityClass$1("MuiAutocomplete",a)}const autocompleteClasses=generateUtilityClasses$1("MuiAutocomplete",["root","expanded","fullWidth","focused","focusVisible","tag","tagSizeSmall","tagSizeMedium","hasPopupIcon","hasClearIcon","inputRoot","input","inputFocused","endAdornment","clearIndicator","popupIndicator","popupIndicatorOpen","popper","popperDisablePortal","paper","listbox","loading","noOptions","option","groupLabel","groupUl"]);var _ClearIcon,_ArrowDropDownIcon;const _excluded$1p=["autoComplete","autoHighlight","autoSelect","blurOnSelect","ChipProps","className","clearIcon","clearOnBlur","clearOnEscape","clearText","closeText","componentsProps","defaultValue","disableClearable","disableCloseOnSelect","disabled","disabledItemsFocusable","disableListWrap","disablePortal","filterOptions","filterSelectedOptions","forcePopupIcon","freeSolo","fullWidth","getLimitTagsText","getOptionDisabled","getOptionKey","getOptionLabel","isOptionEqualToValue","groupBy","handleHomeEndKeys","id","includeInputInList","inputValue","limitTags","ListboxComponent","ListboxProps","loading","loadingText","multiple","noOptionsText","onChange","onClose","onHighlightChange","onInputChange","onOpen","open","openOnFocus","openText","options","PaperComponent","PopperComponent","popupIcon","readOnly","renderGroup","renderInput","renderOption","renderTags","selectOnFocus","size","slotProps","value"],_excluded2$c=["ref"],useThemeProps$1=createUseThemeProps(),useUtilityClasses$1c=a=>{const{classes:i,disablePortal:o,expanded:s,focused:$,fullWidth:j,hasClearIcon:_e,hasPopupIcon:et,inputFocused:tt,popupOpen:nt,size:at}=a,it={root:["root",s&&"expanded",$&&"focused",j&&"fullWidth",_e&&"hasClearIcon",et&&"hasPopupIcon"],inputRoot:["inputRoot"],input:["input",tt&&"inputFocused"],tag:["tag",`tagSize${capitalize$2(at)}`],endAdornment:["endAdornment"],clearIndicator:["clearIndicator"],popupIndicator:["popupIndicator",nt&&"popupIndicatorOpen"],popper:["popper",o&&"popperDisablePortal"],paper:["paper"],listbox:["listbox"],loading:["loading"],noOptions:["noOptions"],option:["option"],groupLabel:["groupLabel"],groupUl:["groupUl"]};return composeClasses(it,getAutocompleteUtilityClass,i)},AutocompleteRoot=styled("div",{name:"MuiAutocomplete",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a,{fullWidth:s,hasClearIcon:$,hasPopupIcon:j,inputFocused:_e,size:et}=o;return[{[`& .${autocompleteClasses.tag}`]:i.tag},{[`& .${autocompleteClasses.tag}`]:i[`tagSize${capitalize$2(et)}`]},{[`& .${autocompleteClasses.inputRoot}`]:i.inputRoot},{[`& .${autocompleteClasses.input}`]:i.input},{[`& .${autocompleteClasses.input}`]:_e&&i.inputFocused},i.root,s&&i.fullWidth,j&&i.hasPopupIcon,$&&i.hasClearIcon]}})({[`&.${autocompleteClasses.focused} .${autocompleteClasses.clearIndicator}`]:{visibility:"visible"},"@media (pointer: fine)":{[`&:hover .${autocompleteClasses.clearIndicator}`]:{visibility:"visible"}},[`& .${autocompleteClasses.tag}`]:{margin:3,maxWidth:"calc(100% - 6px)"},[`& .${autocompleteClasses.inputRoot}`]:{flexWrap:"wrap",[`.${autocompleteClasses.hasPopupIcon}&, .${autocompleteClasses.hasClearIcon}&`]:{paddingRight:30},[`.${autocompleteClasses.hasPopupIcon}.${autocompleteClasses.hasClearIcon}&`]:{paddingRight:56},[`& .${autocompleteClasses.input}`]:{width:0,minWidth:30}},[`& .${inputClasses$1.root}`]:{paddingBottom:1,"& .MuiInput-input":{padding:"4px 4px 4px 0px"}},[`& .${inputClasses$1.root}.${inputBaseClasses$1.sizeSmall}`]:{[`& .${inputClasses$1.input}`]:{padding:"2px 4px 3px 0"}},[`& .${outlinedInputClasses$1.root}`]:{padding:9,[`.${autocompleteClasses.hasPopupIcon}&, .${autocompleteClasses.hasClearIcon}&`]:{paddingRight:39},[`.${autocompleteClasses.hasPopupIcon}.${autocompleteClasses.hasClearIcon}&`]:{paddingRight:65},[`& .${autocompleteClasses.input}`]:{padding:"7.5px 4px 7.5px 5px"},[`& .${autocompleteClasses.endAdornment}`]:{right:9}},[`& .${outlinedInputClasses$1.root}.${inputBaseClasses$1.sizeSmall}`]:{paddingTop:6,paddingBottom:6,paddingLeft:6,[`& .${autocompleteClasses.input}`]:{padding:"2.5px 4px 2.5px 8px"}},[`& .${filledInputClasses$1.root}`]:{paddingTop:19,paddingLeft:8,[`.${autocompleteClasses.hasPopupIcon}&, .${autocompleteClasses.hasClearIcon}&`]:{paddingRight:39},[`.${autocompleteClasses.hasPopupIcon}.${autocompleteClasses.hasClearIcon}&`]:{paddingRight:65},[`& .${filledInputClasses$1.input}`]:{padding:"7px 4px"},[`& .${autocompleteClasses.endAdornment}`]:{right:9}},[`& .${filledInputClasses$1.root}.${inputBaseClasses$1.sizeSmall}`]:{paddingBottom:1,[`& .${filledInputClasses$1.input}`]:{padding:"2.5px 4px"}},[`& .${inputBaseClasses$1.hiddenLabel}`]:{paddingTop:8},[`& .${filledInputClasses$1.root}.${inputBaseClasses$1.hiddenLabel}`]:{paddingTop:0,paddingBottom:0,[`& .${autocompleteClasses.input}`]:{paddingTop:16,paddingBottom:17}},[`& .${filledInputClasses$1.root}.${inputBaseClasses$1.hiddenLabel}.${inputBaseClasses$1.sizeSmall}`]:{[`& .${autocompleteClasses.input}`]:{paddingTop:8,paddingBottom:9}},[`& .${autocompleteClasses.input}`]:{flexGrow:1,textOverflow:"ellipsis",opacity:0},variants:[{props:{fullWidth:!0},style:{width:"100%"}},{props:{size:"small"},style:{[`& .${autocompleteClasses.tag}`]:{margin:2,maxWidth:"calc(100% - 4px)"}}},{props:{inputFocused:!0},style:{[`& .${autocompleteClasses.input}`]:{opacity:1}}}]}),AutocompleteEndAdornment=styled("div",{name:"MuiAutocomplete",slot:"EndAdornment",overridesResolver:(a,i)=>i.endAdornment})({position:"absolute",right:0,top:"50%",transform:"translate(0, -50%)"}),AutocompleteClearIndicator=styled(IconButton$1,{name:"MuiAutocomplete",slot:"ClearIndicator",overridesResolver:(a,i)=>i.clearIndicator})({marginRight:-2,padding:4,visibility:"hidden"}),AutocompletePopupIndicator=styled(IconButton$1,{name:"MuiAutocomplete",slot:"PopupIndicator",overridesResolver:({ownerState:a},i)=>_extends({},i.popupIndicator,a.popupOpen&&i.popupIndicatorOpen)})({padding:2,marginRight:-2,variants:[{props:{popupOpen:!0},style:{transform:"rotate(180deg)"}}]}),AutocompletePopper=styled(MuiPopper,{name:"MuiAutocomplete",slot:"Popper",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`& .${autocompleteClasses.option}`]:i.option},i.popper,o.disablePortal&&i.popperDisablePortal]}})(({theme:a})=>({zIndex:(a.vars||a).zIndex.modal,variants:[{props:{disablePortal:!0},style:{position:"absolute"}}]})),AutocompletePaper=styled(Paper$1,{name:"MuiAutocomplete",slot:"Paper",overridesResolver:(a,i)=>i.paper})(({theme:a})=>_extends({},a.typography.body1,{overflow:"auto"})),AutocompleteLoading=styled("div",{name:"MuiAutocomplete",slot:"Loading",overridesResolver:(a,i)=>i.loading})(({theme:a})=>({color:(a.vars||a).palette.text.secondary,padding:"14px 16px"})),AutocompleteNoOptions=styled("div",{name:"MuiAutocomplete",slot:"NoOptions",overridesResolver:(a,i)=>i.noOptions})(({theme:a})=>({color:(a.vars||a).palette.text.secondary,padding:"14px 16px"})),AutocompleteListbox=styled("div",{name:"MuiAutocomplete",slot:"Listbox",overridesResolver:(a,i)=>i.listbox})(({theme:a})=>({listStyle:"none",margin:0,padding:"8px 0",maxHeight:"40vh",overflow:"auto",position:"relative",[`& .${autocompleteClasses.option}`]:{minHeight:48,display:"flex",overflow:"hidden",justifyContent:"flex-start",alignItems:"center",cursor:"pointer",paddingTop:6,boxSizing:"border-box",outline:"0",WebkitTapHighlightColor:"transparent",paddingBottom:6,paddingLeft:16,paddingRight:16,[a.breakpoints.up("sm")]:{minHeight:"auto"},[`&.${autocompleteClasses.focused}`]:{backgroundColor:(a.vars||a).palette.action.hover,"@media (hover: none)":{backgroundColor:"transparent"}},'&[aria-disabled="true"]':{opacity:(a.vars||a).palette.action.disabledOpacity,pointerEvents:"none"},[`&.${autocompleteClasses.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},'&[aria-selected="true"]':{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity),[`&.${autocompleteClasses.focused}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:(a.vars||a).palette.action.selected}},[`&.${autocompleteClasses.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)}}}})),AutocompleteGroupLabel=styled(ListSubheader$1,{name:"MuiAutocomplete",slot:"GroupLabel",overridesResolver:(a,i)=>i.groupLabel})(({theme:a})=>({backgroundColor:(a.vars||a).palette.background.paper,top:-8})),AutocompleteGroupUl=styled("ul",{name:"MuiAutocomplete",slot:"GroupUl",overridesResolver:(a,i)=>i.groupUl})({padding:0,[`& .${autocompleteClasses.option}`]:{paddingLeft:24}}),Autocomplete=reactExports.forwardRef(function(i,o){var s,$,j,_e;const et=useThemeProps$1({props:i,name:"MuiAutocomplete"}),{autoComplete:tt=!1,autoHighlight:nt=!1,autoSelect:at=!1,blurOnSelect:it=!1,ChipProps:st,className:lt,clearIcon:ct=_ClearIcon||(_ClearIcon=jsxRuntimeExports.jsx(ClearIcon$1,{fontSize:"small"})),clearOnBlur:rt=!et.freeSolo,clearOnEscape:ut=!1,clearText:ot="Clear",closeText:dt="Close",componentsProps:pt={},defaultValue:mt=et.multiple?[]:null,disableClearable:ft=!1,disableCloseOnSelect:ht=!1,disabled:yt=!1,disabledItemsFocusable:bt=!1,disableListWrap:gt=!1,disablePortal:xt=!1,filterSelectedOptions:vt=!1,forcePopupIcon:Lt="auto",freeSolo:$t=!1,fullWidth:Tt=!1,getLimitTagsText:Et=An=>`+${An}`,getOptionLabel:Dt,groupBy:It,handleHomeEndKeys:Ct=!et.freeSolo,includeInputInList:jt=!1,limitTags:Zt=-1,ListboxComponent:Xt="ul",ListboxProps:sn,loading:Ft=!1,loadingText:wt="Loading…",multiple:kt=!1,noOptionsText:At="No options",openOnFocus:Pt=!1,openText:Mt="Open",PaperComponent:Ot=Paper$1,PopperComponent:Bt=MuiPopper,popupIcon:zt=_ArrowDropDownIcon||(_ArrowDropDownIcon=jsxRuntimeExports.jsx(ArrowDropDownIcon$1,{})),readOnly:Gt=!1,renderGroup:Wt,renderInput:qt,renderOption:tn,renderTags:ln,selectOnFocus:gn=!et.freeSolo,size:yn="medium",slotProps:Pn={}}=et,cn=_objectWithoutPropertiesLoose(et,_excluded$1p),{getRootProps:xn,getInputProps:hn,getInputLabelProps:en,getPopupIndicatorProps:Jt,getClearProps:vn,getTagProps:$n,getListboxProps:Mn,getOptionProps:On,value:En,dirty:Bn,expanded:Hn,id:Wn,popupOpen:_n,focused:Zn,focusedTag:bn,anchorEl:dn,setAnchorEl:an,inputValue:In,groupedOptions:Dn}=useAutocomplete(_extends({},et,{componentName:"Autocomplete"})),Xn=!ft&&!yt&&Bn&&!Gt,Yn=(!$t||Lt===!0)&&Lt!==!1,{onMouseDown:pn}=hn(),{ref:Ht}=sn??{},Kt=Mn(),{ref:nn}=Kt,kn=_objectWithoutPropertiesLoose(Kt,_excluded2$c),Rn=useForkRef(nn,Ht),Tn=Dt||(An=>{var Fn;return(Fn=An.label)!=null?Fn:An}),Nn=_extends({},et,{disablePortal:xt,expanded:Hn,focused:Zn,fullWidth:Tt,getOptionLabel:Tn,hasClearIcon:Xn,hasPopupIcon:Yn,inputFocused:bn===-1,popupOpen:_n,size:yn}),Cn=useUtilityClasses$1c(Nn);let Ut;if(kt&&En.length>0){const An=Fn=>_extends({className:Cn.tag,disabled:yt},$n(Fn));ln?Ut=ln(En,An,Nn):Ut=En.map((Fn,jn)=>jsxRuntimeExports.jsx(Chip$1,_extends({label:Tn(Fn),size:yn},An({index:jn}),st)))}if(Zt>-1&&Array.isArray(Ut)){const An=Ut.length-Zt;!Zn&&An>0&&(Ut=Ut.splice(0,Zt),Ut.push(jsxRuntimeExports.jsx("span",{className:Cn.tag,children:Et(An)},Ut.length)))}const Nt=Wt||(An=>jsxRuntimeExports.jsxs("li",{children:[jsxRuntimeExports.jsx(AutocompleteGroupLabel,{className:Cn.groupLabel,ownerState:Nn,component:"div",children:An.group}),jsxRuntimeExports.jsx(AutocompleteGroupUl,{className:Cn.groupUl,ownerState:Nn,children:An.children})]},An.key)),Qt=tn||((An,Fn)=>reactExports.createElement("li",_extends({},An,{key:An.key}),Tn(Fn))),rn=(An,Fn)=>{const jn=On({option:An,index:Fn});return Qt(_extends({},jn,{className:Cn.option}),An,{selected:jn["aria-selected"],index:Fn,inputValue:In},Nn)},fn=(s=Pn.clearIndicator)!=null?s:pt.clearIndicator,Ln=($=Pn.paper)!=null?$:pt.paper,zn=(j=Pn.popper)!=null?j:pt.popper,on=(_e=Pn.popupIndicator)!=null?_e:pt.popupIndicator,mn=An=>jsxRuntimeExports.jsx(AutocompletePopper,_extends({as:Bt,disablePortal:xt,style:{width:dn?dn.clientWidth:null},ownerState:Nn,role:"presentation",anchorEl:dn,open:_n},zn,{className:clsx(Cn.popper,zn==null?void 0:zn.className),children:jsxRuntimeExports.jsx(AutocompletePaper,_extends({ownerState:Nn,as:Ot},Ln,{className:clsx(Cn.paper,Ln==null?void 0:Ln.className),children:An}))}));let Sn=null;return Dn.length>0?Sn=mn(jsxRuntimeExports.jsx(AutocompleteListbox,_extends({as:Xt,className:Cn.listbox,ownerState:Nn},kn,sn,{ref:Rn,children:Dn.map((An,Fn)=>It?Nt({key:An.key,group:An.group,children:An.options.map((jn,Vn)=>rn(jn,An.index+Vn))}):rn(An,Fn))}))):Ft&&Dn.length===0?Sn=mn(jsxRuntimeExports.jsx(AutocompleteLoading,{className:Cn.loading,ownerState:Nn,children:wt})):Dn.length===0&&!$t&&!Ft&&(Sn=mn(jsxRuntimeExports.jsx(AutocompleteNoOptions,{className:Cn.noOptions,ownerState:Nn,role:"presentation",onMouseDown:An=>{An.preventDefault()},children:At}))),jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx(AutocompleteRoot,_extends({ref:o,className:clsx(Cn.root,lt),ownerState:Nn},xn(cn),{children:qt({id:Wn,disabled:yt,fullWidth:!0,size:yn==="small"?"small":void 0,InputLabelProps:en(),InputProps:_extends({ref:an,className:Cn.inputRoot,startAdornment:Ut,onClick:An=>{An.target===An.currentTarget&&pn(An)}},(Xn||Yn)&&{endAdornment:jsxRuntimeExports.jsxs(AutocompleteEndAdornment,{className:Cn.endAdornment,ownerState:Nn,children:[Xn?jsxRuntimeExports.jsx(AutocompleteClearIndicator,_extends({},vn(),{"aria-label":ot,title:ot,ownerState:Nn},fn,{className:clsx(Cn.clearIndicator,fn==null?void 0:fn.className),children:ct})):null,Yn?jsxRuntimeExports.jsx(AutocompletePopupIndicator,_extends({},Jt(),{disabled:yt,"aria-label":_n?dt:Mt,title:_n?dt:Mt,ownerState:Nn},on,{className:clsx(Cn.popupIndicator,on==null?void 0:on.className),children:zt})):null]})}),inputProps:_extends({className:Cn.input,disabled:yt,readOnly:Gt},hn())})})),dn?Sn:null]})}),Autocomplete$1=Autocomplete,_excluded$1o=["addEndListener","appear","children","easing","in","onEnter","onEntered","onEntering","onExit","onExited","onExiting","style","timeout","TransitionComponent"],styles$1={entering:{opacity:1},entered:{opacity:1}},Fade=reactExports.forwardRef(function(i,o){const s=useTheme$1(),$={enter:s.transitions.duration.enteringScreen,exit:s.transitions.duration.leavingScreen},{addEndListener:j,appear:_e=!0,children:et,easing:tt,in:nt,onEnter:at,onEntered:it,onEntering:st,onExit:lt,onExited:ct,onExiting:rt,style:ut,timeout:ot=$,TransitionComponent:dt=Transition$1}=i,pt=_objectWithoutPropertiesLoose(i,_excluded$1o),mt=reactExports.useRef(null),ft=useForkRef(mt,et.ref,o),ht=Tt=>Et=>{if(Tt){const Dt=mt.current;Et===void 0?Tt(Dt):Tt(Dt,Et)}},yt=ht(st),bt=ht((Tt,Et)=>{reflow(Tt);const Dt=getTransitionProps({style:ut,timeout:ot,easing:tt},{mode:"enter"});Tt.style.webkitTransition=s.transitions.create("opacity",Dt),Tt.style.transition=s.transitions.create("opacity",Dt),at&&at(Tt,Et)}),gt=ht(it),xt=ht(rt),vt=ht(Tt=>{const Et=getTransitionProps({style:ut,timeout:ot,easing:tt},{mode:"exit"});Tt.style.webkitTransition=s.transitions.create("opacity",Et),Tt.style.transition=s.transitions.create("opacity",Et),lt&&lt(Tt)}),Lt=ht(ct),$t=Tt=>{j&&j(mt.current,Tt)};return jsxRuntimeExports.jsx(dt,_extends({appear:_e,in:nt,nodeRef:mt,onEnter:bt,onEntered:gt,onEntering:yt,onExit:vt,onExited:Lt,onExiting:xt,addEndListener:$t,timeout:ot},pt,{children:(Tt,Et)=>reactExports.cloneElement(et,_extends({style:_extends({opacity:0,visibility:Tt==="exited"&&!nt?"hidden":void 0},styles$1[Tt],ut,et.props.style),ref:ft},Et))}))}),Fade$1=Fade;function getBackdropUtilityClass(a){return generateUtilityClass$1("MuiBackdrop",a)}generateUtilityClasses$1("MuiBackdrop",["root","invisible"]);const _excluded$1n=["children","className","component","components","componentsProps","invisible","open","slotProps","slots","TransitionComponent","transitionDuration"],useUtilityClasses$1b=a=>{const{classes:i,invisible:o}=a;return composeClasses({root:["root",o&&"invisible"]},getBackdropUtilityClass,i)},BackdropRoot=styled("div",{name:"MuiBackdrop",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.invisible&&i.invisible]}})(({ownerState:a})=>_extends({position:"fixed",display:"flex",alignItems:"center",justifyContent:"center",right:0,bottom:0,top:0,left:0,backgroundColor:"rgba(0, 0, 0, 0.5)",WebkitTapHighlightColor:"transparent"},a.invisible&&{backgroundColor:"transparent"})),Backdrop=reactExports.forwardRef(function(i,o){var s,$,j;const _e=useThemeProps$6({props:i,name:"MuiBackdrop"}),{children:et,className:tt,component:nt="div",components:at={},componentsProps:it={},invisible:st=!1,open:lt,slotProps:ct={},slots:rt={},TransitionComponent:ut=Fade$1,transitionDuration:ot}=_e,dt=_objectWithoutPropertiesLoose(_e,_excluded$1n),pt=_extends({},_e,{component:nt,invisible:st}),mt=useUtilityClasses$1b(pt),ft=(s=ct.root)!=null?s:it.root;return jsxRuntimeExports.jsx(ut,_extends({in:lt,timeout:ot},dt,{children:jsxRuntimeExports.jsx(BackdropRoot,_extends({"aria-hidden":!0},ft,{as:($=(j=rt.root)!=null?j:at.Root)!=null?$:nt,className:clsx(mt.root,tt,ft==null?void 0:ft.className),ownerState:_extends({},pt,ft==null?void 0:ft.ownerState),classes:mt,ref:o,children:et}))}))}),Backdrop$1=Backdrop;function getBadgeUtilityClass(a){return generateUtilityClass$1("MuiBadge",a)}const badgeClasses=generateUtilityClasses$1("MuiBadge",["root","badge","dot","standard","anchorOriginTopRight","anchorOriginBottomRight","anchorOriginTopLeft","anchorOriginBottomLeft","invisible","colorError","colorInfo","colorPrimary","colorSecondary","colorSuccess","colorWarning","overlapRectangular","overlapCircular","anchorOriginTopLeftCircular","anchorOriginTopLeftRectangular","anchorOriginTopRightCircular","anchorOriginTopRightRectangular","anchorOriginBottomLeftCircular","anchorOriginBottomLeftRectangular","anchorOriginBottomRightCircular","anchorOriginBottomRightRectangular"]),badgeClasses$1=badgeClasses,_excluded$1m=["anchorOrigin","className","classes","component","components","componentsProps","children","overlap","color","invisible","max","badgeContent","slots","slotProps","showZero","variant"],RADIUS_STANDARD=10,RADIUS_DOT=4,useThemeProps=createUseThemeProps(),useUtilityClasses$1a=a=>{const{color:i,anchorOrigin:o,invisible:s,overlap:$,variant:j,classes:_e={}}=a,et={root:["root"],badge:["badge",j,s&&"invisible",`anchorOrigin${capitalize$2(o.vertical)}${capitalize$2(o.horizontal)}`,`anchorOrigin${capitalize$2(o.vertical)}${capitalize$2(o.horizontal)}${capitalize$2($)}`,`overlap${capitalize$2($)}`,i!=="default"&&`color${capitalize$2(i)}`]};return composeClasses(et,getBadgeUtilityClass,_e)},BadgeRoot=styled("span",{name:"MuiBadge",slot:"Root",overridesResolver:(a,i)=>i.root})({position:"relative",display:"inline-flex",verticalAlign:"middle",flexShrink:0}),BadgeBadge=styled("span",{name:"MuiBadge",slot:"Badge",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.badge,i[o.variant],i[`anchorOrigin${capitalize$2(o.anchorOrigin.vertical)}${capitalize$2(o.anchorOrigin.horizontal)}${capitalize$2(o.overlap)}`],o.color!=="default"&&i[`color${capitalize$2(o.color)}`],o.invisible&&i.invisible]}})(({theme:a})=>{var i;return{display:"flex",flexDirection:"row",flexWrap:"wrap",justifyContent:"center",alignContent:"center",alignItems:"center",position:"absolute",boxSizing:"border-box",fontFamily:a.typography.fontFamily,fontWeight:a.typography.fontWeightMedium,fontSize:a.typography.pxToRem(12),minWidth:RADIUS_STANDARD*2,lineHeight:1,padding:"0 6px",height:RADIUS_STANDARD*2,borderRadius:RADIUS_STANDARD,zIndex:1,transition:a.transitions.create("transform",{easing:a.transitions.easing.easeInOut,duration:a.transitions.duration.enteringScreen}),variants:[...Object.keys(((i=a.vars)!=null?i:a).palette).filter(o=>{var s,$;return((s=a.vars)!=null?s:a).palette[o].main&&(($=a.vars)!=null?$:a).palette[o].contrastText}).map(o=>({props:{color:o},style:{backgroundColor:(a.vars||a).palette[o].main,color:(a.vars||a).palette[o].contrastText}})),{props:{variant:"dot"},style:{borderRadius:RADIUS_DOT,height:RADIUS_DOT*2,minWidth:RADIUS_DOT*2,padding:0}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="top"&&o.anchorOrigin.horizontal==="right"&&o.overlap==="rectangular",style:{top:0,right:0,transform:"scale(1) translate(50%, -50%)",transformOrigin:"100% 0%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(50%, -50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="bottom"&&o.anchorOrigin.horizontal==="right"&&o.overlap==="rectangular",style:{bottom:0,right:0,transform:"scale(1) translate(50%, 50%)",transformOrigin:"100% 100%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(50%, 50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="top"&&o.anchorOrigin.horizontal==="left"&&o.overlap==="rectangular",style:{top:0,left:0,transform:"scale(1) translate(-50%, -50%)",transformOrigin:"0% 0%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(-50%, -50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="bottom"&&o.anchorOrigin.horizontal==="left"&&o.overlap==="rectangular",style:{bottom:0,left:0,transform:"scale(1) translate(-50%, 50%)",transformOrigin:"0% 100%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(-50%, 50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="top"&&o.anchorOrigin.horizontal==="right"&&o.overlap==="circular",style:{top:"14%",right:"14%",transform:"scale(1) translate(50%, -50%)",transformOrigin:"100% 0%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(50%, -50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="bottom"&&o.anchorOrigin.horizontal==="right"&&o.overlap==="circular",style:{bottom:"14%",right:"14%",transform:"scale(1) translate(50%, 50%)",transformOrigin:"100% 100%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(50%, 50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="top"&&o.anchorOrigin.horizontal==="left"&&o.overlap==="circular",style:{top:"14%",left:"14%",transform:"scale(1) translate(-50%, -50%)",transformOrigin:"0% 0%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(-50%, -50%)"}}},{props:({ownerState:o})=>o.anchorOrigin.vertical==="bottom"&&o.anchorOrigin.horizontal==="left"&&o.overlap==="circular",style:{bottom:"14%",left:"14%",transform:"scale(1) translate(-50%, 50%)",transformOrigin:"0% 100%",[`&.${badgeClasses$1.invisible}`]:{transform:"scale(0) translate(-50%, 50%)"}}},{props:{invisible:!0},style:{transition:a.transitions.create("transform",{easing:a.transitions.easing.easeInOut,duration:a.transitions.duration.leavingScreen})}}]}}),Badge=reactExports.forwardRef(function(i,o){var s,$,j,_e,et,tt;const nt=useThemeProps({props:i,name:"MuiBadge"}),{anchorOrigin:at={vertical:"top",horizontal:"right"},className:it,component:st,components:lt={},componentsProps:ct={},children:rt,overlap:ut="rectangular",color:ot="default",invisible:dt=!1,max:pt=99,badgeContent:mt,slots:ft,slotProps:ht,showZero:yt=!1,variant:bt="standard"}=nt,gt=_objectWithoutPropertiesLoose(nt,_excluded$1m),{badgeContent:xt,invisible:vt,max:Lt,displayValue:$t}=useBadge({max:pt,invisible:dt,badgeContent:mt,showZero:yt}),Tt=usePreviousProps({anchorOrigin:at,color:ot,overlap:ut,variant:bt,badgeContent:mt}),Et=vt||xt==null&&bt!=="dot",{color:Dt=ot,overlap:It=ut,anchorOrigin:Ct=at,variant:jt=bt}=Et?Tt:nt,Zt=jt!=="dot"?$t:void 0,Xt=_extends({},nt,{badgeContent:xt,invisible:Et,max:Lt,displayValue:Zt,showZero:yt,anchorOrigin:Ct,color:Dt,overlap:It,variant:jt}),sn=useUtilityClasses$1a(Xt),Ft=(s=($=ft==null?void 0:ft.root)!=null?$:lt.Root)!=null?s:BadgeRoot,wt=(j=(_e=ft==null?void 0:ft.badge)!=null?_e:lt.Badge)!=null?j:BadgeBadge,kt=(et=ht==null?void 0:ht.root)!=null?et:ct.root,At=(tt=ht==null?void 0:ht.badge)!=null?tt:ct.badge,Pt=useSlotProps({elementType:Ft,externalSlotProps:kt,externalForwardedProps:gt,additionalProps:{ref:o,as:st},ownerState:Xt,className:clsx(kt==null?void 0:kt.className,sn.root,it)}),Mt=useSlotProps({elementType:wt,externalSlotProps:At,ownerState:Xt,className:clsx(sn.badge,At==null?void 0:At.className)});return jsxRuntimeExports.jsxs(Ft,_extends({},Pt,{children:[rt,jsxRuntimeExports.jsx(wt,_extends({},Mt,{children:Zt}))]}))}),Badge$1=Badge,boxClasses=generateUtilityClasses$1("MuiBox",["root"]),boxClasses$1=boxClasses,defaultTheme=createTheme(),Box=createBox({themeId:THEME_ID,defaultTheme,defaultClassName:boxClasses$1.root,generateClassName:ClassNameGenerator$1.generate}),Box$1=Box;function getButtonUtilityClass(a){return generateUtilityClass$1("MuiButton",a)}const buttonClasses=generateUtilityClasses$1("MuiButton",["root","text","textInherit","textPrimary","textSecondary","textSuccess","textError","textInfo","textWarning","outlined","outlinedInherit","outlinedPrimary","outlinedSecondary","outlinedSuccess","outlinedError","outlinedInfo","outlinedWarning","contained","containedInherit","containedPrimary","containedSecondary","containedSuccess","containedError","containedInfo","containedWarning","disableElevation","focusVisible","disabled","colorInherit","colorPrimary","colorSecondary","colorSuccess","colorError","colorInfo","colorWarning","textSizeSmall","textSizeMedium","textSizeLarge","outlinedSizeSmall","outlinedSizeMedium","outlinedSizeLarge","containedSizeSmall","containedSizeMedium","containedSizeLarge","sizeMedium","sizeSmall","sizeLarge","fullWidth","startIcon","endIcon","icon","iconSizeSmall","iconSizeMedium","iconSizeLarge"]),buttonClasses$1=buttonClasses,ButtonGroupContext=reactExports.createContext({}),ButtonGroupContext$1=ButtonGroupContext,ButtonGroupButtonContext=reactExports.createContext(void 0),ButtonGroupButtonContext$1=ButtonGroupButtonContext,_excluded$1l=["children","color","component","className","disabled","disableElevation","disableFocusRipple","endIcon","focusVisibleClassName","fullWidth","size","startIcon","type","variant"],useUtilityClasses$19=a=>{const{color:i,disableElevation:o,fullWidth:s,size:$,variant:j,classes:_e}=a,et={root:["root",j,`${j}${capitalize$2(i)}`,`size${capitalize$2($)}`,`${j}Size${capitalize$2($)}`,`color${capitalize$2(i)}`,o&&"disableElevation",s&&"fullWidth"],label:["label"],startIcon:["icon","startIcon",`iconSize${capitalize$2($)}`],endIcon:["icon","endIcon",`iconSize${capitalize$2($)}`]},tt=composeClasses(et,getButtonUtilityClass,_e);return _extends({},_e,tt)},commonIconStyles=a=>_extends({},a.size==="small"&&{"& > *:nth-of-type(1)":{fontSize:18}},a.size==="medium"&&{"& > *:nth-of-type(1)":{fontSize:20}},a.size==="large"&&{"& > *:nth-of-type(1)":{fontSize:22}}),ButtonRoot=styled(ButtonBase$1,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiButton",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],i[`${o.variant}${capitalize$2(o.color)}`],i[`size${capitalize$2(o.size)}`],i[`${o.variant}Size${capitalize$2(o.size)}`],o.color==="inherit"&&i.colorInherit,o.disableElevation&&i.disableElevation,o.fullWidth&&i.fullWidth]}})(({theme:a,ownerState:i})=>{var o,s;const $=a.palette.mode==="light"?a.palette.grey[300]:a.palette.grey[800],j=a.palette.mode==="light"?a.palette.grey.A100:a.palette.grey[700];return _extends({},a.typography.button,{minWidth:64,padding:"6px 16px",borderRadius:(a.vars||a).shape.borderRadius,transition:a.transitions.create(["background-color","box-shadow","border-color","color"],{duration:a.transitions.duration.short}),"&:hover":_extends({textDecoration:"none",backgroundColor:a.vars?`rgba(${a.vars.palette.text.primaryChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(a.palette.text.primary,a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}},i.variant==="text"&&i.color!=="inherit"&&{backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(a.palette[i.color].main,a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}},i.variant==="outlined"&&i.color!=="inherit"&&{border:`1px solid ${(a.vars||a).palette[i.color].main}`,backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(a.palette[i.color].main,a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}},i.variant==="contained"&&{backgroundColor:a.vars?a.vars.palette.Button.inheritContainedHoverBg:j,boxShadow:(a.vars||a).shadows[4],"@media (hover: none)":{boxShadow:(a.vars||a).shadows[2],backgroundColor:(a.vars||a).palette.grey[300]}},i.variant==="contained"&&i.color!=="inherit"&&{backgroundColor:(a.vars||a).palette[i.color].dark,"@media (hover: none)":{backgroundColor:(a.vars||a).palette[i.color].main}}),"&:active":_extends({},i.variant==="contained"&&{boxShadow:(a.vars||a).shadows[8]}),[`&.${buttonClasses$1.focusVisible}`]:_extends({},i.variant==="contained"&&{boxShadow:(a.vars||a).shadows[6]}),[`&.${buttonClasses$1.disabled}`]:_extends({color:(a.vars||a).palette.action.disabled},i.variant==="outlined"&&{border:`1px solid ${(a.vars||a).palette.action.disabledBackground}`},i.variant==="contained"&&{color:(a.vars||a).palette.action.disabled,boxShadow:(a.vars||a).shadows[0],backgroundColor:(a.vars||a).palette.action.disabledBackground})},i.variant==="text"&&{padding:"6px 8px"},i.variant==="text"&&i.color!=="inherit"&&{color:(a.vars||a).palette[i.color].main},i.variant==="outlined"&&{padding:"5px 15px",border:"1px solid currentColor"},i.variant==="outlined"&&i.color!=="inherit"&&{color:(a.vars||a).palette[i.color].main,border:a.vars?`1px solid rgba(${a.vars.palette[i.color].mainChannel} / 0.5)`:`1px solid ${alpha_1(a.palette[i.color].main,.5)}`},i.variant==="contained"&&{color:a.vars?a.vars.palette.text.primary:(o=(s=a.palette).getContrastText)==null?void 0:o.call(s,a.palette.grey[300]),backgroundColor:a.vars?a.vars.palette.Button.inheritContainedBg:$,boxShadow:(a.vars||a).shadows[2]},i.variant==="contained"&&i.color!=="inherit"&&{color:(a.vars||a).palette[i.color].contrastText,backgroundColor:(a.vars||a).palette[i.color].main},i.color==="inherit"&&{color:"inherit",borderColor:"currentColor"},i.size==="small"&&i.variant==="text"&&{padding:"4px 5px",fontSize:a.typography.pxToRem(13)},i.size==="large"&&i.variant==="text"&&{padding:"8px 11px",fontSize:a.typography.pxToRem(15)},i.size==="small"&&i.variant==="outlined"&&{padding:"3px 9px",fontSize:a.typography.pxToRem(13)},i.size==="large"&&i.variant==="outlined"&&{padding:"7px 21px",fontSize:a.typography.pxToRem(15)},i.size==="small"&&i.variant==="contained"&&{padding:"4px 10px",fontSize:a.typography.pxToRem(13)},i.size==="large"&&i.variant==="contained"&&{padding:"8px 22px",fontSize:a.typography.pxToRem(15)},i.fullWidth&&{width:"100%"})},({ownerState:a})=>a.disableElevation&&{boxShadow:"none","&:hover":{boxShadow:"none"},[`&.${buttonClasses$1.focusVisible}`]:{boxShadow:"none"},"&:active":{boxShadow:"none"},[`&.${buttonClasses$1.disabled}`]:{boxShadow:"none"}}),ButtonStartIcon=styled("span",{name:"MuiButton",slot:"StartIcon",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.startIcon,i[`iconSize${capitalize$2(o.size)}`]]}})(({ownerState:a})=>_extends({display:"inherit",marginRight:8,marginLeft:-4},a.size==="small"&&{marginLeft:-2},commonIconStyles(a))),ButtonEndIcon=styled("span",{name:"MuiButton",slot:"EndIcon",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.endIcon,i[`iconSize${capitalize$2(o.size)}`]]}})(({ownerState:a})=>_extends({display:"inherit",marginRight:-4,marginLeft:8},a.size==="small"&&{marginRight:-2},commonIconStyles(a))),Button=reactExports.forwardRef(function(i,o){const s=reactExports.useContext(ButtonGroupContext$1),$=reactExports.useContext(ButtonGroupButtonContext$1),j=resolveProps(s,i),_e=useThemeProps$6({props:j,name:"MuiButton"}),{children:et,color:tt="primary",component:nt="button",className:at,disabled:it=!1,disableElevation:st=!1,disableFocusRipple:lt=!1,endIcon:ct,focusVisibleClassName:rt,fullWidth:ut=!1,size:ot="medium",startIcon:dt,type:pt,variant:mt="text"}=_e,ft=_objectWithoutPropertiesLoose(_e,_excluded$1l),ht=_extends({},_e,{color:tt,component:nt,disabled:it,disableElevation:st,disableFocusRipple:lt,fullWidth:ut,size:ot,type:pt,variant:mt}),yt=useUtilityClasses$19(ht),bt=dt&&jsxRuntimeExports.jsx(ButtonStartIcon,{className:yt.startIcon,ownerState:ht,children:dt}),gt=ct&&jsxRuntimeExports.jsx(ButtonEndIcon,{className:yt.endIcon,ownerState:ht,children:ct}),xt=$||"";return jsxRuntimeExports.jsxs(ButtonRoot,_extends({ownerState:ht,className:clsx(s.className,yt.root,at,xt),component:nt,disabled:it,focusRipple:!lt,focusVisibleClassName:clsx(yt.focusVisible,rt),ref:o,type:pt},ft,{classes:yt,children:[bt,et,gt]}))}),Button$1=Button;function getButtonGroupUtilityClass(a){return generateUtilityClass$1("MuiButtonGroup",a)}const buttonGroupClasses=generateUtilityClasses$1("MuiButtonGroup",["root","contained","outlined","text","disableElevation","disabled","firstButton","fullWidth","vertical","grouped","groupedHorizontal","groupedVertical","groupedText","groupedTextHorizontal","groupedTextVertical","groupedTextPrimary","groupedTextSecondary","groupedOutlined","groupedOutlinedHorizontal","groupedOutlinedVertical","groupedOutlinedPrimary","groupedOutlinedSecondary","groupedContained","groupedContainedHorizontal","groupedContainedVertical","groupedContainedPrimary","groupedContainedSecondary","lastButton","middleButton"]),_excluded$1k=["children","className","color","component","disabled","disableElevation","disableFocusRipple","disableRipple","fullWidth","orientation","size","variant"],overridesResolver$6=(a,i)=>{const{ownerState:o}=a;return[{[`& .${buttonGroupClasses.grouped}`]:i.grouped},{[`& .${buttonGroupClasses.grouped}`]:i[`grouped${capitalize$2(o.orientation)}`]},{[`& .${buttonGroupClasses.grouped}`]:i[`grouped${capitalize$2(o.variant)}`]},{[`& .${buttonGroupClasses.grouped}`]:i[`grouped${capitalize$2(o.variant)}${capitalize$2(o.orientation)}`]},{[`& .${buttonGroupClasses.grouped}`]:i[`grouped${capitalize$2(o.variant)}${capitalize$2(o.color)}`]},{[`& .${buttonGroupClasses.firstButton}`]:i.firstButton},{[`& .${buttonGroupClasses.lastButton}`]:i.lastButton},{[`& .${buttonGroupClasses.middleButton}`]:i.middleButton},i.root,i[o.variant],o.disableElevation===!0&&i.disableElevation,o.fullWidth&&i.fullWidth,o.orientation==="vertical"&&i.vertical]},useUtilityClasses$18=a=>{const{classes:i,color:o,disabled:s,disableElevation:$,fullWidth:j,orientation:_e,variant:et}=a,tt={root:["root",et,_e==="vertical"&&"vertical",j&&"fullWidth",$&&"disableElevation"],grouped:["grouped",`grouped${capitalize$2(_e)}`,`grouped${capitalize$2(et)}`,`grouped${capitalize$2(et)}${capitalize$2(_e)}`,`grouped${capitalize$2(et)}${capitalize$2(o)}`,s&&"disabled"],firstButton:["firstButton"],lastButton:["lastButton"],middleButton:["middleButton"]};return composeClasses(tt,getButtonGroupUtilityClass,i)},ButtonGroupRoot=styled("div",{name:"MuiButtonGroup",slot:"Root",overridesResolver:overridesResolver$6})(({theme:a,ownerState:i})=>_extends({display:"inline-flex",borderRadius:(a.vars||a).shape.borderRadius},i.variant==="contained"&&{boxShadow:(a.vars||a).shadows[2]},i.disableElevation&&{boxShadow:"none"},i.fullWidth&&{width:"100%"},i.orientation==="vertical"&&{flexDirection:"column"},{[`& .${buttonGroupClasses.grouped}`]:_extends({minWidth:40,"&:hover":_extends({},i.variant==="contained"&&{boxShadow:"none"})},i.variant==="contained"&&{boxShadow:"none"}),[`& .${buttonGroupClasses.firstButton},& .${buttonGroupClasses.middleButton}`]:_extends({},i.orientation==="horizontal"&&{borderTopRightRadius:0,borderBottomRightRadius:0},i.orientation==="vertical"&&{borderBottomRightRadius:0,borderBottomLeftRadius:0},i.variant==="text"&&i.orientation==="horizontal"&&{borderRight:a.vars?`1px solid rgba(${a.vars.palette.common.onBackgroundChannel} / 0.23)`:`1px solid ${a.palette.mode==="light"?"rgba(0, 0, 0, 0.23)":"rgba(255, 255, 255, 0.23)"}`,[`&.${buttonGroupClasses.disabled}`]:{borderRight:`1px solid ${(a.vars||a).palette.action.disabled}`}},i.variant==="text"&&i.orientation==="vertical"&&{borderBottom:a.vars?`1px solid rgba(${a.vars.palette.common.onBackgroundChannel} / 0.23)`:`1px solid ${a.palette.mode==="light"?"rgba(0, 0, 0, 0.23)":"rgba(255, 255, 255, 0.23)"}`,[`&.${buttonGroupClasses.disabled}`]:{borderBottom:`1px solid ${(a.vars||a).palette.action.disabled}`}},i.variant==="text"&&i.color!=="inherit"&&{borderColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / 0.5)`:alpha_1(a.palette[i.color].main,.5)},i.variant==="outlined"&&i.orientation==="horizontal"&&{borderRightColor:"transparent"},i.variant==="outlined"&&i.orientation==="vertical"&&{borderBottomColor:"transparent"},i.variant==="contained"&&i.orientation==="horizontal"&&{borderRight:`1px solid ${(a.vars||a).palette.grey[400]}`,[`&.${buttonGroupClasses.disabled}`]:{borderRight:`1px solid ${(a.vars||a).palette.action.disabled}`}},i.variant==="contained"&&i.orientation==="vertical"&&{borderBottom:`1px solid ${(a.vars||a).palette.grey[400]}`,[`&.${buttonGroupClasses.disabled}`]:{borderBottom:`1px solid ${(a.vars||a).palette.action.disabled}`}},i.variant==="contained"&&i.color!=="inherit"&&{borderColor:(a.vars||a).palette[i.color].dark},{"&:hover":_extends({},i.variant==="outlined"&&i.orientation==="horizontal"&&{borderRightColor:"currentColor"},i.variant==="outlined"&&i.orientation==="vertical"&&{borderBottomColor:"currentColor"})}),[`& .${buttonGroupClasses.lastButton},& .${buttonGroupClasses.middleButton}`]:_extends({},i.orientation==="horizontal"&&{borderTopLeftRadius:0,borderBottomLeftRadius:0},i.orientation==="vertical"&&{borderTopRightRadius:0,borderTopLeftRadius:0},i.variant==="outlined"&&i.orientation==="horizontal"&&{marginLeft:-1},i.variant==="outlined"&&i.orientation==="vertical"&&{marginTop:-1})})),ButtonGroup=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiButtonGroup"}),{children:$,className:j,color:_e="primary",component:et="div",disabled:tt=!1,disableElevation:nt=!1,disableFocusRipple:at=!1,disableRipple:it=!1,fullWidth:st=!1,orientation:lt="horizontal",size:ct="medium",variant:rt="outlined"}=s,ut=_objectWithoutPropertiesLoose(s,_excluded$1k),ot=_extends({},s,{color:_e,component:et,disabled:tt,disableElevation:nt,disableFocusRipple:at,disableRipple:it,fullWidth:st,orientation:lt,size:ct,variant:rt}),dt=useUtilityClasses$18(ot),pt=reactExports.useMemo(()=>({className:dt.grouped,color:_e,disabled:tt,disableElevation:nt,disableFocusRipple:at,disableRipple:it,fullWidth:st,size:ct,variant:rt}),[_e,tt,nt,at,it,st,ct,rt,dt.grouped]),mt=getValidReactChildren($),ft=mt.length,ht=yt=>{const bt=yt===0,gt=yt===ft-1;return bt&&gt?"":bt?dt.firstButton:gt?dt.lastButton:dt.middleButton};return jsxRuntimeExports.jsx(ButtonGroupRoot,_extends({as:et,role:"group",className:clsx(dt.root,j),ref:o,ownerState:ot},ut,{children:jsxRuntimeExports.jsx(ButtonGroupContext$1.Provider,{value:pt,children:mt.map((yt,bt)=>jsxRuntimeExports.jsx(ButtonGroupButtonContext$1.Provider,{value:ht(bt),children:yt},bt))})}))}),ButtonGroup$1=ButtonGroup;function getCardMediaUtilityClass(a){return generateUtilityClass$1("MuiCardMedia",a)}generateUtilityClasses$1("MuiCardMedia",["root","media","img"]);const _excluded$1j=["children","className","component","image","src","style"],useUtilityClasses$17=a=>{const{classes:i,isMediaComponent:o,isImageComponent:s}=a;return composeClasses({root:["root",o&&"media",s&&"img"]},getCardMediaUtilityClass,i)},CardMediaRoot=styled("div",{name:"MuiCardMedia",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a,{isMediaComponent:s,isImageComponent:$}=o;return[i.root,s&&i.media,$&&i.img]}})(({ownerState:a})=>_extends({display:"block",backgroundSize:"cover",backgroundRepeat:"no-repeat",backgroundPosition:"center"},a.isMediaComponent&&{width:"100%"},a.isImageComponent&&{objectFit:"cover"})),MEDIA_COMPONENTS=["video","audio","picture","iframe","img"],IMAGE_COMPONENTS=["picture","img"],CardMedia=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiCardMedia"}),{children:$,className:j,component:_e="div",image:et,src:tt,style:nt}=s,at=_objectWithoutPropertiesLoose(s,_excluded$1j),it=MEDIA_COMPONENTS.indexOf(_e)!==-1,st=!it&&et?_extends({backgroundImage:`url("${et}")`},nt):nt,lt=_extends({},s,{component:_e,isMediaComponent:it,isImageComponent:IMAGE_COMPONENTS.indexOf(_e)!==-1}),ct=useUtilityClasses$17(lt);return jsxRuntimeExports.jsx(CardMediaRoot,_extends({className:clsx(ct.root,j),as:_e,role:!it&&et?"img":void 0,ref:o,style:st,ownerState:lt,src:it?et||tt:void 0},at,{children:$}))}),CardMedia$1=CardMedia;function getSwitchBaseUtilityClass(a){return generateUtilityClass$1("PrivateSwitchBase",a)}generateUtilityClasses$1("PrivateSwitchBase",["root","checked","disabled","input","edgeStart","edgeEnd"]);const _excluded$1i=["autoFocus","checked","checkedIcon","className","defaultChecked","disabled","disableFocusRipple","edge","icon","id","inputProps","inputRef","name","onBlur","onChange","onFocus","readOnly","required","tabIndex","type","value"],useUtilityClasses$16=a=>{const{classes:i,checked:o,disabled:s,edge:$}=a,j={root:["root",o&&"checked",s&&"disabled",$&&`edge${capitalize$2($)}`],input:["input"]};return composeClasses(j,getSwitchBaseUtilityClass,i)},SwitchBaseRoot=styled(ButtonBase$1)(({ownerState:a})=>_extends({padding:9,borderRadius:"50%"},a.edge==="start"&&{marginLeft:a.size==="small"?-3:-12},a.edge==="end"&&{marginRight:a.size==="small"?-3:-12})),SwitchBaseInput=styled("input",{shouldForwardProp:rootShouldForwardProp$2})({cursor:"inherit",position:"absolute",opacity:0,width:"100%",height:"100%",top:0,left:0,margin:0,padding:0,zIndex:1}),SwitchBase=reactExports.forwardRef(function(i,o){const{autoFocus:s,checked:$,checkedIcon:j,className:_e,defaultChecked:et,disabled:tt,disableFocusRipple:nt=!1,edge:at=!1,icon:it,id:st,inputProps:lt,inputRef:ct,name:rt,onBlur:ut,onChange:ot,onFocus:dt,readOnly:pt,required:mt=!1,tabIndex:ft,type:ht,value:yt}=i,bt=_objectWithoutPropertiesLoose(i,_excluded$1i),[gt,xt]=useControlled({controlled:$,default:!!et,name:"SwitchBase",state:"checked"}),vt=useFormControl(),Lt=jt=>{dt&&dt(jt),vt&&vt.onFocus&&vt.onFocus(jt)},$t=jt=>{ut&&ut(jt),vt&&vt.onBlur&&vt.onBlur(jt)},Tt=jt=>{if(jt.nativeEvent.defaultPrevented)return;const Zt=jt.target.checked;xt(Zt),ot&&ot(jt,Zt)};let Et=tt;vt&&typeof Et>"u"&&(Et=vt.disabled);const Dt=ht==="checkbox"||ht==="radio",It=_extends({},i,{checked:gt,disabled:Et,disableFocusRipple:nt,edge:at}),Ct=useUtilityClasses$16(It);return jsxRuntimeExports.jsxs(SwitchBaseRoot,_extends({component:"span",className:clsx(Ct.root,_e),centerRipple:!0,focusRipple:!nt,disabled:Et,tabIndex:null,role:void 0,onFocus:Lt,onBlur:$t,ownerState:It,ref:o},bt,{children:[jsxRuntimeExports.jsx(SwitchBaseInput,_extends({autoFocus:s,checked:$,defaultChecked:et,className:Ct.input,disabled:Et,id:Dt?st:void 0,name:rt,onChange:Tt,readOnly:pt,ref:ct,required:mt,ownerState:It,tabIndex:ft,type:ht},ht==="checkbox"&&yt===void 0?{}:{value:yt},lt)),gt?j:it]}))}),SwitchBase$1=SwitchBase,CheckBoxOutlineBlankIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M19 5v14H5V5h14m0-2H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2z"}),"CheckBoxOutlineBlank"),CheckBoxIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M19 3H5c-1.11 0-2 .9-2 2v14c0 1.1.89 2 2 2h14c1.11 0 2-.9 2-2V5c0-1.1-.89-2-2-2zm-9 14l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"}),"CheckBox"),IndeterminateCheckBoxIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-2 10H7v-2h10v2z"}),"IndeterminateCheckBox");function getCheckboxUtilityClass(a){return generateUtilityClass$1("MuiCheckbox",a)}const checkboxClasses=generateUtilityClasses$1("MuiCheckbox",["root","checked","disabled","indeterminate","colorPrimary","colorSecondary","sizeSmall","sizeMedium"]),checkboxClasses$1=checkboxClasses,_excluded$1h=["checkedIcon","color","icon","indeterminate","indeterminateIcon","inputProps","size","className"],useUtilityClasses$15=a=>{const{classes:i,indeterminate:o,color:s,size:$}=a,j={root:["root",o&&"indeterminate",`color${capitalize$2(s)}`,`size${capitalize$2($)}`]},_e=composeClasses(j,getCheckboxUtilityClass,i);return _extends({},i,_e)},CheckboxRoot=styled(SwitchBase$1,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiCheckbox",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.indeterminate&&i.indeterminate,i[`size${capitalize$2(o.size)}`],o.color!=="default"&&i[`color${capitalize$2(o.color)}`]]}})(({theme:a,ownerState:i})=>_extends({color:(a.vars||a).palette.text.secondary},!i.disableRipple&&{"&:hover":{backgroundColor:a.vars?`rgba(${i.color==="default"?a.vars.palette.action.activeChannel:a.vars.palette[i.color].mainChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha_1(i.color==="default"?a.palette.action.active:a.palette[i.color].main,a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}}},i.color!=="default"&&{[`&.${checkboxClasses$1.checked}, &.${checkboxClasses$1.indeterminate}`]:{color:(a.vars||a).palette[i.color].main},[`&.${checkboxClasses$1.disabled}`]:{color:(a.vars||a).palette.action.disabled}})),defaultCheckedIcon=jsxRuntimeExports.jsx(CheckBoxIcon,{}),defaultIcon=jsxRuntimeExports.jsx(CheckBoxOutlineBlankIcon,{}),defaultIndeterminateIcon=jsxRuntimeExports.jsx(IndeterminateCheckBoxIcon,{}),Checkbox=reactExports.forwardRef(function(i,o){var s,$;const j=useThemeProps$6({props:i,name:"MuiCheckbox"}),{checkedIcon:_e=defaultCheckedIcon,color:et="primary",icon:tt=defaultIcon,indeterminate:nt=!1,indeterminateIcon:at=defaultIndeterminateIcon,inputProps:it,size:st="medium",className:lt}=j,ct=_objectWithoutPropertiesLoose(j,_excluded$1h),rt=nt?at:tt,ut=nt?at:_e,ot=_extends({},j,{color:et,indeterminate:nt,size:st}),dt=useUtilityClasses$15(ot);return jsxRuntimeExports.jsx(CheckboxRoot,_extends({type:"checkbox",inputProps:_extends({"data-indeterminate":nt},it),icon:reactExports.cloneElement(rt,{fontSize:(s=rt.props.fontSize)!=null?s:st}),checkedIcon:reactExports.cloneElement(ut,{fontSize:($=ut.props.fontSize)!=null?$:st}),ownerState:ot,ref:o,className:clsx(dt.root,lt)},ct,{classes:dt}))}),Checkbox$1=Checkbox;function getCircularProgressUtilityClass(a){return generateUtilityClass$1("MuiCircularProgress",a)}generateUtilityClasses$1("MuiCircularProgress",["root","determinate","indeterminate","colorPrimary","colorSecondary","svg","circle","circleDeterminate","circleIndeterminate","circleDisableShrink"]);const _excluded$1g=["className","color","disableShrink","size","style","thickness","value","variant"];let _$2=a=>a,_t$2,_t2$2,_t3$2,_t4$2;const SIZE=44,circularRotateKeyframe=keyframes(_t$2||(_t$2=_$2`
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
`)),circularDashKeyframe=keyframes(_t2$2||(_t2$2=_$2`
  0% {
    stroke-dasharray: 1px, 200px;
    stroke-dashoffset: 0;
  }

  50% {
    stroke-dasharray: 100px, 200px;
    stroke-dashoffset: -15px;
  }

  100% {
    stroke-dasharray: 100px, 200px;
    stroke-dashoffset: -125px;
  }
`)),useUtilityClasses$14=a=>{const{classes:i,variant:o,color:s,disableShrink:$}=a,j={root:["root",o,`color${capitalize$2(s)}`],svg:["svg"],circle:["circle",`circle${capitalize$2(o)}`,$&&"circleDisableShrink"]};return composeClasses(j,getCircularProgressUtilityClass,i)},CircularProgressRoot=styled("span",{name:"MuiCircularProgress",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],i[`color${capitalize$2(o.color)}`]]}})(({ownerState:a,theme:i})=>_extends({display:"inline-block"},a.variant==="determinate"&&{transition:i.transitions.create("transform")},a.color!=="inherit"&&{color:(i.vars||i).palette[a.color].main}),({ownerState:a})=>a.variant==="indeterminate"&&css(_t3$2||(_t3$2=_$2`
      animation: ${0} 1.4s linear infinite;
    `),circularRotateKeyframe)),CircularProgressSVG=styled("svg",{name:"MuiCircularProgress",slot:"Svg",overridesResolver:(a,i)=>i.svg})({display:"block"}),CircularProgressCircle=styled("circle",{name:"MuiCircularProgress",slot:"Circle",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.circle,i[`circle${capitalize$2(o.variant)}`],o.disableShrink&&i.circleDisableShrink]}})(({ownerState:a,theme:i})=>_extends({stroke:"currentColor"},a.variant==="determinate"&&{transition:i.transitions.create("stroke-dashoffset")},a.variant==="indeterminate"&&{strokeDasharray:"80px, 200px",strokeDashoffset:0}),({ownerState:a})=>a.variant==="indeterminate"&&!a.disableShrink&&css(_t4$2||(_t4$2=_$2`
      animation: ${0} 1.4s ease-in-out infinite;
    `),circularDashKeyframe)),CircularProgress=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiCircularProgress"}),{className:$,color:j="primary",disableShrink:_e=!1,size:et=40,style:tt,thickness:nt=3.6,value:at=0,variant:it="indeterminate"}=s,st=_objectWithoutPropertiesLoose(s,_excluded$1g),lt=_extends({},s,{color:j,disableShrink:_e,size:et,thickness:nt,value:at,variant:it}),ct=useUtilityClasses$14(lt),rt={},ut={},ot={};if(it==="determinate"){const dt=2*Math.PI*((SIZE-nt)/2);rt.strokeDasharray=dt.toFixed(3),ot["aria-valuenow"]=Math.round(at),rt.strokeDashoffset=`${((100-at)/100*dt).toFixed(3)}px`,ut.transform="rotate(-90deg)"}return jsxRuntimeExports.jsx(CircularProgressRoot,_extends({className:clsx(ct.root,$),style:_extends({width:et,height:et},ut,tt),ownerState:lt,ref:o,role:"progressbar"},ot,st,{children:jsxRuntimeExports.jsx(CircularProgressSVG,{className:ct.svg,ownerState:lt,viewBox:`${SIZE/2} ${SIZE/2} ${SIZE} ${SIZE}`,children:jsxRuntimeExports.jsx(CircularProgressCircle,{className:ct.circle,style:rt,ownerState:lt,cx:SIZE,cy:SIZE,r:(SIZE-nt)/2,fill:"none",strokeWidth:nt})})}))}),CircularProgress$1=CircularProgress;function getModalUtilityClass(a){return generateUtilityClass$1("MuiModal",a)}generateUtilityClasses$1("MuiModal",["root","hidden","backdrop"]);const _excluded$1f=["BackdropComponent","BackdropProps","classes","className","closeAfterTransition","children","container","component","components","componentsProps","disableAutoFocus","disableEnforceFocus","disableEscapeKeyDown","disablePortal","disableRestoreFocus","disableScrollLock","hideBackdrop","keepMounted","onBackdropClick","onClose","onTransitionEnter","onTransitionExited","open","slotProps","slots","theme"],useUtilityClasses$13=a=>{const{open:i,exited:o,classes:s}=a;return composeClasses({root:["root",!i&&o&&"hidden"],backdrop:["backdrop"]},getModalUtilityClass,s)},ModalRoot=styled("div",{name:"MuiModal",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,!o.open&&o.exited&&i.hidden]}})(({theme:a,ownerState:i})=>_extends({position:"fixed",zIndex:(a.vars||a).zIndex.modal,right:0,bottom:0,top:0,left:0},!i.open&&i.exited&&{visibility:"hidden"})),ModalBackdrop=styled(Backdrop$1,{name:"MuiModal",slot:"Backdrop",overridesResolver:(a,i)=>i.backdrop})({zIndex:-1}),Modal=reactExports.forwardRef(function(i,o){var s,$,j,_e,et,tt;const nt=useThemeProps$6({name:"MuiModal",props:i}),{BackdropComponent:at=ModalBackdrop,BackdropProps:it,className:st,closeAfterTransition:lt=!1,children:ct,container:rt,component:ut,components:ot={},componentsProps:dt={},disableAutoFocus:pt=!1,disableEnforceFocus:mt=!1,disableEscapeKeyDown:ft=!1,disablePortal:ht=!1,disableRestoreFocus:yt=!1,disableScrollLock:bt=!1,hideBackdrop:gt=!1,keepMounted:xt=!1,onBackdropClick:vt,open:Lt,slotProps:$t,slots:Tt}=nt,Et=_objectWithoutPropertiesLoose(nt,_excluded$1f),Dt=_extends({},nt,{closeAfterTransition:lt,disableAutoFocus:pt,disableEnforceFocus:mt,disableEscapeKeyDown:ft,disablePortal:ht,disableRestoreFocus:yt,disableScrollLock:bt,hideBackdrop:gt,keepMounted:xt}),{getRootProps:It,getBackdropProps:Ct,getTransitionProps:jt,portalRef:Zt,isTopModal:Xt,exited:sn,hasTransition:Ft}=useModal(_extends({},Dt,{rootRef:o})),wt=_extends({},Dt,{exited:sn}),kt=useUtilityClasses$13(wt),At={};if(ct.props.tabIndex===void 0&&(At.tabIndex="-1"),Ft){const{onEnter:Wt,onExited:qt}=jt();At.onEnter=Wt,At.onExited=qt}const Pt=(s=($=Tt==null?void 0:Tt.root)!=null?$:ot.Root)!=null?s:ModalRoot,Mt=(j=(_e=Tt==null?void 0:Tt.backdrop)!=null?_e:ot.Backdrop)!=null?j:at,Ot=(et=$t==null?void 0:$t.root)!=null?et:dt.root,Bt=(tt=$t==null?void 0:$t.backdrop)!=null?tt:dt.backdrop,zt=useSlotProps({elementType:Pt,externalSlotProps:Ot,externalForwardedProps:Et,getSlotProps:It,additionalProps:{ref:o,as:ut},ownerState:wt,className:clsx(st,Ot==null?void 0:Ot.className,kt==null?void 0:kt.root,!wt.open&&wt.exited&&(kt==null?void 0:kt.hidden))}),Gt=useSlotProps({elementType:Mt,externalSlotProps:Bt,additionalProps:it,getSlotProps:Wt=>Ct(_extends({},Wt,{onClick:qt=>{vt&&vt(qt),Wt!=null&&Wt.onClick&&Wt.onClick(qt)}})),className:clsx(Bt==null?void 0:Bt.className,it==null?void 0:it.className,kt==null?void 0:kt.backdrop),ownerState:wt});return!xt&&!Lt&&(!Ft||sn)?null:jsxRuntimeExports.jsx(Portal,{ref:Zt,container:rt,disablePortal:ht,children:jsxRuntimeExports.jsxs(Pt,_extends({},zt,{children:[!gt&&at?jsxRuntimeExports.jsx(Mt,_extends({},Gt)):null,jsxRuntimeExports.jsx(FocusTrap,{disableEnforceFocus:mt,disableAutoFocus:pt,disableRestoreFocus:yt,isEnabled:Xt,open:Lt,children:reactExports.cloneElement(ct,At)})]}))})}),Modal$1=Modal;function getDialogUtilityClass(a){return generateUtilityClass$1("MuiDialog",a)}const dialogClasses=generateUtilityClasses$1("MuiDialog",["root","scrollPaper","scrollBody","container","paper","paperScrollPaper","paperScrollBody","paperWidthFalse","paperWidthXs","paperWidthSm","paperWidthMd","paperWidthLg","paperWidthXl","paperFullWidth","paperFullScreen"]),DialogContext=reactExports.createContext({}),DialogContext$1=DialogContext,_excluded$1e=["aria-describedby","aria-labelledby","BackdropComponent","BackdropProps","children","className","disableEscapeKeyDown","fullScreen","fullWidth","maxWidth","onBackdropClick","onClose","open","PaperComponent","PaperProps","scroll","TransitionComponent","transitionDuration","TransitionProps"],DialogBackdrop=styled(Backdrop$1,{name:"MuiDialog",slot:"Backdrop",overrides:(a,i)=>i.backdrop})({zIndex:-1}),useUtilityClasses$12=a=>{const{classes:i,scroll:o,maxWidth:s,fullWidth:$,fullScreen:j}=a,_e={root:["root"],container:["container",`scroll${capitalize$2(o)}`],paper:["paper",`paperScroll${capitalize$2(o)}`,`paperWidth${capitalize$2(String(s))}`,$&&"paperFullWidth",j&&"paperFullScreen"]};return composeClasses(_e,getDialogUtilityClass,i)},DialogRoot=styled(Modal$1,{name:"MuiDialog",slot:"Root",overridesResolver:(a,i)=>i.root})({"@media print":{position:"absolute !important"}}),DialogContainer=styled("div",{name:"MuiDialog",slot:"Container",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.container,i[`scroll${capitalize$2(o.scroll)}`]]}})(({ownerState:a})=>_extends({height:"100%","@media print":{height:"auto"},outline:0},a.scroll==="paper"&&{display:"flex",justifyContent:"center",alignItems:"center"},a.scroll==="body"&&{overflowY:"auto",overflowX:"hidden",textAlign:"center","&::after":{content:'""',display:"inline-block",verticalAlign:"middle",height:"100%",width:"0"}})),DialogPaper=styled(Paper$1,{name:"MuiDialog",slot:"Paper",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.paper,i[`scrollPaper${capitalize$2(o.scroll)}`],i[`paperWidth${capitalize$2(String(o.maxWidth))}`],o.fullWidth&&i.paperFullWidth,o.fullScreen&&i.paperFullScreen]}})(({theme:a,ownerState:i})=>_extends({margin:32,position:"relative",overflowY:"auto","@media print":{overflowY:"visible",boxShadow:"none"}},i.scroll==="paper"&&{display:"flex",flexDirection:"column",maxHeight:"calc(100% - 64px)"},i.scroll==="body"&&{display:"inline-block",verticalAlign:"middle",textAlign:"left"},!i.maxWidth&&{maxWidth:"calc(100% - 64px)"},i.maxWidth==="xs"&&{maxWidth:a.breakpoints.unit==="px"?Math.max(a.breakpoints.values.xs,444):`max(${a.breakpoints.values.xs}${a.breakpoints.unit}, 444px)`,[`&.${dialogClasses.paperScrollBody}`]:{[a.breakpoints.down(Math.max(a.breakpoints.values.xs,444)+32*2)]:{maxWidth:"calc(100% - 64px)"}}},i.maxWidth&&i.maxWidth!=="xs"&&{maxWidth:`${a.breakpoints.values[i.maxWidth]}${a.breakpoints.unit}`,[`&.${dialogClasses.paperScrollBody}`]:{[a.breakpoints.down(a.breakpoints.values[i.maxWidth]+32*2)]:{maxWidth:"calc(100% - 64px)"}}},i.fullWidth&&{width:"calc(100% - 64px)"},i.fullScreen&&{margin:0,width:"100%",maxWidth:"100%",height:"100%",maxHeight:"none",borderRadius:0,[`&.${dialogClasses.paperScrollBody}`]:{margin:0,maxWidth:"100%"}})),Dialog=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiDialog"}),$=useTheme$1(),j={enter:$.transitions.duration.enteringScreen,exit:$.transitions.duration.leavingScreen},{"aria-describedby":_e,"aria-labelledby":et,BackdropComponent:tt,BackdropProps:nt,children:at,className:it,disableEscapeKeyDown:st=!1,fullScreen:lt=!1,fullWidth:ct=!1,maxWidth:rt="sm",onBackdropClick:ut,onClose:ot,open:dt,PaperComponent:pt=Paper$1,PaperProps:mt={},scroll:ft="paper",TransitionComponent:ht=Fade$1,transitionDuration:yt=j,TransitionProps:bt}=s,gt=_objectWithoutPropertiesLoose(s,_excluded$1e),xt=_extends({},s,{disableEscapeKeyDown:st,fullScreen:lt,fullWidth:ct,maxWidth:rt,scroll:ft}),vt=useUtilityClasses$12(xt),Lt=reactExports.useRef(),$t=It=>{Lt.current=It.target===It.currentTarget},Tt=It=>{Lt.current&&(Lt.current=null,ut&&ut(It),ot&&ot(It,"backdropClick"))},Et=useId(et),Dt=reactExports.useMemo(()=>({titleId:Et}),[Et]);return jsxRuntimeExports.jsx(DialogRoot,_extends({className:clsx(vt.root,it),closeAfterTransition:!0,components:{Backdrop:DialogBackdrop},componentsProps:{backdrop:_extends({transitionDuration:yt,as:tt},nt)},disableEscapeKeyDown:st,onClose:ot,open:dt,ref:o,onClick:Tt,ownerState:xt},gt,{children:jsxRuntimeExports.jsx(ht,_extends({appear:!0,in:dt,timeout:yt,role:"presentation"},bt,{children:jsxRuntimeExports.jsx(DialogContainer,{className:clsx(vt.container),onMouseDown:$t,ownerState:xt,children:jsxRuntimeExports.jsx(DialogPaper,_extends({as:pt,elevation:24,role:"dialog","aria-describedby":_e,"aria-labelledby":Et},mt,{className:clsx(vt.paper,mt.className),ownerState:xt,children:jsxRuntimeExports.jsx(DialogContext$1.Provider,{value:Dt,children:at})}))})}))}))}),MuiDialog=Dialog;function getDialogActionsUtilityClass(a){return generateUtilityClass$1("MuiDialogActions",a)}generateUtilityClasses$1("MuiDialogActions",["root","spacing"]);const _excluded$1d=["className","disableSpacing"],useUtilityClasses$11=a=>{const{classes:i,disableSpacing:o}=a;return composeClasses({root:["root",!o&&"spacing"]},getDialogActionsUtilityClass,i)},DialogActionsRoot=styled("div",{name:"MuiDialogActions",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,!o.disableSpacing&&i.spacing]}})(({ownerState:a})=>_extends({display:"flex",alignItems:"center",padding:8,justifyContent:"flex-end",flex:"0 0 auto"},!a.disableSpacing&&{"& > :not(style) ~ :not(style)":{marginLeft:8}})),DialogActions=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiDialogActions"}),{className:$,disableSpacing:j=!1}=s,_e=_objectWithoutPropertiesLoose(s,_excluded$1d),et=_extends({},s,{disableSpacing:j}),tt=useUtilityClasses$11(et);return jsxRuntimeExports.jsx(DialogActionsRoot,_extends({className:clsx(tt.root,$),ownerState:et,ref:o},_e))}),DialogActions$1=DialogActions;function getDialogContentUtilityClass(a){return generateUtilityClass$1("MuiDialogContent",a)}generateUtilityClasses$1("MuiDialogContent",["root","dividers"]);const dialogTitleClasses=generateUtilityClasses$1("MuiDialogTitle",["root"]),dialogTitleClasses$1=dialogTitleClasses,_excluded$1c=["className","dividers"],useUtilityClasses$10=a=>{const{classes:i,dividers:o}=a;return composeClasses({root:["root",o&&"dividers"]},getDialogContentUtilityClass,i)},DialogContentRoot=styled("div",{name:"MuiDialogContent",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.dividers&&i.dividers]}})(({theme:a,ownerState:i})=>_extends({flex:"1 1 auto",WebkitOverflowScrolling:"touch",overflowY:"auto",padding:"20px 24px"},i.dividers?{padding:"16px 24px",borderTop:`1px solid ${(a.vars||a).palette.divider}`,borderBottom:`1px solid ${(a.vars||a).palette.divider}`}:{[`.${dialogTitleClasses$1.root} + &`]:{paddingTop:0}})),DialogContent=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiDialogContent"}),{className:$,dividers:j=!1}=s,_e=_objectWithoutPropertiesLoose(s,_excluded$1c),et=_extends({},s,{dividers:j}),tt=useUtilityClasses$10(et);return jsxRuntimeExports.jsx(DialogContentRoot,_extends({className:clsx(tt.root,$),ownerState:et,ref:o},_e))}),DialogContent$1=DialogContent;function getDividerUtilityClass(a){return generateUtilityClass$1("MuiDivider",a)}const dividerClasses=generateUtilityClasses$1("MuiDivider",["root","absolute","fullWidth","inset","middle","flexItem","light","vertical","withChildren","withChildrenVertical","textAlignRight","textAlignLeft","wrapper","wrapperVertical"]),dividerClasses$1=dividerClasses,_excluded$1b=["absolute","children","className","component","flexItem","light","orientation","role","textAlign","variant"],useUtilityClasses$$=a=>{const{absolute:i,children:o,classes:s,flexItem:$,light:j,orientation:_e,textAlign:et,variant:tt}=a;return composeClasses({root:["root",i&&"absolute",tt,j&&"light",_e==="vertical"&&"vertical",$&&"flexItem",o&&"withChildren",o&&_e==="vertical"&&"withChildrenVertical",et==="right"&&_e!=="vertical"&&"textAlignRight",et==="left"&&_e!=="vertical"&&"textAlignLeft"],wrapper:["wrapper",_e==="vertical"&&"wrapperVertical"]},getDividerUtilityClass,s)},DividerRoot=styled("div",{name:"MuiDivider",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.absolute&&i.absolute,i[o.variant],o.light&&i.light,o.orientation==="vertical"&&i.vertical,o.flexItem&&i.flexItem,o.children&&i.withChildren,o.children&&o.orientation==="vertical"&&i.withChildrenVertical,o.textAlign==="right"&&o.orientation!=="vertical"&&i.textAlignRight,o.textAlign==="left"&&o.orientation!=="vertical"&&i.textAlignLeft]}})(({theme:a,ownerState:i})=>_extends({margin:0,flexShrink:0,borderWidth:0,borderStyle:"solid",borderColor:(a.vars||a).palette.divider,borderBottomWidth:"thin"},i.absolute&&{position:"absolute",bottom:0,left:0,width:"100%"},i.light&&{borderColor:a.vars?`rgba(${a.vars.palette.dividerChannel} / 0.08)`:alpha_1(a.palette.divider,.08)},i.variant==="inset"&&{marginLeft:72},i.variant==="middle"&&i.orientation==="horizontal"&&{marginLeft:a.spacing(2),marginRight:a.spacing(2)},i.variant==="middle"&&i.orientation==="vertical"&&{marginTop:a.spacing(1),marginBottom:a.spacing(1)},i.orientation==="vertical"&&{height:"100%",borderBottomWidth:0,borderRightWidth:"thin"},i.flexItem&&{alignSelf:"stretch",height:"auto"}),({ownerState:a})=>_extends({},a.children&&{display:"flex",whiteSpace:"nowrap",textAlign:"center",border:0,"&::before, &::after":{content:'""',alignSelf:"center"}}),({theme:a,ownerState:i})=>_extends({},i.children&&i.orientation!=="vertical"&&{"&::before, &::after":{width:"100%",borderTop:`thin solid ${(a.vars||a).palette.divider}`}}),({theme:a,ownerState:i})=>_extends({},i.children&&i.orientation==="vertical"&&{flexDirection:"column","&::before, &::after":{height:"100%",borderLeft:`thin solid ${(a.vars||a).palette.divider}`}}),({ownerState:a})=>_extends({},a.textAlign==="right"&&a.orientation!=="vertical"&&{"&::before":{width:"90%"},"&::after":{width:"10%"}},a.textAlign==="left"&&a.orientation!=="vertical"&&{"&::before":{width:"10%"},"&::after":{width:"90%"}})),DividerWrapper=styled("span",{name:"MuiDivider",slot:"Wrapper",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.wrapper,o.orientation==="vertical"&&i.wrapperVertical]}})(({theme:a,ownerState:i})=>_extends({display:"inline-block",paddingLeft:`calc(${a.spacing(1)} * 1.2)`,paddingRight:`calc(${a.spacing(1)} * 1.2)`},i.orientation==="vertical"&&{paddingTop:`calc(${a.spacing(1)} * 1.2)`,paddingBottom:`calc(${a.spacing(1)} * 1.2)`})),Divider=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiDivider"}),{absolute:$=!1,children:j,className:_e,component:et=j?"div":"hr",flexItem:tt=!1,light:nt=!1,orientation:at="horizontal",role:it=et!=="hr"?"separator":void 0,textAlign:st="center",variant:lt="fullWidth"}=s,ct=_objectWithoutPropertiesLoose(s,_excluded$1b),rt=_extends({},s,{absolute:$,component:et,flexItem:tt,light:nt,orientation:at,role:it,textAlign:st,variant:lt}),ut=useUtilityClasses$$(rt);return jsxRuntimeExports.jsx(DividerRoot,_extends({as:et,className:clsx(ut.root,_e),role:it,ref:o,ownerState:rt},ct,{children:j?jsxRuntimeExports.jsx(DividerWrapper,{className:ut.wrapper,ownerState:rt,children:j}):null}))});Divider.muiSkipListHighlight=!0;const Divider$1=Divider,_excluded$1a=["addEndListener","appear","children","container","direction","easing","in","onEnter","onEntered","onEntering","onExit","onExited","onExiting","style","timeout","TransitionComponent"];function getTranslateValue(a,i,o){const s=i.getBoundingClientRect(),$=o&&o.getBoundingClientRect(),j=ownerWindow(i);let _e;if(i.fakeTransform)_e=i.fakeTransform;else{const nt=j.getComputedStyle(i);_e=nt.getPropertyValue("-webkit-transform")||nt.getPropertyValue("transform")}let et=0,tt=0;if(_e&&_e!=="none"&&typeof _e=="string"){const nt=_e.split("(")[1].split(")")[0].split(",");et=parseInt(nt[4],10),tt=parseInt(nt[5],10)}return a==="left"?$?`translateX(${$.right+et-s.left}px)`:`translateX(${j.innerWidth+et-s.left}px)`:a==="right"?$?`translateX(-${s.right-$.left-et}px)`:`translateX(-${s.left+s.width-et}px)`:a==="up"?$?`translateY(${$.bottom+tt-s.top}px)`:`translateY(${j.innerHeight+tt-s.top}px)`:$?`translateY(-${s.top-$.top+s.height-tt}px)`:`translateY(-${s.top+s.height-tt}px)`}function resolveContainer(a){return typeof a=="function"?a():a}function setTranslateValue(a,i,o){const s=resolveContainer(o),$=getTranslateValue(a,i,s);$&&(i.style.webkitTransform=$,i.style.transform=$)}const Slide=reactExports.forwardRef(function(i,o){const s=useTheme$1(),$={enter:s.transitions.easing.easeOut,exit:s.transitions.easing.sharp},j={enter:s.transitions.duration.enteringScreen,exit:s.transitions.duration.leavingScreen},{addEndListener:_e,appear:et=!0,children:tt,container:nt,direction:at="down",easing:it=$,in:st,onEnter:lt,onEntered:ct,onEntering:rt,onExit:ut,onExited:ot,onExiting:dt,style:pt,timeout:mt=j,TransitionComponent:ft=Transition$1}=i,ht=_objectWithoutPropertiesLoose(i,_excluded$1a),yt=reactExports.useRef(null),bt=useForkRef(tt.ref,yt,o),gt=Ct=>jt=>{Ct&&(jt===void 0?Ct(yt.current):Ct(yt.current,jt))},xt=gt((Ct,jt)=>{setTranslateValue(at,Ct,nt),reflow(Ct),lt&&lt(Ct,jt)}),vt=gt((Ct,jt)=>{const Zt=getTransitionProps({timeout:mt,style:pt,easing:it},{mode:"enter"});Ct.style.webkitTransition=s.transitions.create("-webkit-transform",_extends({},Zt)),Ct.style.transition=s.transitions.create("transform",_extends({},Zt)),Ct.style.webkitTransform="none",Ct.style.transform="none",rt&&rt(Ct,jt)}),Lt=gt(ct),$t=gt(dt),Tt=gt(Ct=>{const jt=getTransitionProps({timeout:mt,style:pt,easing:it},{mode:"exit"});Ct.style.webkitTransition=s.transitions.create("-webkit-transform",jt),Ct.style.transition=s.transitions.create("transform",jt),setTranslateValue(at,Ct,nt),ut&&ut(Ct)}),Et=gt(Ct=>{Ct.style.webkitTransition="",Ct.style.transition="",ot&&ot(Ct)}),Dt=Ct=>{_e&&_e(yt.current,Ct)},It=reactExports.useCallback(()=>{yt.current&&setTranslateValue(at,yt.current,nt)},[at,nt]);return reactExports.useEffect(()=>{if(st||at==="down"||at==="right")return;const Ct=debounce$1(()=>{yt.current&&setTranslateValue(at,yt.current,nt)}),jt=ownerWindow(yt.current);return jt.addEventListener("resize",Ct),()=>{Ct.clear(),jt.removeEventListener("resize",Ct)}},[at,st,nt]),reactExports.useEffect(()=>{st||It()},[st,It]),jsxRuntimeExports.jsx(ft,_extends({nodeRef:yt,onEnter:xt,onEntered:Lt,onEntering:vt,onExit:Tt,onExited:Et,onExiting:$t,addEndListener:Dt,appear:et,in:st,timeout:mt},ht,{children:(Ct,jt)=>reactExports.cloneElement(tt,_extends({ref:bt,style:_extends({visibility:Ct==="exited"&&!st?"hidden":void 0},pt,tt.props.style)},jt))}))}),Slide$1=Slide;function getFabUtilityClass(a){return generateUtilityClass$1("MuiFab",a)}const fabClasses=generateUtilityClasses$1("MuiFab",["root","primary","secondary","extended","circular","focusVisible","disabled","colorInherit","sizeSmall","sizeMedium","sizeLarge","info","error","warning","success"]),fabClasses$1=fabClasses,_excluded$19=["children","className","color","component","disabled","disableFocusRipple","focusVisibleClassName","size","variant"],useUtilityClasses$_=a=>{const{color:i,variant:o,classes:s,size:$}=a,j={root:["root",o,`size${capitalize$2($)}`,i==="inherit"?"colorInherit":i]},_e=composeClasses(j,getFabUtilityClass,s);return _extends({},s,_e)},FabRoot=styled(ButtonBase$1,{name:"MuiFab",slot:"Root",shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],i[`size${capitalize$2(o.size)}`],o.color==="inherit"&&i.colorInherit,i[capitalize$2(o.size)],i[o.color]]}})(({theme:a,ownerState:i})=>{var o,s;return _extends({},a.typography.button,{minHeight:36,transition:a.transitions.create(["background-color","box-shadow","border-color"],{duration:a.transitions.duration.short}),borderRadius:"50%",padding:0,minWidth:0,width:56,height:56,zIndex:(a.vars||a).zIndex.fab,boxShadow:(a.vars||a).shadows[6],"&:active":{boxShadow:(a.vars||a).shadows[12]},color:a.vars?a.vars.palette.text.primary:(o=(s=a.palette).getContrastText)==null?void 0:o.call(s,a.palette.grey[300]),backgroundColor:(a.vars||a).palette.grey[300],"&:hover":{backgroundColor:(a.vars||a).palette.grey.A100,"@media (hover: none)":{backgroundColor:(a.vars||a).palette.grey[300]},textDecoration:"none"},[`&.${fabClasses$1.focusVisible}`]:{boxShadow:(a.vars||a).shadows[6]}},i.size==="small"&&{width:40,height:40},i.size==="medium"&&{width:48,height:48},i.variant==="extended"&&{borderRadius:48/2,padding:"0 16px",width:"auto",minHeight:"auto",minWidth:48,height:48},i.variant==="extended"&&i.size==="small"&&{width:"auto",padding:"0 8px",borderRadius:34/2,minWidth:34,height:34},i.variant==="extended"&&i.size==="medium"&&{width:"auto",padding:"0 16px",borderRadius:40/2,minWidth:40,height:40},i.color==="inherit"&&{color:"inherit"})},({theme:a,ownerState:i})=>_extends({},i.color!=="inherit"&&i.color!=="default"&&(a.vars||a).palette[i.color]!=null&&{color:(a.vars||a).palette[i.color].contrastText,backgroundColor:(a.vars||a).palette[i.color].main,"&:hover":{backgroundColor:(a.vars||a).palette[i.color].dark,"@media (hover: none)":{backgroundColor:(a.vars||a).palette[i.color].main}}}),({theme:a})=>({[`&.${fabClasses$1.disabled}`]:{color:(a.vars||a).palette.action.disabled,boxShadow:(a.vars||a).shadows[0],backgroundColor:(a.vars||a).palette.action.disabledBackground}})),Fab=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiFab"}),{children:$,className:j,color:_e="default",component:et="button",disabled:tt=!1,disableFocusRipple:nt=!1,focusVisibleClassName:at,size:it="large",variant:st="circular"}=s,lt=_objectWithoutPropertiesLoose(s,_excluded$19),ct=_extends({},s,{color:_e,component:et,disabled:tt,disableFocusRipple:nt,size:it,variant:st}),rt=useUtilityClasses$_(ct);return jsxRuntimeExports.jsx(FabRoot,_extends({className:clsx(rt.root,j),component:et,disabled:tt,focusRipple:!nt,focusVisibleClassName:clsx(rt.focusVisible,at),ownerState:ct,ref:o},lt,{classes:rt,children:$}))}),Fab$1=Fab,_excluded$18=["disableUnderline","components","componentsProps","fullWidth","hiddenLabel","inputComponent","multiline","slotProps","slots","type"],useUtilityClasses$Z=a=>{const{classes:i,disableUnderline:o}=a,$=composeClasses({root:["root",!o&&"underline"],input:["input"]},getFilledInputUtilityClass,i);return _extends({},i,$)},FilledInputRoot=styled(InputBaseRoot,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiFilledInput",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[...rootOverridesResolver(a,i),!o.disableUnderline&&i.underline]}})(({theme:a,ownerState:i})=>{var o;const s=a.palette.mode==="light",$=s?"rgba(0, 0, 0, 0.42)":"rgba(255, 255, 255, 0.7)",j=s?"rgba(0, 0, 0, 0.06)":"rgba(255, 255, 255, 0.09)",_e=s?"rgba(0, 0, 0, 0.09)":"rgba(255, 255, 255, 0.13)",et=s?"rgba(0, 0, 0, 0.12)":"rgba(255, 255, 255, 0.12)";return _extends({position:"relative",backgroundColor:a.vars?a.vars.palette.FilledInput.bg:j,borderTopLeftRadius:(a.vars||a).shape.borderRadius,borderTopRightRadius:(a.vars||a).shape.borderRadius,transition:a.transitions.create("background-color",{duration:a.transitions.duration.shorter,easing:a.transitions.easing.easeOut}),"&:hover":{backgroundColor:a.vars?a.vars.palette.FilledInput.hoverBg:_e,"@media (hover: none)":{backgroundColor:a.vars?a.vars.palette.FilledInput.bg:j}},[`&.${filledInputClasses$1.focused}`]:{backgroundColor:a.vars?a.vars.palette.FilledInput.bg:j},[`&.${filledInputClasses$1.disabled}`]:{backgroundColor:a.vars?a.vars.palette.FilledInput.disabledBg:et}},!i.disableUnderline&&{"&::after":{borderBottom:`2px solid ${(o=(a.vars||a).palette[i.color||"primary"])==null?void 0:o.main}`,left:0,bottom:0,content:'""',position:"absolute",right:0,transform:"scaleX(0)",transition:a.transitions.create("transform",{duration:a.transitions.duration.shorter,easing:a.transitions.easing.easeOut}),pointerEvents:"none"},[`&.${filledInputClasses$1.focused}:after`]:{transform:"scaleX(1) translateX(0)"},[`&.${filledInputClasses$1.error}`]:{"&::before, &::after":{borderBottomColor:(a.vars||a).palette.error.main}},"&::before":{borderBottom:`1px solid ${a.vars?`rgba(${a.vars.palette.common.onBackgroundChannel} / ${a.vars.opacity.inputUnderline})`:$}`,left:0,bottom:0,content:'"\\00a0"',position:"absolute",right:0,transition:a.transitions.create("border-bottom-color",{duration:a.transitions.duration.shorter}),pointerEvents:"none"},[`&:hover:not(.${filledInputClasses$1.disabled}, .${filledInputClasses$1.error}):before`]:{borderBottom:`1px solid ${(a.vars||a).palette.text.primary}`},[`&.${filledInputClasses$1.disabled}:before`]:{borderBottomStyle:"dotted"}},i.startAdornment&&{paddingLeft:12},i.endAdornment&&{paddingRight:12},i.multiline&&_extends({padding:"25px 12px 8px"},i.size==="small"&&{paddingTop:21,paddingBottom:4},i.hiddenLabel&&{paddingTop:16,paddingBottom:17},i.hiddenLabel&&i.size==="small"&&{paddingTop:8,paddingBottom:9}))}),FilledInputInput=styled(InputBaseComponent,{name:"MuiFilledInput",slot:"Input",overridesResolver:inputOverridesResolver})(({theme:a,ownerState:i})=>_extends({paddingTop:25,paddingRight:12,paddingBottom:8,paddingLeft:12},!a.vars&&{"&:-webkit-autofill":{WebkitBoxShadow:a.palette.mode==="light"?null:"0 0 0 100px #266798 inset",WebkitTextFillColor:a.palette.mode==="light"?null:"#fff",caretColor:a.palette.mode==="light"?null:"#fff",borderTopLeftRadius:"inherit",borderTopRightRadius:"inherit"}},a.vars&&{"&:-webkit-autofill":{borderTopLeftRadius:"inherit",borderTopRightRadius:"inherit"},[a.getColorSchemeSelector("dark")]:{"&:-webkit-autofill":{WebkitBoxShadow:"0 0 0 100px #266798 inset",WebkitTextFillColor:"#fff",caretColor:"#fff"}}},i.size==="small"&&{paddingTop:21,paddingBottom:4},i.hiddenLabel&&{paddingTop:16,paddingBottom:17},i.startAdornment&&{paddingLeft:0},i.endAdornment&&{paddingRight:0},i.hiddenLabel&&i.size==="small"&&{paddingTop:8,paddingBottom:9},i.multiline&&{paddingTop:0,paddingBottom:0,paddingLeft:0,paddingRight:0})),FilledInput=reactExports.forwardRef(function(i,o){var s,$,j,_e;const et=useThemeProps$6({props:i,name:"MuiFilledInput"}),{components:tt={},componentsProps:nt,fullWidth:at=!1,inputComponent:it="input",multiline:st=!1,slotProps:lt,slots:ct={},type:rt="text"}=et,ut=_objectWithoutPropertiesLoose(et,_excluded$18),ot=_extends({},et,{fullWidth:at,inputComponent:it,multiline:st,type:rt}),dt=useUtilityClasses$Z(et),pt={root:{ownerState:ot},input:{ownerState:ot}},mt=lt??nt?deepmerge$1(pt,lt??nt):pt,ft=(s=($=ct.root)!=null?$:tt.Root)!=null?s:FilledInputRoot,ht=(j=(_e=ct.input)!=null?_e:tt.Input)!=null?j:FilledInputInput;return jsxRuntimeExports.jsx(InputBase$1,_extends({slots:{root:ft,input:ht},componentsProps:mt,fullWidth:at,inputComponent:it,multiline:st,ref:o,type:rt},ut,{classes:dt}))});FilledInput.muiName="Input";const FilledInput$1=FilledInput;function getFormControlUtilityClasses(a){return generateUtilityClass$1("MuiFormControl",a)}generateUtilityClasses$1("MuiFormControl",["root","marginNone","marginNormal","marginDense","fullWidth","disabled"]);const _excluded$17=["children","className","color","component","disabled","error","focused","fullWidth","hiddenLabel","margin","required","size","variant"],useUtilityClasses$Y=a=>{const{classes:i,margin:o,fullWidth:s}=a,$={root:["root",o!=="none"&&`margin${capitalize$2(o)}`,s&&"fullWidth"]};return composeClasses($,getFormControlUtilityClasses,i)},FormControlRoot=styled("div",{name:"MuiFormControl",slot:"Root",overridesResolver:({ownerState:a},i)=>_extends({},i.root,i[`margin${capitalize$2(a.margin)}`],a.fullWidth&&i.fullWidth)})(({ownerState:a})=>_extends({display:"inline-flex",flexDirection:"column",position:"relative",minWidth:0,padding:0,margin:0,border:0,verticalAlign:"top"},a.margin==="normal"&&{marginTop:16,marginBottom:8},a.margin==="dense"&&{marginTop:8,marginBottom:4},a.fullWidth&&{width:"100%"})),FormControl=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiFormControl"}),{children:$,className:j,color:_e="primary",component:et="div",disabled:tt=!1,error:nt=!1,focused:at,fullWidth:it=!1,hiddenLabel:st=!1,margin:lt="none",required:ct=!1,size:rt="medium",variant:ut="outlined"}=s,ot=_objectWithoutPropertiesLoose(s,_excluded$17),dt=_extends({},s,{color:_e,component:et,disabled:tt,error:nt,fullWidth:it,hiddenLabel:st,margin:lt,required:ct,size:rt,variant:ut}),pt=useUtilityClasses$Y(dt),[mt,ft]=reactExports.useState(()=>{let $t=!1;return $&&reactExports.Children.forEach($,Tt=>{if(!isMuiElement(Tt,["Input","Select"]))return;const Et=isMuiElement(Tt,["Select"])?Tt.props.input:Tt;Et&&isAdornedStart(Et.props)&&($t=!0)}),$t}),[ht,yt]=reactExports.useState(()=>{let $t=!1;return $&&reactExports.Children.forEach($,Tt=>{isMuiElement(Tt,["Input","Select"])&&(isFilled(Tt.props,!0)||isFilled(Tt.props.inputProps,!0))&&($t=!0)}),$t}),[bt,gt]=reactExports.useState(!1);tt&&bt&&gt(!1);const xt=at!==void 0&&!tt?at:bt;let vt;const Lt=reactExports.useMemo(()=>({adornedStart:mt,setAdornedStart:ft,color:_e,disabled:tt,error:nt,filled:ht,focused:xt,fullWidth:it,hiddenLabel:st,size:rt,onBlur:()=>{gt(!1)},onEmpty:()=>{yt(!1)},onFilled:()=>{yt(!0)},onFocus:()=>{gt(!0)},registerEffect:vt,required:ct,variant:ut}),[mt,_e,tt,nt,ht,xt,it,st,vt,ct,rt,ut]);return jsxRuntimeExports.jsx(FormControlContext$1.Provider,{value:Lt,children:jsxRuntimeExports.jsx(FormControlRoot,_extends({as:et,ownerState:dt,className:clsx(pt.root,j),ref:o},ot,{children:$}))})}),FormControl$1=FormControl,Stack=createStack({createStyledComponent:styled("div",{name:"MuiStack",slot:"Root",overridesResolver:(a,i)=>i.root}),useThemeProps:a=>useThemeProps$6({props:a,name:"MuiStack"})}),Stack$1=Stack;function getFormControlLabelUtilityClasses(a){return generateUtilityClass$1("MuiFormControlLabel",a)}const formControlLabelClasses=generateUtilityClasses$1("MuiFormControlLabel",["root","labelPlacementStart","labelPlacementTop","labelPlacementBottom","disabled","label","error","required","asterisk"]),formControlLabelClasses$1=formControlLabelClasses,_excluded$16=["checked","className","componentsProps","control","disabled","disableTypography","inputRef","label","labelPlacement","name","onChange","required","slotProps","value"],useUtilityClasses$X=a=>{const{classes:i,disabled:o,labelPlacement:s,error:$,required:j}=a,_e={root:["root",o&&"disabled",`labelPlacement${capitalize$2(s)}`,$&&"error",j&&"required"],label:["label",o&&"disabled"],asterisk:["asterisk",$&&"error"]};return composeClasses(_e,getFormControlLabelUtilityClasses,i)},FormControlLabelRoot=styled("label",{name:"MuiFormControlLabel",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`& .${formControlLabelClasses$1.label}`]:i.label},i.root,i[`labelPlacement${capitalize$2(o.labelPlacement)}`]]}})(({theme:a,ownerState:i})=>_extends({display:"inline-flex",alignItems:"center",cursor:"pointer",verticalAlign:"middle",WebkitTapHighlightColor:"transparent",marginLeft:-11,marginRight:16,[`&.${formControlLabelClasses$1.disabled}`]:{cursor:"default"}},i.labelPlacement==="start"&&{flexDirection:"row-reverse",marginLeft:16,marginRight:-11},i.labelPlacement==="top"&&{flexDirection:"column-reverse",marginLeft:16},i.labelPlacement==="bottom"&&{flexDirection:"column",marginLeft:16},{[`& .${formControlLabelClasses$1.label}`]:{[`&.${formControlLabelClasses$1.disabled}`]:{color:(a.vars||a).palette.text.disabled}}})),AsteriskComponent$1=styled("span",{name:"MuiFormControlLabel",slot:"Asterisk",overridesResolver:(a,i)=>i.asterisk})(({theme:a})=>({[`&.${formControlLabelClasses$1.error}`]:{color:(a.vars||a).palette.error.main}})),FormControlLabel=reactExports.forwardRef(function(i,o){var s,$;const j=useThemeProps$6({props:i,name:"MuiFormControlLabel"}),{className:_e,componentsProps:et={},control:tt,disabled:nt,disableTypography:at,label:it,labelPlacement:st="end",required:lt,slotProps:ct={}}=j,rt=_objectWithoutPropertiesLoose(j,_excluded$16),ut=useFormControl(),ot=(s=nt??tt.props.disabled)!=null?s:ut==null?void 0:ut.disabled,dt=lt??tt.props.required,pt={disabled:ot,required:dt};["checked","name","onChange","value","inputRef"].forEach(gt=>{typeof tt.props[gt]>"u"&&typeof j[gt]<"u"&&(pt[gt]=j[gt])});const mt=formControlState({props:j,muiFormControl:ut,states:["error"]}),ft=_extends({},j,{disabled:ot,labelPlacement:st,required:dt,error:mt.error}),ht=useUtilityClasses$X(ft),yt=($=ct.typography)!=null?$:et.typography;let bt=it;return bt!=null&&bt.type!==Typography$1&&!at&&(bt=jsxRuntimeExports.jsx(Typography$1,_extends({component:"span"},yt,{className:clsx(ht.label,yt==null?void 0:yt.className),children:bt}))),jsxRuntimeExports.jsxs(FormControlLabelRoot,_extends({className:clsx(ht.root,_e),ownerState:ft,ref:o},rt,{children:[reactExports.cloneElement(tt,pt),dt?jsxRuntimeExports.jsxs(Stack$1,{display:"block",children:[bt,jsxRuntimeExports.jsxs(AsteriskComponent$1,{ownerState:ft,"aria-hidden":!0,className:ht.asterisk,children:[" ","*"]})]}):bt]}))}),FormControlLabel$1=FormControlLabel;function getFormHelperTextUtilityClasses(a){return generateUtilityClass$1("MuiFormHelperText",a)}const formHelperTextClasses=generateUtilityClasses$1("MuiFormHelperText",["root","error","disabled","sizeSmall","sizeMedium","contained","focused","filled","required"]),formHelperTextClasses$1=formHelperTextClasses;var _span$3;const _excluded$15=["children","className","component","disabled","error","filled","focused","margin","required","variant"],useUtilityClasses$W=a=>{const{classes:i,contained:o,size:s,disabled:$,error:j,filled:_e,focused:et,required:tt}=a,nt={root:["root",$&&"disabled",j&&"error",s&&`size${capitalize$2(s)}`,o&&"contained",et&&"focused",_e&&"filled",tt&&"required"]};return composeClasses(nt,getFormHelperTextUtilityClasses,i)},FormHelperTextRoot=styled("p",{name:"MuiFormHelperText",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.size&&i[`size${capitalize$2(o.size)}`],o.contained&&i.contained,o.filled&&i.filled]}})(({theme:a,ownerState:i})=>_extends({color:(a.vars||a).palette.text.secondary},a.typography.caption,{textAlign:"left",marginTop:3,marginRight:0,marginBottom:0,marginLeft:0,[`&.${formHelperTextClasses$1.disabled}`]:{color:(a.vars||a).palette.text.disabled},[`&.${formHelperTextClasses$1.error}`]:{color:(a.vars||a).palette.error.main}},i.size==="small"&&{marginTop:4},i.contained&&{marginLeft:14,marginRight:14})),FormHelperText=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiFormHelperText"}),{children:$,className:j,component:_e="p"}=s,et=_objectWithoutPropertiesLoose(s,_excluded$15),tt=useFormControl(),nt=formControlState({props:s,muiFormControl:tt,states:["variant","size","disabled","error","filled","focused","required"]}),at=_extends({},s,{component:_e,contained:nt.variant==="filled"||nt.variant==="outlined",variant:nt.variant,size:nt.size,disabled:nt.disabled,error:nt.error,filled:nt.filled,focused:nt.focused,required:nt.required}),it=useUtilityClasses$W(at);return jsxRuntimeExports.jsx(FormHelperTextRoot,_extends({as:_e,ownerState:at,className:clsx(it.root,j),ref:o},et,{children:$===" "?_span$3||(_span$3=jsxRuntimeExports.jsx("span",{className:"notranslate",children:"​"})):$}))}),FormHelperText$1=FormHelperText;function getFormLabelUtilityClasses(a){return generateUtilityClass$1("MuiFormLabel",a)}const formLabelClasses=generateUtilityClasses$1("MuiFormLabel",["root","colorSecondary","focused","disabled","error","filled","required","asterisk"]),formLabelClasses$1=formLabelClasses,_excluded$14=["children","className","color","component","disabled","error","filled","focused","required"],useUtilityClasses$V=a=>{const{classes:i,color:o,focused:s,disabled:$,error:j,filled:_e,required:et}=a,tt={root:["root",`color${capitalize$2(o)}`,$&&"disabled",j&&"error",_e&&"filled",s&&"focused",et&&"required"],asterisk:["asterisk",j&&"error"]};return composeClasses(tt,getFormLabelUtilityClasses,i)},FormLabelRoot=styled("label",{name:"MuiFormLabel",slot:"Root",overridesResolver:({ownerState:a},i)=>_extends({},i.root,a.color==="secondary"&&i.colorSecondary,a.filled&&i.filled)})(({theme:a,ownerState:i})=>_extends({color:(a.vars||a).palette.text.secondary},a.typography.body1,{lineHeight:"1.4375em",padding:0,position:"relative",[`&.${formLabelClasses$1.focused}`]:{color:(a.vars||a).palette[i.color].main},[`&.${formLabelClasses$1.disabled}`]:{color:(a.vars||a).palette.text.disabled},[`&.${formLabelClasses$1.error}`]:{color:(a.vars||a).palette.error.main}})),AsteriskComponent=styled("span",{name:"MuiFormLabel",slot:"Asterisk",overridesResolver:(a,i)=>i.asterisk})(({theme:a})=>({[`&.${formLabelClasses$1.error}`]:{color:(a.vars||a).palette.error.main}})),FormLabel=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiFormLabel"}),{children:$,className:j,component:_e="label"}=s,et=_objectWithoutPropertiesLoose(s,_excluded$14),tt=useFormControl(),nt=formControlState({props:s,muiFormControl:tt,states:["color","required","focused","disabled","error","filled"]}),at=_extends({},s,{color:nt.color||"primary",component:_e,disabled:nt.disabled,error:nt.error,filled:nt.filled,focused:nt.focused,required:nt.required}),it=useUtilityClasses$V(at);return jsxRuntimeExports.jsxs(FormLabelRoot,_extends({as:_e,ownerState:at,className:clsx(it.root,j),ref:o},et,{children:[$,nt.required&&jsxRuntimeExports.jsxs(AsteriskComponent,{ownerState:at,"aria-hidden":!0,className:it.asterisk,children:[" ","*"]})]}))}),FormLabel$1=FormLabel,GridContext=reactExports.createContext(),GridContext$1=GridContext;function getGridUtilityClass(a){return generateUtilityClass$1("MuiGrid",a)}const SPACINGS=[0,1,2,3,4,5,6,7,8,9,10],DIRECTIONS=["column-reverse","column","row-reverse","row"],WRAPS=["nowrap","wrap-reverse","wrap"],GRID_SIZES=["auto",!0,1,2,3,4,5,6,7,8,9,10,11,12],gridClasses=generateUtilityClasses$1("MuiGrid",["root","container","item","zeroMinWidth",...SPACINGS.map(a=>`spacing-xs-${a}`),...DIRECTIONS.map(a=>`direction-xs-${a}`),...WRAPS.map(a=>`wrap-xs-${a}`),...GRID_SIZES.map(a=>`grid-xs-${a}`),...GRID_SIZES.map(a=>`grid-sm-${a}`),...GRID_SIZES.map(a=>`grid-md-${a}`),...GRID_SIZES.map(a=>`grid-lg-${a}`),...GRID_SIZES.map(a=>`grid-xl-${a}`)]),_excluded$13=["className","columns","columnSpacing","component","container","direction","item","rowSpacing","spacing","wrap","zeroMinWidth"];function getOffset(a){const i=parseFloat(a);return`${i}${String(a).replace(String(i),"")||"px"}`}function generateGrid({theme:a,ownerState:i}){let o;return a.breakpoints.keys.reduce((s,$)=>{let j={};if(i[$]&&(o=i[$]),!o)return s;if(o===!0)j={flexBasis:0,flexGrow:1,maxWidth:"100%"};else if(o==="auto")j={flexBasis:"auto",flexGrow:0,flexShrink:0,maxWidth:"none",width:"auto"};else{const _e=resolveBreakpointValues({values:i.columns,breakpoints:a.breakpoints.values}),et=typeof _e=="object"?_e[$]:_e;if(et==null)return s;const tt=`${Math.round(o/et*1e8)/1e6}%`;let nt={};if(i.container&&i.item&&i.columnSpacing!==0){const at=a.spacing(i.columnSpacing);if(at!=="0px"){const it=`calc(${tt} + ${getOffset(at)})`;nt={flexBasis:it,maxWidth:it}}}j=_extends({flexBasis:tt,flexGrow:0,maxWidth:tt},nt)}return a.breakpoints.values[$]===0?Object.assign(s,j):s[a.breakpoints.up($)]=j,s},{})}function generateDirection({theme:a,ownerState:i}){const o=resolveBreakpointValues({values:i.direction,breakpoints:a.breakpoints.values});return handleBreakpoints({theme:a},o,s=>{const $={flexDirection:s};return s.indexOf("column")===0&&($[`& > .${gridClasses.item}`]={maxWidth:"none"}),$})}function extractZeroValueBreakpointKeys({breakpoints:a,values:i}){let o="";Object.keys(i).forEach($=>{o===""&&i[$]!==0&&(o=$)});const s=Object.keys(a).sort(($,j)=>a[$]-a[j]);return s.slice(0,s.indexOf(o))}function generateRowGap({theme:a,ownerState:i}){const{container:o,rowSpacing:s}=i;let $={};if(o&&s!==0){const j=resolveBreakpointValues({values:s,breakpoints:a.breakpoints.values});let _e;typeof j=="object"&&(_e=extractZeroValueBreakpointKeys({breakpoints:a.breakpoints.values,values:j})),$=handleBreakpoints({theme:a},j,(et,tt)=>{var nt;const at=a.spacing(et);return at!=="0px"?{marginTop:`-${getOffset(at)}`,[`& > .${gridClasses.item}`]:{paddingTop:getOffset(at)}}:(nt=_e)!=null&&nt.includes(tt)?{}:{marginTop:0,[`& > .${gridClasses.item}`]:{paddingTop:0}}})}return $}function generateColumnGap({theme:a,ownerState:i}){const{container:o,columnSpacing:s}=i;let $={};if(o&&s!==0){const j=resolveBreakpointValues({values:s,breakpoints:a.breakpoints.values});let _e;typeof j=="object"&&(_e=extractZeroValueBreakpointKeys({breakpoints:a.breakpoints.values,values:j})),$=handleBreakpoints({theme:a},j,(et,tt)=>{var nt;const at=a.spacing(et);return at!=="0px"?{width:`calc(100% + ${getOffset(at)})`,marginLeft:`-${getOffset(at)}`,[`& > .${gridClasses.item}`]:{paddingLeft:getOffset(at)}}:(nt=_e)!=null&&nt.includes(tt)?{}:{width:"100%",marginLeft:0,[`& > .${gridClasses.item}`]:{paddingLeft:0}}})}return $}function resolveSpacingStyles(a,i,o={}){if(!a||a<=0)return[];if(typeof a=="string"&&!Number.isNaN(Number(a))||typeof a=="number")return[o[`spacing-xs-${String(a)}`]];const s=[];return i.forEach($=>{const j=a[$];Number(j)>0&&s.push(o[`spacing-${$}-${String(j)}`])}),s}const GridRoot=styled("div",{name:"MuiGrid",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a,{container:s,direction:$,item:j,spacing:_e,wrap:et,zeroMinWidth:tt,breakpoints:nt}=o;let at=[];s&&(at=resolveSpacingStyles(_e,nt,i));const it=[];return nt.forEach(st=>{const lt=o[st];lt&&it.push(i[`grid-${st}-${String(lt)}`])}),[i.root,s&&i.container,j&&i.item,tt&&i.zeroMinWidth,...at,$!=="row"&&i[`direction-xs-${String($)}`],et!=="wrap"&&i[`wrap-xs-${String(et)}`],...it]}})(({ownerState:a})=>_extends({boxSizing:"border-box"},a.container&&{display:"flex",flexWrap:"wrap",width:"100%"},a.item&&{margin:0},a.zeroMinWidth&&{minWidth:0},a.wrap!=="wrap"&&{flexWrap:a.wrap}),generateDirection,generateRowGap,generateColumnGap,generateGrid);function resolveSpacingClasses(a,i){if(!a||a<=0)return[];if(typeof a=="string"&&!Number.isNaN(Number(a))||typeof a=="number")return[`spacing-xs-${String(a)}`];const o=[];return i.forEach(s=>{const $=a[s];if(Number($)>0){const j=`spacing-${s}-${String($)}`;o.push(j)}}),o}const useUtilityClasses$U=a=>{const{classes:i,container:o,direction:s,item:$,spacing:j,wrap:_e,zeroMinWidth:et,breakpoints:tt}=a;let nt=[];o&&(nt=resolveSpacingClasses(j,tt));const at=[];tt.forEach(st=>{const lt=a[st];lt&&at.push(`grid-${st}-${String(lt)}`)});const it={root:["root",o&&"container",$&&"item",et&&"zeroMinWidth",...nt,s!=="row"&&`direction-xs-${String(s)}`,_e!=="wrap"&&`wrap-xs-${String(_e)}`,...at]};return composeClasses(it,getGridUtilityClass,i)},Grid=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiGrid"}),{breakpoints:$}=useTheme$1(),j=extendSxProp(s),{className:_e,columns:et,columnSpacing:tt,component:nt="div",container:at=!1,direction:it="row",item:st=!1,rowSpacing:lt,spacing:ct=0,wrap:rt="wrap",zeroMinWidth:ut=!1}=j,ot=_objectWithoutPropertiesLoose(j,_excluded$13),dt=lt||ct,pt=tt||ct,mt=reactExports.useContext(GridContext$1),ft=at?et||12:mt,ht={},yt=_extends({},ot);$.keys.forEach(xt=>{ot[xt]!=null&&(ht[xt]=ot[xt],delete yt[xt])});const bt=_extends({},j,{columns:ft,container:at,direction:it,item:st,rowSpacing:dt,columnSpacing:pt,wrap:rt,zeroMinWidth:ut,spacing:ct},ht,{breakpoints:$.keys}),gt=useUtilityClasses$U(bt);return jsxRuntimeExports.jsx(GridContext$1.Provider,{value:ft,children:jsxRuntimeExports.jsx(GridRoot,_extends({ownerState:bt,className:clsx(gt.root,_e),as:nt,ref:o},yt))})}),Grid$1=Grid,_excluded$12=["addEndListener","appear","children","easing","in","onEnter","onEntered","onEntering","onExit","onExited","onExiting","style","timeout","TransitionComponent"];function getScale(a){return`scale(${a}, ${a**2})`}const styles={entering:{opacity:1,transform:getScale(1)},entered:{opacity:1,transform:"none"}},isWebKit154=typeof navigator<"u"&&/^((?!chrome|android).)*(safari|mobile)/i.test(navigator.userAgent)&&/(os |version\/)15(.|_)4/i.test(navigator.userAgent),Grow=reactExports.forwardRef(function(i,o){const{addEndListener:s,appear:$=!0,children:j,easing:_e,in:et,onEnter:tt,onEntered:nt,onEntering:at,onExit:it,onExited:st,onExiting:lt,style:ct,timeout:rt="auto",TransitionComponent:ut=Transition$1}=i,ot=_objectWithoutPropertiesLoose(i,_excluded$12),dt=useTimeout(),pt=reactExports.useRef(),mt=useTheme$1(),ft=reactExports.useRef(null),ht=useForkRef(ft,j.ref,o),yt=Et=>Dt=>{if(Et){const It=ft.current;Dt===void 0?Et(It):Et(It,Dt)}},bt=yt(at),gt=yt((Et,Dt)=>{reflow(Et);const{duration:It,delay:Ct,easing:jt}=getTransitionProps({style:ct,timeout:rt,easing:_e},{mode:"enter"});let Zt;rt==="auto"?(Zt=mt.transitions.getAutoHeightDuration(Et.clientHeight),pt.current=Zt):Zt=It,Et.style.transition=[mt.transitions.create("opacity",{duration:Zt,delay:Ct}),mt.transitions.create("transform",{duration:isWebKit154?Zt:Zt*.666,delay:Ct,easing:jt})].join(","),tt&&tt(Et,Dt)}),xt=yt(nt),vt=yt(lt),Lt=yt(Et=>{const{duration:Dt,delay:It,easing:Ct}=getTransitionProps({style:ct,timeout:rt,easing:_e},{mode:"exit"});let jt;rt==="auto"?(jt=mt.transitions.getAutoHeightDuration(Et.clientHeight),pt.current=jt):jt=Dt,Et.style.transition=[mt.transitions.create("opacity",{duration:jt,delay:It}),mt.transitions.create("transform",{duration:isWebKit154?jt:jt*.666,delay:isWebKit154?It:It||jt*.333,easing:Ct})].join(","),Et.style.opacity=0,Et.style.transform=getScale(.75),it&&it(Et)}),$t=yt(st),Tt=Et=>{rt==="auto"&&dt.start(pt.current||0,Et),s&&s(ft.current,Et)};return jsxRuntimeExports.jsx(ut,_extends({appear:$,in:et,nodeRef:ft,onEnter:gt,onEntered:xt,onEntering:bt,onExit:Lt,onExited:$t,onExiting:vt,addEndListener:Tt,timeout:rt==="auto"?null:rt},ot,{children:(Et,Dt)=>reactExports.cloneElement(j,_extends({style:_extends({opacity:0,transform:getScale(.75),visibility:Et==="exited"&&!et?"hidden":void 0},styles[Et],ct,j.props.style),ref:ht},Dt))}))});Grow.muiSupportAuto=!0;const Grow$1=Grow,_excluded$11=["disableUnderline","components","componentsProps","fullWidth","inputComponent","multiline","slotProps","slots","type"],useUtilityClasses$T=a=>{const{classes:i,disableUnderline:o}=a,$=composeClasses({root:["root",!o&&"underline"],input:["input"]},getInputUtilityClass,i);return _extends({},i,$)},InputRoot=styled(InputBaseRoot,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiInput",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[...rootOverridesResolver(a,i),!o.disableUnderline&&i.underline]}})(({theme:a,ownerState:i})=>{let s=a.palette.mode==="light"?"rgba(0, 0, 0, 0.42)":"rgba(255, 255, 255, 0.7)";return a.vars&&(s=`rgba(${a.vars.palette.common.onBackgroundChannel} / ${a.vars.opacity.inputUnderline})`),_extends({position:"relative"},i.formControl&&{"label + &":{marginTop:16}},!i.disableUnderline&&{"&::after":{borderBottom:`2px solid ${(a.vars||a).palette[i.color].main}`,left:0,bottom:0,content:'""',position:"absolute",right:0,transform:"scaleX(0)",transition:a.transitions.create("transform",{duration:a.transitions.duration.shorter,easing:a.transitions.easing.easeOut}),pointerEvents:"none"},[`&.${inputClasses$1.focused}:after`]:{transform:"scaleX(1) translateX(0)"},[`&.${inputClasses$1.error}`]:{"&::before, &::after":{borderBottomColor:(a.vars||a).palette.error.main}},"&::before":{borderBottom:`1px solid ${s}`,left:0,bottom:0,content:'"\\00a0"',position:"absolute",right:0,transition:a.transitions.create("border-bottom-color",{duration:a.transitions.duration.shorter}),pointerEvents:"none"},[`&:hover:not(.${inputClasses$1.disabled}, .${inputClasses$1.error}):before`]:{borderBottom:`2px solid ${(a.vars||a).palette.text.primary}`,"@media (hover: none)":{borderBottom:`1px solid ${s}`}},[`&.${inputClasses$1.disabled}:before`]:{borderBottomStyle:"dotted"}})}),InputInput=styled(InputBaseComponent,{name:"MuiInput",slot:"Input",overridesResolver:inputOverridesResolver})({}),Input=reactExports.forwardRef(function(i,o){var s,$,j,_e;const et=useThemeProps$6({props:i,name:"MuiInput"}),{disableUnderline:tt,components:nt={},componentsProps:at,fullWidth:it=!1,inputComponent:st="input",multiline:lt=!1,slotProps:ct,slots:rt={},type:ut="text"}=et,ot=_objectWithoutPropertiesLoose(et,_excluded$11),dt=useUtilityClasses$T(et),mt={root:{ownerState:{disableUnderline:tt}}},ft=ct??at?deepmerge$1(ct??at,mt):mt,ht=(s=($=rt.root)!=null?$:nt.Root)!=null?s:InputRoot,yt=(j=(_e=rt.input)!=null?_e:nt.Input)!=null?j:InputInput;return jsxRuntimeExports.jsx(InputBase$1,_extends({slots:{root:ht,input:yt},slotProps:ft,fullWidth:it,inputComponent:st,multiline:lt,ref:o,type:ut},ot,{classes:dt}))});Input.muiName="Input";const Input$1=Input;function getInputAdornmentUtilityClass(a){return generateUtilityClass$1("MuiInputAdornment",a)}const inputAdornmentClasses=generateUtilityClasses$1("MuiInputAdornment",["root","filled","standard","outlined","positionStart","positionEnd","disablePointerEvents","hiddenLabel","sizeSmall"]),inputAdornmentClasses$1=inputAdornmentClasses;var _span$2;const _excluded$10=["children","className","component","disablePointerEvents","disableTypography","position","variant"],overridesResolver$5=(a,i)=>{const{ownerState:o}=a;return[i.root,i[`position${capitalize$2(o.position)}`],o.disablePointerEvents===!0&&i.disablePointerEvents,i[o.variant]]},useUtilityClasses$S=a=>{const{classes:i,disablePointerEvents:o,hiddenLabel:s,position:$,size:j,variant:_e}=a,et={root:["root",o&&"disablePointerEvents",$&&`position${capitalize$2($)}`,_e,s&&"hiddenLabel",j&&`size${capitalize$2(j)}`]};return composeClasses(et,getInputAdornmentUtilityClass,i)},InputAdornmentRoot=styled("div",{name:"MuiInputAdornment",slot:"Root",overridesResolver:overridesResolver$5})(({theme:a,ownerState:i})=>_extends({display:"flex",height:"0.01em",maxHeight:"2em",alignItems:"center",whiteSpace:"nowrap",color:(a.vars||a).palette.action.active},i.variant==="filled"&&{[`&.${inputAdornmentClasses$1.positionStart}&:not(.${inputAdornmentClasses$1.hiddenLabel})`]:{marginTop:16}},i.position==="start"&&{marginRight:8},i.position==="end"&&{marginLeft:8},i.disablePointerEvents===!0&&{pointerEvents:"none"})),InputAdornment=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiInputAdornment"}),{children:$,className:j,component:_e="div",disablePointerEvents:et=!1,disableTypography:tt=!1,position:nt,variant:at}=s,it=_objectWithoutPropertiesLoose(s,_excluded$10),st=useFormControl()||{};let lt=at;at&&st.variant,st&&!lt&&(lt=st.variant);const ct=_extends({},s,{hiddenLabel:st.hiddenLabel,size:st.size,disablePointerEvents:et,position:nt,variant:lt}),rt=useUtilityClasses$S(ct);return jsxRuntimeExports.jsx(FormControlContext$1.Provider,{value:null,children:jsxRuntimeExports.jsx(InputAdornmentRoot,_extends({as:_e,ownerState:ct,className:clsx(rt.root,j),ref:o},it,{children:typeof $=="string"&&!tt?jsxRuntimeExports.jsx(Typography$1,{color:"text.secondary",children:$}):jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[nt==="start"?_span$2||(_span$2=jsxRuntimeExports.jsx("span",{className:"notranslate",children:"​"})):null,$]})}))})}),InputAdornment$1=InputAdornment;function getInputLabelUtilityClasses(a){return generateUtilityClass$1("MuiInputLabel",a)}generateUtilityClasses$1("MuiInputLabel",["root","focused","disabled","error","required","asterisk","formControl","sizeSmall","shrink","animated","standard","filled","outlined"]);const _excluded$$=["disableAnimation","margin","shrink","variant","className"],useUtilityClasses$R=a=>{const{classes:i,formControl:o,size:s,shrink:$,disableAnimation:j,variant:_e,required:et}=a,tt={root:["root",o&&"formControl",!j&&"animated",$&&"shrink",s&&s!=="normal"&&`size${capitalize$2(s)}`,_e],asterisk:[et&&"asterisk"]},nt=composeClasses(tt,getInputLabelUtilityClasses,i);return _extends({},i,nt)},InputLabelRoot=styled(FormLabel$1,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiInputLabel",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`& .${formLabelClasses$1.asterisk}`]:i.asterisk},i.root,o.formControl&&i.formControl,o.size==="small"&&i.sizeSmall,o.shrink&&i.shrink,!o.disableAnimation&&i.animated,o.focused&&i.focused,i[o.variant]]}})(({theme:a,ownerState:i})=>_extends({display:"block",transformOrigin:"top left",whiteSpace:"nowrap",overflow:"hidden",textOverflow:"ellipsis",maxWidth:"100%"},i.formControl&&{position:"absolute",left:0,top:0,transform:"translate(0, 20px) scale(1)"},i.size==="small"&&{transform:"translate(0, 17px) scale(1)"},i.shrink&&{transform:"translate(0, -1.5px) scale(0.75)",transformOrigin:"top left",maxWidth:"133%"},!i.disableAnimation&&{transition:a.transitions.create(["color","transform","max-width"],{duration:a.transitions.duration.shorter,easing:a.transitions.easing.easeOut})},i.variant==="filled"&&_extends({zIndex:1,pointerEvents:"none",transform:"translate(12px, 16px) scale(1)",maxWidth:"calc(100% - 24px)"},i.size==="small"&&{transform:"translate(12px, 13px) scale(1)"},i.shrink&&_extends({userSelect:"none",pointerEvents:"auto",transform:"translate(12px, 7px) scale(0.75)",maxWidth:"calc(133% - 24px)"},i.size==="small"&&{transform:"translate(12px, 4px) scale(0.75)"})),i.variant==="outlined"&&_extends({zIndex:1,pointerEvents:"none",transform:"translate(14px, 16px) scale(1)",maxWidth:"calc(100% - 24px)"},i.size==="small"&&{transform:"translate(14px, 9px) scale(1)"},i.shrink&&{userSelect:"none",pointerEvents:"auto",maxWidth:"calc(133% - 32px)",transform:"translate(14px, -9px) scale(0.75)"}))),InputLabel=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({name:"MuiInputLabel",props:i}),{disableAnimation:$=!1,shrink:j,className:_e}=s,et=_objectWithoutPropertiesLoose(s,_excluded$$),tt=useFormControl();let nt=j;typeof nt>"u"&&tt&&(nt=tt.filled||tt.focused||tt.adornedStart);const at=formControlState({props:s,muiFormControl:tt,states:["size","variant","required","focused"]}),it=_extends({},s,{disableAnimation:$,formControl:tt,shrink:nt,size:at.size,variant:at.variant,required:at.required,focused:at.focused}),st=useUtilityClasses$R(it);return jsxRuntimeExports.jsx(InputLabelRoot,_extends({"data-shrink":nt,ownerState:it,ref:o,className:clsx(st.root,_e)},et,{classes:st}))}),InputLabel$1=InputLabel;function getLinearProgressUtilityClass(a){return generateUtilityClass$1("MuiLinearProgress",a)}generateUtilityClasses$1("MuiLinearProgress",["root","colorPrimary","colorSecondary","determinate","indeterminate","buffer","query","dashed","dashedColorPrimary","dashedColorSecondary","bar","barColorPrimary","barColorSecondary","bar1Indeterminate","bar1Determinate","bar1Buffer","bar2Indeterminate","bar2Buffer"]);const _excluded$_=["className","color","value","valueBuffer","variant"];let _$1=a=>a,_t$1,_t2$1,_t3$1,_t4$1,_t5,_t6;const TRANSITION_DURATION=4,indeterminate1Keyframe=keyframes(_t$1||(_t$1=_$1`
  0% {
    left: -35%;
    right: 100%;
  }

  60% {
    left: 100%;
    right: -90%;
  }

  100% {
    left: 100%;
    right: -90%;
  }
`)),indeterminate2Keyframe=keyframes(_t2$1||(_t2$1=_$1`
  0% {
    left: -200%;
    right: 100%;
  }

  60% {
    left: 107%;
    right: -8%;
  }

  100% {
    left: 107%;
    right: -8%;
  }
`)),bufferKeyframe=keyframes(_t3$1||(_t3$1=_$1`
  0% {
    opacity: 1;
    background-position: 0 -23px;
  }

  60% {
    opacity: 0;
    background-position: 0 -23px;
  }

  100% {
    opacity: 1;
    background-position: -200px -23px;
  }
`)),useUtilityClasses$Q=a=>{const{classes:i,variant:o,color:s}=a,$={root:["root",`color${capitalize$2(s)}`,o],dashed:["dashed",`dashedColor${capitalize$2(s)}`],bar1:["bar",`barColor${capitalize$2(s)}`,(o==="indeterminate"||o==="query")&&"bar1Indeterminate",o==="determinate"&&"bar1Determinate",o==="buffer"&&"bar1Buffer"],bar2:["bar",o!=="buffer"&&`barColor${capitalize$2(s)}`,o==="buffer"&&`color${capitalize$2(s)}`,(o==="indeterminate"||o==="query")&&"bar2Indeterminate",o==="buffer"&&"bar2Buffer"]};return composeClasses($,getLinearProgressUtilityClass,i)},getColorShade=(a,i)=>i==="inherit"?"currentColor":a.vars?a.vars.palette.LinearProgress[`${i}Bg`]:a.palette.mode==="light"?lighten_1(a.palette[i].main,.62):darken_1(a.palette[i].main,.5),LinearProgressRoot=styled("span",{name:"MuiLinearProgress",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[`color${capitalize$2(o.color)}`],i[o.variant]]}})(({ownerState:a,theme:i})=>_extends({position:"relative",overflow:"hidden",display:"block",height:4,zIndex:0,"@media print":{colorAdjust:"exact"},backgroundColor:getColorShade(i,a.color)},a.color==="inherit"&&a.variant!=="buffer"&&{backgroundColor:"none","&::before":{content:'""',position:"absolute",left:0,top:0,right:0,bottom:0,backgroundColor:"currentColor",opacity:.3}},a.variant==="buffer"&&{backgroundColor:"transparent"},a.variant==="query"&&{transform:"rotate(180deg)"})),LinearProgressDashed=styled("span",{name:"MuiLinearProgress",slot:"Dashed",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.dashed,i[`dashedColor${capitalize$2(o.color)}`]]}})(({ownerState:a,theme:i})=>{const o=getColorShade(i,a.color);return _extends({position:"absolute",marginTop:0,height:"100%",width:"100%"},a.color==="inherit"&&{opacity:.3},{backgroundImage:`radial-gradient(${o} 0%, ${o} 16%, transparent 42%)`,backgroundSize:"10px 10px",backgroundPosition:"0 -23px"})},css(_t4$1||(_t4$1=_$1`
    animation: ${0} 3s infinite linear;
  `),bufferKeyframe)),LinearProgressBar1=styled("span",{name:"MuiLinearProgress",slot:"Bar1",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.bar,i[`barColor${capitalize$2(o.color)}`],(o.variant==="indeterminate"||o.variant==="query")&&i.bar1Indeterminate,o.variant==="determinate"&&i.bar1Determinate,o.variant==="buffer"&&i.bar1Buffer]}})(({ownerState:a,theme:i})=>_extends({width:"100%",position:"absolute",left:0,bottom:0,top:0,transition:"transform 0.2s linear",transformOrigin:"left",backgroundColor:a.color==="inherit"?"currentColor":(i.vars||i).palette[a.color].main},a.variant==="determinate"&&{transition:`transform .${TRANSITION_DURATION}s linear`},a.variant==="buffer"&&{zIndex:1,transition:`transform .${TRANSITION_DURATION}s linear`}),({ownerState:a})=>(a.variant==="indeterminate"||a.variant==="query")&&css(_t5||(_t5=_$1`
      width: auto;
      animation: ${0} 2.1s cubic-bezier(0.65, 0.815, 0.735, 0.395) infinite;
    `),indeterminate1Keyframe)),LinearProgressBar2=styled("span",{name:"MuiLinearProgress",slot:"Bar2",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.bar,i[`barColor${capitalize$2(o.color)}`],(o.variant==="indeterminate"||o.variant==="query")&&i.bar2Indeterminate,o.variant==="buffer"&&i.bar2Buffer]}})(({ownerState:a,theme:i})=>_extends({width:"100%",position:"absolute",left:0,bottom:0,top:0,transition:"transform 0.2s linear",transformOrigin:"left"},a.variant!=="buffer"&&{backgroundColor:a.color==="inherit"?"currentColor":(i.vars||i).palette[a.color].main},a.color==="inherit"&&{opacity:.3},a.variant==="buffer"&&{backgroundColor:getColorShade(i,a.color),transition:`transform .${TRANSITION_DURATION}s linear`}),({ownerState:a})=>(a.variant==="indeterminate"||a.variant==="query")&&css(_t6||(_t6=_$1`
      width: auto;
      animation: ${0} 2.1s cubic-bezier(0.165, 0.84, 0.44, 1) 1.15s infinite;
    `),indeterminate2Keyframe)),LinearProgress=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiLinearProgress"}),{className:$,color:j="primary",value:_e,valueBuffer:et,variant:tt="indeterminate"}=s,nt=_objectWithoutPropertiesLoose(s,_excluded$_),at=_extends({},s,{color:j,variant:tt}),it=useUtilityClasses$Q(at),st=useRtl(),lt={},ct={bar1:{},bar2:{}};if((tt==="determinate"||tt==="buffer")&&_e!==void 0){lt["aria-valuenow"]=Math.round(_e),lt["aria-valuemin"]=0,lt["aria-valuemax"]=100;let rt=_e-100;st&&(rt=-rt),ct.bar1.transform=`translateX(${rt}%)`}if(tt==="buffer"&&et!==void 0){let rt=(et||0)-100;st&&(rt=-rt),ct.bar2.transform=`translateX(${rt}%)`}return jsxRuntimeExports.jsxs(LinearProgressRoot,_extends({className:clsx(it.root,$),ownerState:at,role:"progressbar"},lt,{ref:o},nt,{children:[tt==="buffer"?jsxRuntimeExports.jsx(LinearProgressDashed,{className:it.dashed,ownerState:at}):null,jsxRuntimeExports.jsx(LinearProgressBar1,{className:it.bar1,ownerState:at,style:ct.bar1}),tt==="determinate"?null:jsxRuntimeExports.jsx(LinearProgressBar2,{className:it.bar2,ownerState:at,style:ct.bar2})]}))}),LinearProgress$1=LinearProgress;function getLinkUtilityClass(a){return generateUtilityClass$1("MuiLink",a)}const linkClasses=generateUtilityClasses$1("MuiLink",["root","underlineNone","underlineHover","underlineAlways","button","focusVisible"]),linkClasses$1=linkClasses,colorTransformations={primary:"primary.main",textPrimary:"text.primary",secondary:"secondary.main",textSecondary:"text.secondary",error:"error.main"},transformDeprecatedColors=a=>colorTransformations[a]||a,getTextDecoration=({theme:a,ownerState:i})=>{const o=transformDeprecatedColors(i.color),s=getPath(a,`palette.${o}`,!1)||i.color,$=getPath(a,`palette.${o}Channel`);return"vars"in a&&$?`rgba(${$} / 0.4)`:alpha_1(s,.4)},_excluded$Z=["className","color","component","onBlur","onFocus","TypographyClasses","underline","variant","sx"],useUtilityClasses$P=a=>{const{classes:i,component:o,focusVisible:s,underline:$}=a,j={root:["root",`underline${capitalize$2($)}`,o==="button"&&"button",s&&"focusVisible"]};return composeClasses(j,getLinkUtilityClass,i)},LinkRoot=styled(Typography$1,{name:"MuiLink",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[`underline${capitalize$2(o.underline)}`],o.component==="button"&&i.button]}})(({theme:a,ownerState:i})=>_extends({},i.underline==="none"&&{textDecoration:"none"},i.underline==="hover"&&{textDecoration:"none","&:hover":{textDecoration:"underline"}},i.underline==="always"&&_extends({textDecoration:"underline"},i.color!=="inherit"&&{textDecorationColor:getTextDecoration({theme:a,ownerState:i})},{"&:hover":{textDecorationColor:"inherit"}}),i.component==="button"&&{position:"relative",WebkitTapHighlightColor:"transparent",backgroundColor:"transparent",outline:0,border:0,margin:0,borderRadius:0,padding:0,cursor:"pointer",userSelect:"none",verticalAlign:"middle",MozAppearance:"none",WebkitAppearance:"none","&::-moz-focus-inner":{borderStyle:"none"},[`&.${linkClasses$1.focusVisible}`]:{outline:"auto"}})),Link=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiLink"}),{className:$,color:j="primary",component:_e="a",onBlur:et,onFocus:tt,TypographyClasses:nt,underline:at="always",variant:it="inherit",sx:st}=s,lt=_objectWithoutPropertiesLoose(s,_excluded$Z),{isFocusVisibleRef:ct,onBlur:rt,onFocus:ut,ref:ot}=useIsFocusVisible(),[dt,pt]=reactExports.useState(!1),mt=useForkRef(o,ot),ft=gt=>{rt(gt),ct.current===!1&&pt(!1),et&&et(gt)},ht=gt=>{ut(gt),ct.current===!0&&pt(!0),tt&&tt(gt)},yt=_extends({},s,{color:j,component:_e,focusVisible:dt,underline:at,variant:it}),bt=useUtilityClasses$P(yt);return jsxRuntimeExports.jsx(LinkRoot,_extends({color:j,className:clsx(bt.root,$),classes:nt,component:_e,onBlur:ft,onFocus:ht,ref:mt,ownerState:yt,variant:it,sx:[...Object.keys(colorTransformations).includes(j)?[]:[{color:j}],...Array.isArray(st)?st:[st]]},lt))}),Link$1=Link,ListContext=reactExports.createContext({}),ListContext$1=ListContext;function getListUtilityClass(a){return generateUtilityClass$1("MuiList",a)}generateUtilityClasses$1("MuiList",["root","padding","dense","subheader"]);const _excluded$Y=["children","className","component","dense","disablePadding","subheader"],useUtilityClasses$O=a=>{const{classes:i,disablePadding:o,dense:s,subheader:$}=a;return composeClasses({root:["root",!o&&"padding",s&&"dense",$&&"subheader"]},getListUtilityClass,i)},ListRoot=styled("ul",{name:"MuiList",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,!o.disablePadding&&i.padding,o.dense&&i.dense,o.subheader&&i.subheader]}})(({ownerState:a})=>_extends({listStyle:"none",margin:0,padding:0,position:"relative"},!a.disablePadding&&{paddingTop:8,paddingBottom:8},a.subheader&&{paddingTop:0})),List$1=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiList"}),{children:$,className:j,component:_e="ul",dense:et=!1,disablePadding:tt=!1,subheader:nt}=s,at=_objectWithoutPropertiesLoose(s,_excluded$Y),it=reactExports.useMemo(()=>({dense:et}),[et]),st=_extends({},s,{component:_e,dense:et,disablePadding:tt}),lt=useUtilityClasses$O(st);return jsxRuntimeExports.jsx(ListContext$1.Provider,{value:it,children:jsxRuntimeExports.jsxs(ListRoot,_extends({as:_e,className:clsx(lt.root,j),ref:o,ownerState:st},at,{children:[nt,$]}))})}),List$2=List$1;function getListItemUtilityClass(a){return generateUtilityClass$1("MuiListItem",a)}const listItemClasses=generateUtilityClasses$1("MuiListItem",["root","container","focusVisible","dense","alignItemsFlexStart","disabled","divider","gutters","padding","button","secondaryAction","selected"]),listItemClasses$1=listItemClasses;function getListItemButtonUtilityClass(a){return generateUtilityClass$1("MuiListItemButton",a)}const listItemButtonClasses=generateUtilityClasses$1("MuiListItemButton",["root","focusVisible","dense","alignItemsFlexStart","disabled","divider","gutters","selected"]),listItemButtonClasses$1=listItemButtonClasses,_excluded$X=["alignItems","autoFocus","component","children","dense","disableGutters","divider","focusVisibleClassName","selected","className"],overridesResolver$4=(a,i)=>{const{ownerState:o}=a;return[i.root,o.dense&&i.dense,o.alignItems==="flex-start"&&i.alignItemsFlexStart,o.divider&&i.divider,!o.disableGutters&&i.gutters]},useUtilityClasses$N=a=>{const{alignItems:i,classes:o,dense:s,disabled:$,disableGutters:j,divider:_e,selected:et}=a,nt=composeClasses({root:["root",s&&"dense",!j&&"gutters",_e&&"divider",$&&"disabled",i==="flex-start"&&"alignItemsFlexStart",et&&"selected"]},getListItemButtonUtilityClass,o);return _extends({},o,nt)},ListItemButtonRoot=styled(ButtonBase$1,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiListItemButton",slot:"Root",overridesResolver:overridesResolver$4})(({theme:a,ownerState:i})=>_extends({display:"flex",flexGrow:1,justifyContent:"flex-start",alignItems:"center",position:"relative",textDecoration:"none",minWidth:0,boxSizing:"border-box",textAlign:"left",paddingTop:8,paddingBottom:8,transition:a.transitions.create("background-color",{duration:a.transitions.duration.shortest}),"&:hover":{textDecoration:"none",backgroundColor:(a.vars||a).palette.action.hover,"@media (hover: none)":{backgroundColor:"transparent"}},[`&.${listItemButtonClasses$1.selected}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity),[`&.${listItemButtonClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)}},[`&.${listItemButtonClasses$1.selected}:hover`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity)}},[`&.${listItemButtonClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},[`&.${listItemButtonClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity}},i.divider&&{borderBottom:`1px solid ${(a.vars||a).palette.divider}`,backgroundClip:"padding-box"},i.alignItems==="flex-start"&&{alignItems:"flex-start"},!i.disableGutters&&{paddingLeft:16,paddingRight:16},i.dense&&{paddingTop:4,paddingBottom:4})),ListItemButton=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiListItemButton"}),{alignItems:$="center",autoFocus:j=!1,component:_e="div",children:et,dense:tt=!1,disableGutters:nt=!1,divider:at=!1,focusVisibleClassName:it,selected:st=!1,className:lt}=s,ct=_objectWithoutPropertiesLoose(s,_excluded$X),rt=reactExports.useContext(ListContext$1),ut=reactExports.useMemo(()=>({dense:tt||rt.dense||!1,alignItems:$,disableGutters:nt}),[$,rt.dense,tt,nt]),ot=reactExports.useRef(null);useEnhancedEffect(()=>{j&&ot.current&&ot.current.focus()},[j]);const dt=_extends({},s,{alignItems:$,dense:ut.dense,disableGutters:nt,divider:at,selected:st}),pt=useUtilityClasses$N(dt),mt=useForkRef(ot,o);return jsxRuntimeExports.jsx(ListContext$1.Provider,{value:ut,children:jsxRuntimeExports.jsx(ListItemButtonRoot,_extends({ref:mt,href:ct.href||ct.to,component:(ct.href||ct.to)&&_e==="div"?"button":_e,focusVisibleClassName:clsx(pt.focusVisible,it),ownerState:dt,className:clsx(pt.root,lt)},ct,{classes:pt,children:et}))})}),ListItemButton$1=ListItemButton;function getListItemSecondaryActionClassesUtilityClass(a){return generateUtilityClass$1("MuiListItemSecondaryAction",a)}generateUtilityClasses$1("MuiListItemSecondaryAction",["root","disableGutters"]);const _excluded$W=["className"],useUtilityClasses$M=a=>{const{disableGutters:i,classes:o}=a;return composeClasses({root:["root",i&&"disableGutters"]},getListItemSecondaryActionClassesUtilityClass,o)},ListItemSecondaryActionRoot=styled("div",{name:"MuiListItemSecondaryAction",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.disableGutters&&i.disableGutters]}})(({ownerState:a})=>_extends({position:"absolute",right:16,top:"50%",transform:"translateY(-50%)"},a.disableGutters&&{right:0})),ListItemSecondaryAction=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiListItemSecondaryAction"}),{className:$}=s,j=_objectWithoutPropertiesLoose(s,_excluded$W),_e=reactExports.useContext(ListContext$1),et=_extends({},s,{disableGutters:_e.disableGutters}),tt=useUtilityClasses$M(et);return jsxRuntimeExports.jsx(ListItemSecondaryActionRoot,_extends({className:clsx(tt.root,$),ownerState:et,ref:o},j))});ListItemSecondaryAction.muiName="ListItemSecondaryAction";const ListItemSecondaryAction$1=ListItemSecondaryAction,_excluded$V=["className"],_excluded2$b=["alignItems","autoFocus","button","children","className","component","components","componentsProps","ContainerComponent","ContainerProps","dense","disabled","disableGutters","disablePadding","divider","focusVisibleClassName","secondaryAction","selected","slotProps","slots"],overridesResolver$3=(a,i)=>{const{ownerState:o}=a;return[i.root,o.dense&&i.dense,o.alignItems==="flex-start"&&i.alignItemsFlexStart,o.divider&&i.divider,!o.disableGutters&&i.gutters,!o.disablePadding&&i.padding,o.button&&i.button,o.hasSecondaryAction&&i.secondaryAction]},useUtilityClasses$L=a=>{const{alignItems:i,button:o,classes:s,dense:$,disabled:j,disableGutters:_e,disablePadding:et,divider:tt,hasSecondaryAction:nt,selected:at}=a;return composeClasses({root:["root",$&&"dense",!_e&&"gutters",!et&&"padding",tt&&"divider",j&&"disabled",o&&"button",i==="flex-start"&&"alignItemsFlexStart",nt&&"secondaryAction",at&&"selected"],container:["container"]},getListItemUtilityClass,s)},ListItemRoot=styled("div",{name:"MuiListItem",slot:"Root",overridesResolver:overridesResolver$3})(({theme:a,ownerState:i})=>_extends({display:"flex",justifyContent:"flex-start",alignItems:"center",position:"relative",textDecoration:"none",width:"100%",boxSizing:"border-box",textAlign:"left"},!i.disablePadding&&_extends({paddingTop:8,paddingBottom:8},i.dense&&{paddingTop:4,paddingBottom:4},!i.disableGutters&&{paddingLeft:16,paddingRight:16},!!i.secondaryAction&&{paddingRight:48}),!!i.secondaryAction&&{[`& > .${listItemButtonClasses$1.root}`]:{paddingRight:48}},{[`&.${listItemClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},[`&.${listItemClasses$1.selected}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity),[`&.${listItemClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)}},[`&.${listItemClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity}},i.alignItems==="flex-start"&&{alignItems:"flex-start"},i.divider&&{borderBottom:`1px solid ${(a.vars||a).palette.divider}`,backgroundClip:"padding-box"},i.button&&{transition:a.transitions.create("background-color",{duration:a.transitions.duration.shortest}),"&:hover":{textDecoration:"none",backgroundColor:(a.vars||a).palette.action.hover,"@media (hover: none)":{backgroundColor:"transparent"}},[`&.${listItemClasses$1.selected}:hover`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity)}}},i.hasSecondaryAction&&{paddingRight:48})),ListItemContainer=styled("li",{name:"MuiListItem",slot:"Container",overridesResolver:(a,i)=>i.container})({position:"relative"}),ListItem=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiListItem"}),{alignItems:$="center",autoFocus:j=!1,button:_e=!1,children:et,className:tt,component:nt,components:at={},componentsProps:it={},ContainerComponent:st="li",ContainerProps:{className:lt}={},dense:ct=!1,disabled:rt=!1,disableGutters:ut=!1,disablePadding:ot=!1,divider:dt=!1,focusVisibleClassName:pt,secondaryAction:mt,selected:ft=!1,slotProps:ht={},slots:yt={}}=s,bt=_objectWithoutPropertiesLoose(s.ContainerProps,_excluded$V),gt=_objectWithoutPropertiesLoose(s,_excluded2$b),xt=reactExports.useContext(ListContext$1),vt=reactExports.useMemo(()=>({dense:ct||xt.dense||!1,alignItems:$,disableGutters:ut}),[$,xt.dense,ct,ut]),Lt=reactExports.useRef(null);useEnhancedEffect(()=>{j&&Lt.current&&Lt.current.focus()},[j]);const $t=reactExports.Children.toArray(et),Tt=$t.length&&isMuiElement($t[$t.length-1],["ListItemSecondaryAction"]),Et=_extends({},s,{alignItems:$,autoFocus:j,button:_e,dense:vt.dense,disabled:rt,disableGutters:ut,disablePadding:ot,divider:dt,hasSecondaryAction:Tt,selected:ft}),Dt=useUtilityClasses$L(Et),It=useForkRef(Lt,o),Ct=yt.root||at.Root||ListItemRoot,jt=ht.root||it.root||{},Zt=_extends({className:clsx(Dt.root,jt.className,tt),disabled:rt},gt);let Xt=nt||"li";return _e&&(Zt.component=nt||"div",Zt.focusVisibleClassName=clsx(listItemClasses$1.focusVisible,pt),Xt=ButtonBase$1),Tt?(Xt=!Zt.component&&!nt?"div":Xt,st==="li"&&(Xt==="li"?Xt="div":Zt.component==="li"&&(Zt.component="div")),jsxRuntimeExports.jsx(ListContext$1.Provider,{value:vt,children:jsxRuntimeExports.jsxs(ListItemContainer,_extends({as:st,className:clsx(Dt.container,lt),ref:It,ownerState:Et},bt,{children:[jsxRuntimeExports.jsx(Ct,_extends({},jt,!isHostComponent(Ct)&&{as:Xt,ownerState:_extends({},Et,jt.ownerState)},Zt,{children:$t})),$t.pop()]}))})):jsxRuntimeExports.jsx(ListContext$1.Provider,{value:vt,children:jsxRuntimeExports.jsxs(Ct,_extends({},jt,{as:Xt,ref:It},!isHostComponent(Ct)&&{ownerState:_extends({},Et,jt.ownerState)},Zt,{children:[$t,mt&&jsxRuntimeExports.jsx(ListItemSecondaryAction$1,{children:mt})]}))})}),ListItem$1=ListItem;function getListItemIconUtilityClass(a){return generateUtilityClass$1("MuiListItemIcon",a)}const listItemIconClasses=generateUtilityClasses$1("MuiListItemIcon",["root","alignItemsFlexStart"]),listItemIconClasses$1=listItemIconClasses,_excluded$U=["className"],useUtilityClasses$K=a=>{const{alignItems:i,classes:o}=a;return composeClasses({root:["root",i==="flex-start"&&"alignItemsFlexStart"]},getListItemIconUtilityClass,o)},ListItemIconRoot=styled("div",{name:"MuiListItemIcon",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.alignItems==="flex-start"&&i.alignItemsFlexStart]}})(({theme:a,ownerState:i})=>_extends({minWidth:56,color:(a.vars||a).palette.action.active,flexShrink:0,display:"inline-flex"},i.alignItems==="flex-start"&&{marginTop:8})),ListItemIcon=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiListItemIcon"}),{className:$}=s,j=_objectWithoutPropertiesLoose(s,_excluded$U),_e=reactExports.useContext(ListContext$1),et=_extends({},s,{alignItems:_e.alignItems}),tt=useUtilityClasses$K(et);return jsxRuntimeExports.jsx(ListItemIconRoot,_extends({className:clsx(tt.root,$),ownerState:et,ref:o},j))}),ListItemIcon$1=ListItemIcon;function getListItemTextUtilityClass(a){return generateUtilityClass$1("MuiListItemText",a)}const listItemTextClasses=generateUtilityClasses$1("MuiListItemText",["root","multiline","dense","inset","primary","secondary"]),listItemTextClasses$1=listItemTextClasses,_excluded$T=["children","className","disableTypography","inset","primary","primaryTypographyProps","secondary","secondaryTypographyProps"],useUtilityClasses$J=a=>{const{classes:i,inset:o,primary:s,secondary:$,dense:j}=a;return composeClasses({root:["root",o&&"inset",j&&"dense",s&&$&&"multiline"],primary:["primary"],secondary:["secondary"]},getListItemTextUtilityClass,i)},ListItemTextRoot=styled("div",{name:"MuiListItemText",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`& .${listItemTextClasses$1.primary}`]:i.primary},{[`& .${listItemTextClasses$1.secondary}`]:i.secondary},i.root,o.inset&&i.inset,o.primary&&o.secondary&&i.multiline,o.dense&&i.dense]}})(({ownerState:a})=>_extends({flex:"1 1 auto",minWidth:0,marginTop:4,marginBottom:4},a.primary&&a.secondary&&{marginTop:6,marginBottom:6},a.inset&&{paddingLeft:56})),ListItemText=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiListItemText"}),{children:$,className:j,disableTypography:_e=!1,inset:et=!1,primary:tt,primaryTypographyProps:nt,secondary:at,secondaryTypographyProps:it}=s,st=_objectWithoutPropertiesLoose(s,_excluded$T),{dense:lt}=reactExports.useContext(ListContext$1);let ct=tt??$,rt=at;const ut=_extends({},s,{disableTypography:_e,inset:et,primary:!!ct,secondary:!!rt,dense:lt}),ot=useUtilityClasses$J(ut);return ct!=null&&ct.type!==Typography$1&&!_e&&(ct=jsxRuntimeExports.jsx(Typography$1,_extends({variant:lt?"body2":"body1",className:ot.primary,component:nt!=null&&nt.variant?void 0:"span",display:"block"},nt,{children:ct}))),rt!=null&&rt.type!==Typography$1&&!_e&&(rt=jsxRuntimeExports.jsx(Typography$1,_extends({variant:"body2",className:ot.secondary,color:"text.secondary",display:"block"},it,{children:rt}))),jsxRuntimeExports.jsxs(ListItemTextRoot,_extends({className:clsx(ot.root,j),ownerState:ut,ref:o},st,{children:[ct,rt]}))}),ListItemText$1=ListItemText,_excluded$S=["actions","autoFocus","autoFocusItem","children","className","disabledItemsFocusable","disableListWrap","onKeyDown","variant"];function nextItem(a,i,o){return a===i?a.firstChild:i&&i.nextElementSibling?i.nextElementSibling:o?null:a.firstChild}function previousItem(a,i,o){return a===i?o?a.firstChild:a.lastChild:i&&i.previousElementSibling?i.previousElementSibling:o?null:a.lastChild}function textCriteriaMatches(a,i){if(i===void 0)return!0;let o=a.innerText;return o===void 0&&(o=a.textContent),o=o.trim().toLowerCase(),o.length===0?!1:i.repeating?o[0]===i.keys[0]:o.indexOf(i.keys.join(""))===0}function moveFocus(a,i,o,s,$,j){let _e=!1,et=$(a,i,i?o:!1);for(;et;){if(et===a.firstChild){if(_e)return!1;_e=!0}const tt=s?!1:et.disabled||et.getAttribute("aria-disabled")==="true";if(!et.hasAttribute("tabindex")||!textCriteriaMatches(et,j)||tt)et=$(a,et,o);else return et.focus(),!0}return!1}const MenuList=reactExports.forwardRef(function(i,o){const{actions:s,autoFocus:$=!1,autoFocusItem:j=!1,children:_e,className:et,disabledItemsFocusable:tt=!1,disableListWrap:nt=!1,onKeyDown:at,variant:it="selectedMenu"}=i,st=_objectWithoutPropertiesLoose(i,_excluded$S),lt=reactExports.useRef(null),ct=reactExports.useRef({keys:[],repeating:!0,previousKeyMatched:!0,lastTime:null});useEnhancedEffect(()=>{$&&lt.current.focus()},[$]),reactExports.useImperativeHandle(s,()=>({adjustStyleForScrollbar:(pt,{direction:mt})=>{const ft=!lt.current.style.width;if(pt.clientHeight<lt.current.clientHeight&&ft){const ht=`${getScrollbarSize(ownerDocument(pt))}px`;lt.current.style[mt==="rtl"?"paddingLeft":"paddingRight"]=ht,lt.current.style.width=`calc(100% + ${ht})`}return lt.current}}),[]);const rt=pt=>{const mt=lt.current,ft=pt.key,ht=ownerDocument(mt).activeElement;if(ft==="ArrowDown")pt.preventDefault(),moveFocus(mt,ht,nt,tt,nextItem);else if(ft==="ArrowUp")pt.preventDefault(),moveFocus(mt,ht,nt,tt,previousItem);else if(ft==="Home")pt.preventDefault(),moveFocus(mt,null,nt,tt,nextItem);else if(ft==="End")pt.preventDefault(),moveFocus(mt,null,nt,tt,previousItem);else if(ft.length===1){const yt=ct.current,bt=ft.toLowerCase(),gt=performance.now();yt.keys.length>0&&(gt-yt.lastTime>500?(yt.keys=[],yt.repeating=!0,yt.previousKeyMatched=!0):yt.repeating&&bt!==yt.keys[0]&&(yt.repeating=!1)),yt.lastTime=gt,yt.keys.push(bt);const xt=ht&&!yt.repeating&&textCriteriaMatches(ht,yt);yt.previousKeyMatched&&(xt||moveFocus(mt,ht,!1,tt,nextItem,yt))?pt.preventDefault():yt.previousKeyMatched=!1}at&&at(pt)},ut=useForkRef(lt,o);let ot=-1;reactExports.Children.forEach(_e,(pt,mt)=>{if(!reactExports.isValidElement(pt)){ot===mt&&(ot+=1,ot>=_e.length&&(ot=-1));return}pt.props.disabled||(it==="selectedMenu"&&pt.props.selected||ot===-1)&&(ot=mt),ot===mt&&(pt.props.disabled||pt.props.muiSkipListHighlight||pt.type.muiSkipListHighlight)&&(ot+=1,ot>=_e.length&&(ot=-1))});const dt=reactExports.Children.map(_e,(pt,mt)=>{if(mt===ot){const ft={};return j&&(ft.autoFocus=!0),pt.props.tabIndex===void 0&&it==="selectedMenu"&&(ft.tabIndex=0),reactExports.cloneElement(pt,ft)}return pt});return jsxRuntimeExports.jsx(List$2,_extends({role:"menu",ref:ut,className:et,onKeyDown:rt,tabIndex:$?0:-1},st,{children:dt}))}),MenuList$1=MenuList;function getPopoverUtilityClass(a){return generateUtilityClass$1("MuiPopover",a)}generateUtilityClasses$1("MuiPopover",["root","paper"]);const _excluded$R=["onEntering"],_excluded2$a=["action","anchorEl","anchorOrigin","anchorPosition","anchorReference","children","className","container","elevation","marginThreshold","open","PaperProps","slots","slotProps","transformOrigin","TransitionComponent","transitionDuration","TransitionProps","disableScrollLock"],_excluded3$3=["slotProps"];function getOffsetTop(a,i){let o=0;return typeof i=="number"?o=i:i==="center"?o=a.height/2:i==="bottom"&&(o=a.height),o}function getOffsetLeft(a,i){let o=0;return typeof i=="number"?o=i:i==="center"?o=a.width/2:i==="right"&&(o=a.width),o}function getTransformOriginValue(a){return[a.horizontal,a.vertical].map(i=>typeof i=="number"?`${i}px`:i).join(" ")}function resolveAnchorEl(a){return typeof a=="function"?a():a}const useUtilityClasses$I=a=>{const{classes:i}=a;return composeClasses({root:["root"],paper:["paper"]},getPopoverUtilityClass,i)},PopoverRoot=styled(Modal$1,{name:"MuiPopover",slot:"Root",overridesResolver:(a,i)=>i.root})({}),PopoverPaper=styled(Paper$1,{name:"MuiPopover",slot:"Paper",overridesResolver:(a,i)=>i.paper})({position:"absolute",overflowY:"auto",overflowX:"hidden",minWidth:16,minHeight:16,maxWidth:"calc(100% - 32px)",maxHeight:"calc(100% - 32px)",outline:0}),Popover=reactExports.forwardRef(function(i,o){var s,$,j;const _e=useThemeProps$6({props:i,name:"MuiPopover"}),{action:et,anchorEl:tt,anchorOrigin:nt={vertical:"top",horizontal:"left"},anchorPosition:at,anchorReference:it="anchorEl",children:st,className:lt,container:ct,elevation:rt=8,marginThreshold:ut=16,open:ot,PaperProps:dt={},slots:pt,slotProps:mt,transformOrigin:ft={vertical:"top",horizontal:"left"},TransitionComponent:ht=Grow$1,transitionDuration:yt="auto",TransitionProps:{onEntering:bt}={},disableScrollLock:gt=!1}=_e,xt=_objectWithoutPropertiesLoose(_e.TransitionProps,_excluded$R),vt=_objectWithoutPropertiesLoose(_e,_excluded2$a),Lt=(s=mt==null?void 0:mt.paper)!=null?s:dt,$t=reactExports.useRef(),Tt=useForkRef($t,Lt.ref),Et=_extends({},_e,{anchorOrigin:nt,anchorReference:it,elevation:rt,marginThreshold:ut,externalPaperSlotProps:Lt,transformOrigin:ft,TransitionComponent:ht,transitionDuration:yt,TransitionProps:xt}),Dt=useUtilityClasses$I(Et),It=reactExports.useCallback(()=>{if(it==="anchorPosition")return at;const Wt=resolveAnchorEl(tt),tn=(Wt&&Wt.nodeType===1?Wt:ownerDocument($t.current).body).getBoundingClientRect();return{top:tn.top+getOffsetTop(tn,nt.vertical),left:tn.left+getOffsetLeft(tn,nt.horizontal)}},[tt,nt.horizontal,nt.vertical,at,it]),Ct=reactExports.useCallback(Wt=>({vertical:getOffsetTop(Wt,ft.vertical),horizontal:getOffsetLeft(Wt,ft.horizontal)}),[ft.horizontal,ft.vertical]),jt=reactExports.useCallback(Wt=>{const qt={width:Wt.offsetWidth,height:Wt.offsetHeight},tn=Ct(qt);if(it==="none")return{top:null,left:null,transformOrigin:getTransformOriginValue(tn)};const ln=It();let gn=ln.top-tn.vertical,yn=ln.left-tn.horizontal;const Pn=gn+qt.height,cn=yn+qt.width,xn=ownerWindow(resolveAnchorEl(tt)),hn=xn.innerHeight-ut,en=xn.innerWidth-ut;if(ut!==null&&gn<ut){const Jt=gn-ut;gn-=Jt,tn.vertical+=Jt}else if(ut!==null&&Pn>hn){const Jt=Pn-hn;gn-=Jt,tn.vertical+=Jt}if(ut!==null&&yn<ut){const Jt=yn-ut;yn-=Jt,tn.horizontal+=Jt}else if(cn>en){const Jt=cn-en;yn-=Jt,tn.horizontal+=Jt}return{top:`${Math.round(gn)}px`,left:`${Math.round(yn)}px`,transformOrigin:getTransformOriginValue(tn)}},[tt,it,It,Ct,ut]),[Zt,Xt]=reactExports.useState(ot),sn=reactExports.useCallback(()=>{const Wt=$t.current;if(!Wt)return;const qt=jt(Wt);qt.top!==null&&(Wt.style.top=qt.top),qt.left!==null&&(Wt.style.left=qt.left),Wt.style.transformOrigin=qt.transformOrigin,Xt(!0)},[jt]);reactExports.useEffect(()=>(gt&&window.addEventListener("scroll",sn),()=>window.removeEventListener("scroll",sn)),[tt,gt,sn]);const Ft=(Wt,qt)=>{bt&&bt(Wt,qt),sn()},wt=()=>{Xt(!1)};reactExports.useEffect(()=>{ot&&sn()}),reactExports.useImperativeHandle(et,()=>ot?{updatePosition:()=>{sn()}}:null,[ot,sn]),reactExports.useEffect(()=>{if(!ot)return;const Wt=debounce$1(()=>{sn()}),qt=ownerWindow(tt);return qt.addEventListener("resize",Wt),()=>{Wt.clear(),qt.removeEventListener("resize",Wt)}},[tt,ot,sn]);let kt=yt;yt==="auto"&&!ht.muiSupportAuto&&(kt=void 0);const At=ct||(tt?ownerDocument(resolveAnchorEl(tt)).body:void 0),Pt=($=pt==null?void 0:pt.root)!=null?$:PopoverRoot,Mt=(j=pt==null?void 0:pt.paper)!=null?j:PopoverPaper,Ot=useSlotProps({elementType:Mt,externalSlotProps:_extends({},Lt,{style:Zt?Lt.style:_extends({},Lt.style,{opacity:0})}),additionalProps:{elevation:rt,ref:Tt},ownerState:Et,className:clsx(Dt.paper,Lt==null?void 0:Lt.className)}),Bt=useSlotProps({elementType:Pt,externalSlotProps:(mt==null?void 0:mt.root)||{},externalForwardedProps:vt,additionalProps:{ref:o,slotProps:{backdrop:{invisible:!0}},container:At,open:ot},ownerState:Et,className:clsx(Dt.root,lt)}),{slotProps:zt}=Bt,Gt=_objectWithoutPropertiesLoose(Bt,_excluded3$3);return jsxRuntimeExports.jsx(Pt,_extends({},Gt,!isHostComponent(Pt)&&{slotProps:zt,disableScrollLock:gt},{children:jsxRuntimeExports.jsx(ht,_extends({appear:!0,in:ot,onEntering:Ft,onExited:wt,timeout:kt},xt,{children:jsxRuntimeExports.jsx(Mt,_extends({},Ot,{children:st}))}))}))}),Popover$1=Popover;function getMenuUtilityClass(a){return generateUtilityClass$1("MuiMenu",a)}generateUtilityClasses$1("MuiMenu",["root","paper","list"]);const _excluded$Q=["onEntering"],_excluded2$9=["autoFocus","children","className","disableAutoFocusItem","MenuListProps","onClose","open","PaperProps","PopoverClasses","transitionDuration","TransitionProps","variant","slots","slotProps"],RTL_ORIGIN={vertical:"top",horizontal:"right"},LTR_ORIGIN={vertical:"top",horizontal:"left"},useUtilityClasses$H=a=>{const{classes:i}=a;return composeClasses({root:["root"],paper:["paper"],list:["list"]},getMenuUtilityClass,i)},MenuRoot=styled(Popover$1,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiMenu",slot:"Root",overridesResolver:(a,i)=>i.root})({}),MenuPaper=styled(PopoverPaper,{name:"MuiMenu",slot:"Paper",overridesResolver:(a,i)=>i.paper})({maxHeight:"calc(100% - 96px)",WebkitOverflowScrolling:"touch"}),MenuMenuList=styled(MenuList$1,{name:"MuiMenu",slot:"List",overridesResolver:(a,i)=>i.list})({outline:0}),Menu$1=reactExports.forwardRef(function(i,o){var s,$;const j=useThemeProps$6({props:i,name:"MuiMenu"}),{autoFocus:_e=!0,children:et,className:tt,disableAutoFocusItem:nt=!1,MenuListProps:at={},onClose:it,open:st,PaperProps:lt={},PopoverClasses:ct,transitionDuration:rt="auto",TransitionProps:{onEntering:ut}={},variant:ot="selectedMenu",slots:dt={},slotProps:pt={}}=j,mt=_objectWithoutPropertiesLoose(j.TransitionProps,_excluded$Q),ft=_objectWithoutPropertiesLoose(j,_excluded2$9),ht=useRtl(),yt=_extends({},j,{autoFocus:_e,disableAutoFocusItem:nt,MenuListProps:at,onEntering:ut,PaperProps:lt,transitionDuration:rt,TransitionProps:mt,variant:ot}),bt=useUtilityClasses$H(yt),gt=_e&&!nt&&st,xt=reactExports.useRef(null),vt=(Ct,jt)=>{xt.current&&xt.current.adjustStyleForScrollbar(Ct,{direction:ht?"rtl":"ltr"}),ut&&ut(Ct,jt)},Lt=Ct=>{Ct.key==="Tab"&&(Ct.preventDefault(),it&&it(Ct,"tabKeyDown"))};let $t=-1;reactExports.Children.map(et,(Ct,jt)=>{reactExports.isValidElement(Ct)&&(Ct.props.disabled||(ot==="selectedMenu"&&Ct.props.selected||$t===-1)&&($t=jt))});const Tt=(s=dt.paper)!=null?s:MenuPaper,Et=($=pt.paper)!=null?$:lt,Dt=useSlotProps({elementType:dt.root,externalSlotProps:pt.root,ownerState:yt,className:[bt.root,tt]}),It=useSlotProps({elementType:Tt,externalSlotProps:Et,ownerState:yt,className:bt.paper});return jsxRuntimeExports.jsx(MenuRoot,_extends({onClose:it,anchorOrigin:{vertical:"bottom",horizontal:ht?"right":"left"},transformOrigin:ht?RTL_ORIGIN:LTR_ORIGIN,slots:{paper:Tt,root:dt.root},slotProps:{root:Dt,paper:It},open:st,ref:o,transitionDuration:rt,TransitionProps:_extends({onEntering:vt},mt),ownerState:yt},ft,{classes:ct,children:jsxRuntimeExports.jsx(MenuMenuList,_extends({onKeyDown:Lt,actions:xt,autoFocus:_e&&($t===-1||nt),autoFocusItem:gt,variant:ot},at,{className:clsx(bt.list,at.className),children:et}))}))}),Menu$2=Menu$1;function getMenuItemUtilityClass(a){return generateUtilityClass$1("MuiMenuItem",a)}const menuItemClasses=generateUtilityClasses$1("MuiMenuItem",["root","focusVisible","dense","disabled","divider","gutters","selected"]),menuItemClasses$1=menuItemClasses,_excluded$P=["autoFocus","component","dense","divider","disableGutters","focusVisibleClassName","role","tabIndex","className"],overridesResolver$2=(a,i)=>{const{ownerState:o}=a;return[i.root,o.dense&&i.dense,o.divider&&i.divider,!o.disableGutters&&i.gutters]},useUtilityClasses$G=a=>{const{disabled:i,dense:o,divider:s,disableGutters:$,selected:j,classes:_e}=a,tt=composeClasses({root:["root",o&&"dense",i&&"disabled",!$&&"gutters",s&&"divider",j&&"selected"]},getMenuItemUtilityClass,_e);return _extends({},_e,tt)},MenuItemRoot=styled(ButtonBase$1,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiMenuItem",slot:"Root",overridesResolver:overridesResolver$2})(({theme:a,ownerState:i})=>_extends({},a.typography.body1,{display:"flex",justifyContent:"flex-start",alignItems:"center",position:"relative",textDecoration:"none",minHeight:48,paddingTop:6,paddingBottom:6,boxSizing:"border-box",whiteSpace:"nowrap"},!i.disableGutters&&{paddingLeft:16,paddingRight:16},i.divider&&{borderBottom:`1px solid ${(a.vars||a).palette.divider}`,backgroundClip:"padding-box"},{"&:hover":{textDecoration:"none",backgroundColor:(a.vars||a).palette.action.hover,"@media (hover: none)":{backgroundColor:"transparent"}},[`&.${menuItemClasses$1.selected}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity),[`&.${menuItemClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)}},[`&.${menuItemClasses$1.selected}:hover`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity)}},[`&.${menuItemClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},[`&.${menuItemClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity},[`& + .${dividerClasses$1.root}`]:{marginTop:a.spacing(1),marginBottom:a.spacing(1)},[`& + .${dividerClasses$1.inset}`]:{marginLeft:52},[`& .${listItemTextClasses$1.root}`]:{marginTop:0,marginBottom:0},[`& .${listItemTextClasses$1.inset}`]:{paddingLeft:36},[`& .${listItemIconClasses$1.root}`]:{minWidth:36}},!i.dense&&{[a.breakpoints.up("sm")]:{minHeight:"auto"}},i.dense&&_extends({minHeight:32,paddingTop:4,paddingBottom:4},a.typography.body2,{[`& .${listItemIconClasses$1.root} svg`]:{fontSize:"1.25rem"}}))),MenuItem=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiMenuItem"}),{autoFocus:$=!1,component:j="li",dense:_e=!1,divider:et=!1,disableGutters:tt=!1,focusVisibleClassName:nt,role:at="menuitem",tabIndex:it,className:st}=s,lt=_objectWithoutPropertiesLoose(s,_excluded$P),ct=reactExports.useContext(ListContext$1),rt=reactExports.useMemo(()=>({dense:_e||ct.dense||!1,disableGutters:tt}),[ct.dense,_e,tt]),ut=reactExports.useRef(null);useEnhancedEffect(()=>{$&&ut.current&&ut.current.focus()},[$]);const ot=_extends({},s,{dense:rt.dense,divider:et,disableGutters:tt}),dt=useUtilityClasses$G(s),pt=useForkRef(ut,o);let mt;return s.disabled||(mt=it!==void 0?it:-1),jsxRuntimeExports.jsx(ListContext$1.Provider,{value:rt,children:jsxRuntimeExports.jsx(MenuItemRoot,_extends({ref:pt,role:at,tabIndex:mt,component:j,focusVisibleClassName:clsx(dt.focusVisible,nt),className:clsx(dt.root,st)},lt,{ownerState:ot,classes:dt}))})}),MenuItem$1=MenuItem;function getNativeSelectUtilityClasses(a){return generateUtilityClass$1("MuiNativeSelect",a)}const nativeSelectClasses=generateUtilityClasses$1("MuiNativeSelect",["root","select","multiple","filled","outlined","standard","disabled","icon","iconOpen","iconFilled","iconOutlined","iconStandard","nativeInput","error"]),nativeSelectClasses$1=nativeSelectClasses,_excluded$O=["className","disabled","error","IconComponent","inputRef","variant"],useUtilityClasses$F=a=>{const{classes:i,variant:o,disabled:s,multiple:$,open:j,error:_e}=a,et={select:["select",o,s&&"disabled",$&&"multiple",_e&&"error"],icon:["icon",`icon${capitalize$2(o)}`,j&&"iconOpen",s&&"disabled"]};return composeClasses(et,getNativeSelectUtilityClasses,i)},nativeSelectSelectStyles=({ownerState:a,theme:i})=>_extends({MozAppearance:"none",WebkitAppearance:"none",userSelect:"none",borderRadius:0,cursor:"pointer","&:focus":_extends({},i.vars?{backgroundColor:`rgba(${i.vars.palette.common.onBackgroundChannel} / 0.05)`}:{backgroundColor:i.palette.mode==="light"?"rgba(0, 0, 0, 0.05)":"rgba(255, 255, 255, 0.05)"},{borderRadius:0}),"&::-ms-expand":{display:"none"},[`&.${nativeSelectClasses$1.disabled}`]:{cursor:"default"},"&[multiple]":{height:"auto"},"&:not([multiple]) option, &:not([multiple]) optgroup":{backgroundColor:(i.vars||i).palette.background.paper},"&&&":{paddingRight:24,minWidth:16}},a.variant==="filled"&&{"&&&":{paddingRight:32}},a.variant==="outlined"&&{borderRadius:(i.vars||i).shape.borderRadius,"&:focus":{borderRadius:(i.vars||i).shape.borderRadius},"&&&":{paddingRight:32}}),NativeSelectSelect=styled("select",{name:"MuiNativeSelect",slot:"Select",shouldForwardProp:rootShouldForwardProp$2,overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.select,i[o.variant],o.error&&i.error,{[`&.${nativeSelectClasses$1.multiple}`]:i.multiple}]}})(nativeSelectSelectStyles),nativeSelectIconStyles=({ownerState:a,theme:i})=>_extends({position:"absolute",right:0,top:"calc(50% - .5em)",pointerEvents:"none",color:(i.vars||i).palette.action.active,[`&.${nativeSelectClasses$1.disabled}`]:{color:(i.vars||i).palette.action.disabled}},a.open&&{transform:"rotate(180deg)"},a.variant==="filled"&&{right:7},a.variant==="outlined"&&{right:7}),NativeSelectIcon=styled("svg",{name:"MuiNativeSelect",slot:"Icon",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.icon,o.variant&&i[`icon${capitalize$2(o.variant)}`],o.open&&i.iconOpen]}})(nativeSelectIconStyles),NativeSelectInput=reactExports.forwardRef(function(i,o){const{className:s,disabled:$,error:j,IconComponent:_e,inputRef:et,variant:tt="standard"}=i,nt=_objectWithoutPropertiesLoose(i,_excluded$O),at=_extends({},i,{disabled:$,variant:tt,error:j}),it=useUtilityClasses$F(at);return jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx(NativeSelectSelect,_extends({ownerState:at,className:clsx(it.select,s),disabled:$,ref:et||o},nt)),i.multiple?null:jsxRuntimeExports.jsx(NativeSelectIcon,{as:_e,ownerState:at,className:it.icon})]})}),NativeSelectInput$1=NativeSelectInput;var _span$1;const _excluded$N=["children","classes","className","label","notched"],NotchedOutlineRoot$1=styled("fieldset",{shouldForwardProp:rootShouldForwardProp$2})({textAlign:"left",position:"absolute",bottom:0,right:0,top:-5,left:0,margin:0,padding:"0 8px",pointerEvents:"none",borderRadius:"inherit",borderStyle:"solid",borderWidth:1,overflow:"hidden",minWidth:"0%"}),NotchedOutlineLegend=styled("legend",{shouldForwardProp:rootShouldForwardProp$2})(({ownerState:a,theme:i})=>_extends({float:"unset",width:"auto",overflow:"hidden"},!a.withLabel&&{padding:0,lineHeight:"11px",transition:i.transitions.create("width",{duration:150,easing:i.transitions.easing.easeOut})},a.withLabel&&_extends({display:"block",padding:0,height:11,fontSize:"0.75em",visibility:"hidden",maxWidth:.01,transition:i.transitions.create("max-width",{duration:50,easing:i.transitions.easing.easeOut}),whiteSpace:"nowrap","& > span":{paddingLeft:5,paddingRight:5,display:"inline-block",opacity:0,visibility:"visible"}},a.notched&&{maxWidth:"100%",transition:i.transitions.create("max-width",{duration:100,easing:i.transitions.easing.easeOut,delay:50})})));function NotchedOutline(a){const{className:i,label:o,notched:s}=a,$=_objectWithoutPropertiesLoose(a,_excluded$N),j=o!=null&&o!=="",_e=_extends({},a,{notched:s,withLabel:j});return jsxRuntimeExports.jsx(NotchedOutlineRoot$1,_extends({"aria-hidden":!0,className:i,ownerState:_e},$,{children:jsxRuntimeExports.jsx(NotchedOutlineLegend,{ownerState:_e,children:j?jsxRuntimeExports.jsx("span",{children:o}):_span$1||(_span$1=jsxRuntimeExports.jsx("span",{className:"notranslate",children:"​"}))})}))}const _excluded$M=["components","fullWidth","inputComponent","label","multiline","notched","slots","type"],useUtilityClasses$E=a=>{const{classes:i}=a,s=composeClasses({root:["root"],notchedOutline:["notchedOutline"],input:["input"]},getOutlinedInputUtilityClass,i);return _extends({},i,s)},OutlinedInputRoot=styled(InputBaseRoot,{shouldForwardProp:a=>rootShouldForwardProp$2(a)||a==="classes",name:"MuiOutlinedInput",slot:"Root",overridesResolver:rootOverridesResolver})(({theme:a,ownerState:i})=>{const o=a.palette.mode==="light"?"rgba(0, 0, 0, 0.23)":"rgba(255, 255, 255, 0.23)";return _extends({position:"relative",borderRadius:(a.vars||a).shape.borderRadius,[`&:hover .${outlinedInputClasses$1.notchedOutline}`]:{borderColor:(a.vars||a).palette.text.primary},"@media (hover: none)":{[`&:hover .${outlinedInputClasses$1.notchedOutline}`]:{borderColor:a.vars?`rgba(${a.vars.palette.common.onBackgroundChannel} / 0.23)`:o}},[`&.${outlinedInputClasses$1.focused} .${outlinedInputClasses$1.notchedOutline}`]:{borderColor:(a.vars||a).palette[i.color].main,borderWidth:2},[`&.${outlinedInputClasses$1.error} .${outlinedInputClasses$1.notchedOutline}`]:{borderColor:(a.vars||a).palette.error.main},[`&.${outlinedInputClasses$1.disabled} .${outlinedInputClasses$1.notchedOutline}`]:{borderColor:(a.vars||a).palette.action.disabled}},i.startAdornment&&{paddingLeft:14},i.endAdornment&&{paddingRight:14},i.multiline&&_extends({padding:"16.5px 14px"},i.size==="small"&&{padding:"8.5px 14px"}))}),NotchedOutlineRoot=styled(NotchedOutline,{name:"MuiOutlinedInput",slot:"NotchedOutline",overridesResolver:(a,i)=>i.notchedOutline})(({theme:a})=>{const i=a.palette.mode==="light"?"rgba(0, 0, 0, 0.23)":"rgba(255, 255, 255, 0.23)";return{borderColor:a.vars?`rgba(${a.vars.palette.common.onBackgroundChannel} / 0.23)`:i}}),OutlinedInputInput=styled(InputBaseComponent,{name:"MuiOutlinedInput",slot:"Input",overridesResolver:inputOverridesResolver})(({theme:a,ownerState:i})=>_extends({padding:"16.5px 14px"},!a.vars&&{"&:-webkit-autofill":{WebkitBoxShadow:a.palette.mode==="light"?null:"0 0 0 100px #266798 inset",WebkitTextFillColor:a.palette.mode==="light"?null:"#fff",caretColor:a.palette.mode==="light"?null:"#fff",borderRadius:"inherit"}},a.vars&&{"&:-webkit-autofill":{borderRadius:"inherit"},[a.getColorSchemeSelector("dark")]:{"&:-webkit-autofill":{WebkitBoxShadow:"0 0 0 100px #266798 inset",WebkitTextFillColor:"#fff",caretColor:"#fff"}}},i.size==="small"&&{padding:"8.5px 14px"},i.multiline&&{padding:0},i.startAdornment&&{paddingLeft:0},i.endAdornment&&{paddingRight:0})),OutlinedInput=reactExports.forwardRef(function(i,o){var s,$,j,_e,et;const tt=useThemeProps$6({props:i,name:"MuiOutlinedInput"}),{components:nt={},fullWidth:at=!1,inputComponent:it="input",label:st,multiline:lt=!1,notched:ct,slots:rt={},type:ut="text"}=tt,ot=_objectWithoutPropertiesLoose(tt,_excluded$M),dt=useUtilityClasses$E(tt),pt=useFormControl(),mt=formControlState({props:tt,muiFormControl:pt,states:["color","disabled","error","focused","hiddenLabel","size","required"]}),ft=_extends({},tt,{color:mt.color||"primary",disabled:mt.disabled,error:mt.error,focused:mt.focused,formControl:pt,fullWidth:at,hiddenLabel:mt.hiddenLabel,multiline:lt,size:mt.size,type:ut}),ht=(s=($=rt.root)!=null?$:nt.Root)!=null?s:OutlinedInputRoot,yt=(j=(_e=rt.input)!=null?_e:nt.Input)!=null?j:OutlinedInputInput;return jsxRuntimeExports.jsx(InputBase$1,_extends({slots:{root:ht,input:yt},renderSuffix:bt=>jsxRuntimeExports.jsx(NotchedOutlineRoot,{ownerState:ft,className:dt.notchedOutline,label:st!=null&&st!==""&&mt.required?et||(et=jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[st," ","*"]})):st,notched:typeof ct<"u"?ct:!!(bt.startAdornment||bt.filled||bt.focused)}),fullWidth:at,inputComponent:it,multiline:lt,ref:o,type:ut},ot,{classes:_extends({},dt,{notchedOutline:null})}))});OutlinedInput.muiName="Input";const OutlinedInput$1=OutlinedInput;function getPaginationUtilityClass(a){return generateUtilityClass$1("MuiPagination",a)}generateUtilityClasses$1("MuiPagination",["root","ul","outlined","text"]);const _excluded$L=["boundaryCount","componentName","count","defaultPage","disabled","hideNextButton","hidePrevButton","onChange","page","showFirstButton","showLastButton","siblingCount"];function usePagination(a={}){const{boundaryCount:i=1,componentName:o="usePagination",count:s=1,defaultPage:$=1,disabled:j=!1,hideNextButton:_e=!1,hidePrevButton:et=!1,onChange:tt,page:nt,showFirstButton:at=!1,showLastButton:it=!1,siblingCount:st=1}=a,lt=_objectWithoutPropertiesLoose(a,_excluded$L),[ct,rt]=useControlled({controlled:nt,default:$,name:o,state:"page"}),ut=(gt,xt)=>{nt||rt(xt),tt&&tt(gt,xt)},ot=(gt,xt)=>{const vt=xt-gt+1;return Array.from({length:vt},(Lt,$t)=>gt+$t)},dt=ot(1,Math.min(i,s)),pt=ot(Math.max(s-i+1,i+1),s),mt=Math.max(Math.min(ct-st,s-i-st*2-1),i+2),ft=Math.min(Math.max(ct+st,i+st*2+2),pt.length>0?pt[0]-2:s-1),ht=[...at?["first"]:[],...et?[]:["previous"],...dt,...mt>i+2?["start-ellipsis"]:i+1<s-i?[i+1]:[],...ot(mt,ft),...ft<s-i-1?["end-ellipsis"]:s-i>i?[s-i]:[],...pt,..._e?[]:["next"],...it?["last"]:[]],yt=gt=>{switch(gt){case"first":return 1;case"previous":return ct-1;case"next":return ct+1;case"last":return s;default:return null}},bt=ht.map(gt=>typeof gt=="number"?{onClick:xt=>{ut(xt,gt)},type:"page",page:gt,selected:gt===ct,disabled:j,"aria-current":gt===ct?"true":void 0}:{onClick:xt=>{ut(xt,yt(gt))},type:gt,page:yt(gt),selected:!1,disabled:j||gt.indexOf("ellipsis")===-1&&(gt==="next"||gt==="last"?ct>=s:ct<=1)});return _extends({items:bt},lt)}function getPaginationItemUtilityClass(a){return generateUtilityClass$1("MuiPaginationItem",a)}const paginationItemClasses=generateUtilityClasses$1("MuiPaginationItem",["root","page","sizeSmall","sizeLarge","text","textPrimary","textSecondary","outlined","outlinedPrimary","outlinedSecondary","rounded","ellipsis","firstLast","previousNext","focusVisible","disabled","selected","icon","colorPrimary","colorSecondary"]),paginationItemClasses$1=paginationItemClasses,FirstPageIconDefault=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M18.41 16.59L13.82 12l4.59-4.59L17 6l-6 6 6 6zM6 6h2v12H6z"}),"FirstPage"),LastPageIconDefault=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M5.59 7.41L10.18 12l-4.59 4.59L7 18l6-6-6-6zM16 6h2v12h-2z"}),"LastPage"),NavigateBeforeIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M15.41 7.41L14 6l-6 6 6 6 1.41-1.41L10.83 12z"}),"NavigateBefore"),NavigateNextIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M10 6L8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6z"}),"NavigateNext"),_excluded$K=["className","color","component","components","disabled","page","selected","shape","size","slots","type","variant"],overridesResolver$1=(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],i[`size${capitalize$2(o.size)}`],o.variant==="text"&&i[`text${capitalize$2(o.color)}`],o.variant==="outlined"&&i[`outlined${capitalize$2(o.color)}`],o.shape==="rounded"&&i.rounded,o.type==="page"&&i.page,(o.type==="start-ellipsis"||o.type==="end-ellipsis")&&i.ellipsis,(o.type==="previous"||o.type==="next")&&i.previousNext,(o.type==="first"||o.type==="last")&&i.firstLast]},useUtilityClasses$D=a=>{const{classes:i,color:o,disabled:s,selected:$,size:j,shape:_e,type:et,variant:tt}=a,nt={root:["root",`size${capitalize$2(j)}`,tt,_e,o!=="standard"&&`color${capitalize$2(o)}`,o!=="standard"&&`${tt}${capitalize$2(o)}`,s&&"disabled",$&&"selected",{page:"page",first:"firstLast",last:"firstLast","start-ellipsis":"ellipsis","end-ellipsis":"ellipsis",previous:"previousNext",next:"previousNext"}[et]],icon:["icon"]};return composeClasses(nt,getPaginationItemUtilityClass,i)},PaginationItemEllipsis=styled("div",{name:"MuiPaginationItem",slot:"Root",overridesResolver:overridesResolver$1})(({theme:a,ownerState:i})=>_extends({},a.typography.body2,{borderRadius:32/2,textAlign:"center",boxSizing:"border-box",minWidth:32,padding:"0 6px",margin:"0 3px",color:(a.vars||a).palette.text.primary,height:"auto",[`&.${paginationItemClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity}},i.size==="small"&&{minWidth:26,borderRadius:26/2,margin:"0 1px",padding:"0 4px"},i.size==="large"&&{minWidth:40,borderRadius:40/2,padding:"0 10px",fontSize:a.typography.pxToRem(15)})),PaginationItemPage=styled(ButtonBase$1,{name:"MuiPaginationItem",slot:"Root",overridesResolver:overridesResolver$1})(({theme:a,ownerState:i})=>_extends({},a.typography.body2,{borderRadius:32/2,textAlign:"center",boxSizing:"border-box",minWidth:32,height:32,padding:"0 6px",margin:"0 3px",color:(a.vars||a).palette.text.primary,[`&.${paginationItemClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette.action.focus},[`&.${paginationItemClasses$1.disabled}`]:{opacity:(a.vars||a).palette.action.disabledOpacity},transition:a.transitions.create(["color","background-color"],{duration:a.transitions.duration.short}),"&:hover":{backgroundColor:(a.vars||a).palette.action.hover,"@media (hover: none)":{backgroundColor:"transparent"}},[`&.${paginationItemClasses$1.selected}`]:{backgroundColor:(a.vars||a).palette.action.selected,"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.selectedChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.action.selected,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:(a.vars||a).palette.action.selected}},[`&.${paginationItemClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.action.selectedChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette.action.selected,a.palette.action.selectedOpacity+a.palette.action.focusOpacity)},[`&.${paginationItemClasses$1.disabled}`]:{opacity:1,color:(a.vars||a).palette.action.disabled,backgroundColor:(a.vars||a).palette.action.selected}}},i.size==="small"&&{minWidth:26,height:26,borderRadius:26/2,margin:"0 1px",padding:"0 4px"},i.size==="large"&&{minWidth:40,height:40,borderRadius:40/2,padding:"0 10px",fontSize:a.typography.pxToRem(15)},i.shape==="rounded"&&{borderRadius:(a.vars||a).shape.borderRadius}),({theme:a,ownerState:i})=>_extends({},i.variant==="text"&&{[`&.${paginationItemClasses$1.selected}`]:_extends({},i.color!=="standard"&&{color:(a.vars||a).palette[i.color].contrastText,backgroundColor:(a.vars||a).palette[i.color].main,"&:hover":{backgroundColor:(a.vars||a).palette[i.color].dark,"@media (hover: none)":{backgroundColor:(a.vars||a).palette[i.color].main}},[`&.${paginationItemClasses$1.focusVisible}`]:{backgroundColor:(a.vars||a).palette[i.color].dark}},{[`&.${paginationItemClasses$1.disabled}`]:{color:(a.vars||a).palette.action.disabled}})},i.variant==="outlined"&&{border:a.vars?`1px solid rgba(${a.vars.palette.common.onBackgroundChannel} / 0.23)`:`1px solid ${a.palette.mode==="light"?"rgba(0, 0, 0, 0.23)":"rgba(255, 255, 255, 0.23)"}`,[`&.${paginationItemClasses$1.selected}`]:_extends({},i.color!=="standard"&&{color:(a.vars||a).palette[i.color].main,border:`1px solid ${a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / 0.5)`:alpha_1(a.palette[i.color].main,.5)}`,backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / ${a.vars.palette.action.activatedOpacity})`:alpha_1(a.palette[i.color].main,a.palette.action.activatedOpacity),"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / calc(${a.vars.palette.action.activatedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette[i.color].main,a.palette.action.activatedOpacity+a.palette.action.focusOpacity),"@media (hover: none)":{backgroundColor:"transparent"}},[`&.${paginationItemClasses$1.focusVisible}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette[i.color].mainChannel} / calc(${a.vars.palette.action.activatedOpacity} + ${a.vars.palette.action.focusOpacity}))`:alpha_1(a.palette[i.color].main,a.palette.action.activatedOpacity+a.palette.action.focusOpacity)}},{[`&.${paginationItemClasses$1.disabled}`]:{borderColor:(a.vars||a).palette.action.disabledBackground,color:(a.vars||a).palette.action.disabled}})})),PaginationItemPageIcon=styled("div",{name:"MuiPaginationItem",slot:"Icon",overridesResolver:(a,i)=>i.icon})(({theme:a,ownerState:i})=>_extends({fontSize:a.typography.pxToRem(20),margin:"0 -8px"},i.size==="small"&&{fontSize:a.typography.pxToRem(18)},i.size==="large"&&{fontSize:a.typography.pxToRem(22)})),PaginationItem=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiPaginationItem"}),{className:$,color:j="standard",component:_e,components:et={},disabled:tt=!1,page:nt,selected:at=!1,shape:it="circular",size:st="medium",slots:lt={},type:ct="page",variant:rt="text"}=s,ut=_objectWithoutPropertiesLoose(s,_excluded$K),ot=_extends({},s,{color:j,disabled:tt,selected:at,shape:it,size:st,type:ct,variant:rt}),dt=useRtl(),pt=useUtilityClasses$D(ot),ft=(dt?{previous:lt.next||et.next||NavigateNextIcon,next:lt.previous||et.previous||NavigateBeforeIcon,last:lt.first||et.first||FirstPageIconDefault,first:lt.last||et.last||LastPageIconDefault}:{previous:lt.previous||et.previous||NavigateBeforeIcon,next:lt.next||et.next||NavigateNextIcon,first:lt.first||et.first||FirstPageIconDefault,last:lt.last||et.last||LastPageIconDefault})[ct];return ct==="start-ellipsis"||ct==="end-ellipsis"?jsxRuntimeExports.jsx(PaginationItemEllipsis,{ref:o,ownerState:ot,className:clsx(pt.root,$),children:"…"}):jsxRuntimeExports.jsxs(PaginationItemPage,_extends({ref:o,ownerState:ot,component:_e,disabled:tt,className:clsx(pt.root,$)},ut,{children:[ct==="page"&&nt,ft?jsxRuntimeExports.jsx(PaginationItemPageIcon,{as:ft,ownerState:ot,className:pt.icon}):null]}))}),PaginationItem$1=PaginationItem,_excluded$J=["boundaryCount","className","color","count","defaultPage","disabled","getItemAriaLabel","hideNextButton","hidePrevButton","onChange","page","renderItem","shape","showFirstButton","showLastButton","siblingCount","size","variant"],useUtilityClasses$C=a=>{const{classes:i,variant:o}=a;return composeClasses({root:["root",o],ul:["ul"]},getPaginationUtilityClass,i)},PaginationRoot=styled("nav",{name:"MuiPagination",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant]]}})({}),PaginationUl=styled("ul",{name:"MuiPagination",slot:"Ul",overridesResolver:(a,i)=>i.ul})({display:"flex",flexWrap:"wrap",alignItems:"center",padding:0,margin:0,listStyle:"none"});function defaultGetAriaLabel(a,i,o){return a==="page"?`${o?"":"Go to "}page ${i}`:`Go to ${a} page`}const Pagination$1=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiPagination"}),{boundaryCount:$=1,className:j,color:_e="standard",count:et=1,defaultPage:tt=1,disabled:nt=!1,getItemAriaLabel:at=defaultGetAriaLabel,hideNextButton:it=!1,hidePrevButton:st=!1,renderItem:lt=bt=>jsxRuntimeExports.jsx(PaginationItem$1,_extends({},bt)),shape:ct="circular",showFirstButton:rt=!1,showLastButton:ut=!1,siblingCount:ot=1,size:dt="medium",variant:pt="text"}=s,mt=_objectWithoutPropertiesLoose(s,_excluded$J),{items:ft}=usePagination(_extends({},s,{componentName:"Pagination"})),ht=_extends({},s,{boundaryCount:$,color:_e,count:et,defaultPage:tt,disabled:nt,getItemAriaLabel:at,hideNextButton:it,hidePrevButton:st,renderItem:lt,shape:ct,showFirstButton:rt,showLastButton:ut,siblingCount:ot,size:dt,variant:pt}),yt=useUtilityClasses$C(ht);return jsxRuntimeExports.jsx(PaginationRoot,_extends({"aria-label":"pagination navigation",className:clsx(yt.root,j),ownerState:ht,ref:o},mt,{children:jsxRuntimeExports.jsx(PaginationUl,{className:yt.ul,ownerState:ht,children:ft.map((bt,gt)=>jsxRuntimeExports.jsx("li",{children:lt(_extends({},bt,{color:_e,"aria-label":at(bt.type,bt.page,bt.selected),shape:ct,size:dt,variant:pt}))},gt))})}))}),Pagination$2=Pagination$1;function getSelectUtilityClasses(a){return generateUtilityClass$1("MuiSelect",a)}const selectClasses=generateUtilityClasses$1("MuiSelect",["root","select","multiple","filled","outlined","standard","disabled","focused","icon","iconOpen","iconFilled","iconOutlined","iconStandard","nativeInput","error"]);var _span;const _excluded$I=["aria-describedby","aria-label","autoFocus","autoWidth","children","className","defaultOpen","defaultValue","disabled","displayEmpty","error","IconComponent","inputRef","labelId","MenuProps","multiple","name","onBlur","onChange","onClose","onFocus","onOpen","open","readOnly","renderValue","SelectDisplayProps","tabIndex","type","value","variant"],SelectSelect=styled("div",{name:"MuiSelect",slot:"Select",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`&.${selectClasses.select}`]:i.select},{[`&.${selectClasses.select}`]:i[o.variant]},{[`&.${selectClasses.error}`]:i.error},{[`&.${selectClasses.multiple}`]:i.multiple}]}})(nativeSelectSelectStyles,{[`&.${selectClasses.select}`]:{height:"auto",minHeight:"1.4375em",textOverflow:"ellipsis",whiteSpace:"nowrap",overflow:"hidden"}}),SelectIcon=styled("svg",{name:"MuiSelect",slot:"Icon",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.icon,o.variant&&i[`icon${capitalize$2(o.variant)}`],o.open&&i.iconOpen]}})(nativeSelectIconStyles),SelectNativeInput=styled("input",{shouldForwardProp:a=>slotShouldForwardProp(a)&&a!=="classes",name:"MuiSelect",slot:"NativeInput",overridesResolver:(a,i)=>i.nativeInput})({bottom:0,left:0,position:"absolute",opacity:0,pointerEvents:"none",width:"100%",boxSizing:"border-box"});function areEqualValues(a,i){return typeof i=="object"&&i!==null?a===i:String(a)===String(i)}function isEmpty(a){return a==null||typeof a=="string"&&!a.trim()}const useUtilityClasses$B=a=>{const{classes:i,variant:o,disabled:s,multiple:$,open:j,error:_e}=a,et={select:["select",o,s&&"disabled",$&&"multiple",_e&&"error"],icon:["icon",`icon${capitalize$2(o)}`,j&&"iconOpen",s&&"disabled"],nativeInput:["nativeInput"]};return composeClasses(et,getSelectUtilityClasses,i)},SelectInput=reactExports.forwardRef(function(i,o){var s;const{"aria-describedby":$,"aria-label":j,autoFocus:_e,autoWidth:et,children:tt,className:nt,defaultOpen:at,defaultValue:it,disabled:st,displayEmpty:lt,error:ct=!1,IconComponent:rt,inputRef:ut,labelId:ot,MenuProps:dt={},multiple:pt,name:mt,onBlur:ft,onChange:ht,onClose:yt,onFocus:bt,onOpen:gt,open:xt,readOnly:vt,renderValue:Lt,SelectDisplayProps:$t={},tabIndex:Tt,value:Et,variant:Dt="standard"}=i,It=_objectWithoutPropertiesLoose(i,_excluded$I),[Ct,jt]=useControlled({controlled:Et,default:it,name:"Select"}),[Zt,Xt]=useControlled({controlled:xt,default:at,name:"Select"}),sn=reactExports.useRef(null),Ft=reactExports.useRef(null),[wt,kt]=reactExports.useState(null),{current:At}=reactExports.useRef(xt!=null),[Pt,Mt]=reactExports.useState(),Ot=useForkRef(o,ut),Bt=reactExports.useCallback(_n=>{Ft.current=_n,_n&&kt(_n)},[]),zt=wt==null?void 0:wt.parentNode;reactExports.useImperativeHandle(Ot,()=>({focus:()=>{Ft.current.focus()},node:sn.current,value:Ct}),[Ct]),reactExports.useEffect(()=>{at&&Zt&&wt&&!At&&(Mt(et?null:zt.clientWidth),Ft.current.focus())},[wt,et]),reactExports.useEffect(()=>{_e&&Ft.current.focus()},[_e]),reactExports.useEffect(()=>{if(!ot)return;const _n=ownerDocument(Ft.current).getElementById(ot);if(_n){const Zn=()=>{getSelection().isCollapsed&&Ft.current.focus()};return _n.addEventListener("click",Zn),()=>{_n.removeEventListener("click",Zn)}}},[ot]);const Gt=(_n,Zn)=>{_n?gt&&gt(Zn):yt&&yt(Zn),At||(Mt(et?null:zt.clientWidth),Xt(_n))},Wt=_n=>{_n.button===0&&(_n.preventDefault(),Ft.current.focus(),Gt(!0,_n))},qt=_n=>{Gt(!1,_n)},tn=reactExports.Children.toArray(tt),ln=_n=>{const Zn=tn.find(bn=>bn.props.value===_n.target.value);Zn!==void 0&&(jt(Zn.props.value),ht&&ht(_n,Zn))},gn=_n=>Zn=>{let bn;if(Zn.currentTarget.hasAttribute("tabindex")){if(pt){bn=Array.isArray(Ct)?Ct.slice():[];const dn=Ct.indexOf(_n.props.value);dn===-1?bn.push(_n.props.value):bn.splice(dn,1)}else bn=_n.props.value;if(_n.props.onClick&&_n.props.onClick(Zn),Ct!==bn&&(jt(bn),ht)){const dn=Zn.nativeEvent||Zn,an=new dn.constructor(dn.type,dn);Object.defineProperty(an,"target",{writable:!0,value:{value:bn,name:mt}}),ht(an,_n)}pt||Gt(!1,Zn)}},yn=_n=>{vt||[" ","ArrowUp","ArrowDown","Enter"].indexOf(_n.key)!==-1&&(_n.preventDefault(),Gt(!0,_n))},Pn=wt!==null&&Zt,cn=_n=>{!Pn&&ft&&(Object.defineProperty(_n,"target",{writable:!0,value:{value:Ct,name:mt}}),ft(_n))};delete It["aria-invalid"];let xn,hn;const en=[];let Jt=!1;(isFilled({value:Ct})||lt)&&(Lt?xn=Lt(Ct):Jt=!0);const vn=tn.map(_n=>{if(!reactExports.isValidElement(_n))return null;let Zn;if(pt){if(!Array.isArray(Ct))throw new Error(formatMuiErrorMessage$1(2));Zn=Ct.some(bn=>areEqualValues(bn,_n.props.value)),Zn&&Jt&&en.push(_n.props.children)}else Zn=areEqualValues(Ct,_n.props.value),Zn&&Jt&&(hn=_n.props.children);return reactExports.cloneElement(_n,{"aria-selected":Zn?"true":"false",onClick:gn(_n),onKeyUp:bn=>{bn.key===" "&&bn.preventDefault(),_n.props.onKeyUp&&_n.props.onKeyUp(bn)},role:"option",selected:Zn,value:void 0,"data-value":_n.props.value})});Jt&&(pt?en.length===0?xn=null:xn=en.reduce((_n,Zn,bn)=>(_n.push(Zn),bn<en.length-1&&_n.push(", "),_n),[]):xn=hn);let $n=Pt;!et&&At&&wt&&($n=zt.clientWidth);let Mn;typeof Tt<"u"?Mn=Tt:Mn=st?null:0;const On=$t.id||(mt?`mui-component-select-${mt}`:void 0),En=_extends({},i,{variant:Dt,value:Ct,open:Pn,error:ct}),Bn=useUtilityClasses$B(En),Hn=_extends({},dt.PaperProps,(s=dt.slotProps)==null?void 0:s.paper),Wn=useId();return jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx(SelectSelect,_extends({ref:Bt,tabIndex:Mn,role:"combobox","aria-controls":Wn,"aria-disabled":st?"true":void 0,"aria-expanded":Pn?"true":"false","aria-haspopup":"listbox","aria-label":j,"aria-labelledby":[ot,On].filter(Boolean).join(" ")||void 0,"aria-describedby":$,onKeyDown:yn,onMouseDown:st||vt?null:Wt,onBlur:cn,onFocus:bt},$t,{ownerState:En,className:clsx($t.className,Bn.select,nt),id:On,children:isEmpty(xn)?_span||(_span=jsxRuntimeExports.jsx("span",{className:"notranslate",children:"​"})):xn})),jsxRuntimeExports.jsx(SelectNativeInput,_extends({"aria-invalid":ct,value:Array.isArray(Ct)?Ct.join(","):Ct,name:mt,ref:sn,"aria-hidden":!0,onChange:ln,tabIndex:-1,disabled:st,className:Bn.nativeInput,autoFocus:_e,ownerState:En},It)),jsxRuntimeExports.jsx(SelectIcon,{as:rt,className:Bn.icon,ownerState:En}),jsxRuntimeExports.jsx(Menu$2,_extends({id:`menu-${mt||""}`,anchorEl:zt,open:Pn,onClose:qt,anchorOrigin:{vertical:"bottom",horizontal:"center"},transformOrigin:{vertical:"top",horizontal:"center"}},dt,{MenuListProps:_extends({"aria-labelledby":ot,role:"listbox","aria-multiselectable":pt?"true":void 0,disableListWrap:!0,id:Wn},dt.MenuListProps),slotProps:_extends({},dt.slotProps,{paper:_extends({},Hn,{style:_extends({minWidth:$n},Hn!=null?Hn.style:null)})}),children:vn}))]})}),SelectInput$1=SelectInput,_excluded$H=["autoWidth","children","classes","className","defaultOpen","displayEmpty","IconComponent","id","input","inputProps","label","labelId","MenuProps","multiple","native","onClose","onOpen","open","renderValue","SelectDisplayProps","variant"],_excluded2$8=["root"],useUtilityClasses$A=a=>{const{classes:i}=a;return i},styledRootConfig={name:"MuiSelect",overridesResolver:(a,i)=>i.root,shouldForwardProp:a=>rootShouldForwardProp$2(a)&&a!=="variant",slot:"Root"},StyledInput=styled(Input$1,styledRootConfig)(""),StyledOutlinedInput=styled(OutlinedInput$1,styledRootConfig)(""),StyledFilledInput=styled(FilledInput$1,styledRootConfig)(""),Select=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({name:"MuiSelect",props:i}),{autoWidth:$=!1,children:j,classes:_e={},className:et,defaultOpen:tt=!1,displayEmpty:nt=!1,IconComponent:at=ArrowDropDownIcon$1,id:it,input:st,inputProps:lt,label:ct,labelId:rt,MenuProps:ut,multiple:ot=!1,native:dt=!1,onClose:pt,onOpen:mt,open:ft,renderValue:ht,SelectDisplayProps:yt,variant:bt="outlined"}=s,gt=_objectWithoutPropertiesLoose(s,_excluded$H),xt=dt?NativeSelectInput$1:SelectInput$1,vt=useFormControl(),Lt=formControlState({props:s,muiFormControl:vt,states:["variant","error"]}),$t=Lt.variant||bt,Tt=_extends({},s,{variant:$t,classes:_e}),Et=useUtilityClasses$A(Tt),Dt=_objectWithoutPropertiesLoose(Et,_excluded2$8),It=st||{standard:jsxRuntimeExports.jsx(StyledInput,{ownerState:Tt}),outlined:jsxRuntimeExports.jsx(StyledOutlinedInput,{label:ct,ownerState:Tt}),filled:jsxRuntimeExports.jsx(StyledFilledInput,{ownerState:Tt})}[$t],Ct=useForkRef(o,It.ref);return jsxRuntimeExports.jsx(reactExports.Fragment,{children:reactExports.cloneElement(It,_extends({inputComponent:xt,inputProps:_extends({children:j,error:Lt.error,IconComponent:at,variant:$t,type:void 0,multiple:ot},dt?{id:it}:{autoWidth:$,defaultOpen:tt,displayEmpty:nt,labelId:rt,MenuProps:ut,onClose:pt,onOpen:mt,open:ft,renderValue:ht,SelectDisplayProps:_extends({id:it},yt)},lt,{classes:lt?deepmerge$1(Dt,lt.classes):Dt},st?st.props.inputProps:{})},(ot&&dt||nt)&&$t==="outlined"?{notched:!0}:{},{ref:Ct,className:clsx(It.props.className,et,Et.root)},!st&&{variant:$t},gt))})});Select.muiName="Select";const Select$1=Select;function getSkeletonUtilityClass(a){return generateUtilityClass$1("MuiSkeleton",a)}generateUtilityClasses$1("MuiSkeleton",["root","text","rectangular","rounded","circular","pulse","wave","withChildren","fitContent","heightAuto"]);const _excluded$G=["animation","className","component","height","style","variant","width"];let _=a=>a,_t,_t2,_t3,_t4;const useUtilityClasses$z=a=>{const{classes:i,variant:o,animation:s,hasChildren:$,width:j,height:_e}=a;return composeClasses({root:["root",o,s,$&&"withChildren",$&&!j&&"fitContent",$&&!_e&&"heightAuto"]},getSkeletonUtilityClass,i)},pulseKeyframe=keyframes(_t||(_t=_`
  0% {
    opacity: 1;
  }

  50% {
    opacity: 0.4;
  }

  100% {
    opacity: 1;
  }
`)),waveKeyframe=keyframes(_t2||(_t2=_`
  0% {
    transform: translateX(-100%);
  }

  50% {
    /* +0.5s of delay between each loop */
    transform: translateX(100%);
  }

  100% {
    transform: translateX(100%);
  }
`)),SkeletonRoot=styled("span",{name:"MuiSkeleton",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],o.animation!==!1&&i[o.animation],o.hasChildren&&i.withChildren,o.hasChildren&&!o.width&&i.fitContent,o.hasChildren&&!o.height&&i.heightAuto]}})(({theme:a,ownerState:i})=>{const o=getUnit(a.shape.borderRadius)||"px",s=toUnitless(a.shape.borderRadius);return _extends({display:"block",backgroundColor:a.vars?a.vars.palette.Skeleton.bg:alpha$1(a.palette.text.primary,a.palette.mode==="light"?.11:.13),height:"1.2em"},i.variant==="text"&&{marginTop:0,marginBottom:0,height:"auto",transformOrigin:"0 55%",transform:"scale(1, 0.60)",borderRadius:`${s}${o}/${Math.round(s/.6*10)/10}${o}`,"&:empty:before":{content:'"\\00a0"'}},i.variant==="circular"&&{borderRadius:"50%"},i.variant==="rounded"&&{borderRadius:(a.vars||a).shape.borderRadius},i.hasChildren&&{"& > *":{visibility:"hidden"}},i.hasChildren&&!i.width&&{maxWidth:"fit-content"},i.hasChildren&&!i.height&&{height:"auto"})},({ownerState:a})=>a.animation==="pulse"&&css(_t3||(_t3=_`
      animation: ${0} 2s ease-in-out 0.5s infinite;
    `),pulseKeyframe),({ownerState:a,theme:i})=>a.animation==="wave"&&css(_t4||(_t4=_`
      position: relative;
      overflow: hidden;

      /* Fix bug in Safari https://bugs.webkit.org/show_bug.cgi?id=68196 */
      -webkit-mask-image: -webkit-radial-gradient(white, black);

      &::after {
        animation: ${0} 2s linear 0.5s infinite;
        background: linear-gradient(
          90deg,
          transparent,
          ${0},
          transparent
        );
        content: '';
        position: absolute;
        transform: translateX(-100%); /* Avoid flash during server-side hydration */
        bottom: 0;
        left: 0;
        right: 0;
        top: 0;
      }
    `),waveKeyframe,(i.vars||i).palette.action.hover)),Skeleton=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiSkeleton"}),{animation:$="pulse",className:j,component:_e="span",height:et,style:tt,variant:nt="text",width:at}=s,it=_objectWithoutPropertiesLoose(s,_excluded$G),st=_extends({},s,{animation:$,component:_e,variant:nt,hasChildren:!!it.children}),lt=useUtilityClasses$z(st);return jsxRuntimeExports.jsx(SkeletonRoot,_extends({as:_e,ref:o,className:clsx(lt.root,j),ownerState:st},it,{style:_extends({width:at,height:et},tt)}))}),Skeleton$1=Skeleton;function getSnackbarContentUtilityClass(a){return generateUtilityClass$1("MuiSnackbarContent",a)}generateUtilityClasses$1("MuiSnackbarContent",["root","message","action"]);const _excluded$F=["action","className","message","role"],useUtilityClasses$y=a=>{const{classes:i}=a;return composeClasses({root:["root"],action:["action"],message:["message"]},getSnackbarContentUtilityClass,i)},SnackbarContentRoot=styled(Paper$1,{name:"MuiSnackbarContent",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a})=>{const i=a.palette.mode==="light"?.8:.98,o=emphasize_1(a.palette.background.default,i);return _extends({},a.typography.body2,{color:a.vars?a.vars.palette.SnackbarContent.color:a.palette.getContrastText(o),backgroundColor:a.vars?a.vars.palette.SnackbarContent.bg:o,display:"flex",alignItems:"center",flexWrap:"wrap",padding:"6px 16px",borderRadius:(a.vars||a).shape.borderRadius,flexGrow:1,[a.breakpoints.up("sm")]:{flexGrow:"initial",minWidth:288}})}),SnackbarContentMessage=styled("div",{name:"MuiSnackbarContent",slot:"Message",overridesResolver:(a,i)=>i.message})({padding:"8px 0"}),SnackbarContentAction=styled("div",{name:"MuiSnackbarContent",slot:"Action",overridesResolver:(a,i)=>i.action})({display:"flex",alignItems:"center",marginLeft:"auto",paddingLeft:16,marginRight:-8}),SnackbarContent=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiSnackbarContent"}),{action:$,className:j,message:_e,role:et="alert"}=s,tt=_objectWithoutPropertiesLoose(s,_excluded$F),nt=s,at=useUtilityClasses$y(nt);return jsxRuntimeExports.jsxs(SnackbarContentRoot,_extends({role:et,square:!0,elevation:6,className:clsx(at.root,j),ownerState:nt,ref:o},tt,{children:[jsxRuntimeExports.jsx(SnackbarContentMessage,{className:at.message,ownerState:nt,children:_e}),$?jsxRuntimeExports.jsx(SnackbarContentAction,{className:at.action,ownerState:nt,children:$}):null]}))}),SnackbarContent$1=SnackbarContent;function getSnackbarUtilityClass(a){return generateUtilityClass$1("MuiSnackbar",a)}generateUtilityClasses$1("MuiSnackbar",["root","anchorOriginTopCenter","anchorOriginBottomCenter","anchorOriginTopRight","anchorOriginBottomRight","anchorOriginTopLeft","anchorOriginBottomLeft"]);const _excluded$E=["onEnter","onExited"],_excluded2$7=["action","anchorOrigin","autoHideDuration","children","className","ClickAwayListenerProps","ContentProps","disableWindowBlurListener","message","onBlur","onClose","onFocus","onMouseEnter","onMouseLeave","open","resumeHideDuration","TransitionComponent","transitionDuration","TransitionProps"],useUtilityClasses$x=a=>{const{classes:i,anchorOrigin:o}=a,s={root:["root",`anchorOrigin${capitalize$2(o.vertical)}${capitalize$2(o.horizontal)}`]};return composeClasses(s,getSnackbarUtilityClass,i)},SnackbarRoot=styled("div",{name:"MuiSnackbar",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[`anchorOrigin${capitalize$2(o.anchorOrigin.vertical)}${capitalize$2(o.anchorOrigin.horizontal)}`]]}})(({theme:a,ownerState:i})=>{const o={left:"50%",right:"auto",transform:"translateX(-50%)"};return _extends({zIndex:(a.vars||a).zIndex.snackbar,position:"fixed",display:"flex",left:8,right:8,justifyContent:"center",alignItems:"center"},i.anchorOrigin.vertical==="top"?{top:8}:{bottom:8},i.anchorOrigin.horizontal==="left"&&{justifyContent:"flex-start"},i.anchorOrigin.horizontal==="right"&&{justifyContent:"flex-end"},{[a.breakpoints.up("sm")]:_extends({},i.anchorOrigin.vertical==="top"?{top:24}:{bottom:24},i.anchorOrigin.horizontal==="center"&&o,i.anchorOrigin.horizontal==="left"&&{left:24,right:"auto"},i.anchorOrigin.horizontal==="right"&&{right:24,left:"auto"})})}),Snackbar=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiSnackbar"}),$=useTheme$1(),j={enter:$.transitions.duration.enteringScreen,exit:$.transitions.duration.leavingScreen},{action:_e,anchorOrigin:{vertical:et,horizontal:tt}={vertical:"bottom",horizontal:"left"},autoHideDuration:nt=null,children:at,className:it,ClickAwayListenerProps:st,ContentProps:lt,disableWindowBlurListener:ct=!1,message:rt,open:ut,TransitionComponent:ot=Grow$1,transitionDuration:dt=j,TransitionProps:{onEnter:pt,onExited:mt}={}}=s,ft=_objectWithoutPropertiesLoose(s.TransitionProps,_excluded$E),ht=_objectWithoutPropertiesLoose(s,_excluded2$7),yt=_extends({},s,{anchorOrigin:{vertical:et,horizontal:tt},autoHideDuration:nt,disableWindowBlurListener:ct,TransitionComponent:ot,transitionDuration:dt}),bt=useUtilityClasses$x(yt),{getRootProps:gt,onClickAway:xt}=useSnackbar(_extends({},yt)),[vt,Lt]=reactExports.useState(!0),$t=useSlotProps({elementType:SnackbarRoot,getSlotProps:gt,externalForwardedProps:ht,ownerState:yt,additionalProps:{ref:o},className:[bt.root,it]}),Tt=Dt=>{Lt(!0),mt&&mt(Dt)},Et=(Dt,It)=>{Lt(!1),pt&&pt(Dt,It)};return!ut&&vt?null:jsxRuntimeExports.jsx(ClickAwayListener,_extends({onClickAway:xt},st,{children:jsxRuntimeExports.jsx(SnackbarRoot,_extends({},$t,{children:jsxRuntimeExports.jsx(ot,_extends({appear:!0,in:ut,timeout:dt,direction:et==="top"?"down":"up",onEnter:Et,onExited:Tt},ft,{children:at||jsxRuntimeExports.jsx(SnackbarContent$1,_extends({message:rt,action:_e},lt))}))}))}))}),Snackbar$1=Snackbar;function getTooltipUtilityClass(a){return generateUtilityClass$1("MuiTooltip",a)}const tooltipClasses=generateUtilityClasses$1("MuiTooltip",["popper","popperInteractive","popperArrow","popperClose","tooltip","tooltipArrow","touch","tooltipPlacementLeft","tooltipPlacementRight","tooltipPlacementTop","tooltipPlacementBottom","arrow"]),tooltipClasses$1=tooltipClasses,_excluded$D=["arrow","children","classes","components","componentsProps","describeChild","disableFocusListener","disableHoverListener","disableInteractive","disableTouchListener","enterDelay","enterNextDelay","enterTouchDelay","followCursor","id","leaveDelay","leaveTouchDelay","onClose","onOpen","open","placement","PopperComponent","PopperProps","slotProps","slots","title","TransitionComponent","TransitionProps"];function round(a){return Math.round(a*1e5)/1e5}const useUtilityClasses$w=a=>{const{classes:i,disableInteractive:o,arrow:s,touch:$,placement:j}=a,_e={popper:["popper",!o&&"popperInteractive",s&&"popperArrow"],tooltip:["tooltip",s&&"tooltipArrow",$&&"touch",`tooltipPlacement${capitalize$2(j.split("-")[0])}`],arrow:["arrow"]};return composeClasses(_e,getTooltipUtilityClass,i)},TooltipPopper=styled(MuiPopper,{name:"MuiTooltip",slot:"Popper",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.popper,!o.disableInteractive&&i.popperInteractive,o.arrow&&i.popperArrow,!o.open&&i.popperClose]}})(({theme:a,ownerState:i,open:o})=>_extends({zIndex:(a.vars||a).zIndex.tooltip,pointerEvents:"none"},!i.disableInteractive&&{pointerEvents:"auto"},!o&&{pointerEvents:"none"},i.arrow&&{[`&[data-popper-placement*="bottom"] .${tooltipClasses$1.arrow}`]:{top:0,marginTop:"-0.71em","&::before":{transformOrigin:"0 100%"}},[`&[data-popper-placement*="top"] .${tooltipClasses$1.arrow}`]:{bottom:0,marginBottom:"-0.71em","&::before":{transformOrigin:"100% 0"}},[`&[data-popper-placement*="right"] .${tooltipClasses$1.arrow}`]:_extends({},i.isRtl?{right:0,marginRight:"-0.71em"}:{left:0,marginLeft:"-0.71em"},{height:"1em",width:"0.71em","&::before":{transformOrigin:"100% 100%"}}),[`&[data-popper-placement*="left"] .${tooltipClasses$1.arrow}`]:_extends({},i.isRtl?{left:0,marginLeft:"-0.71em"}:{right:0,marginRight:"-0.71em"},{height:"1em",width:"0.71em","&::before":{transformOrigin:"0 0"}})})),TooltipTooltip=styled("div",{name:"MuiTooltip",slot:"Tooltip",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.tooltip,o.touch&&i.touch,o.arrow&&i.tooltipArrow,i[`tooltipPlacement${capitalize$2(o.placement.split("-")[0])}`]]}})(({theme:a,ownerState:i})=>_extends({backgroundColor:a.vars?a.vars.palette.Tooltip.bg:alpha_1(a.palette.grey[700],.92),borderRadius:(a.vars||a).shape.borderRadius,color:(a.vars||a).palette.common.white,fontFamily:a.typography.fontFamily,padding:"4px 8px",fontSize:a.typography.pxToRem(11),maxWidth:300,margin:2,wordWrap:"break-word",fontWeight:a.typography.fontWeightMedium},i.arrow&&{position:"relative",margin:0},i.touch&&{padding:"8px 16px",fontSize:a.typography.pxToRem(14),lineHeight:`${round(16/14)}em`,fontWeight:a.typography.fontWeightRegular},{[`.${tooltipClasses$1.popper}[data-popper-placement*="left"] &`]:_extends({transformOrigin:"right center"},i.isRtl?_extends({marginLeft:"14px"},i.touch&&{marginLeft:"24px"}):_extends({marginRight:"14px"},i.touch&&{marginRight:"24px"})),[`.${tooltipClasses$1.popper}[data-popper-placement*="right"] &`]:_extends({transformOrigin:"left center"},i.isRtl?_extends({marginRight:"14px"},i.touch&&{marginRight:"24px"}):_extends({marginLeft:"14px"},i.touch&&{marginLeft:"24px"})),[`.${tooltipClasses$1.popper}[data-popper-placement*="top"] &`]:_extends({transformOrigin:"center bottom",marginBottom:"14px"},i.touch&&{marginBottom:"24px"}),[`.${tooltipClasses$1.popper}[data-popper-placement*="bottom"] &`]:_extends({transformOrigin:"center top",marginTop:"14px"},i.touch&&{marginTop:"24px"})})),TooltipArrow=styled("span",{name:"MuiTooltip",slot:"Arrow",overridesResolver:(a,i)=>i.arrow})(({theme:a})=>({overflow:"hidden",position:"absolute",width:"1em",height:"0.71em",boxSizing:"border-box",color:a.vars?a.vars.palette.Tooltip.bg:alpha_1(a.palette.grey[700],.9),"&::before":{content:'""',margin:"auto",display:"block",width:"100%",height:"100%",backgroundColor:"currentColor",transform:"rotate(45deg)"}}));let hystersisOpen=!1;const hystersisTimer=new Timeout;let cursorPosition={x:0,y:0};function composeEventHandler(a,i){return(o,...s)=>{i&&i(o,...s),a(o,...s)}}const Tooltip=reactExports.forwardRef(function(i,o){var s,$,j,_e,et,tt,nt,at,it,st,lt,ct,rt,ut,ot,dt,pt,mt,ft;const ht=useThemeProps$6({props:i,name:"MuiTooltip"}),{arrow:yt=!1,children:bt,components:gt={},componentsProps:xt={},describeChild:vt=!1,disableFocusListener:Lt=!1,disableHoverListener:$t=!1,disableInteractive:Tt=!1,disableTouchListener:Et=!1,enterDelay:Dt=100,enterNextDelay:It=0,enterTouchDelay:Ct=700,followCursor:jt=!1,id:Zt,leaveDelay:Xt=0,leaveTouchDelay:sn=1500,onClose:Ft,onOpen:wt,open:kt,placement:At="bottom",PopperComponent:Pt,PopperProps:Mt={},slotProps:Ot={},slots:Bt={},title:zt,TransitionComponent:Gt=Grow$1,TransitionProps:Wt}=ht,qt=_objectWithoutPropertiesLoose(ht,_excluded$D),tn=reactExports.isValidElement(bt)?bt:jsxRuntimeExports.jsx("span",{children:bt}),ln=useTheme$1(),gn=useRtl(),[yn,Pn]=reactExports.useState(),[cn,xn]=reactExports.useState(null),hn=reactExports.useRef(!1),en=Tt||jt,Jt=useTimeout(),vn=useTimeout(),$n=useTimeout(),Mn=useTimeout(),[On,En]=useControlled({controlled:kt,default:!1,name:"Tooltip",state:"open"});let Bn=On;const Hn=useId(Zt),Wn=reactExports.useRef(),_n=useEventCallback(()=>{Wn.current!==void 0&&(document.body.style.WebkitUserSelect=Wn.current,Wn.current=void 0),Mn.clear()});reactExports.useEffect(()=>_n,[_n]);const Zn=jn=>{hystersisTimer.clear(),hystersisOpen=!0,En(!0),wt&&!Bn&&wt(jn)},bn=useEventCallback(jn=>{hystersisTimer.start(800+Xt,()=>{hystersisOpen=!1}),En(!1),Ft&&Bn&&Ft(jn),Jt.start(ln.transitions.duration.shortest,()=>{hn.current=!1})}),dn=jn=>{hn.current&&jn.type!=="touchstart"||(yn&&yn.removeAttribute("title"),vn.clear(),$n.clear(),Dt||hystersisOpen&&It?vn.start(hystersisOpen?It:Dt,()=>{Zn(jn)}):Zn(jn))},an=jn=>{vn.clear(),$n.start(Xt,()=>{bn(jn)})},{isFocusVisibleRef:In,onBlur:Dn,onFocus:Xn,ref:Yn}=useIsFocusVisible(),[,pn]=reactExports.useState(!1),Ht=jn=>{Dn(jn),In.current===!1&&(pn(!1),an(jn))},Kt=jn=>{yn||Pn(jn.currentTarget),Xn(jn),In.current===!0&&(pn(!0),dn(jn))},nn=jn=>{hn.current=!0;const Vn=tn.props;Vn.onTouchStart&&Vn.onTouchStart(jn)},kn=jn=>{nn(jn),$n.clear(),Jt.clear(),_n(),Wn.current=document.body.style.WebkitUserSelect,document.body.style.WebkitUserSelect="none",Mn.start(Ct,()=>{document.body.style.WebkitUserSelect=Wn.current,dn(jn)})},Rn=jn=>{tn.props.onTouchEnd&&tn.props.onTouchEnd(jn),_n(),$n.start(sn,()=>{bn(jn)})};reactExports.useEffect(()=>{if(!Bn)return;function jn(Vn){(Vn.key==="Escape"||Vn.key==="Esc")&&bn(Vn)}return document.addEventListener("keydown",jn),()=>{document.removeEventListener("keydown",jn)}},[bn,Bn]);const Un=useForkRef(tn.ref,Yn,Pn,o);!zt&&zt!==0&&(Bn=!1);const Tn=reactExports.useRef(),Nn=jn=>{const Vn=tn.props;Vn.onMouseMove&&Vn.onMouseMove(jn),cursorPosition={x:jn.clientX,y:jn.clientY},Tn.current&&Tn.current.update()},Cn={},Ut=typeof zt=="string";vt?(Cn.title=!Bn&&Ut&&!$t?zt:null,Cn["aria-describedby"]=Bn?Hn:null):(Cn["aria-label"]=Ut?zt:null,Cn["aria-labelledby"]=Bn&&!Ut?Hn:null);const Rt=_extends({},Cn,qt,tn.props,{className:clsx(qt.className,tn.props.className),onTouchStart:nn,ref:Un},jt?{onMouseMove:Nn}:{}),Nt={};Et||(Rt.onTouchStart=kn,Rt.onTouchEnd=Rn),$t||(Rt.onMouseOver=composeEventHandler(dn,Rt.onMouseOver),Rt.onMouseLeave=composeEventHandler(an,Rt.onMouseLeave),en||(Nt.onMouseOver=dn,Nt.onMouseLeave=an)),Lt||(Rt.onFocus=composeEventHandler(Kt,Rt.onFocus),Rt.onBlur=composeEventHandler(Ht,Rt.onBlur),en||(Nt.onFocus=Kt,Nt.onBlur=Ht));const Vt=reactExports.useMemo(()=>{var jn;let Vn=[{name:"arrow",enabled:!!cn,options:{element:cn,padding:4}}];return(jn=Mt.popperOptions)!=null&&jn.modifiers&&(Vn=Vn.concat(Mt.popperOptions.modifiers)),_extends({},Mt.popperOptions,{modifiers:Vn})},[cn,Mt]),Qt=_extends({},ht,{isRtl:gn,arrow:yt,disableInteractive:en,placement:At,PopperComponentProp:Pt,touch:hn.current}),rn=useUtilityClasses$w(Qt),fn=(s=($=Bt.popper)!=null?$:gt.Popper)!=null?s:TooltipPopper,Ln=(j=(_e=(et=Bt.transition)!=null?et:gt.Transition)!=null?_e:Gt)!=null?j:Grow$1,zn=(tt=(nt=Bt.tooltip)!=null?nt:gt.Tooltip)!=null?tt:TooltipTooltip,on=(at=(it=Bt.arrow)!=null?it:gt.Arrow)!=null?at:TooltipArrow,mn=appendOwnerState(fn,_extends({},Mt,(st=Ot.popper)!=null?st:xt.popper,{className:clsx(rn.popper,Mt==null?void 0:Mt.className,(lt=(ct=Ot.popper)!=null?ct:xt.popper)==null?void 0:lt.className)}),Qt),Sn=appendOwnerState(Ln,_extends({},Wt,(rt=Ot.transition)!=null?rt:xt.transition),Qt),An=appendOwnerState(zn,_extends({},(ut=Ot.tooltip)!=null?ut:xt.tooltip,{className:clsx(rn.tooltip,(ot=(dt=Ot.tooltip)!=null?dt:xt.tooltip)==null?void 0:ot.className)}),Qt),Fn=appendOwnerState(on,_extends({},(pt=Ot.arrow)!=null?pt:xt.arrow,{className:clsx(rn.arrow,(mt=(ft=Ot.arrow)!=null?ft:xt.arrow)==null?void 0:mt.className)}),Qt);return jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[reactExports.cloneElement(tn,Rt),jsxRuntimeExports.jsx(fn,_extends({as:Pt??MuiPopper,placement:At,anchorEl:jt?{getBoundingClientRect:()=>({top:cursorPosition.y,left:cursorPosition.x,right:cursorPosition.x,bottom:cursorPosition.y,width:0,height:0})}:yn,popperRef:Tn,open:yn?Bn:!1,id:Hn,transition:!0},Nt,mn,{popperOptions:Vt,children:({TransitionProps:jn})=>jsxRuntimeExports.jsx(Ln,_extends({timeout:ln.transitions.duration.shorter},jn,Sn,{children:jsxRuntimeExports.jsxs(zn,_extends({},An,{children:[zt,yt?jsxRuntimeExports.jsx(on,_extends({},Fn,{ref:xn})):null]}))}))}))]})}),Tooltip$1=Tooltip,StepperContext=reactExports.createContext({}),StepperContext$1=StepperContext,StepContext=reactExports.createContext({}),StepContext$1=StepContext;function getStepUtilityClass(a){return generateUtilityClass$1("MuiStep",a)}generateUtilityClasses$1("MuiStep",["root","horizontal","vertical","alternativeLabel","completed"]);const _excluded$C=["active","children","className","component","completed","disabled","expanded","index","last"],useUtilityClasses$v=a=>{const{classes:i,orientation:o,alternativeLabel:s,completed:$}=a;return composeClasses({root:["root",o,s&&"alternativeLabel",$&&"completed"]},getStepUtilityClass,i)},StepRoot=styled("div",{name:"MuiStep",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.orientation],o.alternativeLabel&&i.alternativeLabel,o.completed&&i.completed]}})(({ownerState:a})=>_extends({},a.orientation==="horizontal"&&{paddingLeft:8,paddingRight:8},a.alternativeLabel&&{flex:1,position:"relative"})),Step=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiStep"}),{active:$,children:j,className:_e,component:et="div",completed:tt,disabled:nt,expanded:at=!1,index:it,last:st}=s,lt=_objectWithoutPropertiesLoose(s,_excluded$C),{activeStep:ct,connector:rt,alternativeLabel:ut,orientation:ot,nonLinear:dt}=reactExports.useContext(StepperContext$1);let[pt=!1,mt=!1,ft=!1]=[$,tt,nt];ct===it?pt=$!==void 0?$:!0:!dt&&ct>it?mt=tt!==void 0?tt:!0:!dt&&ct<it&&(ft=nt!==void 0?nt:!0);const ht=reactExports.useMemo(()=>({index:it,last:st,expanded:at,icon:it+1,active:pt,completed:mt,disabled:ft}),[it,st,at,pt,mt,ft]),yt=_extends({},s,{active:pt,orientation:ot,alternativeLabel:ut,completed:mt,disabled:ft,expanded:at,component:et}),bt=useUtilityClasses$v(yt),gt=jsxRuntimeExports.jsxs(StepRoot,_extends({as:et,className:clsx(bt.root,_e),ref:o,ownerState:yt},lt,{children:[rt&&ut&&it!==0?rt:null,j]}));return jsxRuntimeExports.jsx(StepContext$1.Provider,{value:ht,children:rt&&!ut&&it!==0?jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[rt,gt]}):gt})}),Step$1=Step,CheckCircle=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M12 0a12 12 0 1 0 0 24 12 12 0 0 0 0-24zm-2 17l-5-5 1.4-1.4 3.6 3.6 7.6-7.6L19 8l-9 9z"}),"CheckCircle"),Warning=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M1 21h22L12 2 1 21zm12-3h-2v-2h2v2zm0-4h-2v-4h2v4z"}),"Warning");function getStepIconUtilityClass(a){return generateUtilityClass$1("MuiStepIcon",a)}const stepIconClasses=generateUtilityClasses$1("MuiStepIcon",["root","active","completed","error","text"]),stepIconClasses$1=stepIconClasses;var _circle;const _excluded$B=["active","className","completed","error","icon"],useUtilityClasses$u=a=>{const{classes:i,active:o,completed:s,error:$}=a;return composeClasses({root:["root",o&&"active",s&&"completed",$&&"error"],text:["text"]},getStepIconUtilityClass,i)},StepIconRoot=styled(SvgIcon,{name:"MuiStepIcon",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a})=>({display:"block",transition:a.transitions.create("color",{duration:a.transitions.duration.shortest}),color:(a.vars||a).palette.text.disabled,[`&.${stepIconClasses$1.completed}`]:{color:(a.vars||a).palette.primary.main},[`&.${stepIconClasses$1.active}`]:{color:(a.vars||a).palette.primary.main},[`&.${stepIconClasses$1.error}`]:{color:(a.vars||a).palette.error.main}})),StepIconText=styled("text",{name:"MuiStepIcon",slot:"Text",overridesResolver:(a,i)=>i.text})(({theme:a})=>({fill:(a.vars||a).palette.primary.contrastText,fontSize:a.typography.caption.fontSize,fontFamily:a.typography.fontFamily})),StepIcon=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiStepIcon"}),{active:$=!1,className:j,completed:_e=!1,error:et=!1,icon:tt}=s,nt=_objectWithoutPropertiesLoose(s,_excluded$B),at=_extends({},s,{active:$,completed:_e,error:et}),it=useUtilityClasses$u(at);if(typeof tt=="number"||typeof tt=="string"){const st=clsx(j,it.root);return et?jsxRuntimeExports.jsx(StepIconRoot,_extends({as:Warning,className:st,ref:o,ownerState:at},nt)):_e?jsxRuntimeExports.jsx(StepIconRoot,_extends({as:CheckCircle,className:st,ref:o,ownerState:at},nt)):jsxRuntimeExports.jsxs(StepIconRoot,_extends({className:st,ref:o,ownerState:at},nt,{children:[_circle||(_circle=jsxRuntimeExports.jsx("circle",{cx:"12",cy:"12",r:"12"})),jsxRuntimeExports.jsx(StepIconText,{className:it.text,x:"12",y:"12",textAnchor:"middle",dominantBaseline:"central",ownerState:at,children:tt})]}))}return tt}),StepIcon$1=StepIcon;function getStepLabelUtilityClass(a){return generateUtilityClass$1("MuiStepLabel",a)}const stepLabelClasses=generateUtilityClasses$1("MuiStepLabel",["root","horizontal","vertical","label","active","completed","error","disabled","iconContainer","alternativeLabel","labelContainer"]),stepLabelClasses$1=stepLabelClasses,_excluded$A=["children","className","componentsProps","error","icon","optional","slotProps","StepIconComponent","StepIconProps"],useUtilityClasses$t=a=>{const{classes:i,orientation:o,active:s,completed:$,error:j,disabled:_e,alternativeLabel:et}=a;return composeClasses({root:["root",o,j&&"error",_e&&"disabled",et&&"alternativeLabel"],label:["label",s&&"active",$&&"completed",j&&"error",_e&&"disabled",et&&"alternativeLabel"],iconContainer:["iconContainer",s&&"active",$&&"completed",j&&"error",_e&&"disabled",et&&"alternativeLabel"],labelContainer:["labelContainer",et&&"alternativeLabel"]},getStepLabelUtilityClass,i)},StepLabelRoot=styled("span",{name:"MuiStepLabel",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.orientation]]}})(({ownerState:a})=>_extends({display:"flex",alignItems:"center",[`&.${stepLabelClasses$1.alternativeLabel}`]:{flexDirection:"column"},[`&.${stepLabelClasses$1.disabled}`]:{cursor:"default"}},a.orientation==="vertical"&&{textAlign:"left",padding:"8px 0"})),StepLabelLabel=styled("span",{name:"MuiStepLabel",slot:"Label",overridesResolver:(a,i)=>i.label})(({theme:a})=>_extends({},a.typography.body2,{display:"block",transition:a.transitions.create("color",{duration:a.transitions.duration.shortest}),[`&.${stepLabelClasses$1.active}`]:{color:(a.vars||a).palette.text.primary,fontWeight:500},[`&.${stepLabelClasses$1.completed}`]:{color:(a.vars||a).palette.text.primary,fontWeight:500},[`&.${stepLabelClasses$1.alternativeLabel}`]:{marginTop:16},[`&.${stepLabelClasses$1.error}`]:{color:(a.vars||a).palette.error.main}})),StepLabelIconContainer=styled("span",{name:"MuiStepLabel",slot:"IconContainer",overridesResolver:(a,i)=>i.iconContainer})(()=>({flexShrink:0,display:"flex",paddingRight:8,[`&.${stepLabelClasses$1.alternativeLabel}`]:{paddingRight:0}})),StepLabelLabelContainer=styled("span",{name:"MuiStepLabel",slot:"LabelContainer",overridesResolver:(a,i)=>i.labelContainer})(({theme:a})=>({width:"100%",color:(a.vars||a).palette.text.secondary,[`&.${stepLabelClasses$1.alternativeLabel}`]:{textAlign:"center"}})),StepLabel=reactExports.forwardRef(function(i,o){var s;const $=useThemeProps$6({props:i,name:"MuiStepLabel"}),{children:j,className:_e,componentsProps:et={},error:tt=!1,icon:nt,optional:at,slotProps:it={},StepIconComponent:st,StepIconProps:lt}=$,ct=_objectWithoutPropertiesLoose($,_excluded$A),{alternativeLabel:rt,orientation:ut}=reactExports.useContext(StepperContext$1),{active:ot,disabled:dt,completed:pt,icon:mt}=reactExports.useContext(StepContext$1),ft=nt||mt;let ht=st;ft&&!ht&&(ht=StepIcon$1);const yt=_extends({},$,{active:ot,alternativeLabel:rt,completed:pt,disabled:dt,error:tt,orientation:ut}),bt=useUtilityClasses$t(yt),gt=(s=it.label)!=null?s:et.label;return jsxRuntimeExports.jsxs(StepLabelRoot,_extends({className:clsx(bt.root,_e),ref:o,ownerState:yt},ct,{children:[ft||ht?jsxRuntimeExports.jsx(StepLabelIconContainer,{className:bt.iconContainer,ownerState:yt,children:jsxRuntimeExports.jsx(ht,_extends({completed:pt,active:ot,error:tt,icon:ft},lt))}):null,jsxRuntimeExports.jsxs(StepLabelLabelContainer,{className:bt.labelContainer,ownerState:yt,children:[j?jsxRuntimeExports.jsx(StepLabelLabel,_extends({ownerState:yt},gt,{className:clsx(bt.label,gt==null?void 0:gt.className),children:j})):null,at]})]}))});StepLabel.muiName="StepLabel";const StepLabel$1=StepLabel;function getStepConnectorUtilityClass(a){return generateUtilityClass$1("MuiStepConnector",a)}const stepConnectorClasses=generateUtilityClasses$1("MuiStepConnector",["root","horizontal","vertical","alternativeLabel","active","completed","disabled","line","lineHorizontal","lineVertical"]),stepConnectorClasses$1=stepConnectorClasses,_excluded$z=["className"],useUtilityClasses$s=a=>{const{classes:i,orientation:o,alternativeLabel:s,active:$,completed:j,disabled:_e}=a,et={root:["root",o,s&&"alternativeLabel",$&&"active",j&&"completed",_e&&"disabled"],line:["line",`line${capitalize$2(o)}`]};return composeClasses(et,getStepConnectorUtilityClass,i)},StepConnectorRoot=styled("div",{name:"MuiStepConnector",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.orientation],o.alternativeLabel&&i.alternativeLabel,o.completed&&i.completed]}})(({ownerState:a})=>_extends({flex:"1 1 auto"},a.orientation==="vertical"&&{marginLeft:12},a.alternativeLabel&&{position:"absolute",top:12,left:"calc(-50% + 20px)",right:"calc(50% + 20px)"})),StepConnectorLine=styled("span",{name:"MuiStepConnector",slot:"Line",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.line,i[`line${capitalize$2(o.orientation)}`]]}})(({ownerState:a,theme:i})=>{const o=i.palette.mode==="light"?i.palette.grey[400]:i.palette.grey[600];return _extends({display:"block",borderColor:i.vars?i.vars.palette.StepConnector.border:o},a.orientation==="horizontal"&&{borderTopStyle:"solid",borderTopWidth:1},a.orientation==="vertical"&&{borderLeftStyle:"solid",borderLeftWidth:1,minHeight:24})}),StepConnector=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiStepConnector"}),{className:$}=s,j=_objectWithoutPropertiesLoose(s,_excluded$z),{alternativeLabel:_e,orientation:et="horizontal"}=reactExports.useContext(StepperContext$1),{active:tt,disabled:nt,completed:at}=reactExports.useContext(StepContext$1),it=_extends({},s,{alternativeLabel:_e,orientation:et,active:tt,completed:at,disabled:nt}),st=useUtilityClasses$s(it);return jsxRuntimeExports.jsx(StepConnectorRoot,_extends({className:clsx(st.root,$),ref:o,ownerState:it},j,{children:jsxRuntimeExports.jsx(StepConnectorLine,{className:st.line,ownerState:it})}))}),StepConnector$1=StepConnector;function getStepperUtilityClass(a){return generateUtilityClass$1("MuiStepper",a)}generateUtilityClasses$1("MuiStepper",["root","horizontal","vertical","alternativeLabel"]);const _excluded$y=["activeStep","alternativeLabel","children","className","component","connector","nonLinear","orientation"],useUtilityClasses$r=a=>{const{orientation:i,alternativeLabel:o,classes:s}=a;return composeClasses({root:["root",i,o&&"alternativeLabel"]},getStepperUtilityClass,s)},StepperRoot=styled("div",{name:"MuiStepper",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.orientation],o.alternativeLabel&&i.alternativeLabel]}})(({ownerState:a})=>_extends({display:"flex"},a.orientation==="horizontal"&&{flexDirection:"row",alignItems:"center"},a.orientation==="vertical"&&{flexDirection:"column"},a.alternativeLabel&&{alignItems:"flex-start"})),defaultConnector=jsxRuntimeExports.jsx(StepConnector$1,{}),Stepper=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiStepper"}),{activeStep:$=0,alternativeLabel:j=!1,children:_e,className:et,component:tt="div",connector:nt=defaultConnector,nonLinear:at=!1,orientation:it="horizontal"}=s,st=_objectWithoutPropertiesLoose(s,_excluded$y),lt=_extends({},s,{alternativeLabel:j,orientation:it,component:tt}),ct=useUtilityClasses$r(lt),rt=reactExports.Children.toArray(_e).filter(Boolean),ut=rt.map((dt,pt)=>reactExports.cloneElement(dt,_extends({index:pt,last:pt+1===rt.length},dt.props))),ot=reactExports.useMemo(()=>({activeStep:$,alternativeLabel:j,connector:nt,nonLinear:at,orientation:it}),[$,j,nt,at,it]);return jsxRuntimeExports.jsx(StepperContext$1.Provider,{value:ot,children:jsxRuntimeExports.jsx(StepperRoot,_extends({as:tt,ownerState:lt,className:clsx(ct.root,et),ref:o},st,{children:ut}))})}),Stepper$1=Stepper,TableContext=reactExports.createContext(),TableContext$1=TableContext;function getTableUtilityClass(a){return generateUtilityClass$1("MuiTable",a)}generateUtilityClasses$1("MuiTable",["root","stickyHeader"]);const _excluded$x=["className","component","padding","size","stickyHeader"],useUtilityClasses$q=a=>{const{classes:i,stickyHeader:o}=a;return composeClasses({root:["root",o&&"stickyHeader"]},getTableUtilityClass,i)},TableRoot=styled("table",{name:"MuiTable",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.stickyHeader&&i.stickyHeader]}})(({theme:a,ownerState:i})=>_extends({display:"table",width:"100%",borderCollapse:"collapse",borderSpacing:0,"& caption":_extends({},a.typography.body2,{padding:a.spacing(2),color:(a.vars||a).palette.text.secondary,textAlign:"left",captionSide:"bottom"})},i.stickyHeader&&{borderCollapse:"separate"})),defaultComponent$3="table",Table=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTable"}),{className:$,component:j=defaultComponent$3,padding:_e="normal",size:et="medium",stickyHeader:tt=!1}=s,nt=_objectWithoutPropertiesLoose(s,_excluded$x),at=_extends({},s,{component:j,padding:_e,size:et,stickyHeader:tt}),it=useUtilityClasses$q(at),st=reactExports.useMemo(()=>({padding:_e,size:et,stickyHeader:tt}),[_e,et,tt]);return jsxRuntimeExports.jsx(TableContext$1.Provider,{value:st,children:jsxRuntimeExports.jsx(TableRoot,_extends({as:j,role:j===defaultComponent$3?null:"table",ref:o,className:clsx(it.root,$),ownerState:at},nt))})}),Table$1=Table,Tablelvl2Context=reactExports.createContext(),Tablelvl2Context$1=Tablelvl2Context;function getTableBodyUtilityClass(a){return generateUtilityClass$1("MuiTableBody",a)}generateUtilityClasses$1("MuiTableBody",["root"]);const _excluded$w=["className","component"],useUtilityClasses$p=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getTableBodyUtilityClass,i)},TableBodyRoot=styled("tbody",{name:"MuiTableBody",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"table-row-group"}),tablelvl2$1={variant:"body"},defaultComponent$2="tbody",TableBody=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTableBody"}),{className:$,component:j=defaultComponent$2}=s,_e=_objectWithoutPropertiesLoose(s,_excluded$w),et=_extends({},s,{component:j}),tt=useUtilityClasses$p(et);return jsxRuntimeExports.jsx(Tablelvl2Context$1.Provider,{value:tablelvl2$1,children:jsxRuntimeExports.jsx(TableBodyRoot,_extends({className:clsx(tt.root,$),as:j,ref:o,role:j===defaultComponent$2?null:"rowgroup",ownerState:et},_e))})}),TableBody$1=TableBody;function getTableCellUtilityClass(a){return generateUtilityClass$1("MuiTableCell",a)}const tableCellClasses=generateUtilityClasses$1("MuiTableCell",["root","head","body","footer","sizeSmall","sizeMedium","paddingCheckbox","paddingNone","alignLeft","alignCenter","alignRight","alignJustify","stickyHeader"]),tableCellClasses$1=tableCellClasses,_excluded$v=["align","className","component","padding","scope","size","sortDirection","variant"],useUtilityClasses$o=a=>{const{classes:i,variant:o,align:s,padding:$,size:j,stickyHeader:_e}=a,et={root:["root",o,_e&&"stickyHeader",s!=="inherit"&&`align${capitalize$2(s)}`,$!=="normal"&&`padding${capitalize$2($)}`,`size${capitalize$2(j)}`]};return composeClasses(et,getTableCellUtilityClass,i)},TableCellRoot=styled("td",{name:"MuiTableCell",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[o.variant],i[`size${capitalize$2(o.size)}`],o.padding!=="normal"&&i[`padding${capitalize$2(o.padding)}`],o.align!=="inherit"&&i[`align${capitalize$2(o.align)}`],o.stickyHeader&&i.stickyHeader]}})(({theme:a,ownerState:i})=>_extends({},a.typography.body2,{display:"table-cell",verticalAlign:"inherit",borderBottom:a.vars?`1px solid ${a.vars.palette.TableCell.border}`:`1px solid
    ${a.palette.mode==="light"?lighten_1(alpha_1(a.palette.divider,1),.88):darken_1(alpha_1(a.palette.divider,1),.68)}`,textAlign:"left",padding:16},i.variant==="head"&&{color:(a.vars||a).palette.text.primary,lineHeight:a.typography.pxToRem(24),fontWeight:a.typography.fontWeightMedium},i.variant==="body"&&{color:(a.vars||a).palette.text.primary},i.variant==="footer"&&{color:(a.vars||a).palette.text.secondary,lineHeight:a.typography.pxToRem(21),fontSize:a.typography.pxToRem(12)},i.size==="small"&&{padding:"6px 16px",[`&.${tableCellClasses$1.paddingCheckbox}`]:{width:24,padding:"0 12px 0 16px","& > *":{padding:0}}},i.padding==="checkbox"&&{width:48,padding:"0 0 0 4px"},i.padding==="none"&&{padding:0},i.align==="left"&&{textAlign:"left"},i.align==="center"&&{textAlign:"center"},i.align==="right"&&{textAlign:"right",flexDirection:"row-reverse"},i.align==="justify"&&{textAlign:"justify"},i.stickyHeader&&{position:"sticky",top:0,zIndex:2,backgroundColor:(a.vars||a).palette.background.default})),TableCell=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTableCell"}),{align:$="inherit",className:j,component:_e,padding:et,scope:tt,size:nt,sortDirection:at,variant:it}=s,st=_objectWithoutPropertiesLoose(s,_excluded$v),lt=reactExports.useContext(TableContext$1),ct=reactExports.useContext(Tablelvl2Context$1),rt=ct&&ct.variant==="head";let ut;_e?ut=_e:ut=rt?"th":"td";let ot=tt;ut==="td"?ot=void 0:!ot&&rt&&(ot="col");const dt=it||ct&&ct.variant,pt=_extends({},s,{align:$,component:ut,padding:et||(lt&&lt.padding?lt.padding:"normal"),size:nt||(lt&&lt.size?lt.size:"medium"),sortDirection:at,stickyHeader:dt==="head"&&lt&&lt.stickyHeader,variant:dt}),mt=useUtilityClasses$o(pt);let ft=null;return at&&(ft=at==="asc"?"ascending":"descending"),jsxRuntimeExports.jsx(TableCellRoot,_extends({as:ut,ref:o,className:clsx(mt.root,j),"aria-sort":ft,scope:ot,ownerState:pt},st))}),TableCell$1=TableCell;function getTableContainerUtilityClass(a){return generateUtilityClass$1("MuiTableContainer",a)}generateUtilityClasses$1("MuiTableContainer",["root"]);const _excluded$u=["className","component"],useUtilityClasses$n=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getTableContainerUtilityClass,i)},TableContainerRoot=styled("div",{name:"MuiTableContainer",slot:"Root",overridesResolver:(a,i)=>i.root})({width:"100%",overflowX:"auto"}),TableContainer=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTableContainer"}),{className:$,component:j="div"}=s,_e=_objectWithoutPropertiesLoose(s,_excluded$u),et=_extends({},s,{component:j}),tt=useUtilityClasses$n(et);return jsxRuntimeExports.jsx(TableContainerRoot,_extends({ref:o,as:j,className:clsx(tt.root,$),ownerState:et},_e))}),TableContainer$1=TableContainer;function getTableHeadUtilityClass(a){return generateUtilityClass$1("MuiTableHead",a)}generateUtilityClasses$1("MuiTableHead",["root"]);const _excluded$t=["className","component"],useUtilityClasses$m=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getTableHeadUtilityClass,i)},TableHeadRoot=styled("thead",{name:"MuiTableHead",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"table-header-group"}),tablelvl2={variant:"head"},defaultComponent$1="thead",TableHead=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTableHead"}),{className:$,component:j=defaultComponent$1}=s,_e=_objectWithoutPropertiesLoose(s,_excluded$t),et=_extends({},s,{component:j}),tt=useUtilityClasses$m(et);return jsxRuntimeExports.jsx(Tablelvl2Context$1.Provider,{value:tablelvl2,children:jsxRuntimeExports.jsx(TableHeadRoot,_extends({as:j,className:clsx(tt.root,$),ref:o,role:j===defaultComponent$1?null:"rowgroup",ownerState:et},_e))})}),TableHead$1=TableHead;function getToolbarUtilityClass(a){return generateUtilityClass$1("MuiToolbar",a)}generateUtilityClasses$1("MuiToolbar",["root","gutters","regular","dense"]);const _excluded$s=["className","component","disableGutters","variant"],useUtilityClasses$l=a=>{const{classes:i,disableGutters:o,variant:s}=a;return composeClasses({root:["root",!o&&"gutters",s]},getToolbarUtilityClass,i)},ToolbarRoot=styled("div",{name:"MuiToolbar",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,!o.disableGutters&&i.gutters,i[o.variant]]}})(({theme:a,ownerState:i})=>_extends({position:"relative",display:"flex",alignItems:"center"},!i.disableGutters&&{paddingLeft:a.spacing(2),paddingRight:a.spacing(2),[a.breakpoints.up("sm")]:{paddingLeft:a.spacing(3),paddingRight:a.spacing(3)}},i.variant==="dense"&&{minHeight:48}),({theme:a,ownerState:i})=>i.variant==="regular"&&a.mixins.toolbar),Toolbar=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiToolbar"}),{className:$,component:j="div",disableGutters:_e=!1,variant:et="regular"}=s,tt=_objectWithoutPropertiesLoose(s,_excluded$s),nt=_extends({},s,{component:j,disableGutters:_e,variant:et}),at=useUtilityClasses$l(nt);return jsxRuntimeExports.jsx(ToolbarRoot,_extends({as:j,className:clsx(at.root,$),ref:o,ownerState:nt},tt))}),Toolbar$1=Toolbar;function getTableRowUtilityClass(a){return generateUtilityClass$1("MuiTableRow",a)}const tableRowClasses=generateUtilityClasses$1("MuiTableRow",["root","selected","hover","head","footer"]),tableRowClasses$1=tableRowClasses,_excluded$r=["className","component","hover","selected"],useUtilityClasses$k=a=>{const{classes:i,selected:o,hover:s,head:$,footer:j}=a;return composeClasses({root:["root",o&&"selected",s&&"hover",$&&"head",j&&"footer"]},getTableRowUtilityClass,i)},TableRowRoot=styled("tr",{name:"MuiTableRow",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,o.head&&i.head,o.footer&&i.footer]}})(({theme:a})=>({color:"inherit",display:"table-row",verticalAlign:"middle",outline:0,[`&.${tableRowClasses$1.hover}:hover`]:{backgroundColor:(a.vars||a).palette.action.hover},[`&.${tableRowClasses$1.selected}`]:{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.selectedOpacity})`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity),"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha_1(a.palette.primary.main,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity)}}})),defaultComponent="tr",TableRow=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTableRow"}),{className:$,component:j=defaultComponent,hover:_e=!1,selected:et=!1}=s,tt=_objectWithoutPropertiesLoose(s,_excluded$r),nt=reactExports.useContext(Tablelvl2Context$1),at=_extends({},s,{component:j,hover:_e,selected:et,head:nt&&nt.variant==="head",footer:nt&&nt.variant==="footer"}),it=useUtilityClasses$k(at);return jsxRuntimeExports.jsx(TableRowRoot,_extends({as:j,ref:o,className:clsx(it.root,$),role:j===defaultComponent?null:"row",ownerState:at},tt))}),TableRow$1=TableRow;function getTextFieldUtilityClass(a){return generateUtilityClass$1("MuiTextField",a)}generateUtilityClasses$1("MuiTextField",["root"]);const _excluded$q=["autoComplete","autoFocus","children","className","color","defaultValue","disabled","error","FormHelperTextProps","fullWidth","helperText","id","InputLabelProps","inputProps","InputProps","inputRef","label","maxRows","minRows","multiline","name","onBlur","onChange","onFocus","placeholder","required","rows","select","SelectProps","type","value","variant"],variantComponent={standard:Input$1,filled:FilledInput$1,outlined:OutlinedInput$1},useUtilityClasses$j=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getTextFieldUtilityClass,i)},TextFieldRoot=styled(FormControl$1,{name:"MuiTextField",slot:"Root",overridesResolver:(a,i)=>i.root})({}),TextField=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiTextField"}),{autoComplete:$,autoFocus:j=!1,children:_e,className:et,color:tt="primary",defaultValue:nt,disabled:at=!1,error:it=!1,FormHelperTextProps:st,fullWidth:lt=!1,helperText:ct,id:rt,InputLabelProps:ut,inputProps:ot,InputProps:dt,inputRef:pt,label:mt,maxRows:ft,minRows:ht,multiline:yt=!1,name:bt,onBlur:gt,onChange:xt,onFocus:vt,placeholder:Lt,required:$t=!1,rows:Tt,select:Et=!1,SelectProps:Dt,type:It,value:Ct,variant:jt="outlined"}=s,Zt=_objectWithoutPropertiesLoose(s,_excluded$q),Xt=_extends({},s,{autoFocus:j,color:tt,disabled:at,error:it,fullWidth:lt,multiline:yt,required:$t,select:Et,variant:jt}),sn=useUtilityClasses$j(Xt),Ft={};jt==="outlined"&&(ut&&typeof ut.shrink<"u"&&(Ft.notched=ut.shrink),Ft.label=mt),Et&&((!Dt||!Dt.native)&&(Ft.id=void 0),Ft["aria-describedby"]=void 0);const wt=useId(rt),kt=ct&&wt?`${wt}-helper-text`:void 0,At=mt&&wt?`${wt}-label`:void 0,Pt=variantComponent[jt],Mt=jsxRuntimeExports.jsx(Pt,_extends({"aria-describedby":kt,autoComplete:$,autoFocus:j,defaultValue:nt,fullWidth:lt,multiline:yt,name:bt,rows:Tt,maxRows:ft,minRows:ht,type:It,value:Ct,id:wt,inputRef:pt,onBlur:gt,onChange:xt,onFocus:vt,placeholder:Lt,inputProps:ot},Ft,dt));return jsxRuntimeExports.jsxs(TextFieldRoot,_extends({className:clsx(sn.root,et),disabled:at,error:it,fullWidth:lt,ref:o,required:$t,color:tt,variant:jt,ownerState:Xt},Zt,{children:[mt!=null&&mt!==""&&jsxRuntimeExports.jsx(InputLabel$1,_extends({htmlFor:wt,id:At},ut,{children:mt})),Et?jsxRuntimeExports.jsx(Select$1,_extends({"aria-describedby":kt,id:wt,labelId:At,value:Ct,input:Mt},Dt,{children:_e})):Mt,ct&&jsxRuntimeExports.jsx(FormHelperText$1,_extends({id:kt},st,{children:ct}))]}))}),TextField$1=TextField;function getToggleButtonUtilityClass(a){return generateUtilityClass$1("MuiToggleButton",a)}const toggleButtonClasses=generateUtilityClasses$1("MuiToggleButton",["root","disabled","selected","standard","primary","secondary","sizeSmall","sizeMedium","sizeLarge","fullWidth"]),toggleButtonClasses$1=toggleButtonClasses,ToggleButtonGroupContext=reactExports.createContext({}),ToggleButtonGroupContext$1=ToggleButtonGroupContext,ToggleButtonGroupButtonContext=reactExports.createContext(void 0),ToggleButtonGroupButtonContext$1=ToggleButtonGroupButtonContext;function isValueSelected(a,i){return i===void 0||a===void 0?!1:Array.isArray(i)?i.indexOf(a)>=0:a===i}const _excluded$p=["value"],_excluded2$6=["children","className","color","disabled","disableFocusRipple","fullWidth","onChange","onClick","selected","size","value"],useUtilityClasses$i=a=>{const{classes:i,fullWidth:o,selected:s,disabled:$,size:j,color:_e}=a,et={root:["root",s&&"selected",$&&"disabled",o&&"fullWidth",`size${capitalize$2(j)}`,_e]};return composeClasses(et,getToggleButtonUtilityClass,i)},ToggleButtonRoot=styled(ButtonBase$1,{name:"MuiToggleButton",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.root,i[`size${capitalize$2(o.size)}`]]}})(({theme:a,ownerState:i})=>{let o=i.color==="standard"?a.palette.text.primary:a.palette[i.color].main,s;return a.vars&&(o=i.color==="standard"?a.vars.palette.text.primary:a.vars.palette[i.color].main,s=i.color==="standard"?a.vars.palette.text.primaryChannel:a.vars.palette[i.color].mainChannel),_extends({},a.typography.button,{borderRadius:(a.vars||a).shape.borderRadius,padding:11,border:`1px solid ${(a.vars||a).palette.divider}`,color:(a.vars||a).palette.action.active},i.fullWidth&&{width:"100%"},{[`&.${toggleButtonClasses$1.disabled}`]:{color:(a.vars||a).palette.action.disabled,border:`1px solid ${(a.vars||a).palette.action.disabledBackground}`},"&:hover":{textDecoration:"none",backgroundColor:a.vars?`rgba(${a.vars.palette.text.primaryChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha$1(a.palette.text.primary,a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:"transparent"}},[`&.${toggleButtonClasses$1.selected}`]:{color:o,backgroundColor:a.vars?`rgba(${s} / ${a.vars.palette.action.selectedOpacity})`:alpha$1(o,a.palette.action.selectedOpacity),"&:hover":{backgroundColor:a.vars?`rgba(${s} / calc(${a.vars.palette.action.selectedOpacity} + ${a.vars.palette.action.hoverOpacity}))`:alpha$1(o,a.palette.action.selectedOpacity+a.palette.action.hoverOpacity),"@media (hover: none)":{backgroundColor:a.vars?`rgba(${s} / ${a.vars.palette.action.selectedOpacity})`:alpha$1(o,a.palette.action.selectedOpacity)}}}},i.size==="small"&&{padding:7,fontSize:a.typography.pxToRem(13)},i.size==="large"&&{padding:15,fontSize:a.typography.pxToRem(15)})}),ToggleButton=reactExports.forwardRef(function(i,o){const s=reactExports.useContext(ToggleButtonGroupContext$1),{value:$}=s,j=_objectWithoutPropertiesLoose(s,_excluded$p),_e=reactExports.useContext(ToggleButtonGroupButtonContext$1),et=resolveProps(_extends({},j,{selected:isValueSelected(i.value,$)}),i),tt=useThemeProps$6({props:et,name:"MuiToggleButton"}),{children:nt,className:at,color:it="standard",disabled:st=!1,disableFocusRipple:lt=!1,fullWidth:ct=!1,onChange:rt,onClick:ut,selected:ot,size:dt="medium",value:pt}=tt,mt=_objectWithoutPropertiesLoose(tt,_excluded2$6),ft=_extends({},tt,{color:it,disabled:st,disableFocusRipple:lt,fullWidth:ct,size:dt}),ht=useUtilityClasses$i(ft),yt=gt=>{ut&&(ut(gt,pt),gt.defaultPrevented)||rt&&rt(gt,pt)},bt=_e||"";return jsxRuntimeExports.jsx(ToggleButtonRoot,_extends({className:clsx(j.className,ht.root,at,bt),disabled:st,focusRipple:!lt,ref:o,onClick:yt,onChange:rt,value:pt,ownerState:ft,"aria-pressed":ot},mt,{children:nt}))}),ToggleButton$1=ToggleButton;function getToggleButtonGroupUtilityClass(a){return generateUtilityClass$1("MuiToggleButtonGroup",a)}const toggleButtonGroupClasses=generateUtilityClasses$1("MuiToggleButtonGroup",["root","selected","vertical","disabled","grouped","groupedHorizontal","groupedVertical","fullWidth","firstButton","lastButton","middleButton"]),toggleButtonGroupClasses$1=toggleButtonGroupClasses,_excluded$o=["children","className","color","disabled","exclusive","fullWidth","onChange","orientation","size","value"],useUtilityClasses$h=a=>{const{classes:i,orientation:o,fullWidth:s,disabled:$}=a,j={root:["root",o==="vertical"&&"vertical",s&&"fullWidth"],grouped:["grouped",`grouped${capitalize$2(o)}`,$&&"disabled"],firstButton:["firstButton"],lastButton:["lastButton"],middleButton:["middleButton"]};return composeClasses(j,getToggleButtonGroupUtilityClass,i)},ToggleButtonGroupRoot=styled("div",{name:"MuiToggleButtonGroup",slot:"Root",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[{[`& .${toggleButtonGroupClasses$1.grouped}`]:i.grouped},{[`& .${toggleButtonGroupClasses$1.grouped}`]:i[`grouped${capitalize$2(o.orientation)}`]},{[`& .${toggleButtonGroupClasses$1.firstButton}`]:i.firstButton},{[`& .${toggleButtonGroupClasses$1.lastButton}`]:i.lastButton},{[`& .${toggleButtonGroupClasses$1.middleButton}`]:i.middleButton},i.root,o.orientation==="vertical"&&i.vertical,o.fullWidth&&i.fullWidth]}})(({ownerState:a,theme:i})=>_extends({display:"inline-flex",borderRadius:(i.vars||i).shape.borderRadius},a.orientation==="vertical"&&{flexDirection:"column"},a.fullWidth&&{width:"100%"},{[`& .${toggleButtonGroupClasses$1.grouped}`]:_extends({},a.orientation==="horizontal"?{[`&.${toggleButtonGroupClasses$1.selected} + .${toggleButtonGroupClasses$1.grouped}.${toggleButtonGroupClasses$1.selected}`]:{borderLeft:0,marginLeft:0}}:{[`&.${toggleButtonGroupClasses$1.selected} + .${toggleButtonGroupClasses$1.grouped}.${toggleButtonGroupClasses$1.selected}`]:{borderTop:0,marginTop:0}})},a.orientation==="horizontal"?{[`& .${toggleButtonGroupClasses$1.firstButton},& .${toggleButtonGroupClasses$1.middleButton}`]:{borderTopRightRadius:0,borderBottomRightRadius:0},[`& .${toggleButtonGroupClasses$1.lastButton},& .${toggleButtonGroupClasses$1.middleButton}`]:{marginLeft:-1,borderLeft:"1px solid transparent",borderTopLeftRadius:0,borderBottomLeftRadius:0}}:{[`& .${toggleButtonGroupClasses$1.firstButton},& .${toggleButtonGroupClasses$1.middleButton}`]:{borderBottomLeftRadius:0,borderBottomRightRadius:0},[`& .${toggleButtonGroupClasses$1.lastButton},& .${toggleButtonGroupClasses$1.middleButton}`]:{marginTop:-1,borderTop:"1px solid transparent",borderTopLeftRadius:0,borderTopRightRadius:0}},a.orientation==="horizontal"?{[`& .${toggleButtonGroupClasses$1.lastButton}.${toggleButtonClasses$1.disabled},& .${toggleButtonGroupClasses$1.middleButton}.${toggleButtonClasses$1.disabled}`]:{borderLeft:"1px solid transparent"}}:{[`& .${toggleButtonGroupClasses$1.lastButton}.${toggleButtonClasses$1.disabled},& .${toggleButtonGroupClasses$1.middleButton}.${toggleButtonClasses$1.disabled}`]:{borderTop:"1px solid transparent"}})),ToggleButtonGroup=reactExports.forwardRef(function(i,o){const s=useThemeProps$6({props:i,name:"MuiToggleButtonGroup"}),{children:$,className:j,color:_e="standard",disabled:et=!1,exclusive:tt=!1,fullWidth:nt=!1,onChange:at,orientation:it="horizontal",size:st="medium",value:lt}=s,ct=_objectWithoutPropertiesLoose(s,_excluded$o),rt=_extends({},s,{disabled:et,fullWidth:nt,orientation:it,size:st}),ut=useUtilityClasses$h(rt),ot=reactExports.useCallback((yt,bt)=>{if(!at)return;const gt=lt&&lt.indexOf(bt);let xt;lt&&gt>=0?(xt=lt.slice(),xt.splice(gt,1)):xt=lt?lt.concat(bt):[bt],at(yt,xt)},[at,lt]),dt=reactExports.useCallback((yt,bt)=>{at&&at(yt,lt===bt?null:bt)},[at,lt]),pt=reactExports.useMemo(()=>({className:ut.grouped,onChange:tt?dt:ot,value:lt,size:st,fullWidth:nt,color:_e,disabled:et}),[ut.grouped,tt,dt,ot,lt,st,nt,_e,et]),mt=getValidReactChildren($),ft=mt.length,ht=yt=>{const bt=yt===0,gt=yt===ft-1;return bt&&gt?"":bt?ut.firstButton:gt?ut.lastButton:ut.middleButton};return jsxRuntimeExports.jsx(ToggleButtonGroupRoot,_extends({role:"group",className:clsx(ut.root,j),ref:o,ownerState:rt},ct,{children:jsxRuntimeExports.jsx(ToggleButtonGroupContext$1.Provider,{value:pt,children:mt.map((yt,bt)=>jsxRuntimeExports.jsx(ToggleButtonGroupButtonContext$1.Provider,{value:ht(bt),children:yt},bt))})}))}),ToggleButtonGroup$1=ToggleButtonGroup;var Search={},createSvgIcon={};const require$$0=getAugmentedNamespace(utils$2);var hasRequiredCreateSvgIcon;function requireCreateSvgIcon(){return hasRequiredCreateSvgIcon||(hasRequiredCreateSvgIcon=1,function(a){"use client";Object.defineProperty(a,"__esModule",{value:!0}),Object.defineProperty(a,"default",{enumerable:!0,get:function(){return i.createSvgIcon}});var i=require$$0}(createSvgIcon)),createSvgIcon}var _interopRequireDefault$t=interopRequireDefaultExports;Object.defineProperty(Search,"__esModule",{value:!0});var default_1$t=Search.default=void 0,_createSvgIcon$t=_interopRequireDefault$t(requireCreateSvgIcon()),_jsxRuntime$t=jsxRuntimeExports;default_1$t=Search.default=(0,_createSvgIcon$t.default)((0,_jsxRuntime$t.jsx)("path",{d:"M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14"}),"Search");var Checklist={},_interopRequireDefault$s=interopRequireDefaultExports;Object.defineProperty(Checklist,"__esModule",{value:!0});var default_1$s=Checklist.default=void 0,_createSvgIcon$s=_interopRequireDefault$s(requireCreateSvgIcon()),_jsxRuntime$s=jsxRuntimeExports;default_1$s=Checklist.default=(0,_createSvgIcon$s.default)((0,_jsxRuntime$s.jsx)("path",{d:"M22 7h-9v2h9zm0 8h-9v2h9zM5.54 11 2 7.46l1.41-1.41 2.12 2.12 4.24-4.24 1.41 1.41zm0 8L2 15.46l1.41-1.41 2.12 2.12 4.24-4.24 1.41 1.41z"}),"Checklist");const selectedDateAtom=atom(""),emptyListAtom=atom([]),emptyObjectAtom=atom({}),updatePaperAtom=atom(null,(a,i,{paperAtom:o,id:s,field:$,newValue:j})=>{const _e=a(o);i(o,{..._e,[$]:j})}),updatePaperInListAtom=atom(null,(a,i,{papersListAtom:o,id:s,field:$,newValue:j})=>{const et=a(o).map(tt=>tt.id===s?{...tt,[$]:j}:tt);i(o,et)});function adjustDateToUserTimezone(a){const i=a.getTimezoneOffset()*6e4;return new Date(a.getTime()+i)}function formatDate$1(a,i){const o=new Date(a);return adjustDateToUserTimezone(o).toLocaleDateString("en-US",i)}function formatDateParts(a,i){return formatDate$1(a,i).split(/[,\s]+/)}var dayjs_min={exports:{}};(function(a,i){(function(o,s){a.exports=s()})(commonjsGlobal,function(){var o=1e3,s=6e4,$=36e5,j="millisecond",_e="second",et="minute",tt="hour",nt="day",at="week",it="month",st="quarter",lt="year",ct="date",rt="Invalid Date",ut=/^(\d{4})[-/]?(\d{1,2})?[-/]?(\d{0,2})[Tt\s]*(\d{1,2})?:?(\d{1,2})?:?(\d{1,2})?[.:]?(\d+)?$/,ot=/\[([^\]]+)]|Y{1,4}|M{1,4}|D{1,2}|d{1,4}|H{1,2}|h{1,2}|a|A|m{1,2}|s{1,2}|Z{1,2}|SSS/g,dt={name:"en",weekdays:"Sunday_Monday_Tuesday_Wednesday_Thursday_Friday_Saturday".split("_"),months:"January_February_March_April_May_June_July_August_September_October_November_December".split("_"),ordinal:function(Tt){var Et=["th","st","nd","rd"],Dt=Tt%100;return"["+Tt+(Et[(Dt-20)%10]||Et[Dt]||Et[0])+"]"}},pt=function(Tt,Et,Dt){var It=String(Tt);return!It||It.length>=Et?Tt:""+Array(Et+1-It.length).join(Dt)+Tt},mt={s:pt,z:function(Tt){var Et=-Tt.utcOffset(),Dt=Math.abs(Et),It=Math.floor(Dt/60),Ct=Dt%60;return(Et<=0?"+":"-")+pt(It,2,"0")+":"+pt(Ct,2,"0")},m:function Tt(Et,Dt){if(Et.date()<Dt.date())return-Tt(Dt,Et);var It=12*(Dt.year()-Et.year())+(Dt.month()-Et.month()),Ct=Et.clone().add(It,it),jt=Dt-Ct<0,Zt=Et.clone().add(It+(jt?-1:1),it);return+(-(It+(Dt-Ct)/(jt?Ct-Zt:Zt-Ct))||0)},a:function(Tt){return Tt<0?Math.ceil(Tt)||0:Math.floor(Tt)},p:function(Tt){return{M:it,y:lt,w:at,d:nt,D:ct,h:tt,m:et,s:_e,ms:j,Q:st}[Tt]||String(Tt||"").toLowerCase().replace(/s$/,"")},u:function(Tt){return Tt===void 0}},ft="en",ht={};ht[ft]=dt;var yt="$isDayjsObject",bt=function(Tt){return Tt instanceof Lt||!(!Tt||!Tt[yt])},gt=function Tt(Et,Dt,It){var Ct;if(!Et)return ft;if(typeof Et=="string"){var jt=Et.toLowerCase();ht[jt]&&(Ct=jt),Dt&&(ht[jt]=Dt,Ct=jt);var Zt=Et.split("-");if(!Ct&&Zt.length>1)return Tt(Zt[0])}else{var Xt=Et.name;ht[Xt]=Et,Ct=Xt}return!It&&Ct&&(ft=Ct),Ct||!It&&ft},xt=function(Tt,Et){if(bt(Tt))return Tt.clone();var Dt=typeof Et=="object"?Et:{};return Dt.date=Tt,Dt.args=arguments,new Lt(Dt)},vt=mt;vt.l=gt,vt.i=bt,vt.w=function(Tt,Et){return xt(Tt,{locale:Et.$L,utc:Et.$u,x:Et.$x,$offset:Et.$offset})};var Lt=function(){function Tt(Dt){this.$L=gt(Dt.locale,null,!0),this.parse(Dt),this.$x=this.$x||Dt.x||{},this[yt]=!0}var Et=Tt.prototype;return Et.parse=function(Dt){this.$d=function(It){var Ct=It.date,jt=It.utc;if(Ct===null)return new Date(NaN);if(vt.u(Ct))return new Date;if(Ct instanceof Date)return new Date(Ct);if(typeof Ct=="string"&&!/Z$/i.test(Ct)){var Zt=Ct.match(ut);if(Zt){var Xt=Zt[2]-1||0,sn=(Zt[7]||"0").substring(0,3);return jt?new Date(Date.UTC(Zt[1],Xt,Zt[3]||1,Zt[4]||0,Zt[5]||0,Zt[6]||0,sn)):new Date(Zt[1],Xt,Zt[3]||1,Zt[4]||0,Zt[5]||0,Zt[6]||0,sn)}}return new Date(Ct)}(Dt),this.init()},Et.init=function(){var Dt=this.$d;this.$y=Dt.getFullYear(),this.$M=Dt.getMonth(),this.$D=Dt.getDate(),this.$W=Dt.getDay(),this.$H=Dt.getHours(),this.$m=Dt.getMinutes(),this.$s=Dt.getSeconds(),this.$ms=Dt.getMilliseconds()},Et.$utils=function(){return vt},Et.isValid=function(){return this.$d.toString()!==rt},Et.isSame=function(Dt,It){var Ct=xt(Dt);return this.startOf(It)<=Ct&&Ct<=this.endOf(It)},Et.isAfter=function(Dt,It){return xt(Dt)<this.startOf(It)},Et.isBefore=function(Dt,It){return this.endOf(It)<xt(Dt)},Et.$g=function(Dt,It,Ct){return vt.u(Dt)?this[It]:this.set(Ct,Dt)},Et.unix=function(){return Math.floor(this.valueOf()/1e3)},Et.valueOf=function(){return this.$d.getTime()},Et.startOf=function(Dt,It){var Ct=this,jt=!!vt.u(It)||It,Zt=vt.p(Dt),Xt=function(Ot,Bt){var zt=vt.w(Ct.$u?Date.UTC(Ct.$y,Bt,Ot):new Date(Ct.$y,Bt,Ot),Ct);return jt?zt:zt.endOf(nt)},sn=function(Ot,Bt){return vt.w(Ct.toDate()[Ot].apply(Ct.toDate("s"),(jt?[0,0,0,0]:[23,59,59,999]).slice(Bt)),Ct)},Ft=this.$W,wt=this.$M,kt=this.$D,At="set"+(this.$u?"UTC":"");switch(Zt){case lt:return jt?Xt(1,0):Xt(31,11);case it:return jt?Xt(1,wt):Xt(0,wt+1);case at:var Pt=this.$locale().weekStart||0,Mt=(Ft<Pt?Ft+7:Ft)-Pt;return Xt(jt?kt-Mt:kt+(6-Mt),wt);case nt:case ct:return sn(At+"Hours",0);case tt:return sn(At+"Minutes",1);case et:return sn(At+"Seconds",2);case _e:return sn(At+"Milliseconds",3);default:return this.clone()}},Et.endOf=function(Dt){return this.startOf(Dt,!1)},Et.$set=function(Dt,It){var Ct,jt=vt.p(Dt),Zt="set"+(this.$u?"UTC":""),Xt=(Ct={},Ct[nt]=Zt+"Date",Ct[ct]=Zt+"Date",Ct[it]=Zt+"Month",Ct[lt]=Zt+"FullYear",Ct[tt]=Zt+"Hours",Ct[et]=Zt+"Minutes",Ct[_e]=Zt+"Seconds",Ct[j]=Zt+"Milliseconds",Ct)[jt],sn=jt===nt?this.$D+(It-this.$W):It;if(jt===it||jt===lt){var Ft=this.clone().set(ct,1);Ft.$d[Xt](sn),Ft.init(),this.$d=Ft.set(ct,Math.min(this.$D,Ft.daysInMonth())).$d}else Xt&&this.$d[Xt](sn);return this.init(),this},Et.set=function(Dt,It){return this.clone().$set(Dt,It)},Et.get=function(Dt){return this[vt.p(Dt)]()},Et.add=function(Dt,It){var Ct,jt=this;Dt=Number(Dt);var Zt=vt.p(It),Xt=function(wt){var kt=xt(jt);return vt.w(kt.date(kt.date()+Math.round(wt*Dt)),jt)};if(Zt===it)return this.set(it,this.$M+Dt);if(Zt===lt)return this.set(lt,this.$y+Dt);if(Zt===nt)return Xt(1);if(Zt===at)return Xt(7);var sn=(Ct={},Ct[et]=s,Ct[tt]=$,Ct[_e]=o,Ct)[Zt]||1,Ft=this.$d.getTime()+Dt*sn;return vt.w(Ft,this)},Et.subtract=function(Dt,It){return this.add(-1*Dt,It)},Et.format=function(Dt){var It=this,Ct=this.$locale();if(!this.isValid())return Ct.invalidDate||rt;var jt=Dt||"YYYY-MM-DDTHH:mm:ssZ",Zt=vt.z(this),Xt=this.$H,sn=this.$m,Ft=this.$M,wt=Ct.weekdays,kt=Ct.months,At=Ct.meridiem,Pt=function(Bt,zt,Gt,Wt){return Bt&&(Bt[zt]||Bt(It,jt))||Gt[zt].slice(0,Wt)},Mt=function(Bt){return vt.s(Xt%12||12,Bt,"0")},Ot=At||function(Bt,zt,Gt){var Wt=Bt<12?"AM":"PM";return Gt?Wt.toLowerCase():Wt};return jt.replace(ot,function(Bt,zt){return zt||function(Gt){switch(Gt){case"YY":return String(It.$y).slice(-2);case"YYYY":return vt.s(It.$y,4,"0");case"M":return Ft+1;case"MM":return vt.s(Ft+1,2,"0");case"MMM":return Pt(Ct.monthsShort,Ft,kt,3);case"MMMM":return Pt(kt,Ft);case"D":return It.$D;case"DD":return vt.s(It.$D,2,"0");case"d":return String(It.$W);case"dd":return Pt(Ct.weekdaysMin,It.$W,wt,2);case"ddd":return Pt(Ct.weekdaysShort,It.$W,wt,3);case"dddd":return wt[It.$W];case"H":return String(Xt);case"HH":return vt.s(Xt,2,"0");case"h":return Mt(1);case"hh":return Mt(2);case"a":return Ot(Xt,sn,!0);case"A":return Ot(Xt,sn,!1);case"m":return String(sn);case"mm":return vt.s(sn,2,"0");case"s":return String(It.$s);case"ss":return vt.s(It.$s,2,"0");case"SSS":return vt.s(It.$ms,3,"0");case"Z":return Zt}return null}(Bt)||Zt.replace(":","")})},Et.utcOffset=function(){return 15*-Math.round(this.$d.getTimezoneOffset()/15)},Et.diff=function(Dt,It,Ct){var jt,Zt=this,Xt=vt.p(It),sn=xt(Dt),Ft=(sn.utcOffset()-this.utcOffset())*s,wt=this-sn,kt=function(){return vt.m(Zt,sn)};switch(Xt){case lt:jt=kt()/12;break;case it:jt=kt();break;case st:jt=kt()/3;break;case at:jt=(wt-Ft)/6048e5;break;case nt:jt=(wt-Ft)/864e5;break;case tt:jt=wt/$;break;case et:jt=wt/s;break;case _e:jt=wt/o;break;default:jt=wt}return Ct?jt:vt.a(jt)},Et.daysInMonth=function(){return this.endOf(it).$D},Et.$locale=function(){return ht[this.$L]},Et.locale=function(Dt,It){if(!Dt)return this.$L;var Ct=this.clone(),jt=gt(Dt,It,!0);return jt&&(Ct.$L=jt),Ct},Et.clone=function(){return vt.w(this.$d,this)},Et.toDate=function(){return new Date(this.valueOf())},Et.toJSON=function(){return this.isValid()?this.toISOString():null},Et.toISOString=function(){return this.$d.toISOString()},Et.toString=function(){return this.$d.toUTCString()},Tt}(),$t=Lt.prototype;return xt.prototype=$t,[["$ms",j],["$s",_e],["$m",et],["$H",tt],["$W",nt],["$M",it],["$y",lt],["$D",ct]].forEach(function(Tt){$t[Tt[1]]=function(Et){return this.$g(Et,Tt[0],Tt[1])}}),xt.extend=function(Tt,Et){return Tt.$i||(Tt(Et,Lt,xt),Tt.$i=!0),xt},xt.locale=gt,xt.isDayjs=bt,xt.unix=function(Tt){return xt(1e3*Tt)},xt.en=ht[ft],xt.Ls=ht,xt.p={},xt})})(dayjs_min);var dayjs_minExports=dayjs_min.exports;const dayjs=getDefaultExportFromCjs(dayjs_minExports),dates=[{month:"May 2024",dates:[{value:"2024-05-31",status:"pending",count:null,createdAt:"2024-05-31 04:00:01.010 +00:00",updatedAt:"2024-05-31 13:50:50.309 +00:00"},{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:58:28.953 +00:00"},{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 05:21:04.775 +00:00"},{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:51:45.389 +00:00"},{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:51:34.031 +00:00"},{value:"2024-05-26",status:"complete",count:63,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 05:12:18.499 +00:00"},{value:"2024-05-25",status:"complete",count:55,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 05:12:24.590 +00:00"},{value:"2024-05-24",status:"complete",count:122,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 05:12:40.931 +00:00"},{value:"2024-05-23",status:"complete",count:181,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 05:22:53.342 +00:00"},{value:"2024-05-22",status:"complete",count:111,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:07:02.684 +00:00"},{value:"2024-05-21",status:"complete",count:92,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:52.950 +00:00"},{value:"2024-05-20",status:"complete",count:71,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:52.818 +00:00"},{value:"2024-05-19",status:"complete",count:42,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:41.663 +00:00"},{value:"2024-05-18",status:"complete",count:35,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:23.755 +00:00"},{value:"2024-05-17",status:"complete",count:72,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:29.664 +00:00"},{value:"2024-05-16",status:"complete",count:95,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:29.905 +00:00"},{value:"2024-05-15",status:"complete",count:72,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:15.893 +00:00"},{value:"2024-05-14",status:"complete",count:60,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:08.172 +00:00"},{value:"2024-05-13",status:"complete",count:97,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:06:16.110 +00:00"},{value:"2024-05-12",status:"complete",count:45,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:55.044 +00:00"},{value:"2024-05-11",status:"complete",count:25,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:49.161 +00:00"},{value:"2024-05-10",status:"complete",count:67,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:55.138 +00:00"},{value:"2024-05-09",status:"complete",count:68,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:28.940 +00:00"},{value:"2024-05-08",status:"complete",count:85,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:43.021 +00:00"},{value:"2024-05-07",status:"complete",count:105,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:43.139 +00:00"},{value:"2024-05-06",status:"complete",count:90,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:06.390 +00:00"},{value:"2024-05-05",status:"complete",count:37,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:06.160 +00:00"},{value:"2024-05-04",status:"complete",count:46,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:05:06.460 +00:00"},{value:"2024-05-03",status:"complete",count:84,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:47.894 +00:00"},{value:"2024-05-02",status:"complete",count:97,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:40.874 +00:00"},{value:"2024-05-01",status:"complete",count:66,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:35.193 +00:00"}]},{month:"April 2024",dates:[{value:"2024-04-30",status:"complete",count:79,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:40.754 +00:00"},{value:"2024-04-29",status:"complete",count:90,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:24.230 +00:00"},{value:"2024-04-28",status:"complete",count:43,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:09.255 +00:00"},{value:"2024-04-27",status:"complete",count:51,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:24.100 +00:00"},{value:"2024-04-26",status:"complete",count:64,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:59.133 +00:00"},{value:"2024-04-25",status:"complete",count:101,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:05.692 +00:00"},{value:"2024-04-24",status:"complete",count:88,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:25:05.570 +00:00"},{value:"2024-04-23",status:"complete",count:108,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:48.191 +00:00"},{value:"2024-04-22",status:"complete",count:79,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:41.231 +00:00"},{value:"2024-04-21",status:"complete",count:56,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:30.419 +00:00"},{value:"2024-04-20",status:"complete",count:40,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:30.365 +00:00"},{value:"2024-04-19",status:"complete",count:75,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:17.445 +00:00"},{value:"2024-04-18",status:"complete",count:93,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:24.303 +00:00"},{value:"2024-04-17",status:"complete",count:106,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:24.426 +00:00"},{value:"2024-04-16",status:"complete",count:101,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:03.288 +00:00"},{value:"2024-04-15",status:"complete",count:104,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:03.496 +00:00"},{value:"2024-04-14",status:"complete",count:48,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:44:03.274 +00:00"},{value:"2024-04-13",status:"complete",count:36,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:41.979 +00:00"},{value:"2024-04-12",status:"complete",count:77,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:47.722 +00:00"},{value:"2024-04-11",status:"complete",count:102,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:47.818 +00:00"},{value:"2024-04-10",status:"complete",count:83,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:26.849 +00:00"},{value:"2024-04-09",status:"complete",count:96,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:32.544 +00:00"},{value:"2024-04-08",status:"complete",count:82,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:32.265 +00:00"},{value:"2024-04-07",status:"complete",count:47,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:16.730 +00:00"},{value:"2024-04-06",status:"complete",count:44,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:13.083 +00:00"},{value:"2024-04-05",status:"complete",count:66,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:16.793 +00:00"},{value:"2024-04-04",status:"complete",count:86,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:06.825 +00:00"},{value:"2024-04-03",status:"complete",count:113,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:06.988 +00:00"},{value:"2024-04-02",status:"complete",count:102,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:43:06.837 +00:00"},{value:"2024-04-01",status:"complete",count:89,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:48.062 +00:00"}]},{month:"March 2024",dates:[{value:"2024-03-31",status:"complete",count:46,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:29.387 +00:00"},{value:"2024-03-30",status:"complete",count:44,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:25.426 +00:00"},{value:"2024-03-29",status:"complete",count:71,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:29.489 +00:00"},{value:"2024-03-28",status:"complete",count:86,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:19.196 +00:00"},{value:"2024-03-27",status:"complete",count:105,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:12.088 +00:00"},{value:"2024-03-26",status:"complete",count:118,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:24:19.382 +00:00"},{value:"2024-03-25",status:"complete",count:119,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:00.086 +00:00"},{value:"2024-03-24",status:"complete",count:50,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:50.955 +00:00"},{value:"2024-03-23",status:"complete",count:47,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:46.128 +00:00"},{value:"2024-03-22",status:"complete",count:80,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:51.094 +00:00"},{value:"2024-03-21",status:"complete",count:101,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:37.899 +00:00"},{value:"2024-03-20",status:"complete",count:86,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:32.948 +00:00"},{value:"2024-03-19",status:"complete",count:88,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:37.910 +00:00"},{value:"2024-03-18",status:"complete",count:112,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:21.407 +00:00"},{value:"2024-03-17",status:"complete",count:54,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:21.250 +00:00"},{value:"2024-03-16",status:"complete",count:44,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:14.450 +00:00"},{value:"2024-03-15",status:"complete",count:110,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:01.804 +00:00"},{value:"2024-03-14",status:"complete",count:111,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:07.881 +00:00"},{value:"2024-03-13",status:"complete",count:118,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:27:07.944 +00:00"},{value:"2024-03-12",status:"complete",count:120,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:26:49.929 +00:00"},{value:"2024-03-11",status:"complete",count:91,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:26:49.731 +00:00"},{value:"2024-03-10",status:"complete",count:40,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:26:43.153 +00:00"},{value:"2024-03-09",status:"complete",count:43,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:26:34.134 +00:00"},{value:"2024-03-08",status:"complete",count:92,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:26:34.535 +00:00"},{value:"2024-03-07",status:"complete",count:99,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:26:34.457 +00:00"},{value:"2024-03-06",status:"complete",count:94,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:30:12.437 +00:00"},{value:"2024-03-05",status:"complete",count:113,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:30:05.785 +00:00"},{value:"2024-03-04",status:"complete",count:107,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:30:05.644 +00:00"},{value:"2024-03-03",status:"complete",count:35,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:59.443 +00:00"},{value:"2024-03-02",status:"complete",count:49,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:41.831 +00:00"},{value:"2024-03-01",status:"complete",count:81,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:47.389 +00:00"}]},{month:"February 2024",dates:[{value:"2024-02-29",status:"complete",count:108,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:47.559 +00:00"},{value:"2024-02-28",status:"complete",count:112,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:34.567 +00:00"},{value:"2024-02-27",status:"complete",count:104,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:34.107 +00:00"},{value:"2024-02-26",status:"complete",count:116,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:34.560 +00:00"},{value:"2024-02-25",status:"complete",count:49,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:07.144 +00:00"},{value:"2024-02-24",status:"complete",count:56,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:14.152 +00:00"},{value:"2024-02-23",status:"complete",count:98,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:29:14.304 +00:00"},{value:"2024-02-22",status:"complete",count:120,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:59.234 +00:00"},{value:"2024-02-21",status:"complete",count:119,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:58.854 +00:00"},{value:"2024-02-20",status:"complete",count:117,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:59.288 +00:00"},{value:"2024-02-19",status:"complete",count:136,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:36.125 +00:00"},{value:"2024-02-18",status:"complete",count:56,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:35.935 +00:00"},{value:"2024-02-17",status:"complete",count:40,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:28:27.593 +00:00"},{value:"2024-02-16",status:"complete",count:118,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:41:20.440 +00:00"},{value:"2024-02-15",status:"complete",count:110,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:41:04.541 +00:00"},{value:"2024-02-14",status:"complete",count:119,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:41:04.833 +00:00"},{value:"2024-02-13",status:"complete",count:98,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:57.376 +00:00"},{value:"2024-02-12",status:"complete",count:98,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:42.351 +00:00"},{value:"2024-02-11",status:"complete",count:42,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:42.185 +00:00"},{value:"2024-02-10",status:"complete",count:39,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:37.061 +00:00"},{value:"2024-02-09",status:"complete",count:79,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:25.435 +00:00"},{value:"2024-02-08",status:"complete",count:111,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:32.416 +00:00"},{value:"2024-02-07",status:"complete",count:100,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:32.234 +00:00"},{value:"2024-02-06",status:"complete",count:129,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:12.584 +00:00"},{value:"2024-02-05",status:"complete",count:128,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:12.431 +00:00"},{value:"2024-02-04",status:"complete",count:67,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:40:06.192 +00:00"},{value:"2024-02-03",status:"complete",count:54,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:39:45.854 +00:00"},{value:"2024-02-02",status:"complete",count:122,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:39:53.891 +00:00"},{value:"2024-02-01",status:"complete",count:101,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:39:53.625 +00:00"}]},{month:"January 2024",dates:[{value:"2024-01-31",status:"complete",count:82,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:39:24.700 +00:00"},{value:"2024-01-30",status:"complete",count:101,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:39:32.590 +00:00"},{value:"2024-01-29",status:"complete",count:110,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:39:32.792 +00:00"},{value:"2024-01-28",status:"complete",count:30,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:33:20.056 +00:00"},{value:"2024-01-27",status:"complete",count:36,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:33:20.099 +00:00"},{value:"2024-01-26",status:"complete",count:57,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:33:09.910 +00:00"},{value:"2024-01-25",status:"complete",count:95,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:33:15.609 +00:00"},{value:"2024-01-24",status:"complete",count:76,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:33:15.374 +00:00"},{value:"2024-01-23",status:"complete",count:88,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:58.731 +00:00"},{value:"2024-01-22",status:"complete",count:103,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:58.892 +00:00"},{value:"2024-01-21",status:"complete",count:34,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:51.046 +00:00"},{value:"2024-01-20",status:"complete",count:36,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:37.255 +00:00"},{value:"2024-01-19",status:"complete",count:65,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:42.812 +00:00"},{value:"2024-01-18",status:"complete",count:78,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:42.951 +00:00"},{value:"2024-01-17",status:"complete",count:61,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:25.260 +00:00"},{value:"2024-01-16",status:"complete",count:70,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:31.110 +00:00"},{value:"2024-01-15",status:"complete",count:61,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:31.022 +00:00"},{value:"2024-01-14",status:"complete",count:30,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:17.524 +00:00"},{value:"2024-01-13",status:"complete",count:36,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:13.067 +00:00"},{value:"2024-01-12",status:"complete",count:75,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:17.657 +00:00"},{value:"2024-01-11",status:"complete",count:66,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:01.505 +00:00"},{value:"2024-01-10",status:"complete",count:76,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:06.985 +00:00"},{value:"2024-01-09",status:"complete",count:68,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:32:07.033 +00:00"},{value:"2024-01-08",status:"complete",count:58,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:35:08.492 +00:00"},{value:"2024-01-07",status:"complete",count:47,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:35:08.399 +00:00"},{value:"2024-01-06",status:"complete",count:45,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:34:58.723 +00:00"},{value:"2024-01-05",status:"complete",count:60,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:35:02.248 +00:00"},{value:"2024-01-04",status:"complete",count:57,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:35:02.161 +00:00"},{value:"2024-01-03",status:"complete",count:56,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:34:49.121 +00:00"},{value:"2024-01-02",status:"complete",count:54,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:34:49.438 +00:00"},{value:"2024-01-01",status:"complete",count:35,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:34:49.113 +00:00"}]},{month:"December 2023",dates:[{value:"2023-12-31",status:"complete",count:37,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 07:34:38.959 +00:00"}]},{month:"August 2023",dates:[{value:"2023-08-28",status:"complete",count:85,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:39.128 +00:00"},{value:"2023-08-26",status:"complete",count:26,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:34.190 +00:00"},{value:"2023-08-10",status:"complete",count:52,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:26.067 +00:00"}]},{month:"July 2023",dates:[{value:"2023-07-20",status:"complete",count:73,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:29.888 +00:00"},{value:"2023-07-19",status:"complete",count:69,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:29.786 +00:00"},{value:"2023-07-18",status:"complete",count:66,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:19.279 +00:00"},{value:"2023-07-14",status:"complete",count:39,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:14.756 +00:00"},{value:"2023-07-13",status:"complete",count:59,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:19.187 +00:00"},{value:"2023-07-12",status:"complete",count:48,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:00.502 +00:00"},{value:"2023-07-11",status:"complete",count:80,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:06.342 +00:00"},{value:"2023-07-05",status:"complete",count:68,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:34:06.249 +00:00"},{value:"2023-07-04",status:"complete",count:70,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:43:29.834 +00:00"}]},{month:"June 2023",dates:[{value:"2023-06-26",status:"complete",count:64,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:43:29.979 +00:00"}]},{month:"May 2023",dates:[{value:"2023-05-18",status:"complete",count:97,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:43:18.102 +00:00"},{value:"2023-05-02",status:"complete",count:62,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:43:17.905 +00:00"}]},{month:"October 2022",dates:[{value:"2022-10-05",status:"complete",count:51,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:43:10.572 +00:00"}]},{month:"August 2022",dates:[{value:"2022-08-05",status:"complete",count:34,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:42:58.920 +00:00"}]},{month:"June 2022",dates:[{value:"2022-06-17",status:"complete",count:58,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:42:59.084 +00:00"}]},{month:"January 2018",dates:[{value:"2018-01-22",status:"complete",count:9,createdAt:"2024-05-31 00:09:48.711 +00:00",updatedAt:"2024-05-31 07:42:54.898 +00:00"}]}],datesRowsAtom=atom([]),openMonthAtom=atom(""),lastOpenMonthAtom=atom(""),setSidebarDataAtom=atom(null,async(a,i,o)=>{var s;i(datesRowsAtom,o),i(openMonthAtom,((s=o[0])==null?void 0:s.month)??"")}),fetchDatesSidebarDataAtom=atom(null,async(a,i)=>{try{const o=dates;console.log("Sidebar dates:",{dateList:o}),i(setSidebarDataAtom,o)}catch(o){console.error("Failed to fetch calendar",o)}}),updateSidebarDataAtom=atom(null,async(a,i,{key:o,count:s,status:$})=>{i(datesRowsAtom,j=>{const _e=dayjs(o).format("MMMM YYYY");return j.map(tt=>tt.month===_e?{...tt,dates:tt.dates.map(nt=>nt.value===o?{...nt,status:$,count:s||void 0}:nt)}:tt)})});function bind(a,i){return function(){return a.apply(i,arguments)}}const{toString}=Object.prototype,{getPrototypeOf}=Object,kindOf=(a=>i=>{const o=toString.call(i);return a[o]||(a[o]=o.slice(8,-1).toLowerCase())})(Object.create(null)),kindOfTest=a=>(a=a.toLowerCase(),i=>kindOf(i)===a),typeOfTest=a=>i=>typeof i===a,{isArray}=Array,isUndefined=typeOfTest("undefined");function isBuffer(a){return a!==null&&!isUndefined(a)&&a.constructor!==null&&!isUndefined(a.constructor)&&isFunction$1(a.constructor.isBuffer)&&a.constructor.isBuffer(a)}const isArrayBuffer$1=kindOfTest("ArrayBuffer");function isArrayBufferView(a){let i;return typeof ArrayBuffer<"u"&&ArrayBuffer.isView?i=ArrayBuffer.isView(a):i=a&&a.buffer&&isArrayBuffer$1(a.buffer),i}const isString$1=typeOfTest("string"),isFunction$1=typeOfTest("function"),isNumber=typeOfTest("number"),isObject=a=>a!==null&&typeof a=="object",isBoolean=a=>a===!0||a===!1,isPlainObject=a=>{if(kindOf(a)!=="object")return!1;const i=getPrototypeOf(a);return(i===null||i===Object.prototype||Object.getPrototypeOf(i)===null)&&!(Symbol.toStringTag in a)&&!(Symbol.iterator in a)},isDate=kindOfTest("Date"),isFile$1=kindOfTest("File"),isBlob$1=kindOfTest("Blob"),isFileList=kindOfTest("FileList"),isStream=a=>isObject(a)&&isFunction$1(a.pipe),isFormData=a=>{let i;return a&&(typeof FormData=="function"&&a instanceof FormData||isFunction$1(a.append)&&((i=kindOf(a))==="formdata"||i==="object"&&isFunction$1(a.toString)&&a.toString()==="[object FormData]"))},isURLSearchParams=kindOfTest("URLSearchParams"),trim=a=>a.trim?a.trim():a.replace(/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,"");function forEach(a,i,{allOwnKeys:o=!1}={}){if(a===null||typeof a>"u")return;let s,$;if(typeof a!="object"&&(a=[a]),isArray(a))for(s=0,$=a.length;s<$;s++)i.call(null,a[s],s,a);else{const j=o?Object.getOwnPropertyNames(a):Object.keys(a),_e=j.length;let et;for(s=0;s<_e;s++)et=j[s],i.call(null,a[et],et,a)}}function findKey(a,i){i=i.toLowerCase();const o=Object.keys(a);let s=o.length,$;for(;s-- >0;)if($=o[s],i===$.toLowerCase())return $;return null}const _global=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:global,isContextDefined=a=>!isUndefined(a)&&a!==_global;function merge(){const{caseless:a}=isContextDefined(this)&&this||{},i={},o=(s,$)=>{const j=a&&findKey(i,$)||$;isPlainObject(i[j])&&isPlainObject(s)?i[j]=merge(i[j],s):isPlainObject(s)?i[j]=merge({},s):isArray(s)?i[j]=s.slice():i[j]=s};for(let s=0,$=arguments.length;s<$;s++)arguments[s]&&forEach(arguments[s],o);return i}const extend=(a,i,o,{allOwnKeys:s}={})=>(forEach(i,($,j)=>{o&&isFunction$1($)?a[j]=bind($,o):a[j]=$},{allOwnKeys:s}),a),stripBOM=a=>(a.charCodeAt(0)===65279&&(a=a.slice(1)),a),inherits=(a,i,o,s)=>{a.prototype=Object.create(i.prototype,s),a.prototype.constructor=a,Object.defineProperty(a,"super",{value:i.prototype}),o&&Object.assign(a.prototype,o)},toFlatObject=(a,i,o,s)=>{let $,j,_e;const et={};if(i=i||{},a==null)return i;do{for($=Object.getOwnPropertyNames(a),j=$.length;j-- >0;)_e=$[j],(!s||s(_e,a,i))&&!et[_e]&&(i[_e]=a[_e],et[_e]=!0);a=o!==!1&&getPrototypeOf(a)}while(a&&(!o||o(a,i))&&a!==Object.prototype);return i},endsWith=(a,i,o)=>{a=String(a),(o===void 0||o>a.length)&&(o=a.length),o-=i.length;const s=a.indexOf(i,o);return s!==-1&&s===o},toArray=a=>{if(!a)return null;if(isArray(a))return a;let i=a.length;if(!isNumber(i))return null;const o=new Array(i);for(;i-- >0;)o[i]=a[i];return o},isTypedArray$1=(a=>i=>a&&i instanceof a)(typeof Uint8Array<"u"&&getPrototypeOf(Uint8Array)),forEachEntry=(a,i)=>{const s=(a&&a[Symbol.iterator]).call(a);let $;for(;($=s.next())&&!$.done;){const j=$.value;i.call(a,j[0],j[1])}},matchAll=(a,i)=>{let o;const s=[];for(;(o=a.exec(i))!==null;)s.push(o);return s},isHTMLForm=kindOfTest("HTMLFormElement"),toCamelCase=a=>a.toLowerCase().replace(/[-_\s]([a-z\d])(\w*)/g,function(o,s,$){return s.toUpperCase()+$}),hasOwnProperty=(({hasOwnProperty:a})=>(i,o)=>a.call(i,o))(Object.prototype),isRegExp=kindOfTest("RegExp"),reduceDescriptors=(a,i)=>{const o=Object.getOwnPropertyDescriptors(a),s={};forEach(o,($,j)=>{let _e;(_e=i($,j,a))!==!1&&(s[j]=_e||$)}),Object.defineProperties(a,s)},freezeMethods=a=>{reduceDescriptors(a,(i,o)=>{if(isFunction$1(a)&&["arguments","caller","callee"].indexOf(o)!==-1)return!1;const s=a[o];if(isFunction$1(s)){if(i.enumerable=!1,"writable"in i){i.writable=!1;return}i.set||(i.set=()=>{throw Error("Can not rewrite read-only method '"+o+"'")})}})},toObjectSet=(a,i)=>{const o={},s=$=>{$.forEach(j=>{o[j]=!0})};return isArray(a)?s(a):s(String(a).split(i)),o},noop$1=()=>{},toFiniteNumber=(a,i)=>(a=+a,Number.isFinite(a)?a:i),ALPHA="abcdefghijklmnopqrstuvwxyz",DIGIT="0123456789",ALPHABET={DIGIT,ALPHA,ALPHA_DIGIT:ALPHA+ALPHA.toUpperCase()+DIGIT},generateString=(a=16,i=ALPHABET.ALPHA_DIGIT)=>{let o="";const{length:s}=i;for(;a--;)o+=i[Math.random()*s|0];return o};function isSpecCompliantForm(a){return!!(a&&isFunction$1(a.append)&&a[Symbol.toStringTag]==="FormData"&&a[Symbol.iterator])}const toJSONObject=a=>{const i=new Array(10),o=(s,$)=>{if(isObject(s)){if(i.indexOf(s)>=0)return;if(!("toJSON"in s)){i[$]=s;const j=isArray(s)?[]:{};return forEach(s,(_e,et)=>{const tt=o(_e,$+1);!isUndefined(tt)&&(j[et]=tt)}),i[$]=void 0,j}}return s};return o(a,0)},isAsyncFn=kindOfTest("AsyncFunction"),isThenable=a=>a&&(isObject(a)||isFunction$1(a))&&isFunction$1(a.then)&&isFunction$1(a.catch),utils$1={isArray,isArrayBuffer:isArrayBuffer$1,isBuffer,isFormData,isArrayBufferView,isString:isString$1,isNumber,isBoolean,isObject,isPlainObject,isUndefined,isDate,isFile:isFile$1,isBlob:isBlob$1,isRegExp,isFunction:isFunction$1,isStream,isURLSearchParams,isTypedArray:isTypedArray$1,isFileList,forEach,merge,extend,trim,stripBOM,inherits,toFlatObject,kindOf,kindOfTest,endsWith,toArray,forEachEntry,matchAll,isHTMLForm,hasOwnProperty,hasOwnProp:hasOwnProperty,reduceDescriptors,freezeMethods,toObjectSet,toCamelCase,noop:noop$1,toFiniteNumber,findKey,global:_global,isContextDefined,ALPHABET,generateString,isSpecCompliantForm,toJSONObject,isAsyncFn,isThenable};function AxiosError(a,i,o,s,$){Error.call(this),Error.captureStackTrace?Error.captureStackTrace(this,this.constructor):this.stack=new Error().stack,this.message=a,this.name="AxiosError",i&&(this.code=i),o&&(this.config=o),s&&(this.request=s),$&&(this.response=$)}utils$1.inherits(AxiosError,Error,{toJSON:function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:utils$1.toJSONObject(this.config),code:this.code,status:this.response&&this.response.status?this.response.status:null}}});const prototype$1=AxiosError.prototype,descriptors={};["ERR_BAD_OPTION_VALUE","ERR_BAD_OPTION","ECONNABORTED","ETIMEDOUT","ERR_NETWORK","ERR_FR_TOO_MANY_REDIRECTS","ERR_DEPRECATED","ERR_BAD_RESPONSE","ERR_BAD_REQUEST","ERR_CANCELED","ERR_NOT_SUPPORT","ERR_INVALID_URL"].forEach(a=>{descriptors[a]={value:a}});Object.defineProperties(AxiosError,descriptors);Object.defineProperty(prototype$1,"isAxiosError",{value:!0});AxiosError.from=(a,i,o,s,$,j)=>{const _e=Object.create(prototype$1);return utils$1.toFlatObject(a,_e,function(tt){return tt!==Error.prototype},et=>et!=="isAxiosError"),AxiosError.call(_e,a.message,i,o,s,$),_e.cause=a,_e.name=a.name,j&&Object.assign(_e,j),_e};const httpAdapter=null;function isVisitable(a){return utils$1.isPlainObject(a)||utils$1.isArray(a)}function removeBrackets(a){return utils$1.endsWith(a,"[]")?a.slice(0,-2):a}function renderKey(a,i,o){return a?a.concat(i).map(function($,j){return $=removeBrackets($),!o&&j?"["+$+"]":$}).join(o?".":""):i}function isFlatArray(a){return utils$1.isArray(a)&&!a.some(isVisitable)}const predicates=utils$1.toFlatObject(utils$1,{},null,function(i){return/^is[A-Z]/.test(i)});function toFormData(a,i,o){if(!utils$1.isObject(a))throw new TypeError("target must be an object");i=i||new FormData,o=utils$1.toFlatObject(o,{metaTokens:!0,dots:!1,indexes:!1},!1,function(rt,ut){return!utils$1.isUndefined(ut[rt])});const s=o.metaTokens,$=o.visitor||at,j=o.dots,_e=o.indexes,tt=(o.Blob||typeof Blob<"u"&&Blob)&&utils$1.isSpecCompliantForm(i);if(!utils$1.isFunction($))throw new TypeError("visitor must be a function");function nt(ct){if(ct===null)return"";if(utils$1.isDate(ct))return ct.toISOString();if(!tt&&utils$1.isBlob(ct))throw new AxiosError("Blob is not supported. Use a Buffer instead.");return utils$1.isArrayBuffer(ct)||utils$1.isTypedArray(ct)?tt&&typeof Blob=="function"?new Blob([ct]):Buffer.from(ct):ct}function at(ct,rt,ut){let ot=ct;if(ct&&!ut&&typeof ct=="object"){if(utils$1.endsWith(rt,"{}"))rt=s?rt:rt.slice(0,-2),ct=JSON.stringify(ct);else if(utils$1.isArray(ct)&&isFlatArray(ct)||(utils$1.isFileList(ct)||utils$1.endsWith(rt,"[]"))&&(ot=utils$1.toArray(ct)))return rt=removeBrackets(rt),ot.forEach(function(pt,mt){!(utils$1.isUndefined(pt)||pt===null)&&i.append(_e===!0?renderKey([rt],mt,j):_e===null?rt:rt+"[]",nt(pt))}),!1}return isVisitable(ct)?!0:(i.append(renderKey(ut,rt,j),nt(ct)),!1)}const it=[],st=Object.assign(predicates,{defaultVisitor:at,convertValue:nt,isVisitable});function lt(ct,rt){if(!utils$1.isUndefined(ct)){if(it.indexOf(ct)!==-1)throw Error("Circular reference detected in "+rt.join("."));it.push(ct),utils$1.forEach(ct,function(ot,dt){(!(utils$1.isUndefined(ot)||ot===null)&&$.call(i,ot,utils$1.isString(dt)?dt.trim():dt,rt,st))===!0&&lt(ot,rt?rt.concat(dt):[dt])}),it.pop()}}if(!utils$1.isObject(a))throw new TypeError("data must be an object");return lt(a),i}function encode$1(a){const i={"!":"%21","'":"%27","(":"%28",")":"%29","~":"%7E","%20":"+","%00":"\0"};return encodeURIComponent(a).replace(/[!'()~]|%20|%00/g,function(s){return i[s]})}function AxiosURLSearchParams(a,i){this._pairs=[],a&&toFormData(a,this,i)}const prototype=AxiosURLSearchParams.prototype;prototype.append=function(i,o){this._pairs.push([i,o])};prototype.toString=function(i){const o=i?function(s){return i.call(this,s,encode$1)}:encode$1;return this._pairs.map(function($){return o($[0])+"="+o($[1])},"").join("&")};function encode(a){return encodeURIComponent(a).replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}function buildURL(a,i,o){if(!i)return a;const s=o&&o.encode||encode,$=o&&o.serialize;let j;if($?j=$(i,o):j=utils$1.isURLSearchParams(i)?i.toString():new AxiosURLSearchParams(i,o).toString(s),j){const _e=a.indexOf("#");_e!==-1&&(a=a.slice(0,_e)),a+=(a.indexOf("?")===-1?"?":"&")+j}return a}class InterceptorManager{constructor(){this.handlers=[]}use(i,o,s){return this.handlers.push({fulfilled:i,rejected:o,synchronous:s?s.synchronous:!1,runWhen:s?s.runWhen:null}),this.handlers.length-1}eject(i){this.handlers[i]&&(this.handlers[i]=null)}clear(){this.handlers&&(this.handlers=[])}forEach(i){utils$1.forEach(this.handlers,function(s){s!==null&&i(s)})}}const transitionalDefaults={silentJSONParsing:!0,forcedJSONParsing:!0,clarifyTimeoutError:!1},URLSearchParams$1=typeof URLSearchParams<"u"?URLSearchParams:AxiosURLSearchParams,FormData$1=typeof FormData<"u"?FormData:null,Blob$1=typeof Blob<"u"?Blob:null,platform$1={isBrowser:!0,classes:{URLSearchParams:URLSearchParams$1,FormData:FormData$1,Blob:Blob$1},protocols:["http","https","file","blob","url","data"]},hasBrowserEnv=typeof window<"u"&&typeof document<"u",hasStandardBrowserEnv=(a=>hasBrowserEnv&&["ReactNative","NativeScript","NS"].indexOf(a)<0)(typeof navigator<"u"&&navigator.product),hasStandardBrowserWebWorkerEnv=typeof WorkerGlobalScope<"u"&&self instanceof WorkerGlobalScope&&typeof self.importScripts=="function",utils=Object.freeze(Object.defineProperty({__proto__:null,hasBrowserEnv,hasStandardBrowserEnv,hasStandardBrowserWebWorkerEnv},Symbol.toStringTag,{value:"Module"})),platform={...utils,...platform$1};function toURLEncodedForm(a,i){return toFormData(a,new platform.classes.URLSearchParams,Object.assign({visitor:function(o,s,$,j){return platform.isNode&&utils$1.isBuffer(o)?(this.append(s,o.toString("base64")),!1):j.defaultVisitor.apply(this,arguments)}},i))}function parsePropPath(a){return utils$1.matchAll(/\w+|\[(\w*)]/g,a).map(i=>i[0]==="[]"?"":i[1]||i[0])}function arrayToObject(a){const i={},o=Object.keys(a);let s;const $=o.length;let j;for(s=0;s<$;s++)j=o[s],i[j]=a[j];return i}function formDataToJSON(a){function i(o,s,$,j){let _e=o[j++];if(_e==="__proto__")return!0;const et=Number.isFinite(+_e),tt=j>=o.length;return _e=!_e&&utils$1.isArray($)?$.length:_e,tt?(utils$1.hasOwnProp($,_e)?$[_e]=[$[_e],s]:$[_e]=s,!et):((!$[_e]||!utils$1.isObject($[_e]))&&($[_e]=[]),i(o,s,$[_e],j)&&utils$1.isArray($[_e])&&($[_e]=arrayToObject($[_e])),!et)}if(utils$1.isFormData(a)&&utils$1.isFunction(a.entries)){const o={};return utils$1.forEachEntry(a,(s,$)=>{i(parsePropPath(s),$,o,0)}),o}return null}function stringifySafely(a,i,o){if(utils$1.isString(a))try{return(i||JSON.parse)(a),utils$1.trim(a)}catch(s){if(s.name!=="SyntaxError")throw s}return(o||JSON.stringify)(a)}const defaults={transitional:transitionalDefaults,adapter:["xhr","http"],transformRequest:[function(i,o){const s=o.getContentType()||"",$=s.indexOf("application/json")>-1,j=utils$1.isObject(i);if(j&&utils$1.isHTMLForm(i)&&(i=new FormData(i)),utils$1.isFormData(i))return $?JSON.stringify(formDataToJSON(i)):i;if(utils$1.isArrayBuffer(i)||utils$1.isBuffer(i)||utils$1.isStream(i)||utils$1.isFile(i)||utils$1.isBlob(i))return i;if(utils$1.isArrayBufferView(i))return i.buffer;if(utils$1.isURLSearchParams(i))return o.setContentType("application/x-www-form-urlencoded;charset=utf-8",!1),i.toString();let et;if(j){if(s.indexOf("application/x-www-form-urlencoded")>-1)return toURLEncodedForm(i,this.formSerializer).toString();if((et=utils$1.isFileList(i))||s.indexOf("multipart/form-data")>-1){const tt=this.env&&this.env.FormData;return toFormData(et?{"files[]":i}:i,tt&&new tt,this.formSerializer)}}return j||$?(o.setContentType("application/json",!1),stringifySafely(i)):i}],transformResponse:[function(i){const o=this.transitional||defaults.transitional,s=o&&o.forcedJSONParsing,$=this.responseType==="json";if(i&&utils$1.isString(i)&&(s&&!this.responseType||$)){const _e=!(o&&o.silentJSONParsing)&&$;try{return JSON.parse(i)}catch(et){if(_e)throw et.name==="SyntaxError"?AxiosError.from(et,AxiosError.ERR_BAD_RESPONSE,this,null,this.response):et}}return i}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,maxBodyLength:-1,env:{FormData:platform.classes.FormData,Blob:platform.classes.Blob},validateStatus:function(i){return i>=200&&i<300},headers:{common:{Accept:"application/json, text/plain, */*","Content-Type":void 0}}};utils$1.forEach(["delete","get","head","post","put","patch"],a=>{defaults.headers[a]={}});const defaults$1=defaults,ignoreDuplicateOf=utils$1.toObjectSet(["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"]),parseHeaders=a=>{const i={};let o,s,$;return a&&a.split(`
`).forEach(function(_e){$=_e.indexOf(":"),o=_e.substring(0,$).trim().toLowerCase(),s=_e.substring($+1).trim(),!(!o||i[o]&&ignoreDuplicateOf[o])&&(o==="set-cookie"?i[o]?i[o].push(s):i[o]=[s]:i[o]=i[o]?i[o]+", "+s:s)}),i},$internals=Symbol("internals");function normalizeHeader(a){return a&&String(a).trim().toLowerCase()}function normalizeValue(a){return a===!1||a==null?a:utils$1.isArray(a)?a.map(normalizeValue):String(a)}function parseTokens(a){const i=Object.create(null),o=/([^\s,;=]+)\s*(?:=\s*([^,;]+))?/g;let s;for(;s=o.exec(a);)i[s[1]]=s[2];return i}const isValidHeaderName=a=>/^[-_a-zA-Z0-9^`|~,!#$%&'*+.]+$/.test(a.trim());function matchHeaderValue(a,i,o,s,$){if(utils$1.isFunction(s))return s.call(this,i,o);if($&&(i=o),!!utils$1.isString(i)){if(utils$1.isString(s))return i.indexOf(s)!==-1;if(utils$1.isRegExp(s))return s.test(i)}}function formatHeader(a){return a.trim().toLowerCase().replace(/([a-z\d])(\w*)/g,(i,o,s)=>o.toUpperCase()+s)}function buildAccessors(a,i){const o=utils$1.toCamelCase(" "+i);["get","set","has"].forEach(s=>{Object.defineProperty(a,s+o,{value:function($,j,_e){return this[s].call(this,i,$,j,_e)},configurable:!0})})}class AxiosHeaders{constructor(i){i&&this.set(i)}set(i,o,s){const $=this;function j(et,tt,nt){const at=normalizeHeader(tt);if(!at)throw new Error("header name must be a non-empty string");const it=utils$1.findKey($,at);(!it||$[it]===void 0||nt===!0||nt===void 0&&$[it]!==!1)&&($[it||tt]=normalizeValue(et))}const _e=(et,tt)=>utils$1.forEach(et,(nt,at)=>j(nt,at,tt));return utils$1.isPlainObject(i)||i instanceof this.constructor?_e(i,o):utils$1.isString(i)&&(i=i.trim())&&!isValidHeaderName(i)?_e(parseHeaders(i),o):i!=null&&j(o,i,s),this}get(i,o){if(i=normalizeHeader(i),i){const s=utils$1.findKey(this,i);if(s){const $=this[s];if(!o)return $;if(o===!0)return parseTokens($);if(utils$1.isFunction(o))return o.call(this,$,s);if(utils$1.isRegExp(o))return o.exec($);throw new TypeError("parser must be boolean|regexp|function")}}}has(i,o){if(i=normalizeHeader(i),i){const s=utils$1.findKey(this,i);return!!(s&&this[s]!==void 0&&(!o||matchHeaderValue(this,this[s],s,o)))}return!1}delete(i,o){const s=this;let $=!1;function j(_e){if(_e=normalizeHeader(_e),_e){const et=utils$1.findKey(s,_e);et&&(!o||matchHeaderValue(s,s[et],et,o))&&(delete s[et],$=!0)}}return utils$1.isArray(i)?i.forEach(j):j(i),$}clear(i){const o=Object.keys(this);let s=o.length,$=!1;for(;s--;){const j=o[s];(!i||matchHeaderValue(this,this[j],j,i,!0))&&(delete this[j],$=!0)}return $}normalize(i){const o=this,s={};return utils$1.forEach(this,($,j)=>{const _e=utils$1.findKey(s,j);if(_e){o[_e]=normalizeValue($),delete o[j];return}const et=i?formatHeader(j):String(j).trim();et!==j&&delete o[j],o[et]=normalizeValue($),s[et]=!0}),this}concat(...i){return this.constructor.concat(this,...i)}toJSON(i){const o=Object.create(null);return utils$1.forEach(this,(s,$)=>{s!=null&&s!==!1&&(o[$]=i&&utils$1.isArray(s)?s.join(", "):s)}),o}[Symbol.iterator](){return Object.entries(this.toJSON())[Symbol.iterator]()}toString(){return Object.entries(this.toJSON()).map(([i,o])=>i+": "+o).join(`
`)}get[Symbol.toStringTag](){return"AxiosHeaders"}static from(i){return i instanceof this?i:new this(i)}static concat(i,...o){const s=new this(i);return o.forEach($=>s.set($)),s}static accessor(i){const s=(this[$internals]=this[$internals]={accessors:{}}).accessors,$=this.prototype;function j(_e){const et=normalizeHeader(_e);s[et]||(buildAccessors($,_e),s[et]=!0)}return utils$1.isArray(i)?i.forEach(j):j(i),this}}AxiosHeaders.accessor(["Content-Type","Content-Length","Accept","Accept-Encoding","User-Agent","Authorization"]);utils$1.reduceDescriptors(AxiosHeaders.prototype,({value:a},i)=>{let o=i[0].toUpperCase()+i.slice(1);return{get:()=>a,set(s){this[o]=s}}});utils$1.freezeMethods(AxiosHeaders);const AxiosHeaders$1=AxiosHeaders;function transformData(a,i){const o=this||defaults$1,s=i||o,$=AxiosHeaders$1.from(s.headers);let j=s.data;return utils$1.forEach(a,function(et){j=et.call(o,j,$.normalize(),i?i.status:void 0)}),$.normalize(),j}function isCancel(a){return!!(a&&a.__CANCEL__)}function CanceledError(a,i,o){AxiosError.call(this,a??"canceled",AxiosError.ERR_CANCELED,i,o),this.name="CanceledError"}utils$1.inherits(CanceledError,AxiosError,{__CANCEL__:!0});function settle(a,i,o){const s=o.config.validateStatus;!o.status||!s||s(o.status)?a(o):i(new AxiosError("Request failed with status code "+o.status,[AxiosError.ERR_BAD_REQUEST,AxiosError.ERR_BAD_RESPONSE][Math.floor(o.status/100)-4],o.config,o.request,o))}const cookies=platform.hasStandardBrowserEnv?{write(a,i,o,s,$,j){const _e=[a+"="+encodeURIComponent(i)];utils$1.isNumber(o)&&_e.push("expires="+new Date(o).toGMTString()),utils$1.isString(s)&&_e.push("path="+s),utils$1.isString($)&&_e.push("domain="+$),j===!0&&_e.push("secure"),document.cookie=_e.join("; ")},read(a){const i=document.cookie.match(new RegExp("(^|;\\s*)("+a+")=([^;]*)"));return i?decodeURIComponent(i[3]):null},remove(a){this.write(a,"",Date.now()-864e5)}}:{write(){},read(){return null},remove(){}};function isAbsoluteURL(a){return/^([a-z][a-z\d+\-.]*:)?\/\//i.test(a)}function combineURLs(a,i){return i?a.replace(/\/?\/$/,"")+"/"+i.replace(/^\/+/,""):a}function buildFullPath(a,i){return a&&!isAbsoluteURL(i)?combineURLs(a,i):i}const isURLSameOrigin=platform.hasStandardBrowserEnv?function(){const i=/(msie|trident)/i.test(navigator.userAgent),o=document.createElement("a");let s;function $(j){let _e=j;return i&&(o.setAttribute("href",_e),_e=o.href),o.setAttribute("href",_e),{href:o.href,protocol:o.protocol?o.protocol.replace(/:$/,""):"",host:o.host,search:o.search?o.search.replace(/^\?/,""):"",hash:o.hash?o.hash.replace(/^#/,""):"",hostname:o.hostname,port:o.port,pathname:o.pathname.charAt(0)==="/"?o.pathname:"/"+o.pathname}}return s=$(window.location.href),function(_e){const et=utils$1.isString(_e)?$(_e):_e;return et.protocol===s.protocol&&et.host===s.host}}():function(){return function(){return!0}}();function parseProtocol(a){const i=/^([-+\w]{1,25})(:?\/\/|:)/.exec(a);return i&&i[1]||""}function speedometer(a,i){a=a||10;const o=new Array(a),s=new Array(a);let $=0,j=0,_e;return i=i!==void 0?i:1e3,function(tt){const nt=Date.now(),at=s[j];_e||(_e=nt),o[$]=tt,s[$]=nt;let it=j,st=0;for(;it!==$;)st+=o[it++],it=it%a;if($=($+1)%a,$===j&&(j=(j+1)%a),nt-_e<i)return;const lt=at&&nt-at;return lt?Math.round(st*1e3/lt):void 0}}function progressEventReducer(a,i){let o=0;const s=speedometer(50,250);return $=>{const j=$.loaded,_e=$.lengthComputable?$.total:void 0,et=j-o,tt=s(et),nt=j<=_e;o=j;const at={loaded:j,total:_e,progress:_e?j/_e:void 0,bytes:et,rate:tt||void 0,estimated:tt&&_e&&nt?(_e-j)/tt:void 0,event:$};at[i?"download":"upload"]=!0,a(at)}}const isXHRAdapterSupported=typeof XMLHttpRequest<"u",xhrAdapter=isXHRAdapterSupported&&function(a){return new Promise(function(o,s){let $=a.data;const j=AxiosHeaders$1.from(a.headers).normalize();let{responseType:_e,withXSRFToken:et}=a,tt;function nt(){a.cancelToken&&a.cancelToken.unsubscribe(tt),a.signal&&a.signal.removeEventListener("abort",tt)}let at;if(utils$1.isFormData($)){if(platform.hasStandardBrowserEnv||platform.hasStandardBrowserWebWorkerEnv)j.setContentType(!1);else if((at=j.getContentType())!==!1){const[rt,...ut]=at?at.split(";").map(ot=>ot.trim()).filter(Boolean):[];j.setContentType([rt||"multipart/form-data",...ut].join("; "))}}let it=new XMLHttpRequest;if(a.auth){const rt=a.auth.username||"",ut=a.auth.password?unescape(encodeURIComponent(a.auth.password)):"";j.set("Authorization","Basic "+btoa(rt+":"+ut))}const st=buildFullPath(a.baseURL,a.url);it.open(a.method.toUpperCase(),buildURL(st,a.params,a.paramsSerializer),!0),it.timeout=a.timeout;function lt(){if(!it)return;const rt=AxiosHeaders$1.from("getAllResponseHeaders"in it&&it.getAllResponseHeaders()),ot={data:!_e||_e==="text"||_e==="json"?it.responseText:it.response,status:it.status,statusText:it.statusText,headers:rt,config:a,request:it};settle(function(pt){o(pt),nt()},function(pt){s(pt),nt()},ot),it=null}if("onloadend"in it?it.onloadend=lt:it.onreadystatechange=function(){!it||it.readyState!==4||it.status===0&&!(it.responseURL&&it.responseURL.indexOf("file:")===0)||setTimeout(lt)},it.onabort=function(){it&&(s(new AxiosError("Request aborted",AxiosError.ECONNABORTED,a,it)),it=null)},it.onerror=function(){s(new AxiosError("Network Error",AxiosError.ERR_NETWORK,a,it)),it=null},it.ontimeout=function(){let ut=a.timeout?"timeout of "+a.timeout+"ms exceeded":"timeout exceeded";const ot=a.transitional||transitionalDefaults;a.timeoutErrorMessage&&(ut=a.timeoutErrorMessage),s(new AxiosError(ut,ot.clarifyTimeoutError?AxiosError.ETIMEDOUT:AxiosError.ECONNABORTED,a,it)),it=null},platform.hasStandardBrowserEnv&&(et&&utils$1.isFunction(et)&&(et=et(a)),et||et!==!1&&isURLSameOrigin(st))){const rt=a.xsrfHeaderName&&a.xsrfCookieName&&cookies.read(a.xsrfCookieName);rt&&j.set(a.xsrfHeaderName,rt)}$===void 0&&j.setContentType(null),"setRequestHeader"in it&&utils$1.forEach(j.toJSON(),function(ut,ot){it.setRequestHeader(ot,ut)}),utils$1.isUndefined(a.withCredentials)||(it.withCredentials=!!a.withCredentials),_e&&_e!=="json"&&(it.responseType=a.responseType),typeof a.onDownloadProgress=="function"&&it.addEventListener("progress",progressEventReducer(a.onDownloadProgress,!0)),typeof a.onUploadProgress=="function"&&it.upload&&it.upload.addEventListener("progress",progressEventReducer(a.onUploadProgress)),(a.cancelToken||a.signal)&&(tt=rt=>{it&&(s(!rt||rt.type?new CanceledError(null,a,it):rt),it.abort(),it=null)},a.cancelToken&&a.cancelToken.subscribe(tt),a.signal&&(a.signal.aborted?tt():a.signal.addEventListener("abort",tt)));const ct=parseProtocol(st);if(ct&&platform.protocols.indexOf(ct)===-1){s(new AxiosError("Unsupported protocol "+ct+":",AxiosError.ERR_BAD_REQUEST,a));return}it.send($||null)})},knownAdapters={http:httpAdapter,xhr:xhrAdapter};utils$1.forEach(knownAdapters,(a,i)=>{if(a){try{Object.defineProperty(a,"name",{value:i})}catch{}Object.defineProperty(a,"adapterName",{value:i})}});const renderReason=a=>`- ${a}`,isResolvedHandle=a=>utils$1.isFunction(a)||a===null||a===!1,adapters={getAdapter:a=>{a=utils$1.isArray(a)?a:[a];const{length:i}=a;let o,s;const $={};for(let j=0;j<i;j++){o=a[j];let _e;if(s=o,!isResolvedHandle(o)&&(s=knownAdapters[(_e=String(o)).toLowerCase()],s===void 0))throw new AxiosError(`Unknown adapter '${_e}'`);if(s)break;$[_e||"#"+j]=s}if(!s){const j=Object.entries($).map(([et,tt])=>`adapter ${et} `+(tt===!1?"is not supported by the environment":"is not available in the build"));let _e=i?j.length>1?`since :
`+j.map(renderReason).join(`
`):" "+renderReason(j[0]):"as no adapter specified";throw new AxiosError("There is no suitable adapter to dispatch the request "+_e,"ERR_NOT_SUPPORT")}return s},adapters:knownAdapters};function throwIfCancellationRequested(a){if(a.cancelToken&&a.cancelToken.throwIfRequested(),a.signal&&a.signal.aborted)throw new CanceledError(null,a)}function dispatchRequest(a){return throwIfCancellationRequested(a),a.headers=AxiosHeaders$1.from(a.headers),a.data=transformData.call(a,a.transformRequest),["post","put","patch"].indexOf(a.method)!==-1&&a.headers.setContentType("application/x-www-form-urlencoded",!1),adapters.getAdapter(a.adapter||defaults$1.adapter)(a).then(function(s){return throwIfCancellationRequested(a),s.data=transformData.call(a,a.transformResponse,s),s.headers=AxiosHeaders$1.from(s.headers),s},function(s){return isCancel(s)||(throwIfCancellationRequested(a),s&&s.response&&(s.response.data=transformData.call(a,a.transformResponse,s.response),s.response.headers=AxiosHeaders$1.from(s.response.headers))),Promise.reject(s)})}const headersToObject=a=>a instanceof AxiosHeaders$1?{...a}:a;function mergeConfig(a,i){i=i||{};const o={};function s(nt,at,it){return utils$1.isPlainObject(nt)&&utils$1.isPlainObject(at)?utils$1.merge.call({caseless:it},nt,at):utils$1.isPlainObject(at)?utils$1.merge({},at):utils$1.isArray(at)?at.slice():at}function $(nt,at,it){if(utils$1.isUndefined(at)){if(!utils$1.isUndefined(nt))return s(void 0,nt,it)}else return s(nt,at,it)}function j(nt,at){if(!utils$1.isUndefined(at))return s(void 0,at)}function _e(nt,at){if(utils$1.isUndefined(at)){if(!utils$1.isUndefined(nt))return s(void 0,nt)}else return s(void 0,at)}function et(nt,at,it){if(it in i)return s(nt,at);if(it in a)return s(void 0,nt)}const tt={url:j,method:j,data:j,baseURL:_e,transformRequest:_e,transformResponse:_e,paramsSerializer:_e,timeout:_e,timeoutMessage:_e,withCredentials:_e,withXSRFToken:_e,adapter:_e,responseType:_e,xsrfCookieName:_e,xsrfHeaderName:_e,onUploadProgress:_e,onDownloadProgress:_e,decompress:_e,maxContentLength:_e,maxBodyLength:_e,beforeRedirect:_e,transport:_e,httpAgent:_e,httpsAgent:_e,cancelToken:_e,socketPath:_e,responseEncoding:_e,validateStatus:et,headers:(nt,at)=>$(headersToObject(nt),headersToObject(at),!0)};return utils$1.forEach(Object.keys(Object.assign({},a,i)),function(at){const it=tt[at]||$,st=it(a[at],i[at],at);utils$1.isUndefined(st)&&it!==et||(o[at]=st)}),o}const VERSION="1.6.8",validators$1={};["object","boolean","number","function","string","symbol"].forEach((a,i)=>{validators$1[a]=function(s){return typeof s===a||"a"+(i<1?"n ":" ")+a}});const deprecatedWarnings={};validators$1.transitional=function(i,o,s){function $(j,_e){return"[Axios v"+VERSION+"] Transitional option '"+j+"'"+_e+(s?". "+s:"")}return(j,_e,et)=>{if(i===!1)throw new AxiosError($(_e," has been removed"+(o?" in "+o:"")),AxiosError.ERR_DEPRECATED);return o&&!deprecatedWarnings[_e]&&(deprecatedWarnings[_e]=!0,console.warn($(_e," has been deprecated since v"+o+" and will be removed in the near future"))),i?i(j,_e,et):!0}};function assertOptions(a,i,o){if(typeof a!="object")throw new AxiosError("options must be an object",AxiosError.ERR_BAD_OPTION_VALUE);const s=Object.keys(a);let $=s.length;for(;$-- >0;){const j=s[$],_e=i[j];if(_e){const et=a[j],tt=et===void 0||_e(et,j,a);if(tt!==!0)throw new AxiosError("option "+j+" must be "+tt,AxiosError.ERR_BAD_OPTION_VALUE);continue}if(o!==!0)throw new AxiosError("Unknown option "+j,AxiosError.ERR_BAD_OPTION)}}const validator={assertOptions,validators:validators$1},validators=validator.validators;class Axios{constructor(i){this.defaults=i,this.interceptors={request:new InterceptorManager,response:new InterceptorManager}}async request(i,o){try{return await this._request(i,o)}catch(s){if(s instanceof Error){let $;Error.captureStackTrace?Error.captureStackTrace($={}):$=new Error;const j=$.stack?$.stack.replace(/^.+\n/,""):"";s.stack?j&&!String(s.stack).endsWith(j.replace(/^.+\n.+\n/,""))&&(s.stack+=`
`+j):s.stack=j}throw s}}_request(i,o){typeof i=="string"?(o=o||{},o.url=i):o=i||{},o=mergeConfig(this.defaults,o);const{transitional:s,paramsSerializer:$,headers:j}=o;s!==void 0&&validator.assertOptions(s,{silentJSONParsing:validators.transitional(validators.boolean),forcedJSONParsing:validators.transitional(validators.boolean),clarifyTimeoutError:validators.transitional(validators.boolean)},!1),$!=null&&(utils$1.isFunction($)?o.paramsSerializer={serialize:$}:validator.assertOptions($,{encode:validators.function,serialize:validators.function},!0)),o.method=(o.method||this.defaults.method||"get").toLowerCase();let _e=j&&utils$1.merge(j.common,j[o.method]);j&&utils$1.forEach(["delete","get","head","post","put","patch","common"],ct=>{delete j[ct]}),o.headers=AxiosHeaders$1.concat(_e,j);const et=[];let tt=!0;this.interceptors.request.forEach(function(rt){typeof rt.runWhen=="function"&&rt.runWhen(o)===!1||(tt=tt&&rt.synchronous,et.unshift(rt.fulfilled,rt.rejected))});const nt=[];this.interceptors.response.forEach(function(rt){nt.push(rt.fulfilled,rt.rejected)});let at,it=0,st;if(!tt){const ct=[dispatchRequest.bind(this),void 0];for(ct.unshift.apply(ct,et),ct.push.apply(ct,nt),st=ct.length,at=Promise.resolve(o);it<st;)at=at.then(ct[it++],ct[it++]);return at}st=et.length;let lt=o;for(it=0;it<st;){const ct=et[it++],rt=et[it++];try{lt=ct(lt)}catch(ut){rt.call(this,ut);break}}try{at=dispatchRequest.call(this,lt)}catch(ct){return Promise.reject(ct)}for(it=0,st=nt.length;it<st;)at=at.then(nt[it++],nt[it++]);return at}getUri(i){i=mergeConfig(this.defaults,i);const o=buildFullPath(i.baseURL,i.url);return buildURL(o,i.params,i.paramsSerializer)}}utils$1.forEach(["delete","get","head","options"],function(i){Axios.prototype[i]=function(o,s){return this.request(mergeConfig(s||{},{method:i,url:o,data:(s||{}).data}))}});utils$1.forEach(["post","put","patch"],function(i){function o(s){return function(j,_e,et){return this.request(mergeConfig(et||{},{method:i,headers:s?{"Content-Type":"multipart/form-data"}:{},url:j,data:_e}))}}Axios.prototype[i]=o(),Axios.prototype[i+"Form"]=o(!0)});const Axios$1=Axios;class CancelToken{constructor(i){if(typeof i!="function")throw new TypeError("executor must be a function.");let o;this.promise=new Promise(function(j){o=j});const s=this;this.promise.then($=>{if(!s._listeners)return;let j=s._listeners.length;for(;j-- >0;)s._listeners[j]($);s._listeners=null}),this.promise.then=$=>{let j;const _e=new Promise(et=>{s.subscribe(et),j=et}).then($);return _e.cancel=function(){s.unsubscribe(j)},_e},i(function(j,_e,et){s.reason||(s.reason=new CanceledError(j,_e,et),o(s.reason))})}throwIfRequested(){if(this.reason)throw this.reason}subscribe(i){if(this.reason){i(this.reason);return}this._listeners?this._listeners.push(i):this._listeners=[i]}unsubscribe(i){if(!this._listeners)return;const o=this._listeners.indexOf(i);o!==-1&&this._listeners.splice(o,1)}static source(){let i;return{token:new CancelToken(function($){i=$}),cancel:i}}}const CancelToken$1=CancelToken;function spread(a){return function(o){return a.apply(null,o)}}function isAxiosError(a){return utils$1.isObject(a)&&a.isAxiosError===!0}const HttpStatusCode={Continue:100,SwitchingProtocols:101,Processing:102,EarlyHints:103,Ok:200,Created:201,Accepted:202,NonAuthoritativeInformation:203,NoContent:204,ResetContent:205,PartialContent:206,MultiStatus:207,AlreadyReported:208,ImUsed:226,MultipleChoices:300,MovedPermanently:301,Found:302,SeeOther:303,NotModified:304,UseProxy:305,Unused:306,TemporaryRedirect:307,PermanentRedirect:308,BadRequest:400,Unauthorized:401,PaymentRequired:402,Forbidden:403,NotFound:404,MethodNotAllowed:405,NotAcceptable:406,ProxyAuthenticationRequired:407,RequestTimeout:408,Conflict:409,Gone:410,LengthRequired:411,PreconditionFailed:412,PayloadTooLarge:413,UriTooLong:414,UnsupportedMediaType:415,RangeNotSatisfiable:416,ExpectationFailed:417,ImATeapot:418,MisdirectedRequest:421,UnprocessableEntity:422,Locked:423,FailedDependency:424,TooEarly:425,UpgradeRequired:426,PreconditionRequired:428,TooManyRequests:429,RequestHeaderFieldsTooLarge:431,UnavailableForLegalReasons:451,InternalServerError:500,NotImplemented:501,BadGateway:502,ServiceUnavailable:503,GatewayTimeout:504,HttpVersionNotSupported:505,VariantAlsoNegotiates:506,InsufficientStorage:507,LoopDetected:508,NotExtended:510,NetworkAuthenticationRequired:511};Object.entries(HttpStatusCode).forEach(([a,i])=>{HttpStatusCode[i]=a});const HttpStatusCode$1=HttpStatusCode;function createInstance(a){const i=new Axios$1(a),o=bind(Axios$1.prototype.request,i);return utils$1.extend(o,Axios$1.prototype,i,{allOwnKeys:!0}),utils$1.extend(o,i,null,{allOwnKeys:!0}),o.create=function($){return createInstance(mergeConfig(a,$))},o}const axios=createInstance(defaults$1);axios.Axios=Axios$1;axios.CanceledError=CanceledError;axios.CancelToken=CancelToken$1;axios.isCancel=isCancel;axios.VERSION=VERSION;axios.toFormData=toFormData;axios.AxiosError=AxiosError;axios.Cancel=axios.CanceledError;axios.all=function(i){return Promise.all(i)};axios.spread=spread;axios.isAxiosError=isAxiosError;axios.mergeConfig=mergeConfig;axios.AxiosHeaders=AxiosHeaders$1;axios.formToJSON=a=>formDataToJSON(utils$1.isHTMLForm(a)?new FormData(a):a);axios.getAdapter=adapters.getAdapter;axios.HttpStatusCode=HttpStatusCode$1;axios.default=axios;const apiUrl="http://localhost:3000",fetchPdf=a=>axios.get(apiUrl+"/fetchPdf/"+a,{responseType:"blob"}),initializeChat=a=>axios.get(apiUrl+"/initializeChat/"+a),getThreads=a=>axios.get(apiUrl+"/getThreads/"+a),getMessages=a=>axios.get(apiUrl+"/getMessages/"+a),resetDateStatus=a=>axios.post(apiUrl+"/reset/"+a),scrapeDate=a=>axios.post(apiUrl+"/scrape/"+a),searchPapers=a=>axios.post(apiUrl+"/searchPapers",{form:a}),onboard=a=>axios.post(apiUrl+"/onboardNewUser",{form:a}),backfillDates=a=>axios.post(apiUrl+"/backfillDates",a),scrapeBatch=a=>axios.post(apiUrl+"/scrapeBatch",a),getBatchDates=a=>axios.post(apiUrl+"/getBatchDates",a),createThread=a=>axios.post(apiUrl+"/createThread",a),branchThread=a=>axios.post(apiUrl+"/branchThread",a),sendMessage=a=>axios.post(apiUrl+"/sendMessage",a),streamResponse=a=>axios.post(apiUrl+"/streamResponse",a);var define_import_meta_env_default={BASE_URL:"/",MODE:"production",DEV:!1,PROD:!0,SSR:!1};const RESET=Symbol((define_import_meta_env_default?"production":void 0)!=="production"?"RESET":""),getCached$1=(a,i,o)=>(i.has(o)?i:i.set(o,a())).get(o),cache1$2=new WeakMap,memo2$1=(a,i,o)=>{const s=getCached$1(()=>new WeakMap,cache1$2,i);return getCached$1(a,s,o)},cacheKeyForEmptyKeyExtractor={},isWritable=a=>!!a.write,isFunction=a=>typeof a=="function";function splitAtom(a,i){return memo2$1(()=>{const o=new WeakMap,s=(_e,et)=>{let tt=o.get(_e);if(tt)return tt;const nt=et&&o.get(et),at=[],it=[];return _e.forEach((st,lt)=>{const ct=i?i(st):lt;it[lt]=ct;const rt=nt&&nt.atomList[nt.keyList.indexOf(ct)];if(rt){at[lt]=rt;return}const ut=dt=>{const pt=dt($),mt=dt(a),ht=s(mt,pt==null?void 0:pt.arr).keyList.indexOf(ct);if(ht<0||ht>=mt.length){const yt=_e[s(_e).keyList.indexOf(ct)];if(yt)return yt;throw new Error("splitAtom: index out of bounds for read")}return mt[ht]},ot=(dt,pt,mt)=>{const ft=dt($),ht=dt(a),bt=s(ht,ft==null?void 0:ft.arr).keyList.indexOf(ct);if(bt<0||bt>=ht.length)throw new Error("splitAtom: index out of bounds for write");const gt=isFunction(mt)?mt(ht[bt]):mt;Object.is(ht[bt],gt)||pt(a,[...ht.slice(0,bt),gt,...ht.slice(bt+1)])};at[lt]=isWritable(a)?atom(ut,ot):atom(ut)}),nt&&nt.keyList.length===it.length&&nt.keyList.every((st,lt)=>st===it[lt])?tt=nt:tt={arr:_e,atomList:at,keyList:it},o.set(_e,tt),tt},$=atom(_e=>{const et=_e($),tt=_e(a);return s(tt,et==null?void 0:et.arr)});(define_import_meta_env_default?"production":void 0)!=="production"&&($.debugPrivate=!0),$.init=void 0;const j=isWritable(a)?atom(_e=>_e($).atomList,(_e,et,tt)=>{switch(tt.type){case"remove":{const nt=_e(j).indexOf(tt.atom);if(nt>=0){const at=_e(a);et(a,[...at.slice(0,nt),...at.slice(nt+1)])}break}case"insert":{const nt=tt.before?_e(j).indexOf(tt.before):_e(j).length;if(nt>=0){const at=_e(a);et(a,[...at.slice(0,nt),tt.value,...at.slice(nt)])}break}case"move":{const nt=_e(j).indexOf(tt.atom),at=tt.before?_e(j).indexOf(tt.before):_e(j).length;if(nt>=0&&at>=0){const it=_e(a);nt<at?et(a,[...it.slice(0,nt),...it.slice(nt+1,at),it[nt],...it.slice(at)]):et(a,[...it.slice(0,at),it[nt],...it.slice(at,nt),...it.slice(nt+1)])}break}}}):atom(_e=>_e($).atomList);return j},a,i||cacheKeyForEmptyKeyExtractor)}const isPromiseLike=a=>typeof(a==null?void 0:a.then)=="function";function createJSONStorage(a=()=>{try{return window.localStorage}catch(o){(define_import_meta_env_default?"production":void 0)!=="production"&&typeof window<"u"&&console.warn(o);return}},i){let o,s;const $={getItem:(j,_e)=>{var et,tt;const nt=it=>{if(it=it||"",o!==it){try{s=JSON.parse(it,i==null?void 0:i.reviver)}catch{return _e}o=it}return s},at=(tt=(et=a())==null?void 0:et.getItem(j))!=null?tt:null;return isPromiseLike(at)?at.then(nt):nt(at)},setItem:(j,_e)=>{var et;return(et=a())==null?void 0:et.setItem(j,JSON.stringify(_e,i==null?void 0:i.replacer))},removeItem:j=>{var _e;return(_e=a())==null?void 0:_e.removeItem(j)}};return typeof window<"u"&&typeof window.addEventListener=="function"&&window.Storage&&($.subscribe=(j,_e,et)=>{if(!(a()instanceof window.Storage))return()=>{};const tt=nt=>{if(nt.storageArea===a()&&nt.key===j){let at;try{at=JSON.parse(nt.newValue||"")}catch{at=et}_e(at)}};return window.addEventListener("storage",tt),()=>{window.removeEventListener("storage",tt)}}),$}const defaultStorage=createJSONStorage();function atomWithStorage(a,i,o=defaultStorage,s){const $=s==null?void 0:s.getOnInit,j=atom($?o.getItem(a,i):i);return(define_import_meta_env_default?"production":void 0)!=="production"&&(j.debugPrivate=!0),j.onMount=et=>{et(o.getItem(a,i));let tt;return o.subscribe&&(tt=o.subscribe(a,et,i)),tt},atom(et=>et(j),(et,tt,nt)=>{const at=typeof nt=="function"?nt(et(j)):nt;return at===RESET?(tt(j,i),o.removeItem(a)):at instanceof Promise?at.then(it=>(tt(j,it),o.setItem(a,it))):(tt(j,at),o.setItem(a,at))})}const calendar=[{date:{value:"2024-05-31",status:"pending",count:null,createdAt:"2024-05-31 04:00:01.010 +00:00",updatedAt:"2024-05-31 13:50:50.309 +00:00"},papers:[]},{date:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:58:28.953 +00:00"},papers:[{id:"2405.19686",date:"2024-05-30",title:`Knowledge Graph Tuning: Real-time Large Language Model Personalization
  based on Human Feedback`,abstract:`  Large language models (LLMs) have demonstrated remarkable proficiency in a
range of natural language processing tasks. Once deployed, LLMs encounter users
with personalized factual knowledge, and such personalized knowledge is
consistently reflected through users' interactions with the LLMs. To enhance
user experience, real-time model personalization is essential, allowing LLMs to
adapt user-specific knowledge based on user feedback during human-LLM
interactions. Existing methods mostly require back-propagation to finetune the
model parameters, which incurs high computational and memory costs. In
addition, these methods suffer from low interpretability, which will cause
unforeseen impacts on model performance during long-term use, where the user's
personalized knowledge is accumulated extensively.To address these challenges,
we propose Knowledge Graph Tuning (KGT), a novel approach that leverages
knowledge graphs (KGs) to personalize LLMs. KGT extracts personalized factual
knowledge triples from users' queries and feedback and optimizes KGs without
modifying the LLM parameters. Our method improves computational and memory
efficiency by avoiding back-propagation and ensures interpretability by making
the KG adjustments comprehensible to humans.Experiments with state-of-the-art
LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves
personalization performance while reducing latency and GPU memory costs.
Ultimately, KGT offers a promising solution of effective, efficient, and
interpretable real-time LLM personalization during user interactions with the
LLMs.
`,authors:"Jingwei Sun; Zhixu Du; Yiran Chen",status:0,relevancy:.6376084555961112,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20309",date:"2024-05-30",title:"Large Language Models Can Self-Improve At Web Agent Tasks",abstract:`  Training models to act as agents that can effectively navigate and perform
actions in a complex environment, such as a web browser, has typically been
challenging due to lack of training data. Large language models (LLMs) have
recently demonstrated some capability to navigate novel environments as agents
in a zero-shot or few-shot fashion, purely guided by natural language
instructions as prompts. Recent research has also demonstrated LLMs have the
capability to exceed their base performance through self-improvement, i.e.
fine-tuning on data generated by the model itself. In this work, we explore the
extent to which LLMs can self-improve their performance as agents in
long-horizon tasks in a complex environment using the WebArena benchmark. In
WebArena, an agent must autonomously navigate and perform actions on web pages
to achieve a specified objective. We explore fine-tuning on three distinct
synthetic training data mixtures and achieve a 31\\% improvement in task
completion rate over the base model on the WebArena benchmark through a
self-improvement procedure. We additionally contribute novel evaluation metrics
for assessing the performance, robustness, capabilities, and quality of
trajectories of our fine-tuned agent models to a greater degree than simple,
aggregate-level benchmark scores currently used to measure self-improvement.
`,authors:"Ajay Patel; Markus Hofmarcher; Claudiu Leoveanu-Condrei; Marius-Constantin Dinu; Chris Callison-Burch; Sepp Hochreiter",status:0,relevancy:.6260053599812707,isStarred:!0,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T07:09:19.690Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19737",date:"2024-05-30",title:`Beyond Imitation: Learning Key Reasoning Steps from Dual
  Chain-of-Thoughts in Reasoning Distillation`,abstract:`  As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts
(CoTs) reasoning abilities, practical resource constraints drive efforts to
distill these capabilities into more compact Smaller Language Models (SLMs). We
find that CoTs consist mainly of simple reasoning forms, with a small
proportion ($\\approx 4.7\\%$) of key reasoning steps that truly impact
conclusions. However, previous distillation methods typically involve
supervised fine-tuning student SLMs only on correct CoTs data produced by
teacher LLMs, resulting in students struggling to learn the key reasoning
steps, instead imitating the teacher's reasoning forms and making errors or
omissions on these steps. To address these issues, drawing an analogy to human
learning, where analyzing mistakes according to correct solutions often reveals
the crucial steps leading to successes or failures, we propose
mistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step
distilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs
learning key reasoning steps rather than mere simple fine-tuning. Firstly, to
expose these crucial steps in CoTs, we design specific prompts to generate dual
CoTs data with similar reasoning paths but divergent conclusions. Then, we
apply the minimum edit distance algorithm on the dual CoTs data to locate these
key steps and optimize the likelihood of these steps. Extensive experiments
validate the effectiveness of EDIT across both in-domain and out-of-domain
benchmark reasoning datasets. Further analysis shows that EDIT can generate
high-quality CoTs with more correct key reasoning steps. Notably, we also
explore how different mistake patterns affect performance and find that EDIT
benefits more from logical errors than from knowledge or mathematical
calculation errors in dual CoTs\\footnote{Code can be found at
\\url{https://github.com/C-W-D/EDIT}}.
`,authors:"Chengwei Dai; Kun Li; Wei Zhou; Songlin Hu",status:0,relevancy:.6101105980546383,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20315",date:"2024-05-30",title:"ANAH: Analytical Annotation of Hallucinations in Large Language Models",abstract:`  Reducing the \`$\\textit{hallucination}$' problem of Large Language Models
(LLMs) is crucial for their wide applications. A comprehensive and fine-grained
measurement of the hallucination is the first key step for the governance of
this issue but is under-explored in the community. Thus, we present
$\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical
$\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative
Question Answering. Each answer sentence in our dataset undergoes rigorous
annotation, involving the retrieval of a reference fragment, the judgment of
the hallucination type, and the correction of hallucinated content. ANAH
consists of ~12k sentence-level annotations for ~4.3k LLM responses covering
over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the
fine granularity of the hallucination annotations, we can quantitatively
confirm that the hallucinations of LLMs progressively accumulate in the answer
and use ANAH to train and evaluate hallucination annotators. We conduct
extensive experiments on studying generative and discriminative annotators and
show that, although current open-source LLMs have difficulties in fine-grained
hallucination annotation, the generative annotator trained with ANAH can
surpass all open-source LLMs and GPT-3.5, obtain performance competitive with
GPT-4, and exhibits better generalization ability on unseen questions.
`,authors:"Ziwei Ji; Yuzhe Gu; Wenwei Zhang; Chengqi Lyu; Dahua Lin; Kai Chen",status:0,relevancy:.6035992069948193,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T07:09:34.075Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20179",date:"2024-05-30",title:`Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning
  CodeLLMs`,abstract:`  Large language models (LLMs) have shown great promise at generating robot
programs from natural language given domain-specific robot application
programming interfaces (APIs). However, the performance gap between proprietary
LLMs and smaller open-weight LLMs remains wide. This raises a question: Can we
fine-tune smaller open-weight LLMs for generating domain-specific robot
programs to close the performance gap with proprietary LLMs? While
Self-Instruct is a promising solution by generating a diverse set of training
data, it cannot verify the correctness of these programs. In contrast, a robot
simulator with a well-defined world can identify execution errors but limits
the diversity of programs that it can verify. In this work, we introduce
Robo-Instruct, which brings the best of both worlds -- it promotes the
diversity of Self-Instruct while providing the correctness of simulator-based
checking. Robo-Instruct introduces RoboSim to synthesize a consistent world
state on the fly by inferring properties relevant to the program being checked,
and simulating actions accordingly. Furthermore, the instructions and programs
generated by Self-Instruct may be subtly inconsistent -- such as the program
missing a step implied by the instruction. Robo-Instruct further addresses this
with InstAlign, an instruction-program alignment procedure that revises the
task instruction to reflect the actual results of the generated program. Given
a few seed task descriptions and the robot APIs, Robo-Instruct is capable of
generating a training dataset using only a small open-weight model. This
dataset can then be used to fine-tune small open-weight language models,
enabling them to match or even exceed the performance of several proprietary
LLMs, such as GPT-3.5-Turbo and Gemini-Pro.
`,authors:"Zichao Hu; Junyi Jessy Li; Arjun Guha; Joydeep Biswas",status:0,relevancy:.5998539735913091,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20163",date:"2024-05-30",title:"Reasoning about concepts with LLMs: Inconsistencies abound",abstract:`  The ability to summarize and organize knowledge into abstract concepts is key
to learning and reasoning. Many industrial applications rely on the consistent
and systematic use of concepts, especially when dealing with decision-critical
knowledge. However, we demonstrate that, when methodically questioned, large
language models (LLMs) often display and demonstrate significant
inconsistencies in their knowledge. Computationally, the basic aspects of the
conceptualization of a given domain can be represented as Is-A hierarchies in a
knowledge graph (KG) or ontology, together with a few properties or axioms that
enable straightforward reasoning. We show that even simple ontologies can be
used to reveal conceptual inconsistencies across several LLMs. We also propose
strategies that domain experts can use to evaluate and improve the coverage of
key domain concepts in LLMs of various sizes. In particular, we have been able
to significantly enhance the performance of LLMs of various sizes with openly
available weights using simple knowledge-graph (KG) based prompting strategies.
`,authors:"Rosario Uceda-Sosa; Karthikeyan Natesan Ramamurthy; Maria Chang; Moninder Singh",status:0,relevancy:.5992772276091419,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19616",date:"2024-05-30",title:"Easy Problems That LLMs Get Wrong",abstract:`  We introduce a comprehensive Linguistic Benchmark designed to evaluate the
limitations of Large Language Models (LLMs) in domains such as logical
reasoning, spatial intelligence, and linguistic understanding, among others.
Through a series of straightforward questions, it uncovers the significant
limitations of well-regarded models to perform tasks that humans manage with
ease. It also highlights the potential of prompt engineering to mitigate some
errors and underscores the necessity for better training methodologies. Our
findings stress the importance of grounding LLMs with human reasoning and
common sense, emphasising the need for human-in-the-loop for enterprise
applications. We hope this work paves the way for future research to enhance
the usefulness and reliability of new models.
`,authors:"Sean Williams; James Huckle",status:0,relevancy:.593759085090133,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19842",date:"2024-05-30",title:`Improve Student's Reasoning Generalizability through Cascading
  Decomposed CoTs Distillation`,abstract:`  Large language models (LLMs) exhibit enhanced reasoning at larger scales,
driving efforts to distill these capabilities into smaller models via
teacher-student learning. Previous works simply fine-tune student models on
teachers' generated Chain-of-Thoughts (CoTs) data. Although these methods
enhance in-domain (IND) reasoning performance, they struggle to generalize to
out-of-domain (OOD) tasks. We believe that the widespread spurious correlations
between questions and answers may lead the model to preset a specific answer
which restricts the diversity and generalizability of its reasoning process. In
this paper, we propose Cascading Decomposed CoTs Distillation (CasCoD) to
address these issues by decomposing the traditional single-step learning
process into two cascaded learning steps. Specifically, by restructuring the
training objectives -- removing the answer from outputs and concatenating the
question with the rationale as input -- CasCoD's two-step learning process
ensures that students focus on learning rationales without interference from
the preset answers, thus improving reasoning generalizability. Extensive
experiments demonstrate the effectiveness of CasCoD on both IND and OOD
benchmark reasoning datasets. Code can be found at
https://github.com/C-W-D/CasCoD.
`,authors:"Chengwei Dai; Kun Li; Wei Zhou; Songlin Hu",status:0,relevancy:.5875590148206132,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19877",date:"2024-05-30",title:`KNOW: A Real-World Ontology for Knowledge Capture with Large Language
  Models`,abstract:`  We present KNOW--the Knowledge Navigator Ontology for the World--the first
ontology designed to capture everyday knowledge to augment large language
models (LLMs) in real-world generative AI use cases such as personal AI
assistants. Our domain is human life, both its everyday concerns and its major
milestones. We have limited the initial scope of the modeled concepts to only
established human universals: spacetime (places, events) plus social (people,
groups, organizations). The inclusion criteria for modeled concepts are
pragmatic, beginning with universality and utility. We compare and contrast
previous work such as Schema.org and Cyc--as well as attempts at a synthesis of
knowledge graphs and language models--noting how LLMs already encode internally
much of the commonsense tacit knowledge that took decades to capture in the Cyc
project. We also make available code-generated software libraries for the 12
most popular programming languages, enabling the direct use of ontology
concepts in software engineering. We emphasize simplicity and developer
experience in promoting AI interoperability.
`,authors:"Arto Bendiken",status:0,relevancy:.5819214765405173,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19988",date:"2024-05-30",title:`Video-Language Critic: Transferable Reward Functions for
  Language-Conditioned Robotics`,abstract:`  Natural language is often the easiest and most convenient modality for humans
to specify tasks for robots. However, learning to ground language to behavior
typically requires impractical amounts of diverse, language-annotated
demonstrations collected on each target robot. In this work, we aim to separate
the problem of what to accomplish from how to accomplish it, as the former can
benefit from substantial amounts of external observation-only data, and only
the latter depends on a specific robot embodiment. To this end, we propose
Video-Language Critic, a reward model that can be trained on readily available
cross-embodiment data using contrastive learning and a temporal ranking
objective, and use it to score behavior traces from a separate reinforcement
learning actor. When trained on Open X-Embodiment data, our reward model
enables 2x more sample-efficient policy training on Meta-World tasks than a
sparse reward only, despite a significant domain gap. Using in-domain data but
in a challenging task generalization setting on Meta-World, we further
demonstrate more sample-efficient training than is possible with prior
language-conditioned reward models that are either trained with binary
classification, use static images, or do not leverage the temporal information
present in video data.
`,authors:"Minttu Alakuijala; Reginald McLean; Isaac Woungang; Nariman Farsad; Samuel Kaski; Pekka Marttinen; Kai Yuan",status:0,relevancy:.5796007891930083,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20175",date:"2024-05-30",title:`InstructionCP: A fast approach to transfer Large Language Models into
  target language`,abstract:`  The rapid development of large language models (LLMs) in recent years has
largely focused on English, resulting in models that respond exclusively in
English. To adapt these models to other languages, continual pre-training (CP)
is often employed, followed by supervised fine-tuning (SFT) to maintain
conversational abilities. However, CP and SFT can reduce a model's ability to
filter harmful content. We propose Instruction Continual Pre-training (InsCP),
which integrates instruction tags into the CP process to prevent loss of
conversational proficiency while acquiring new languages. Our experiments
demonstrate that InsCP retains conversational and Reinforcement Learning from
Human Feedback (RLHF) abilities. Empirical evaluations on language alignment,
reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably,
this approach requires only 0.1 billion tokens of high-quality
instruction-following data, thereby reducing resource consumption.
`,authors:"Kuang-Ming Chen; Hung-yi Lee",status:0,relevancy:.5744896107983334,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20318",date:"2024-05-30",title:"CausalQuest: Collecting Natural Causal Questions for AI Agents",abstract:`  Humans have an innate drive to seek out causality. Whether fuelled by
curiosity or specific goals, we constantly question why things happen, how they
are interconnected, and many other related phenomena. To develop AI agents
capable of addressing this natural human quest for causality, we urgently need
a comprehensive dataset of natural causal questions. Unfortunately, existing
datasets either contain only artificially-crafted questions that do not reflect
real AI usage scenarios or have limited coverage of questions from specific
sources. To address this gap, we present CausalQuest, a dataset of 13,500
naturally occurring questions sourced from social networks, search engines, and
AI assistants. We formalize the definition of causal questions and establish a
taxonomy for finer-grained classification. Through a combined effort of human
annotators and large language models (LLMs), we carefully label the dataset. We
find that 42% of the questions humans ask are indeed causal, with the majority
seeking to understand the causes behind given effects. Using this dataset, we
train efficient classifiers (up to 2.85B parameters) for the binary task of
identifying causal questions, achieving high performance with F1 scores of up
to 0.877. We conclude with a rich set of future research directions that can
build upon our data and models.
`,authors:"Roberto Ceraolo; Dmitrii Kharlapenko; Amélie Reymond; Rada Mihalcea; Mrinmaya Sachan; Bernhard Schölkopf; Zhijing Jin",status:0,relevancy:.5681685308949704,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20189",date:"2024-05-30",title:`Nadine: An LLM-driven Intelligent Social Robot with Affective
  Capabilities and Human-like Memory`,abstract:`  In this work, we describe our approach to developing an intelligent and
robust social robotic system for the Nadine social robot platform. We achieve
this by integrating Large Language Models (LLMs) and skilfully leveraging the
powerful reasoning and instruction-following capabilities of these types of
models to achieve advanced human-like affective and cognitive capabilities.
This approach is novel compared to the current state-of-the-art LLM-based
agents which do not implement human-like long-term memory or sophisticated
emotional appraisal. The naturalness of social robots, consisting of multiple
modules, highly depends on the performance and capabilities of each component
of the system and the seamless integration of the components. We built a social
robot system that enables generating appropriate behaviours through multimodal
input processing, bringing episodic memories accordingly to the recognised
user, and simulating the emotional states of the robot induced by the
interaction with the human partner. In particular, we introduce an LLM-agent
frame for social robots, SoR-ReAct, serving as a core component for the
interaction module in our system. This design has brought forth the advancement
of social robots and aims to increase the quality of human-robot interaction.
`,authors:"Hangyeol Kang; Maher Ben Moussa; Nadia Magnenat-Thalmann",status:0,relevancy:.5658114955122293,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20331",date:"2024-05-30",title:"CoSy: Evaluating Textual Explanations of Neurons",abstract:`  A crucial aspect of understanding the complex nature of Deep Neural Networks
(DNNs) is the ability to explain learned concepts within their latent
representations. While various methods exist to connect neurons to textual
descriptions of human-understandable concepts, evaluating the quality of these
explanation methods presents a major challenge in the field due to a lack of
unified, general-purpose quantitative evaluation. In this work, we introduce
CoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to
evaluate the quality of textual explanations for latent neurons. Given textual
explanations, our proposed framework leverages a generative model conditioned
on textual input to create data points representing the textual explanation.
Then, the neuron's response to these explanation data points is compared with
the response to control data points, providing a quality estimate of the given
explanation. We ensure the reliability of our proposed framework in a series of
meta-evaluation experiments and demonstrate practical value through insights
from benchmarking various concept-based textual explanation methods for
Computer Vision tasks, showing that tested explanation methods significantly
differ in quality.
`,authors:"Laura Kopf; Philine Lou Bommer; Anna Hedström; Sebastian Lapuschkin; Marina M. -C. Höhne; Kirill Bykov",status:0,relevancy:.5552265666100827,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19787",date:"2024-05-30",title:`From Symbolic Tasks to Code Generation: Diversification Yields Better
  Task Performers`,abstract:`  Instruction tuning -- tuning large language models on instruction-output
pairs -- is a promising technique for making models better adapted to the real
world. Yet, the key factors driving the model's capability to understand and
follow instructions not seen during training remain under-explored. Our
investigation begins with a series of synthetic experiments within the
theoretical framework of a Turing-complete algorithm called Markov algorithm,
which allows fine-grained control over the instruction-tuning data.
Generalization and robustness with respect to the training distribution emerge
once a diverse enough set of tasks is provided, even though very few examples
are provided for each task. We extend these initial results to a real-world
application scenario of code generation and find that a more diverse
instruction set, extending beyond code-related tasks, improves the performance
of code generation. Our observations suggest that a more diverse semantic space
for instruction-tuning sets greatly improves the model's ability to follow
instructions and perform tasks.
`,authors:"Dylan Zhang; Justin Wang; Francois Charton",status:0,relevancy:.5487411096183784,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20131",date:"2024-05-30",title:"Language Models Need Inductive Biases to Count Inductively",abstract:`  Counting is a fundamental example of generalization, whether viewed through
the mathematical lens of Peano's axioms defining the natural numbers or the
cognitive science literature for children learning to count. The argument holds
for both cases that learning to count means learning to count infinitely. While
few papers have tried to distill transformer "reasoning" to the simplest case
of counting, investigating length generalization does occur throughout the
literature. In the "train short, test long" paradigm of NLP, length refers to
the training sentence length. In formal language recognition, length refers to
the input sequence length, or the maximum stack size induced by a pushdown
automata. In general problem solving, length refers to the number of hops in a
deductive reasoning chain or the recursion depth. For all cases, counting is
central to task success. And crucially, generalizing counting inductively is
central to success on OOD instances. This work provides extensive empirical
results on training language models to count. We experiment with architectures
ranging from RNNs, Transformers, State-Space Models and RWKV. We present
carefully-designed task formats, auxiliary tasks and positional embeddings to
avoid limitations in generalization with OOD-position and OOD-vocabulary. We
find that while traditional RNNs trivially achieve inductive counting,
Transformers have to rely on positional embeddings to count out-of-domain. As
counting is the basis for many arguments concerning the expressivity of
Transformers, our finding calls for the community to reexamine the application
scope of primitive functions defined in formal characterizations. Finally,
modern RNNs also largely underperform traditional RNNs in generalizing counting
inductively. We discuss how design choices that enable parallelized training of
modern RNNs cause them to lose merits of a recurrent nature.
`,authors:"Yingshan Chang; Yonatan Bisk",status:0,relevancy:.5469786419539899,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20046",date:"2024-05-30",title:`Cross-Training with Multi-View Knowledge Fusion for Heterogenous
  Federated Learning`,abstract:`  Federated learning benefits from cross-training strategies, which enables
models to train on data from distinct sources to improve the generalization
capability. However, the data heterogeneity between sources may lead models to
gradually forget previously acquired knowledge when undergoing cross-training
to adapt to new tasks or data sources. We argue that integrating personalized
and global knowledge to gather information from multiple perspectives could
potentially improve performance. To achieve this goal, this paper presents a
novel approach that enhances federated learning through a cross-training scheme
incorporating multi-view information. Specifically, the proposed method, termed
FedCT, includes three main modules, where the consistency-aware knowledge
broadcasting module aims to optimize model assignment strategies, which
enhances collaborative advantages between clients and achieves an efficient
federated learning process. The multi-view knowledge-guided representation
learning module leverages fused prototypical knowledge from both global and
local views to enhance the preservation of local knowledge before and after
model exchange, as well as to ensure consistency between local and global
knowledge. The mixup-based feature augmentation module aggregates rich
information to further increase the diversity of feature spaces, which enables
the model to better discriminate complex samples. Extensive experiments were
conducted on four datasets in terms of performance comparison, ablation study,
in-depth analysis and case study. The results demonstrated that FedCT
alleviates knowledge forgetting from both local and global views, which enables
it outperform state-of-the-art methods.
`,authors:"Zhuang Qi; Lei Meng; Weihao He; Ruohan Zhang; Yu Wang; Xin Qi; Xiangxu Meng",status:0,relevancy:.5465224352747082,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19740",date:"2024-05-30",title:`PertEval: Unveiling Real Knowledge Capacity of LLMs with
  Knowledge-Invariant Perturbations`,abstract:`  Expert-designed close-ended benchmarks serve as vital tools in assessing the
knowledge capacity of large language models (LLMs). Despite their widespread
use, concerns have mounted regarding their reliability due to limited test
scenarios and an unavoidable risk of data contamination. To rectify this, we
present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge
capacity through knowledge-invariant perturbations. These perturbations employ
human-like restatement techniques to generate on-the-fly test samples from
static benchmarks, meticulously retaining knowledge-critical content while
altering irrelevant details. Our toolkit further includes a suite of transition
analyses that compare performance on raw vs. perturbed test sets to precisely
assess LLMs' genuine knowledge capacity. Six state-of-the-art LLMs are
re-evaluated using PertEval. Results reveal significantly inflated performance
of the LLMs on raw benchmarks, including an absolute 21% overestimation for
GPT-4. Additionally, through a nuanced response pattern analysis, we discover
that PertEval retains LLMs' uncertainty to specious knowledge, potentially
being resolved through rote memorization and leading to inflated performance.
We also find that the detailed transition analyses by PertEval could illuminate
weaknesses in existing LLMs' knowledge mastery and guide the development of
refinement. Given these insights, we posit that PertEval can act as an
essential tool that, when applied alongside any close-ended benchmark, unveils
the true knowledge capacity of LLMs, marking a significant step toward more
trustworthy LLM evaluation.
`,authors:"Jiatong Li; Renjun Hu; Kunzhe Huang; Yan Zhuang; Qi Liu; Mengxiao Zhu; Xing Shi; Wei Lin",status:0,relevancy:.5391528916112052,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19846",date:"2024-05-30",title:`Quest: Query-centric Data Synthesis Approach for Long-context Scaling of
  Large Language Model`,abstract:`  Large language models, initially pre-trained with a limited context length,
can better handle longer texts by continuing training on a corpus with extended
contexts. However, obtaining effective long-context data is challenging due to
the scarcity and uneven distribution of long documents across different
domains. To address this issue, we propose a Query-centric data synthesis
method, abbreviated as Quest. Quest is an interpretable method based on the
observation that documents retrieved by similar queries are relevant but
low-redundant, thus well-suited for synthesizing long-context data. The method
is also scalable and capable of constructing large amounts of long-context
data. Using Quest, we synthesize a long-context dataset up to 128k context
length, significantly outperforming other data synthesis methods on multiple
long-context benchmark datasets. In addition, we further verify that the Quest
method is predictable through scaling law experiments, making it a reliable
solution for advancing long-context models.
`,authors:"Chaochen Gao; Xing Wu; Qi Fu; Songlin Hu",status:0,relevancy:.538267137522946,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19744",date:"2024-05-30",title:`X-Instruction: Aligning Language Model in Low-resource Languages with
  Self-curated Cross-lingual Instructions`,abstract:`  Large language models respond well in high-resource languages like English
but struggle in low-resource languages. It may arise from the lack of
high-quality instruction following data in these languages. Directly
translating English samples into these languages can be a solution but
unreliable, leading to responses with translation errors and lacking
language-specific or cultural knowledge. To address this issue, we propose a
novel method to construct cross-lingual instruction following samples with
instruction in English and response in low-resource languages. Specifically,
the language model first learns to generate appropriate English instructions
according to the natural web texts in other languages as responses. The
candidate cross-lingual instruction tuning samples are further refined and
diversified. We have employed this method to build a large-scale cross-lingual
instruction tuning dataset on 10 languages, namely X-Instruction. The
instruction data built using our method incorporate more language-specific
knowledge compared with the naive translation method. Experimental results have
shown that the response quality of the model tuned on X-Instruction greatly
exceeds the model distilled from a powerful teacher model, reaching or even
surpassing the ones of ChatGPT. In addition, we find that models tuned on
cross-lingual instruction following samples can follow the instruction in the
output language without further tuning.
`,authors:"Chong Li; Wen Yang; Jiajun Zhang; Jinliang Lu; Shaonan Wang; Chengqing Zong",status:0,relevancy:.5372564816328679,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19606",date:"2024-05-30",title:"Relation Modeling and Distillation for Learning with Noisy Labels",abstract:`  Learning with noisy labels has become an effective strategy for enhancing the
robustness of models, which enables models to better tolerate inaccurate data.
Existing methods either focus on optimizing the loss function to mitigate the
interference from noise, or design procedures to detect potential noise and
correct errors. However, their effectiveness is often compromised in
representation learning due to the dilemma where models overfit to noisy
labels. To address this issue, this paper proposes a relation modeling and
distillation framework that models inter-sample relationships via
self-supervised learning and employs knowledge distillation to enhance
understanding of latent associations, which mitigate the impact of noisy
labels. Specifically, the proposed method, termed RMDNet, includes two main
modules, where the relation modeling (RM) module implements the contrastive
learning technique to learn representations of all data, an unsupervised
approach that effectively eliminates the interference of noisy tags on feature
extraction. The relation-guided representation learning (RGRL) module utilizes
inter-sample relation learned from the RM module to calibrate the
representation distribution for noisy samples, which is capable of improving
the generalization of the model in the inference phase. Notably, the proposed
RMDNet is a plug-and-play framework that can integrate multiple methods to its
advantage. Extensive experiments were conducted on two datasets, including
performance comparison, ablation study, in-depth analysis and case study. The
results show that RMDNet can learn discriminative representations for noisy
data, which results in superior performance than the existing methods.
`,authors:"Xiaming Che; Junlin Zhang; Zhuang Qi; Xin Qi",status:0,relevancy:.5236682767321168,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19783",date:"2024-05-30",title:"Instruction-Guided Visual Masking",abstract:`  Instruction following is crucial in contemporary LLM. However, when extended
to multimodal setting, it often suffers from misalignment between specific
textual instruction and targeted local region of an image. To achieve more
accurate and nuanced multimodal instruction following, we introduce
Instruction-guided Visual Masking (IVM), a new versatile visual grounding model
that is compatible with diverse multimodal models, such as LMM and robot model.
By constructing visual masks for instruction-irrelevant regions, IVM-enhanced
multimodal models can effectively focus on task-relevant image regions to
better align with complex instructions. Specifically, we design a visual
masking data generation pipeline and create an IVM-Mix-1M dataset with 1
million image-instruction pairs. We further introduce a new learning technique,
Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training
that prioritizes high-quality data samples. Experimental results on generic
multimodal tasks such as VQA and embodied robotic control demonstrate the
versatility of IVM, which as a plug-and-play tool, significantly boosts the
performance of diverse multimodal models, yielding new state-of-the-art results
across challenging multimodal benchmarks. Code is available at
https://github.com/2toinf/IVM.
`,authors:"Jinliang Zheng; Jianxiong Li; Sijie Cheng; Yinan Zheng; Jiaming Li; Jihao Liu; Yu Liu; Jingjing Liu; Xianyuan Zhan",status:0,relevancy:.5236245343669101,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19946",date:"2024-05-30",title:`Learning to Discuss Strategically: A Case Study on One Night Ultimate
  Werewolf`,abstract:`  Communication is a fundamental aspect of human society, facilitating the
exchange of information and beliefs among people. Despite the advancements in
large language models (LLMs), recent agents built with these often neglect the
control over discussion tactics, which are essential in communication scenarios
and games. As a variant of the famous communication game Werewolf, One Night
Ultimate Werewolf (ONUW) requires players to develop strategic discussion
policies due to the potential role changes that increase the uncertainty and
complexity of the game. In this work, we first present the existence of the
Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with
discussion and one without. The results showcase that the discussion greatly
changes players' utilities by affecting their beliefs, emphasizing the
significance of discussion tactics. Based on the insights obtained from the
analyses, we propose an RL-instructed language agent framework, where a
discussion policy trained by reinforcement learning (RL) is employed to
determine appropriate discussion tactics to adopt. Our experimental results on
several ONUW game settings demonstrate the effectiveness and generalizability
of our proposed framework.
`,authors:"Xuanfa Jin; Ziyan Wang; Yali Du; Meng Fang; Haifeng Zhang; Jun Wang",status:0,relevancy:.5235238922992325,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19909",date:"2024-05-30",title:`Adaptive Advantage-Guided Policy Regularization for Offline
  Reinforcement Learning`,abstract:`  In offline reinforcement learning, the challenge of out-of-distribution (OOD)
is pronounced. To address this, existing methods often constrain the learned
policy through policy regularization. However, these methods often suffer from
the issue of unnecessary conservativeness, hampering policy improvement. This
occurs due to the indiscriminate use of all actions from the behavior policy
that generates the offline dataset as constraints. The problem becomes
particularly noticeable when the quality of the dataset is suboptimal. Thus, we
propose Adaptive Advantage-guided Policy Regularization (A2PR), obtaining
high-advantage actions from an augmented behavior policy combined with VAE to
guide the learned policy. A2PR can select high-advantage actions that differ
from those present in the dataset, while still effectively maintaining
conservatism from OOD actions. This is achieved by harnessing the VAE capacity
to generate samples matching the distribution of the data points. We
theoretically prove that the improvement of the behavior policy is guaranteed.
Besides, it effectively mitigates value overestimation with a bounded
performance gap. Empirically, we conduct a series of experiments on the D4RL
benchmark, where A2PR demonstrates state-of-the-art performance. Furthermore,
experimental results on additional suboptimal mixed datasets reveal that A2PR
exhibits superior performance. Code is available at
https://github.com/ltlhuuu/A2PR.
`,authors:"Tenglong Liu; Yang Li; Yixing Lan; Hao Gao; Wei Pan; Xin Xu",status:0,relevancy:.519748050812327,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19592",date:"2024-05-30",title:"Why Larger Language Models Do In-context Learning Differently?",abstract:`  Large language models (LLM) have emerged as a powerful tool for AI, with the
key ability of in-context learning (ICL), where they can perform well on unseen
tasks based on a brief series of task examples without necessitating any
adjustments to the model parameters. One recent interesting mysterious
observation is that models of different scales may have different ICL
behaviors: larger models tend to be more sensitive to noise in the test
context. This work studies this observation theoretically aiming to improve the
understanding of LLM and ICL. We analyze two stylized settings: (1) linear
regression with one-layer single-head linear transformers and (2) parity
classification with two-layer multiple attention heads transformers (non-linear
data and non-linear model). In both settings, we give closed-form optimal
solutions and find that smaller models emphasize important hidden features
while larger ones cover more hidden features; thus, smaller models are more
robust to noise while larger ones are more easily distracted, leading to
different ICL behaviors. This sheds light on where transformers pay attention
to and how that affects ICL. Preliminary experimental results on large base and
chat models provide positive support for our analysis.
`,authors:"Zhenmei Shi; Junyi Wei; Zhuoyan Xu; Yingyu Liang",status:0,relevancy:.5112408702582242,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19933",date:"2024-05-30",title:"Learning Latent Graph Structures and their Uncertainty",abstract:`  Within a prediction task, Graph Neural Networks (GNNs) use relational
information as an inductive bias to enhance the model's accuracy. As
task-relevant relations might be unknown, graph structure learning approaches
have been proposed to learn them while solving the downstream prediction task.
In this paper, we demonstrate that minimization of a point-prediction loss
function, e.g., the mean absolute error, does not guarantee proper learning of
the latent relational information and its associated uncertainty. Conversely,
we prove that a suitable loss function on the stochastic model outputs
simultaneously grants (i) the unknown adjacency matrix latent distribution and
(ii) optimal performance on the prediction task. Finally, we propose a
sampling-based method that solves this joint learning task. Empirical results
validate our theoretical claims and demonstrate the effectiveness of the
proposed approach.
`,authors:"Alessandro Manenti; Daniele Zambon; Cesare Alippi",status:0,relevancy:.5108126705428825,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19874",date:"2024-05-30",title:"Is In-Context Learning Sufficient for Instruction Following in LLMs?",abstract:`  In-context learning (ICL) allows LLMs to learn from examples without changing
their weights, which is a particularly promising capability for long-context
LLMs that can potentially learn from many examples. Recently, Lin et al. (2024)
proposed URIAL, a method using only three in-context examples to align base
LLMs, achieving non-trivial instruction following performance. In this work, we
show that, while effective, ICL alignment with URIAL still underperforms
compared to instruction fine-tuning on established benchmarks such as MT-Bench
and AlpacaEval 2.0 (LC), especially with more capable base LMs. Unlike for
tasks such as classification, translation, or summarization, adding more ICL
demonstrations for long-context LLMs does not systematically improve
instruction following performance. To address this limitation, we derive a
greedy selection approach for ICL examples that noticeably improves
performance, yet without bridging the gap to instruction fine-tuning. Finally,
we provide a series of ablation studies to better understand the reasons behind
the remaining gap, and we show how some aspects of ICL depart from the existing
knowledge and are specific to the instruction tuning setting. Overall, our work
advances the understanding of ICL as an alignment technique. We provide our
code at https://github.com/tml-epfl/icl-alignment.
`,authors:"Hao Zhao; Maksym Andriushchenko; Francesco Croce; Nicolas Flammarion",status:0,relevancy:.5101523277778419,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19795",date:"2024-05-30",title:"SLM as Guardian: Pioneering AI Safety with Small Language Models",abstract:`  Most prior safety research of large language models (LLMs) has focused on
enhancing the alignment of LLMs to better suit the safety requirements of
humans. However, internalizing such safeguard features into larger models
brought challenges of higher training cost and unintended degradation of
helpfulness. To overcome such challenges, a modular approach employing a
smaller LLM to detect harmful user queries is regarded as a convenient solution
in designing LLM-based system with safety requirements.
  In this paper, we leverage a smaller LLM for both harmful query detection and
safeguard response generation. We introduce our safety requirements and the
taxonomy of harmfulness categories, and then propose a multi-task learning
mechanism fusing the two tasks into a single model. We demonstrate the
effectiveness of our approach, providing on par or surpassing harmful query
detection and safeguard response performance compared to the publicly available
LLMs.
`,authors:"Ohjoon Kwon; Donghyeon Jeon; Nayoung Choi; Gyu-Hwung Cho; Changbong Kim; Hyunwoo Lee; Inho Kang; Sun Kim; Taiwoo Park",status:0,relevancy:.4966200069020835,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19581",date:"2024-05-30",title:`Source Code Foundation Models are Transferable Binary Analysis Knowledge
  Bases`,abstract:`  Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of
binary and source code, aiming to lift binary code to human-readable content
relevant to source code, thereby bridging the binary-source semantic gap.
Recent advancements in uni-modal code model pre-training, particularly in
generative Source Code Foundation Models (SCFMs) and binary understanding
models, have laid the groundwork for transfer learning applicable to HOBRE.
However, existing approaches for HOBRE rely heavily on uni-modal models like
SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in
sub-optimal performance. Inspired by recent progress in large multi-modal
models, we propose that it is possible to harness the strengths of uni-modal
code models from both sides to bridge the semantic gap effectively. In this
paper, we introduce a novel probe-and-recover framework that incorporates a
binary-source encoder-decoder model and black-box LLMs for binary analysis. Our
approach leverages the pre-trained knowledge within SCFMs to synthesize
relevant, symbol-rich code fragments as context. This additional context
enables black-box LLMs to enhance recovery accuracy. We demonstrate significant
improvements in zero-shot binary summarization and binary function name
recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a
GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute
increase in token-level precision and recall for name recovery, respectively.
These results highlight the effectiveness of our approach in automating and
improving binary code analysis.
`,authors:"Zian Su; Xiangzhe Xu; Ziyang Huang; Kaiyuan Zhang; Xiangyu Zhang",status:0,relevancy:.49597674797286095,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19690",date:"2024-05-30",title:`Diffusion Policies creating a Trust Region for Offline Reinforcement
  Learning`,abstract:`  Offline reinforcement learning (RL) leverages pre-collected datasets to train
optimal policies. Diffusion Q-Learning (DQL), introducing diffusion models as a
powerful and expressive policy class, significantly boosts the performance of
offline RL. However, its reliance on iterative denoising sampling to generate
actions slows down both training and inference. While several recent attempts
have tried to accelerate diffusion-QL, the improvement in training and/or
inference speed often results in degraded performance. In this paper, we
introduce a dual policy approach, Diffusion Trusted Q-Learning (DTQL), which
comprises a diffusion policy for pure behavior cloning and a practical one-step
policy. We bridge the two polices by a newly introduced diffusion trust region
loss. The diffusion policy maintains expressiveness, while the trust region
loss directs the one-step policy to explore freely and seek modes within the
region defined by the diffusion policy. DTQL eliminates the need for iterative
denoising sampling during both training and inference, making it remarkably
computationally efficient. We evaluate its effectiveness and algorithmic
characteristics against popular Kullback-Leibler (KL) based distillation
methods in 2D bandit scenarios and gym tasks. We then show that DTQL could not
only outperform other methods on the majority of the D4RL benchmark tasks but
also demonstrate efficiency in training and inference speeds. The PyTorch
implementation will be made available.
`,authors:"Tianyu Chen; Zhendong Wang; Mingyuan Zhou",status:0,relevancy:.4947105804434925,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19736",date:"2024-05-30",title:`Learning Task-relevant Sequence Representations via Intrinsic Dynamics
  Characteristics in Reinforcement Learning`,abstract:`  Learning task-relevant state representations is crucial to solving the
problem of scene generalization in visual deep reinforcement learning. Prior
work typically establishes a self-supervised auxiliary learner, introducing
elements (e.g., rewards and actions) to extract task-relevant state information
from observations through behavioral similarity metrics. However, the methods
often ignore the inherent relationships between the elements (e.g., dynamics
relationships) that are essential for learning accurate representations, and
they are also limited to single-step metrics, which impedes the discrimination
of short-term similar task/behavior information in long-term dynamics
transitions. To solve the issues, we propose an intrinsic dynamic
characteristics-driven sequence representation learning method (DSR) over a
common DRL frame. Concretely, inspired by the fact of state transition in the
underlying system, it constrains the optimization of the encoder via modeling
the dynamics equations related to the state transition, which prompts the
latent encoding information to satisfy the state transition process and thereby
distinguishes state space and noise space. Further, to refine the ability of
encoding similar tasks based on dynamics constraints, DSR also sequentially
models inherent dynamics equation relationships from the perspective of
sequence elements' frequency domain and multi-step prediction. Finally,
experimental results show that DSR has achieved a significant performance boost
in the Distracting DMControl Benchmark, with an average of 78.9% over the
backbone baseline. Further results indicate that it also achieves the best
performance in real-world autonomous driving tasks in the CARLA simulator.
Moreover, the qualitative analysis results of t-SNE visualization validate that
our method possesses superior representation ability on visual tasks.
`,authors:"Dayang Liang; Jinyang Lai; Yunlong Liu",status:0,relevancy:.4940873058207249,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19778",date:"2024-05-30",title:`Enhancing Consistency and Role-Specific Knowledge Capturing by
  Rebuilding Fictional Character's Persona`,abstract:`  With the recent introduction of Assistants API, it is expected that
document-based language models will be actively used in various domains,
especially Role-playing. However, a key challenge lies in utilizing
protagonist's persona: Assistants API often fails to achieve with its search
because the information extraction part is different each time and it often
omits important information such as protagonist's backstory or relationships.
It is hard to maintain a consistent persona simply by using the persona
document as input to the Assistants API. To address the challenge of achieving
stable persona consistency, we propose CharacterGPT, a novel persona
reconstruction framework to alleviate the shortcomings of the Assistants API.
Our method involves Character Persona Training (CPT), an effective persona
rebuilding process that updates the character persona by extracting the
character's traits from given summary of the novel for each character as if the
story in a novel progresses. In our experiments, we ask each character to take
the Big Five Inventory personality test in various settings and analyze the
results. To assess whether it can think outside the box, we let each character
generate short novels. Extensive experiments and human evaluation demonstrate
that CharacterGPT presents new possibilities for role-playing agent research.
`,authors:"Jeiyoon Park; Chanjun Park; Heuiseok Lim",status:0,relevancy:.4936318081392568,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19715",date:"2024-05-30",title:"SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths",abstract:`  Speculative decoding reduces the inference latency of a target large language
model via utilizing a smaller and faster draft model. Its performance depends
on a hyperparameter K -- the candidate length, i.e., the number of candidate
tokens for the target model to verify in each round. However, previous methods
often use simple heuristics to choose K, which may result in sub-optimal
performance. We study the choice of the candidate length K and formulate it as
a Markov Decision Process. We theoretically show that the optimal policy of
this Markov decision process takes the form of a threshold policy, i.e., the
current speculation should stop and be verified when the probability of getting
a rejection exceeds a threshold value. Motivated by this theory, we propose
SpecDec++, an enhanced version of speculative decoding that adaptively
determines the candidate length on the fly. We augment the draft model with a
trained acceptance prediction head to predict the conditional acceptance
probability of the candidate tokens. SpecDec++ will stop the current
speculation when the predicted probability that at least one token gets
rejected exceeds a threshold. We implement SpecDec++ and apply it to the
llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup
on the Alpaca dataset (an additional 7.2% improvement over the baseline
speculative decoding). On the GSM8K and HumanEval datasets, our method achieves
a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement),
respectively.
`,authors:"Kaixuan Huang; Xudong Guo; Mengdi Wang",status:0,relevancy:.49233093933001637,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20081",date:"2024-05-30",title:`NoiseBoost: Alleviating Hallucination with Noise Perturbation for
  Multimodal Large Language Models`,abstract:`  Multimodal large language models (MLLMs) contribute a powerful mechanism to
understanding visual information building on large language models. However,
MLLMs are notorious for suffering from hallucinations, especially when
generating lengthy, detailed descriptions for images. Our analysis reveals that
hallucinations stem from the inherent summarization mechanism of large language
models, leading to excessive dependence on linguistic tokens while neglecting
vision information. In this paper, we propose NoiseBoost, a broadly applicable
and simple method for alleviating hallucinations for MLLMs through the
integration of noise feature perturbations. Noise perturbation acts as a
regularizer, facilitating a balanced distribution of attention weights among
visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently
enhances the performance of MLLMs across common training strategies, including
supervised fine-tuning and reinforcement learning. Further, NoiseBoost
pioneerly enables semi-supervised learning for MLLMs, unleashing the power of
unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves
dense caption accuracy by 8.1% with human evaluation and achieves comparable
results with 50% of the data by mining unlabeled data. Code and models are
available at https://kaiwu5.github.io/noiseboost.
`,authors:"Kai Wu; Boyuan Jiang; Zhengkai Jiang; Qingdong He; Donghao Luo; Shengzhi Wang; Qingwen Liu; Chengjie Wang",status:0,relevancy:.4911574666255476,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20247",date:"2024-05-30",title:"KerasCV and KerasNLP: Vision and Language Power-Ups",abstract:`  We present the Keras domain packages KerasCV and KerasNLP, extensions of the
Keras API for Computer Vision and Natural Language Processing workflows,
capable of running on either JAX, TensorFlow, or PyTorch. These domain packages
are designed to enable fast experimentation, with a focus on ease-of-use and
performance. We adopt a modular, layered design: at the library's lowest level
of abstraction, we provide building blocks for creating models and data
preprocessing pipelines, and at the library's highest level of abstraction, we
provide pretrained \`\`task" models for popular architectures such as Stable
Diffusion, YOLOv8, GPT2, BERT, Mistral, CLIP, Gemma, T5, etc. Task models have
built-in preprocessing, pretrained weights, and can be fine-tuned on raw
inputs. To enable efficient training, we support XLA compilation for all
models, and run all preprocessing via a compiled graph of TensorFlow operations
using the tf.data API. The libraries are fully open-source (Apache 2.0 license)
and available on GitHub.
`,authors:"Matthew Watson; Divyashree Shivakumar Sreepathihalli; Francois Chollet; Martin Gorner; Kiranbir Sodhia; Ramesh Sampath; Tirth Patel; Haifeng Jin; Neel Kovelamudi; Gabriel Rasskin; Samaneh Saadat; Luke Wood; Chen Qian; Jonathan Bischof; Ian Stenbit",status:0,relevancy:.48989663697013364,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19723",date:"2024-05-30",title:`Encoding and Controlling Global Semantics for Long-form Video Question
  Answering`,abstract:`  Seeking answers effectively for long videos is essential to build video
question answering (videoQA) systems. Previous methods adaptively select frames
and regions from long videos to save computations. However, this fails to
reason over the whole sequence of video, leading to sub-optimal performance. To
address this problem, we introduce a state space layer (SSL) into multi-modal
Transformer to efficiently integrate global semantics of the video, which
mitigates the video information loss caused by frame and region selection
modules. Our SSL includes a gating unit to enable controllability over the flow
of global semantics into visual representations. To further enhance the
controllability, we introduce a cross-modal compositional congruence (C^3)
objective to encourage global semantics aligned with the question. To
rigorously evaluate long-form videoQA capacity, we construct two new benchmarks
Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5
minutes and 1.9 hours, respectively. Extensive experiments demonstrate the
superiority of our framework on these new as well as existing datasets.
`,authors:"Thong Thanh Nguyen; Zhiyuan Hu; Xiaobao Wu; Cong-Duy T Nguyen; See-Kiong Ng; Anh Tuan Luu",status:0,relevancy:.48418333482341813,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19697",date:"2024-05-30",title:`Bilevel reinforcement learning via the development of hyper-gradient
  without lower-level convexity`,abstract:`  Bilevel reinforcement learning (RL), which features intertwined two-level
problems, has attracted growing interest recently. The inherent non-convexity
of the lower-level RL problem is, however, to be an impediment to developing
bilevel optimization methods. By employing the fixed point equation associated
with the regularized RL, we characterize the hyper-gradient via fully
first-order information, thus circumventing the assumption of lower-level
convexity. This, remarkably, distinguishes our development of hyper-gradient
from the general AID-based bilevel frameworks since we take advantage of the
specific structure of RL problems. Moreover, we propose both model-based and
model-free bilevel reinforcement learning algorithms, facilitated by access to
the fully first-order hyper-gradient. Both algorithms are provable to enjoy the
convergence rate $\\mathcal{O}(\\epsilon^{-1})$. To the best of our knowledge,
this is the first time that AID-based bilevel RL gets rid of additional
assumptions on the lower-level problem. In addition, numerical experiments
demonstrate that the hyper-gradient indeed serves as an integration of
exploitation and exploration.
`,authors:"Yan Yang; Bin Gao; Ya-xiang Yuan",status:0,relevancy:.4836568828964227,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20132",date:"2024-05-30",title:`LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically
  Generating Metaheuristics`,abstract:`  Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to
understand natural language and generate complex code snippets. This paper
introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA)
framework, leveraging GPT models for the automated generation and refinement of
algorithms. Given a set of criteria and a task definition (the search space),
LLaMEA iteratively generates, mutates and selects algorithms based on
performance metrics and feedback from runtime evaluations. This framework
offers a unique approach to generating optimized algorithms without requiring
extensive prior expertise. We show how this framework can be used to generate
novel black-box metaheuristic optimization algorithms automatically. LLaMEA
generates multiple algorithms that outperform state-of-the-art optimization
algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential
Evolution) on the five dimensional black box optimization benchmark (BBOB). The
results demonstrate the feasibility of the framework and identify future
directions for automated generation and optimization of algorithms via LLMs.
`,authors:"Niki van Stein; Thomas Bäck",status:0,relevancy:.48045015810835356,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19950",date:"2024-05-30",title:"MM-Lego: Modular Biomedical Multimodal Models with Minimal Fine-Tuning",abstract:`  Learning holistic computational representations in physical, chemical or
biological systems requires the ability to process information from different
distributions and modalities within the same model. Thus, the demand for
multimodal machine learning models has sharply risen for modalities that go
beyond vision and language, such as sequences, graphs, time series, or tabular
data. While there are many available multimodal fusion and alignment
approaches, most of them require end-to-end training, scale quadratically with
the number of modalities, cannot handle cases of high modality imbalance in the
training set, or are highly topology-specific, making them too restrictive for
many biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego),
a modular and general-purpose fusion and model merging framework to turn any
set of encoders into a competitive multimodal model with no or minimal
fine-tuning. We achieve this by introducing a wrapper for unimodal encoders
that enforces lightweight dimensionality assumptions between modalities and
harmonises their representations by learning features in the frequency domain
to enable model merging with little signal interference. We show that MM-Lego
1) can be used as a model merging method which achieves competitive performance
with end-to-end fusion models without any fine-tuning, 2) can operate on any
unimodal encoder, and 3) is a model fusion method that, with minimal
fine-tuning, achieves state-of-the-art results on six benchmarked multimodal
biomedical tasks.
`,authors:"Konstantin Hemker; Nikola Simidjievski; Mateja Jamnik",status:0,relevancy:.4798264065767728,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20183",date:"2024-05-30",title:`A Survey Study on the State of the Art of Programming Exercise
  Generation using Large Language Models`,abstract:`  This paper analyzes Large Language Models (LLMs) with regard to their
programming exercise generation capabilities. Through a survey study, we
defined the state of the art, extracted their strengths and weaknesses and
finally proposed an evaluation matrix, helping researchers and educators to
decide which LLM is the best fitting for the programming exercise generation
use case. We also found that multiple LLMs are capable of producing useful
programming exercises. Nevertheless, there exist challenges like the ease with
which LLMs might solve exercises generated by LLMs. This paper contributes to
the ongoing discourse on the integration of LLMs in education.
`,authors:"Eduard Frankford; Ingo Höhn; Clemens Sauerwein; Ruth Breu",status:0,relevancy:.47859261643548645,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20245",date:"2024-05-30",title:`Retrieval Augmented Structured Generation: Business Document Information
  Extraction As Tool Use`,abstract:`  Business Document Information Extraction (BDIE) is the problem of
transforming a blob of unstructured information (raw text, scanned documents,
etc.) into a structured format that downstream systems can parse and use. It
has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition
(LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem,
where the tools are these downstream systems. We then present Retrieval
Augmented Structured Generation (RASG), a novel general framework for BDIE that
achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE
benchmarks.
  The contributions of this paper are threefold: (1) We show, with ablation
benchmarks, that Large Language Models (LLMs) with RASG are already competitive
with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on
BDIE benchmarks. (2) We propose a new metric class for Line Items Recognition,
General Line Items Recognition Metric (GLIRM), that is more aligned with
practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE,
and GriTS. (3) We provide a heuristic algorithm for backcalculating bounding
boxes of predicted line items and tables without the need for vision encoders.
Finally, we claim that, while LMMs might sometimes offer marginal performance
benefits, LLMs + RASG is oftentimes superior given real-world applications and
constraints of BDIE.
`,authors:"Franz Louis Cesista; Rui Aguiar; Jason Kim; Paolo Acilo",status:0,relevancy:.4753594426074407,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19673",date:"2024-05-30",title:`Bridging Model-Based Optimization and Generative Modeling via
  Conservative Fine-Tuning of Diffusion Models`,abstract:`  AI-driven design problems, such as DNA/protein sequence design, are commonly
tackled from two angles: generative modeling, which efficiently captures the
feasible design space (e.g., natural images or biological sequences), and
model-based optimization, which utilizes reward models for extrapolation. To
combine the strengths of both approaches, we adopt a hybrid method that
fine-tunes cutting-edge diffusion models by optimizing reward models through
RL. Although prior work has explored similar avenues, they primarily focus on
scenarios where accurate reward models are accessible. In contrast, we
concentrate on an offline setting where a reward model is unknown, and we must
learn from static offline datasets, a common scenario in scientific domains. In
offline scenarios, existing approaches tend to suffer from overoptimization, as
they may be misled by the reward model in out-of-distribution regions. To
address this, we introduce a conservative fine-tuning approach, BRAID, by
optimizing a conservative reward model, which includes additional penalization
outside of offline data distributions. Through empirical and theoretical
analysis, we demonstrate the capability of our approach to outperform the best
designs in offline data, leveraging the extrapolation capabilities of reward
models while avoiding the generation of invalid designs through pre-trained
diffusion models.
`,authors:"Masatoshi Uehara; Yulai Zhao; Ehsan Hajiramezanali; Gabriele Scalia; Gökcen Eraslan; Avantika Lal; Sergey Levine; Tommaso Biancalani",status:0,relevancy:.4743536369081548,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20274",date:"2024-05-30",title:"ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection",abstract:`  Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion
and diversity due to various shared tasks spanning several languages and fields
and organized via SemEval workshops and Germeval. Nonetheless, a few
shortcomings still need to be addressed, such as the lack of low-resource
language evaluations and the emphasis on sentence-level analysis. To thoroughly
assess ABSA techniques in the context of complete reviews, this research
presents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST).
ROAST seeks to close the gap between sentence-level and text-level ABSA by
identifying every ABSA constituent at the review level. We extend the available
datasets to enable ROAST, addressing the drawbacks noted in previous research
by incorporating low-resource languages, numerous languages, and a variety of
topics. Through this effort, ABSA research will be able to cover more ground
and get a deeper comprehension of the task and its practical application in a
variety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).
`,authors:"Siva Uday Sampreeth Chebolu; Franck Dernoncourt; Nedim Lipka; Thamar Solorio",status:0,relevancy:.4727940839780025,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19694",date:"2024-05-30",title:`Grade Like a Human: Rethinking Automated Assessment with Large Language
  Models`,abstract:`  While large language models (LLMs) have been used for automated grading, they
have not yet achieved the same level of performance as humans, especially when
it comes to grading complex questions. Existing research on this topic focuses
on a particular step in the grading procedure: grading using predefined
rubrics. However, grading is a multifaceted procedure that encompasses other
crucial steps, such as grading rubrics design and post-grading review. There
has been a lack of systematic research exploring the potential of LLMs to
enhance the entire grading~process.
  In this paper, we propose an LLM-based grading system that addresses the
entire grading procedure, including the following key components: 1) Developing
grading rubrics that not only consider the questions but also the student
answers, which can more accurately reflect students' performance. 2) Under the
guidance of grading rubrics, providing accurate and consistent scores for each
student, along with customized feedback. 3) Conducting post-grading review to
better ensure accuracy and fairness. Additionally, we collected a new dataset
named OS from a university operating system course and conducted extensive
experiments on both our new dataset and the widely used Mohler dataset.
Experiments demonstrate the effectiveness of our proposed approach, providing
some new insights for developing automated grading systems based on LLMs.
`,authors:"Wenjing Xie; Juxin Niu; Chun Jason Xue; Nan Guan",status:0,relevancy:.46883618289192464,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19708",date:"2024-05-30",title:"Text Guided Image Editing with Automatic Concept Locating and Forgetting",abstract:`  With the advancement of image-to-image diffusion models guided by text,
significant progress has been made in image editing. However, a persistent
challenge remains in seamlessly incorporating objects into images based on
textual instructions, without relying on extra user-provided guidance. Text and
images are inherently distinct modalities, bringing out difficulties in fully
capturing the semantic intent conveyed through language and accurately
translating that into the desired visual modifications. Therefore, text-guided
image editing models often produce generations with residual object attributes
that do not fully align with human expectations. To address this challenge, the
models should comprehend the image content effectively away from a disconnect
between the provided textual editing prompts and the actual modifications made
to the image. In our paper, we propose a novel method called Locate and Forget
(LaF), which effectively locates potential target concepts in the image for
modification by comparing the syntactic trees of the target prompt and scene
descriptions in the input image, intending to forget their existence clues in
the generated image. Compared to the baselines, our method demonstrates its
superiority in text-guided image editing tasks both qualitatively and
quantitatively.
`,authors:"Jia Li; Lijie Hu; Zhixian He; Jingfeng Zhang; Tianhang Zheng; Di Wang",status:0,relevancy:.4677283584526706,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19796",date:"2024-05-30",title:"Explainable Attribute-Based Speaker Verification",abstract:`  This paper proposes a fully explainable approach to speaker verification
(SV), a task that fundamentally relies on individual speaker characteristics.
The opaque use of speaker attributes in current SV systems raises concerns of
trust. Addressing this, we propose an attribute-based explainable SV system
that identifies speakers by comparing personal attributes such as gender,
nationality, and age extracted automatically from voice recordings. We believe
this approach better aligns with human reasoning, making it more understandable
than traditional methods. Evaluated on the Voxceleb1 test set, the best
performance of our system is comparable with the ground truth established when
using all correct attributes, proving its efficacy. Whilst our approach
sacrifices some performance compared to non-explainable methods, we believe
that it moves us closer to the goal of transparent, interpretable AI and lays
the groundwork for future enhancements through attribute expansion.
`,authors:"Xiaoliang Wu; Chau Luu; Peter Bell; Ajitha Rajan",status:0,relevancy:.45955810526014285,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20216",date:"2024-05-30",title:`Boost Your Own Human Image Generation Model via Direct Preference
  Optimization with AI Feedback`,abstract:`  The generation of high-quality human images through text-to-image (T2I)
methods is a significant yet challenging task. Distinct from general image
generation, human image synthesis must satisfy stringent criteria related to
human pose, anatomy, and alignment with textual prompts, making it particularly
difficult to achieve realistic results. Recent advancements in T2I generation
based on diffusion models have shown promise, yet challenges remain in meeting
human-specific preferences. In this paper, we introduce a novel approach
tailored specifically for human image generation utilizing Direct Preference
Optimization (DPO). Specifically, we introduce an efficient method for
constructing a specialized DPO dataset for training human image generation
models without the need for costly human feedback. We also propose a modified
loss function that enhances the DPO training process by minimizing artifacts
and improving image fidelity. Our method demonstrates its versatility and
effectiveness in generating human images, including personalized text-to-image
generation. Through comprehensive evaluations, we show that our approach
significantly advances the state of human image generation, achieving superior
results in terms of natural anatomies, poses, and text-image alignment.
`,authors:"Sanghyeon Na; Yonggyu Kim; Hyunjoon Lee",status:0,relevancy:.4589247821765172,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20003",date:"2024-05-30",title:`Kernel Language Entropy: Fine-grained Uncertainty Quantification for
  LLMs from Semantic Similarities`,abstract:`  Uncertainty quantification in Large Language Models (LLMs) is crucial for
applications where safety and reliability are important. In particular,
uncertainty can be used to improve the trustworthiness of LLMs by detecting
factually incorrect model responses, commonly called hallucinations.
Critically, one should seek to capture the model's semantic uncertainty, i.e.,
the uncertainty over the meanings of LLM outputs, rather than uncertainty over
lexical or syntactic variations that do not affect answer correctness. To
address this problem, we propose Kernel Language Entropy (KLE), a novel method
for uncertainty estimation in white- and black-box LLMs. KLE defines positive
semidefinite unit trace kernels to encode the semantic similarities of LLM
outputs and quantifies uncertainty using the von Neumann entropy. It considers
pairwise semantic dependencies between answers (or semantic clusters),
providing more fine-grained uncertainty estimates than previous methods based
on hard clustering of answers. We theoretically prove that KLE generalizes the
previous state-of-the-art method called semantic entropy and empirically
demonstrate that it improves uncertainty quantification performance across
multiple natural language generation datasets and LLM architectures.
`,authors:"Alexander Nikitin; Jannik Kossen; Yarin Gal; Pekka Marttinen",status:0,relevancy:.4559010398248027,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19815",date:"2024-05-30",title:`Efficient Stimuli Generation using Reinforcement Learning in Design
  Verification`,abstract:`  The increasing design complexity of System-on-Chips (SoCs) has led to
significant verification challenges, particularly in meeting coverage targets
within a timely manner. At present, coverage closure is heavily dependent on
constrained random and coverage driven verification methodologies where the
randomized stimuli are bounded to verify certain scenarios and to reach
coverage goals. This process is said to be exhaustive and to consume a lot of
project time. In this paper, a novel methodology is proposed to generate
efficient stimuli with the help of Reinforcement Learning (RL) to reach the
maximum code coverage of the Design Under Verification (DUV). Additionally, an
automated framework is created using metamodeling to generate a SystemVerilog
testbench and an RL environment for any given design. The proposed approach is
applied to various designs and the produced results proves that the RL agent
provides effective stimuli to achieve code coverage faster in comparison with
baseline random simulations. Furthermore, various RL agents and reward schemes
are analyzed in our work.
`,authors:"Deepak Narayan Gadde; Thomas Nalapat; Aman Kumar; Djones Lettnin; Wolfgang Kunz; Sebastian Simon",status:0,relevancy:.44928077451128423,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19654",date:"2024-05-30",title:`Unlocking the Power of Spatial and Temporal Information in Medical
  Multimodal Pre-training`,abstract:`  Medical vision-language pre-training methods mainly leverage the
correspondence between paired medical images and radiological reports. Although
multi-view spatial images and temporal sequences of image-report pairs are
available in off-the-shelf multi-modal medical datasets, most existing methods
have not thoroughly tapped into such extensive supervision signals. In this
paper, we introduce the Med-ST framework for fine-grained spatial and temporal
modeling to exploit information from multiple spatial views of chest
radiographs and temporal historical records. For spatial modeling, Med-ST
employs the Mixture of View Expert (MoVE) architecture to integrate different
visual features from both frontal and lateral views. To achieve a more
comprehensive alignment, Med-ST not only establishes the global alignment
between whole images and texts but also introduces modality-weighted local
alignment between text tokens and spatial regions of images. For temporal
modeling, we propose a novel cross-modal bidirectional cycle consistency
objective by forward mapping classification (FMC) and reverse mapping
regression (RMR). By perceiving temporal information from simple to complex,
Med-ST can learn temporal semantics. Experimental results across four distinct
tasks demonstrate the effectiveness of Med-ST, especially in temporal
classification tasks. Our code and model are available at
https://github.com/SVT-Yang/MedST.
`,authors:"Jinxia Yang; Bing Su; Wayne Xin Zhao; Ji-Rong Wen",status:0,relevancy:.448925720240238,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19931",date:"2024-05-30",title:`Exploring Diffusion Models' Corruption Stage in Few-Shot Fine-tuning and
  Mitigating with Bayesian Neural Networks`,abstract:`  Few-shot fine-tuning of Diffusion Models (DMs) is a key advancement,
significantly reducing training costs and enabling personalized AI
applications. However, we explore the training dynamics of DMs and observe an
unanticipated phenomenon: during the training process, image fidelity initially
improves, then unexpectedly deteriorates with the emergence of noisy patterns,
only to recover later with severe overfitting. We term the stage with generated
noisy patterns as corruption stage. To understand this corruption stage, we
begin by theoretically modeling the one-shot fine-tuning scenario, and then
extend this modeling to more general cases. Through this modeling, we identify
the primary cause of this corruption stage: a narrowed learning distribution
inherent in the nature of few-shot fine-tuning. To tackle this, we apply
Bayesian Neural Networks (BNNs) on DMs with variational inference to implicitly
broaden the learned distribution, and present that the learning target of the
BNNs can be naturally regarded as an expectation of the diffusion loss and a
further regularization with the pretrained DMs. This approach is highly
compatible with current few-shot fine-tuning methods in DMs and does not
introduce any extra inference costs. Experimental results demonstrate that our
method significantly mitigates corruption, and improves the fidelity, quality
and diversity of the generated images in both object-driven and subject-driven
generation tasks.
`,authors:"Xiaoyu Wu; Jiaru Zhang; Yang Hua; Bohan Lyu; Hao Wang; Tao Song; Haibing Guan",status:0,relevancy:.44860151310205854,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20213",date:"2024-05-30",title:`PostDoc: Generating Poster from a Long Multimodal Document Using Deep
  Submodular Optimization`,abstract:`  A poster from a long input document can be considered as a one-page
easy-to-read multimodal (text and images) summary presented on a nice template
with good design elements. Automatic transformation of a long document into a
poster is a very less studied but challenging task. It involves content
summarization of the input document followed by template generation and
harmonization. In this work, we propose a novel deep submodular function which
can be trained on ground truth summaries to extract multimodal content from the
document and explicitly ensures good coverage, diversity and alignment of text
and images. Then, we use an LLM based paraphraser and propose to generate a
template with various design aspects conditioned on the input content. We show
the merits of our approach through extensive automated and human evaluations.
`,authors:"Vijay Jaisankar; Sambaran Bandyopadhyay; Kalp Vyas; Varre Chaitanya; Shwetha Somasundaram",status:0,relevancy:.44536321945538926,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19958",date:"2024-05-30",title:`Multi-Aspect Controllable Text Generation with Disentangled
  Counterfactual Augmentation`,abstract:`  Multi-aspect controllable text generation aims to control the generated texts
in attributes from multiple aspects (e.g., "positive" from sentiment and
"sport" from topic). For ease of obtaining training samples, existing works
neglect attribute correlations formed by the intertwining of different
attributes. Particularly, the stereotype formed by imbalanced attribute
correlations significantly affects multi-aspect control. In this paper, we
propose MAGIC, a new multi-aspect controllable text generation method with
disentangled counterfactual augmentation. We alleviate the issue of imbalanced
attribute correlations during training using counterfactual feature vectors in
the attribute latent space by disentanglement. During inference, we enhance
attribute correlations by target-guided counterfactual augmentation to further
improve multi-aspect control. Experiments show that MAGIC outperforms
state-of-the-art baselines in both imbalanced and balanced attribute
correlation scenarios. Our source code and data are available at
https://github.com/nju-websoft/MAGIC.
`,authors:"Yi Liu; Xiangyu Liu; Xiangrong Zhu; Wei Hu",status:0,relevancy:.4448100532707844,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20202",date:"2024-05-30",title:`One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient
  Deployments`,abstract:`  Large Language Models (LLMs) have advanced rapidly but face significant
memory demands. While quantization has shown promise for LLMs, current methods
typically require lengthy training to alleviate the performance degradation
from quantization loss. However, deploying LLMs across diverse scenarios with
different resource constraints, e.g., servers and personal computers, requires
repeated training per application, which amplifies the lengthy training
problem. Given that, it is advantageous to train a once-for-all (OFA) supernet
capable of yielding diverse optimal subnets for downstream applications through
one-shot training. Nonetheless, the scale of current language models impedes
efficiency and amplifies interference from weight sharing between subnets. We
make an initial attempt to extend the once-for-all framework to large language
models. Specifically, we decouple shared weights to eliminate the interference
and incorporate Low-Rank adapters for training efficiency. Furthermore, we
observe the imbalance allocation of training resources from the traditional
uniform sampling. A non-parametric scheduler is introduced to adjust the
sampling rate for each quantization configuration, achieving a more balanced
allocation among subnets with varying demands. We validate the approach on
LLaMA2 families, and downstream evaluation confirms our ability to maintain
high performance while significantly reducing deployment time faced with
multiple scenarios.
`,authors:"Ke Yi; Yuhui Xu; Heng Chang; Chen Tang; Yuan Meng; Tong Zhang; Jia Li",status:0,relevancy:.443071356784724,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19816",date:"2024-05-30",title:`Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them
  Optimally`,abstract:`  Machine learning tasks are generally formulated as optimization problems,
where one searches for an optimal function within a certain functional space.
In practice, parameterized functional spaces are considered, in order to be
able to perform gradient descent. Typically, a neural network architecture is
chosen and fixed, and its parameters (connection weights) are optimized,
yielding an architecture-dependent result. This way of proceeding however
forces the evolution of the function during training to lie within the realm of
what is expressible with the chosen architecture, and prevents any optimization
across architectures. Costly architectural hyper-parameter optimization is
often performed to compensate for this. Instead, we propose to adapt the
architecture on the fly during training. We show that the information about
desirable architectural changes, due to expressivity bottlenecks when
attempting to follow the functional gradient, can be extracted from %the
backpropagation. To do this, we propose a mathematical definition of
expressivity bottlenecks, which enables us to detect, quantify and solve them
while training, by adding suitable neurons when and where needed. Thus, while
the standard approach requires large networks, in terms of number of neurons
per layer, for expressivity and optimization reasons, we are able to start with
very small neural networks and let them grow appropriately. As a proof of
concept, we show results~on the CIFAR dataset, matching large neural network
accuracy, with competitive training time, while removing the need for standard
architectural hyper-parameter search.
`,authors:"Manon Verbockhaven; Sylvain Chevallier; Guillaume Charpiat",status:0,relevancy:.44195345501328087,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19982",date:"2024-05-30",title:`A Deep Reinforcement Learning Approach for Trading Optimization in the
  Forex Market with Multi-Agent Asynchronous Distribution`,abstract:`  In today's forex market traders increasingly turn to algorithmic trading,
leveraging computers to seek more profits. Deep learning techniques as
cutting-edge advancements in machine learning, capable of identifying patterns
in financial data. Traders utilize these patterns to execute more effective
trades, adhering to algorithmic trading rules. Deep reinforcement learning
methods (DRL), by directly executing trades based on identified patterns and
assessing their profitability, offer advantages over traditional DL approaches.
This research pioneers the application of a multi-agent (MA) RL framework with
the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) algorithm. The
proposed method employs parallel learning across multiple asynchronous workers,
each specialized in trading across multiple currency pairs to explore the
potential for nuanced strategies tailored to different market conditions and
currency pairs. Two different A3C with lock and without lock MA model was
proposed and trained on single currency and multi-currency. The results
indicate that both model outperform on Proximal Policy Optimization model. A3C
with lock outperforms other in single currency training scenario and A3C
without Lock outperforms other in multi-currency scenario. The findings
demonstrate that this approach facilitates broader and faster exploration of
different currency pairs, significantly enhancing trading returns.
Additionally, the agent can learn a more profitable trading strategy in a
shorter time.
`,authors:"Davoud Sarani; Dr. Parviz Rashidi-Khazaee",status:0,relevancy:.4410347837315911,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19837",date:"2024-05-30",title:`Lifelong learning challenges in the era of artificial intelligence: a
  computational thinking perspective`,abstract:`  The rapid advancement of artificial intelligence (AI) has brought significant
challenges to the education and workforce skills required to take advantage of
AI for human-AI collaboration in the workplace. As AI continues to reshape
industries and job markets, the need to define how AI literacy can be
considered in lifelong learning has become increasingly critical (Cetindamar et
al., 2022; Laupichler et al., 2022; Romero et al., 2023). Like any new
technology, AI is the subject of both hopes and fears, and what it entails
today presents major challenges (Cugurullo \\& Acheampong, 2023; Villani et al.,
2018). It also raises profound questions about our own humanity. Will the
machine surpass the intelligence of the humans who designed it? What will be
the relationship between so-called AI and our human intelligences? How could
human-AI collaboration be regulated in a way that serves the Sustainable
Development Goals (SDGs)? This paper provides a review of the challenges of
lifelong learning in the era of AI from a computational thinking, critical
thinking, and creative competencies perspective, highlighting the implications
for management and leadership in organizations.
`,authors:"Margarida Romero",status:0,relevancy:.43820782155057236,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19861",date:"2024-05-30",title:"Hierarchical Object-Centric Learning with Capsule Networks",abstract:`  Capsule networks (CapsNets) were introduced to address convolutional neural
networks limitations, learning object-centric representations that are more
robust, pose-aware, and interpretable. They organize neurons into groups called
capsules, where each capsule encodes the instantiation parameters of an object
or one of its parts. Moreover, a routing algorithm connects capsules in
different layers, thereby capturing hierarchical part-whole relationships in
the data.
  This thesis investigates the intriguing aspects of CapsNets and focuses on
three key questions to unlock their full potential. First, we explore the
effectiveness of the routing algorithm, particularly in small-sized networks.
We propose a novel method that anneals the number of routing iterations during
training, enhancing performance in architectures with fewer parameters.
  Secondly, we investigate methods to extract more effective first-layer
capsules, also known as primary capsules. By exploiting pruned backbones, we
aim to improve computational efficiency by reducing the number of capsules
while achieving high generalization. This approach reduces CapsNets memory
requirements and computational effort.
  Third, we explore part-relationship learning in CapsNets. Through extensive
research, we demonstrate that capsules with low entropy can extract more
concise and discriminative part-whole relationships compared to traditional
capsule networks, even with reasonable network sizes.
  Lastly, we showcase how CapsNets can be utilized in real-world applications,
including autonomous localization of unmanned aerial vehicles, quaternion-based
rotations prediction in synthetic datasets, and lung nodule segmentation in
biomedical imaging.
  The findings presented in this thesis contribute to a deeper understanding of
CapsNets and highlight their potential to address complex computer vision
challenges.
`,authors:"Riccardo Renzulli",status:0,relevancy:.4375315160940294,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19850",date:"2024-05-30",title:`Deciphering Human Mobility: Inferring Semantics of Trajectories with
  Large Language Models`,abstract:`  Understanding human mobility patterns is essential for various applications,
from urban planning to public safety. The individual trajectory such as mobile
phone location data, while rich in spatio-temporal information, often lacks
semantic detail, limiting its utility for in-depth mobility analysis. Existing
methods can infer basic routine activity sequences from this data, lacking
depth in understanding complex human behaviors and users' characteristics.
Additionally, they struggle with the dependency on hard-to-obtain auxiliary
datasets like travel surveys. To address these limitations, this paper defines
trajectory semantic inference through three key dimensions: user occupation
category, activity sequence, and trajectory description, and proposes the
Trajectory Semantic Inference with Large Language Models (TSI-LLM) framework to
leverage LLMs infer trajectory semantics comprehensively and deeply. We adopt
spatio-temporal attributes enhanced data formatting (STFormat) and design a
context-inclusive prompt, enabling LLMs to more effectively interpret and infer
the semantics of trajectory data. Experimental validation on real-world
trajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex
human mobility patterns. This study explores the potential of LLMs in enhancing
the semantic analysis of trajectory data, paving the way for more sophisticated
and accessible human mobility research.
`,authors:"Yuxiao Luo; Zhongcai Cao; Xin Jin; Kang Liu; Ling Yin",status:0,relevancy:.43706604976747854,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19631",date:"2024-05-30",title:`Leveraging Open-Source Large Language Models for encoding Social
  Determinants of Health using an Intelligent Router`,abstract:`  Social Determinants of Health (SDOH) play a significant role in patient
health outcomes. The Center of Disease Control (CDC) introduced a subset of
ICD-10 codes called Z-codes in an attempt to officially recognize and measure
SDOH in the health care system. However, these codes are rarely annotated in a
patient's Electronic Health Record (EHR), and instead, in many cases, need to
be inferred from clinical notes. Previous research has shown that large
language models (LLMs) show promise on extracting unstructured data from EHRs.
However, with thousands of models to choose from with unique architectures and
training sets, it's difficult to choose one model that performs the best on
coding tasks. Further, clinical notes contain trusted health information making
the use of closed-source language models from commercial vendors difficult, so
the identification of open source LLMs that can be run within health
organizations and exhibits high performance on SDOH tasks is an urgent problem.
Here, we introduce an intelligent routing system for SDOH coding that uses a
language model router to direct medical record data to open source LLMs that
demonstrate optimal performance on specific SDOH codes. The intelligent routing
system exhibits state of the art performance of 97.4% accuracy averaged across
5 codes, including homelessness and food insecurity, on par with closed models
such as GPT-4o. In order to train the routing system and validate models, we
also introduce a synthetic data generation and validation paradigm to increase
the scale of training data without needing privacy protected medical records.
Together, we demonstrate an architecture for intelligent routing of inputs to
task-optimal language models to achieve high performance across a set of
medical coding sub-tasks.
`,authors:"Akul Goel; Surya Narayanan Hari; Belinda Waltman; Matt Thomson",status:0,relevancy:.42855517518640984,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20204",date:"2024-05-30",title:"Jina CLIP: Your CLIP Model Is Also Your Text Retriever",abstract:`  Contrastive Language-Image Pretraining (CLIP) is widely used to train models
to align images and texts in a common embedding space by mapping them to
fixed-sized vectors. These models are key to multimodal information retrieval
and related tasks. However, CLIP models generally underperform in text-only
tasks compared to specialized text models. This creates inefficiencies for
information retrieval systems that keep separate embeddings and models for
text-only and multimodal tasks. We propose a novel, multi-task contrastive
training method to address this issue, which we use to train the jina-clip-v1
model to achieve the state-of-the-art performance on both text-image and
text-text retrieval tasks.
`,authors:"Andreas Koukounas; Georgios Mastrapas; Michael Günther; Bo Wang; Scott Martens; Isabelle Mohr; Saba Sturua; Mohammad Kalim Akram; Joan Fontanals Martínez; Saahil Ognawala; Susana Guzman; Maximilian Werk; Nan Wang; Han Xiao",status:0,relevancy:.4233779231858861,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20218",date:"2024-05-30",title:`ESG-FTSE: A corpus of news articles with ESG relevance labels and use
  cases`,abstract:`  We present ESG-FTSE, the first corpus comprised of news articles with
Environmental, Social and Governance (ESG) relevance annotations. In recent
years, investors and regulators have pushed ESG investing to the mainstream due
to the urgency of climate change. This has led to the rise of ESG scores to
evaluate an investment's credentials as socially responsible. While demand for
ESG scores is high, their quality varies wildly. Quantitative techniques can be
applied to improve ESG scores, thus, responsible investing. To contribute to
resource building for ESG and financial text mining, we pioneer the ESG-FTSE
corpus. We further present the first of its kind ESG annotation schema. It has
three levels: a binary classification (relevant versus irrelevant news
articles), ESG classification (ESG-related news articles), and target company.
Both supervised and unsupervised learning experiments for ESG relevance
detection were conducted to demonstrate that the corpus can be used in
different settings to derive accurate ESG predictions. Keywords: corpus
annotation, ESG labels, annotation schema, news article, natural language
processing
`,authors:"Mariya Pavlova; Bernard Casey; Miaosen Wang",status:0,relevancy:.4120645248126876,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20233",date:"2024-05-30",title:"Grokfast: Accelerated Grokking by Amplifying Slow Gradients",abstract:`  One puzzling artifact in machine learning dubbed grokking is where delayed
generalization is achieved tenfolds of iterations after near perfect
overfitting to the training data. Focusing on the long delay itself on behalf
of machine learning practitioners, our goal is to accelerate generalization of
a model under grokking phenomenon. By regarding a series of gradients of a
parameter over training iterations as a random signal over time, we can
spectrally decompose the parameter trajectories under gradient descent into two
components: the fast-varying, overfitting-yielding component and the
slow-varying, generalization-inducing component. This analysis allows us to
accelerate the grokking phenomenon more than $\\times 50$ with only a few lines
of code that amplifies the slow-varying components of gradients. The
experiments show that our algorithm applies to diverse tasks involving images,
languages, and graphs, enabling practical availability of this peculiar
artifact of sudden generalization. Our code is available at
\\url{https://github.com/ironjr/grokfast}.
`,authors:"Jaerin Lee; Bong Gyun Kang; Kihoon Kim; Kyoung Mu Lee",status:0,relevancy:.4115985844813268,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19678",date:"2024-05-30",title:`View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature
  Fields`,abstract:`  Large-scale vision foundation models such as Segment Anything (SAM)
demonstrate impressive performance in zero-shot image segmentation at multiple
levels of granularity. However, these zero-shot predictions are rarely
3D-consistent. As the camera viewpoint changes in a scene, so do the
segmentation predictions, as well as the characterizations of \`\`coarse" or
\`\`fine" granularity. In this work, we address the challenging task of lifting
multi-granular and view-inconsistent image segmentations into a hierarchical
and 3D-consistent representation. We learn a novel feature field within a
Neural Radiance Field (NeRF) representing a 3D scene, whose segmentation
structure can be revealed at different scales by simply using different
thresholds on feature distance. Our key idea is to learn an ultrametric feature
space, which unlike a Euclidean space, exhibits transitivity in distance-based
grouping, naturally leading to a hierarchical clustering. Put together, our
method takes view-inconsistent multi-granularity 2D segmentations as input and
produces a hierarchy of 3D-consistent segmentations as output. We evaluate our
method and several baselines on synthetic datasets with multi-view images and
multi-granular segmentation, showcasing improved accuracy and
viewpoint-consistency. We additionally provide qualitative examples of our
model's 3D hierarchical segmentations in real world scenes.\\footnote{The code
and dataset are available at:
`,authors:"Haodi He; Colton Stearns; Adam W. Harley; Leonidas J. Guibas",status:0,relevancy:.40922065179453393,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20319",date:"2024-05-30",title:"ParSEL: Parameterized Shape Editing with Language",abstract:`  The ability to edit 3D assets from natural language presents a compelling
paradigm to aid in the democratization of 3D content creation. However, while
natural language is often effective at communicating general intent, it is
poorly suited for specifying precise manipulation. To address this gap, we
introduce ParSEL, a system that enables controllable editing of high-quality 3D
assets from natural language. Given a segmented 3D mesh and an editing request,
ParSEL produces a parameterized editing program. Adjusting the program
parameters allows users to explore shape variations with a precise control over
the magnitudes of edits. To infer editing programs which align with an input
edit request, we leverage the abilities of large-language models (LLMs).
However, while we find that LLMs excel at identifying initial edit operations,
they often fail to infer complete editing programs, and produce outputs that
violate shape semantics. To overcome this issue, we introduce Analytical Edit
Propagation (AEP), an algorithm which extends a seed edit with additional
operations until a complete editing program has been formed. Unlike prior
methods, AEP searches for analytical editing operations compatible with a range
of possible user edits through the integration of computer algebra systems for
geometric analysis. Experimentally we demonstrate ParSEL's effectiveness in
enabling controllable editing of 3D objects through natural language requests
over alternative system designs.
`,authors:"Aditya Ganeshan; Ryan Y. Huang; Xianghao Xu; R. Kenny Jones; Daniel Ritchie",status:0,relevancy:.40658810338109996,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19765",date:"2024-05-30",title:`Towards Unified Multi-granularity Text Detection with Interactive
  Attention`,abstract:`  Existing OCR engines or document image analysis systems typically rely on
training separate models for text detection in varying scenarios and
granularities, leading to significant computational complexity and resource
demands. In this paper, we introduce "Detect Any Text" (DAT), an advanced
paradigm that seamlessly unifies scene text detection, layout analysis, and
document page detection into a cohesive, end-to-end model. This design enables
DAT to efficiently manage text instances at different granularities, including
*word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the
across-granularity interactive attention module, which significantly enhances
the representation learning of text instances at varying granularities by
correlating structural information across different text queries. As a result,
it enables the model to achieve mutually beneficial detection performances
across multiple text granularities. Additionally, a prompt-based segmentation
module refines detection outcomes for texts of arbitrary curvature and complex
layouts, thereby improving DAT's accuracy and expanding its real-world
applicability. Experimental results demonstrate that DAT achieves
state-of-the-art performances across a variety of text-related benchmarks,
including multi-oriented/arbitrarily-shaped scene text detection, document
layout analysis and page detection tasks.
`,authors:"Xingyu Wan; Chengquan Zhang; Pengyuan Lyu; Sen Fan; Zihan Ni; Kun Yao; Errui Ding; Jingdong Wang",status:0,relevancy:.4065289484620488,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20337",date:"2024-05-30",title:`OccSora: 4D Occupancy Generation Models as World Simulators for
  Autonomous Driving`,abstract:`  Understanding the evolution of 3D scenes is important for effective
autonomous driving. While conventional methods mode scene development with the
motion of individual instances, world models emerge as a generative framework
to describe the general scene dynamics. However, most existing methods adopt an
autoregressive framework to perform next-token prediction, which suffer from
inefficiency in modeling long-term temporal evolutions. To address this, we
propose a diffusion-based 4D occupancy generation model, OccSora, to simulate
the development of the 3D world for autonomous driving. We employ a 4D scene
tokenizer to obtain compact discrete spatial-temporal representations for 4D
occupancy input and achieve high-quality reconstruction for long-sequence
occupancy videos. We then learn a diffusion transformer on the spatial-temporal
representations and generate 4D occupancy conditioned on a trajectory prompt.
We conduct extensive experiments on the widely used nuScenes dataset with Occ3D
occupancy annotations. OccSora can generate 16s-videos with authentic 3D layout
and temporal consistency, demonstrating its ability to understand the spatial
and temporal distributions of driving scenes. With trajectory-aware 4D
generation, OccSora has the potential to serve as a world simulator for the
decision-making of autonomous driving. Code is available at:
https://github.com/wzzheng/OccSora.
`,authors:"Lening Wang; Wenzhao Zheng; Yilong Ren; Han Jiang; Zhiyong Cui; Haiyang Yu; Jiwen Lu",status:0,relevancy:.405878936415753,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20015",date:"2024-05-30",title:"Efficient LLM-Jailbreaking by Introducing Visual Modality",abstract:`  This paper focuses on jailbreaking attacks against large language models
(LLMs), eliciting them to generate objectionable content in response to harmful
user queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our
approach begins by constructing a multimodal large language model (MLLM)
through the incorporation of a visual module into the target LLM. Subsequently,
we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings
embJS. Finally, we convert the embJS into text space to facilitate the
jailbreaking of the target LLM. Compared to direct LLM-jailbreaking, our
approach is more efficient, as MLLMs are more vulnerable to jailbreaking than
pure LLM. Additionally, to improve the attack success rate (ASR) of
jailbreaking, we propose an image-text semantic matching scheme to identify a
suitable initial input. Extensive experiments demonstrate that our approach
surpasses current state-of-the-art methods in terms of both efficiency and
effectiveness. Moreover, our approach exhibits superior cross-class
jailbreaking capabilities.
`,authors:"Zhenxing Niu; Yuyao Sun; Haodong Ren; Haoxuan Ji; Quan Wang; Xiaoke Ma; Gang Hua; Rong Jin",status:0,relevancy:.40555543538695793,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20234",date:"2024-05-30",title:"Context Injection Attacks on Large Language Models",abstract:`  Large Language Models (LLMs) such as ChatGPT and Llama-2 have become
prevalent in real-world applications, exhibiting impressive text generation
performance. LLMs are fundamentally developed from a scenario where the input
data remains static and lacks a clear structure. To behave interactively over
time, LLM-based chat systems must integrate additional contextual information
(i.e., chat history) into their inputs, following a pre-defined structure. This
paper identifies how such integration can expose LLMs to misleading context
from untrusted sources and fail to differentiate between system and user
inputs, allowing users to inject context. We present a systematic methodology
for conducting context injection attacks aimed at eliciting disallowed
responses by introducing fabricated context. This could lead to illegal
actions, inappropriate content, or technology misuse. Our context fabrication
strategies, acceptance elicitation and word anonymization, effectively create
misleading contexts that can be structured with attacker-customized prompt
templates, achieving injection through malicious user messages. Comprehensive
evaluations on real-world LLMs such as ChatGPT and Llama-2 confirm the efficacy
of the proposed attack with success rates reaching 97%. We also discuss
potential countermeasures that can be adopted for attack detection and
developing more secure models. Our findings provide insights into the
challenges associated with the real-world deployment of LLMs for interactive
and structured data scenarios.
`,authors:"Cheng'an Wei; Kai Chen; Yue Zhao; Yujia Gong; Lu Xiang; Shenchen Zhu",status:0,relevancy:.4048507842357849,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19642",date:"2024-05-30",title:`Few-shot fault diagnosis based on multi-scale graph convolution
  filtering for industry`,abstract:`  Industrial equipment fault diagnosis often encounter challenges such as the
scarcity of fault data, complex operating conditions, and varied types of
failures. Signal analysis, data statistical learning, and conventional deep
learning techniques face constraints under these conditions due to their
substantial data requirements and the necessity for transfer learning to
accommodate new failure modes. To effectively leverage information and extract
the intrinsic characteristics of faults across different domains under limited
sample conditions, this paper introduces a fault diagnosis approach employing
Multi-Scale Graph Convolution Filtering (MSGCF). MSGCF enhances the traditional
Graph Neural Network (GNN) framework by integrating both local and global
information fusion modules within the graph convolution filter block. This
advancement effectively mitigates the over-smoothing issue associated with
excessive layering of graph convolutional layers while preserving a broad
receptive field. It also reduces the risk of overfitting in few-shot diagnosis,
thereby augmenting the model's representational capacity. Experiments on the
University of Paderborn bearing dataset (PU) demonstrate that the MSGCF method
proposed herein surpasses alternative approaches in accuracy, thereby offering
valuable insights for industrial fault diagnosis in few-shot learning
scenarios.
`,authors:"Mengjie Gan; Penglong Lian; Zhiheng Su; Jiyang Zhang; Jialong Huang; Benhao Wang; Jianxiao Zou; Shicai Fan",status:0,relevancy:.40468896887458816,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19761",date:"2024-05-30",title:"Revisiting CNNs for Trajectory Similarity Learning",abstract:`  Similarity search is a fundamental but expensive operator in querying
trajectory data, due to its quadratic complexity of distance computation. To
mitigate the computational burden for long trajectories, neural networks have
been widely employed for similarity learning and each trajectory is encoded as
a high-dimensional vector for similarity search with linear complexity. Given
the sequential nature of trajectory data, previous efforts have been primarily
devoted to the utilization of RNNs or Transformers.
  In this paper, we argue that the common practice of treating trajectory as
sequential data results in excessive attention to capturing long-term global
dependency between two sequences. Instead, our investigation reveals the
pivotal role of local similarity, prompting a revisit of simple CNNs for
trajectory similarity learning. We introduce ConvTraj, incorporating both 1D
and 2D convolutions to capture sequential and geo-distribution features of
trajectories, respectively. In addition, we conduct a series of theoretical
analyses to justify the effectiveness of ConvTraj. Experimental results on
three real-world large-scale datasets demonstrate that ConvTraj achieves
state-of-the-art accuracy in trajectory similarity search. Owing to the simple
network structure of ConvTraj, the training and inference speed on the Porto
dataset with 1.6 million trajectories are increased by at least $240$x and
$2.16$x, respectively. The source code and dataset can be found at
\\textit{\\url{https://github.com/Proudc/ConvTraj}}.
`,authors:"Zhihao Chang; Linzhu Yu; Huan Li; Sai Wu; Gang Chen; Dongxiang Zhang",status:0,relevancy:.40441262870956474,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20082",date:"2024-05-30",title:`Segment, Shuffle, and Stitch: A Simple Mechanism for Improving
  Time-Series Representations`,abstract:`  Existing approaches for learning representations of time-series keep the
temporal arrangement of the time-steps intact with the presumption that the
original order is the most optimal for learning. However, non-adjacent sections
of real-world time-series may have strong dependencies. Accordingly we raise
the question: Is there an alternative arrangement for time-series which could
enable more effective representation learning? To address this, we propose a
simple plug-and-play mechanism called Segment, Shuffle, and Stitch (S3)
designed to improve time-series representation learning of existing models. S3
works by creating non-overlapping segments from the original sequence and
shuffling them in a learned manner that is the most optimal for the task at
hand. It then re-attaches the shuffled segments back together and performs a
learned weighted sum with the original input to capture both the newly shuffled
sequence along with the original sequence. S3 is modular and can be stacked to
create various degrees of granularity, and can be added to many forms of neural
architectures including CNNs or Transformers with negligible computation
overhead. Through extensive experiments on several datasets and
state-of-the-art baselines, we show that incorporating S3 results in
significant improvements for the tasks of time-series classification and
forecasting, improving performance on certain datasets by up to 68\\%. We also
show that S3 makes the learning more stable with a smoother training loss curve
and loss landscape compared to the original baseline. The code is available at
https://github.com/shivam-grover/S3-TimeSeries .
`,authors:"Shivam Grover; Amin Jalali; Ali Etemad",status:0,relevancy:.4005710419662505,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19818",date:"2024-05-30",title:`WebUOT-1M: Advancing Deep Underwater Object Tracking with A
  Million-Scale Benchmark`,abstract:`  Underwater object tracking (UOT) is a foundational task for identifying and
tracing submerged entities in underwater video sequences. However, current UOT
datasets suffer from limitations in scale, diversity of target categories and
scenarios covered, hindering the training and evaluation of modern tracking
algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M,
\\ie, the largest public UOT benchmark to date, sourced from complex and
realistic underwater environments. It comprises 1.1 million frames across 1,500
video clips filtered from 408 target categories, largely surpassing previous
UOT datasets, \\eg, UVOT400. Through meticulous manual annotation and
verification, we provide high-quality bounding boxes for underwater targets.
Additionally, WebUOT-1M includes language prompts for video sequences,
expanding its application areas, \\eg, underwater vision-language tracking. Most
existing trackers are tailored for open-air environments, leading to
performance degradation when applied to UOT due to domain gaps. Retraining and
fine-tuning these trackers are challenging due to sample imbalances and limited
real-world underwater datasets. To tackle these challenges, we propose a novel
omni-knowledge distillation framework based on WebUOT-1M, incorporating various
strategies to guide the learning of the student Transformer. To the best of our
knowledge, this framework is the first to effectively transfer open-air domain
knowledge to the UOT model through knowledge distillation, as demonstrated by
results on both existing UOT datasets and the newly proposed WebUOT-1M.
Furthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers,
showcasing its value as a benchmark for UOT research by presenting new
challenges and opportunities for future studies. The complete dataset, codes
and tracking results, will be made publicly available.
`,authors:"Chunhui Zhang; Li Liu; Guanjie Huang; Hao Wen; Xi Zhou; Yanfeng Wang",status:0,relevancy:.39869194825448206,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20180",date:"2024-05-30",title:`Transformers and Slot Encoding for Sample Efficient Physical World
  Modelling`,abstract:`  World modelling, i.e. building a representation of the rules that govern the
world so as to predict its evolution, is an essential ability for any agent
interacting with the physical world. Recent applications of the Transformer
architecture to the problem of world modelling from video input show notable
improvements in sample efficiency. However, existing approaches tend to work
only at the image level thus disregarding that the environment is composed of
objects interacting with each other. In this paper, we propose an architecture
combining Transformers for world modelling with the slot-attention paradigm, an
approach for learning representations of objects appearing in a scene. We
describe the resulting neural architecture and report experimental results
showing an improvement over the existing solutions in terms of sample
efficiency and a reduction of the variation of the performance over the
training examples. The code for our architecture and experiments is available
at https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm
`,authors:"Francesco Petri; Luigi Asprino; Aldo Gangemi",status:0,relevancy:.3962625571167996,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19996",date:"2024-05-30",title:`DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in
  the Wild`,abstract:`  Image quality assessment (IQA) plays a critical role in selecting
high-quality images and guiding compression and enhancement methods in a series
of applications. The blind IQA, which assesses the quality of in-the-wild
images containing complex authentic distortions without reference images, poses
greater challenges. Existing methods are limited to modeling a uniform
distribution with local patches and are bothered by the gap between low and
high-level visions (caused by widely adopted pre-trained classification
networks). In this paper, we propose a novel IQA method called diffusion
priors-based IQA (DP-IQA), which leverages the prior knowledge from the
pre-trained diffusion model with its excellent powers to bridge semantic gaps
in the perception of the visual quality of images. Specifically, we use
pre-trained stable diffusion as the backbone, extract multi-level features from
the denoising U-Net during the upsampling process at a specified timestep, and
decode them to estimate the image quality score. The text and image adapters
are adopted to mitigate the domain gap for downstream tasks and correct the
information loss caused by the variational autoencoder bottleneck. Finally, we
distill the knowledge in the above model into a CNN-based student model,
significantly reducing the parameter to enhance applicability, with the student
model performing similarly or even better than the teacher model surprisingly.
Experimental results demonstrate that our DP-IQA achieves state-of-the-art
results on various in-the-wild datasets with better generalization capability,
which shows the superiority of our method in global modeling and utilizing the
hierarchical feature clues of diffusion for evaluating image quality.
`,authors:"Honghao Fu; Yufei Wang; Wenhan Yang; Bihan Wen",status:0,relevancy:.3917643603771399,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19754",date:"2024-05-30",title:`Mitigating annotation shift in cancer classification using single image
  generative models`,abstract:`  Artificial Intelligence (AI) has emerged as a valuable tool for assisting
radiologists in breast cancer detection and diagnosis. However, the success of
AI applications in this domain is restricted by the quantity and quality of
available data, posing challenges due to limited and costly data annotation
procedures that often lead to annotation shifts. This study simulates, analyses
and mitigates annotation shifts in cancer classification in the breast
mammography domain. First, a high-accuracy cancer risk prediction model is
developed, which effectively distinguishes benign from malignant lesions. Next,
model performance is used to quantify the impact of annotation shift. We
uncover a substantial impact of annotation shift on multiclass classification
performance particularly for malignant lesions. We thus propose a training data
augmentation approach based on single-image generative models for the affected
class, requiring as few as four in-domain annotations to considerably mitigate
annotation shift, while also addressing dataset imbalance. Lastly, we further
increase performance by proposing and validating an ensemble architecture based
on multiple models trained under different data augmentation regimes. Our study
offers key insights into annotation shift in deep learning breast cancer
classification and explores the potential of single-image generative models to
overcome domain shift challenges.
`,authors:"Marta Buetas Arcas; Richard Osuala; Karim Lekadir; Oliver Díaz",status:0,relevancy:.39038447492082073,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19600",date:"2024-05-30",title:`Do spectral cues matter in contrast-based graph self-supervised
  learning?`,abstract:`  The recent surge in contrast-based graph self-supervised learning has
prominently featured an intensified exploration of spectral cues. However, an
intriguing paradox emerges, as methods grounded in seemingly conflicting
assumptions or heuristic approaches regarding the spectral domain demonstrate
notable enhancements in learning performance. This paradox prompts a critical
inquiry into the genuine contribution of spectral information to contrast-based
graph self-supervised learning. This study undertakes an extensive
investigation into this inquiry, conducting a thorough study of the
relationship between spectral characteristics and the learning outcomes of
contemporary methodologies. Based on this analysis, we claim that the
effectiveness and significance of spectral information need to be questioned.
Instead, we revisit simple edge perturbation: random edge dropping designed for
node-level self-supervised learning and random edge adding intended for
graph-level self-supervised learning. Compelling evidence is presented that
these simple yet effective strategies consistently yield superior performance
while demanding significantly fewer computational resources compared to all
prior spectral augmentation methods. The proposed insights represent a
significant leap forward in the field, potentially reshaping the understanding
and implementation of graph self-supervised learning.
`,authors:"Xiangru Jian; Xinjian Zhao; Wei Pang; Chaolong Ying; Yimu Wang; Yaoyao Xu; Tianshu Yu",status:0,relevancy:.39015523518223405,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19597",date:"2024-05-30",title:"SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors",abstract:`  Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its
variants, freeze pre-trained model weights \\(W\\) and inject learnable matrices
\\(\\Delta W\\). These \\(\\Delta W\\) matrices are structured for efficient
parameterization, often using techniques like low-rank approximations or
scaling vectors. However, these methods typically show a performance gap
compared to full fine-tuning. Although recent PEFT methods have narrowed this
gap, they do so at the cost of additional learnable parameters. We propose
SVFT, a simple approach that fundamentally differs from existing methods: the
structure imposed on \\(\\Delta W\\) depends on the specific weight matrix \\(W\\).
Specifically, SVFT updates \\(W\\) as a sparse combination of outer products of
its singular vectors, training only the coefficients (scales) of these sparse
combinations. This approach allows fine-grained control over expressivity
through the number of coefficients. Extensive experiments on language and
vision benchmarks show that SVFT recovers up to 96% of full fine-tuning
performance while training only 0.006 to 0.25% of parameters, outperforming
existing methods that only recover up to 85% performance using 0.03 to 0.8% of
the trainable parameter budget.
`,authors:"Vijay Lingam; Atula Tejaswi; Aditya Vavre; Aneesh Shetty; Gautham Krishna Gudur; Joydeep Ghosh; Alex Dimakis; Eunsol Choi; Aleksandar Bojchevski; Sujay Sanghavi",status:0,relevancy:.38953830302401415,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20330",date:"2024-05-30",title:"4DHands: Reconstructing Interactive Hands in 4D with Transformers",abstract:`  In this paper, we introduce 4DHands, a robust approach to recovering
interactive hand meshes and their relative movement from monocular inputs. Our
approach addresses two major limitations of previous methods: lacking a unified
solution for handling various hand image inputs and neglecting the positional
relationship of two hands within images. To overcome these challenges, we
develop a transformer-based architecture with novel tokenization and feature
fusion strategies. Specifically, we propose a Relation-aware Two-Hand
Tokenization (RAT) method to embed positional relation information into the
hand tokens. In this way, our network can handle both single-hand and two-hand
inputs and explicitly leverage relative hand positions, facilitating the
reconstruction of intricate hand interactions in real-world scenarios. As such
tokenization indicates the relative relationship of two hands, it also supports
more effective feature fusion. To this end, we further develop a
Spatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D
with attention and decode them into 3D hand meshes and relative temporal
movements. The efficacy of our approach is validated on several benchmark
datasets. The results on in-the-wild videos and real-world scenarios
demonstrate the superior performances of our approach for interactive hand
reconstruction. More video results can be found on the project page:
https://4dhands.github.io.
`,authors:"Dixuan Lin; Yuxiang Zhang; Mengcheng Li; Yebin Liu; Wei Jing; Qi Yan; Qianying Wang; Hongwen Zhang",status:0,relevancy:.3871932143211807,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19656",date:"2024-05-30",title:"Accurate and Reliable Predictions with Mutual-Transport Ensemble",abstract:`  Deep Neural Networks (DNNs) have achieved remarkable success in a variety of
tasks, especially when it comes to prediction accuracy. However, in complex
real-world scenarios, particularly in safety-critical applications, high
accuracy alone is not enough. Reliable uncertainty estimates are crucial.
Modern DNNs, often trained with cross-entropy loss, tend to be overconfident,
especially with ambiguous samples. To improve uncertainty calibration, many
techniques have been developed, but they often compromise prediction accuracy.
To tackle this challenge, we propose the \`\`mutual-transport ensemble'' (MTE).
This approach introduces a co-trained auxiliary model and adaptively
regularizes the cross-entropy loss using Kullback-Leibler (KL) divergence
between the prediction distributions of the primary and auxiliary models. We
conducted extensive studies on various benchmarks to validate the effectiveness
of our method. The results show that MTE can simultaneously enhance both
accuracy and uncertainty calibration. For example, on the CIFAR-100 dataset,
our MTE method on ResNet34/50 achieved significant improvements compared to
previous state-of-the-art method, with absolute accuracy increases of
2.4%/3.7%, relative reductions in ECE of $42.3%/29.4%, and relative reductions
in classwise-ECE of 11.6%/15.3%.
`,authors:"Han Liu; Peng Cui; Bingning Wang; Jun Zhu; Xiaolin Hu",status:0,relevancy:.38662846418346786,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20289",date:"2024-05-30",title:`DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music
  Generation`,abstract:`  Controllable music generation methods are critical for human-centered
AI-based music creation, but are currently limited by speed, quality, and
control design trade-offs. Diffusion Inference-Time T-optimization (DITTO), in
particular, offers state-of-the-art results, but is over 10x slower than
real-time, limiting practical use. We propose Distilled Diffusion
Inference-Time T -Optimization (or DITTO-2), a new method to speed up
inference-time optimization-based control and unlock faster-than-real-time
generation for a wide-variety of applications such as music inpainting,
outpainting, intensity, melody, and musical structure control. Our method works
by (1) distilling a pre-trained diffusion model for fast sampling via an
efficient, modified consistency or consistency trajectory distillation process
(2) performing inference-time optimization using our distilled model with
one-step sampling as an efficient surrogate optimization task and (3) running a
final multi-step sampling generation (decoding) using our estimated noise
latents for best-quality, fast, controllable generation. Through thorough
evaluation, we find our method not only speeds up generation over 10-20x, but
simultaneously improves control adherence and generation quality all at once.
Furthermore, we apply our approach to a new application of maximizing text
adherence (CLAP score) and show we can convert an unconditional diffusion model
without text inputs into a model that yields state-of-the-art text control.
Sound examples can be found at https://ditto-music.github.io/ditto2/.
`,authors:"Zachary Novack; Julian McAuley; Taylor Berg-Kirkpatrick; Nicholas Bryan",status:0,relevancy:.3818986811651287,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19808",date:"2024-05-30",title:"AI with Alien Content and Alien Metasemantics",abstract:`  AlphaGo plays chess and Go in a creative and novel way. It is natural for us
to attribute contents to it, such as that it doesn't view being several pawns
behind, if it has more board space, as bad. The framework introduced in
Cappelen and Dever (2021) provides a way of thinking about the semantics and
the metasemantics of AI content: does AlphaGo entertain contents like this, and
if so, in virtue of what does a given state of the program mean that particular
content? One salient question Cappelen and Dever didn't consider was the
possibility of alien content. Alien content is content that is not or cannot be
expressed by human beings. It's highly plausible that AlphaGo, or any other
sophisticated AI system, expresses alien contents. That this is so, moreover,
is plausibly a metasemantic fact: a fact that has to do with how AI comes to
entertain content in the first place, one that will heed the vastly different
etiology of AI and human content. This chapter explores the question of alien
content in AI from a semantic and metasemantic perspective. It lays out the
logical space of possible responses to the semantic and metasemantic questions
alien content poses, considers whether and how we humans could communicate with
entities who express alien content, and points out that getting clear about
such questions might be important for more 'applied' issues in the philosophy
of AI, such as existential risk and XAI.
`,authors:"Herman Cappelen; Josh Dever",status:0,relevancy:.38163742004054024,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19784",date:"2024-05-30",title:`PixelsDB: Serverless and Natural-Language-Aided Data Analytics with
  Flexible Service Levels and Prices`,abstract:`  Serverless query processing has become increasingly popular due to its
advantages, including automated hardware and software management, high
elasticity, and pay-as-you-go pricing. For users who are not system experts,
serverless query processing greatly reduces the cost of owning a data analytic
system. However, it is still a significant challenge for non-expert users to
transform their complex and evolving data analytic needs into proper SQL
queries and select a serverless query engine that delivers satisfactory
performance and price for each type of query.
  This paper presents PixelsDB, an open-source data analytic system that allows
users who lack system or SQL expertise to explore data efficiently. It allows
users to generate and debug SQL queries using a natural language interface
powered by fine-tuned language models. The queries are then executed by a
serverless query engine that offers varying prices for different service levels
on query urgency. The service levels are natively supported by dedicated
architecture design and heterogeneous resource scheduling that can apply
cost-efficient resources to process non-urgent queries. We envision that the
combination of a serverless paradigm, a natural-language-aided interface, and
flexible service levels and prices will substantially improve the user
experience in data analysis.
`,authors:"Haoqiong Bian; Dongyang Geng; Haoyang Li; Anastasia Ailamaki",status:0,relevancy:.38030300445777143,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19899",date:"2024-05-30",title:"Open-Set Domain Adaptation for Semantic Segmentation",abstract:`  Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer the pixel-wise knowledge from the labeled source domain to the
unlabeled target domain. However, current UDA methods typically assume a shared
label space between source and target, limiting their applicability in
real-world scenarios where novel categories may emerge in the target domain. In
this paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation
(OSDA-SS) for the first time, where the target domain includes unknown classes.
We identify two major problems in the OSDA-SS scenario as follows: 1) the
existing UDA methods struggle to predict the exact boundary of the unknown
classes, and 2) they fail to accurately predict the shape of the unknown
classes. To address these issues, we propose Boundary and Unknown Shape-Aware
open-set domain adaptation, coined BUS. Our BUS can accurately discern the
boundaries between known and unknown classes in a contrastive manner using a
novel dilation-erosion-based contrastive loss. In addition, we propose
OpenReMix, a new domain mixing augmentation method that guides our model to
effectively learn domain and size-invariant features for improving the shape
detection of the known and unknown classes. Through extensive experiments, we
demonstrate that our proposed BUS effectively detects unknown classes in the
challenging OSDA-SS scenario compared to the previous methods by a large
margin. The code is available at https://github.com/KHU-AGI/BUS.
`,authors:"Seun-An Choe; Ah-Hyung Shin; Keon-Hee Park; Jinwoo Choi; Gyeong-Moon Park",status:0,relevancy:.37924322881298234,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20279",date:"2024-05-30",title:"CV-VAE: A Compatible Video VAE for Latent Generative Video Models",abstract:`  Spatio-temporal compression of videos, utilizing networks such as Variational
Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other
video generative models. For instance, many LLM-like video models learn the
distribution of discrete tokens derived from 3D VAEs within the VQVAE
framework, while most diffusion-based video models capture the distribution of
continuous latent extracted by 2D VAEs without quantization. The temporal
compression is simply realized by uniform frame sampling which results in
unsmooth motion between consecutive frames. Currently, there lacks of a
commonly used continuous video (3D) VAE for latent diffusion-based video models
in the research community. Moreover, since current diffusion-based approaches
are often implemented using pre-trained text-to-image (T2I) models, directly
training a video VAE without considering the compatibility with existing T2I
models will result in a latent space gap between them, which will take huge
computational resources for training to bridge the gap even with the T2I models
as initialization. To address this issue, we propose a method for training a
video VAE of latent video models, namely CV-VAE, whose latent space is
compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion
(SD). The compatibility is achieved by the proposed novel latent space
regularization, which involves formulating a regularization loss using the
image VAE. Benefiting from the latent space compatibility, video models can be
trained seamlessly from pre-trained T2I or video models in a truly
spatio-temporally compressed latent space, rather than simply sampling video
frames at equal intervals. With our CV-VAE, existing video models can generate
four times more frames with minimal finetuning. Extensive experiments are
conducted to demonstrate the effectiveness of the proposed video VAE.
`,authors:"Sijie Zhao; Yong Zhang; Xiaodong Cun; Shaoshu Yang; Muyao Niu; Xiaoyu Li; Wenbo Hu; Ying Shan",status:0,relevancy:.3792280354540144,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20172",date:"2024-05-30",title:"Iterative Feature Boosting for Explainable Speech Emotion Recognition",abstract:`  In speech emotion recognition (SER), using predefined features without
considering their practical importance may lead to high dimensional datasets,
including redundant and irrelevant information. Consequently, high-dimensional
learning often results in decreasing model accuracy while increasing
computational complexity. Our work underlines the importance of carefully
considering and analyzing features in order to build efficient SER systems. We
present a new supervised SER method based on an efficient feature engineering
approach. We pay particular attention to the explainability of results to
evaluate feature relevance and refine feature sets. This is performed
iteratively through feature evaluation loop, using Shapley values to boost
feature selection and improve overall framework performance. Our approach
allows thus to balance the benefits between model performance and transparency.
The proposed method outperforms human-level performance (HLP) and
state-of-the-art machine learning methods in emotion recognition on the TESS
dataset.
`,authors:"Alaa Nfissi; Wassim Bouachir; Nizar Bouguila; Brian Mishara",status:0,relevancy:.3720850311410322,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19832",date:"2024-05-30",title:"AI Safety: A Climb To Armageddon?",abstract:`  This paper presents an argument that certain AI safety measures, rather than
mitigating existential risk, may instead exacerbate it. Under certain key
assumptions - the inevitability of AI failure, the expected correlation between
an AI system's power at the point of failure and the severity of the resulting
harm, and the tendency of safety measures to enable AI systems to become more
powerful before failing - safety efforts have negative expected utility. The
paper examines three response strategies: Optimism, Mitigation, and Holism.
Each faces challenges stemming from intrinsic features of the AI safety
landscape that we term Bottlenecking, the Perfection Barrier, and Equilibrium
Fluctuation. The surprising robustness of the argument forces a re-examination
of core assumptions around AI safety and points to several avenues for further
research.
`,authors:"Herman Cappelen; Josh Dever; John Hawthorne",status:0,relevancy:.36714458446619347,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20222",date:"2024-05-30",title:`MOFA-Video: Controllable Image Animation via Generative Motion Field
  Adaptions in Frozen Image-to-Video Diffusion Model`,abstract:`  We present MOFA-Video, an advanced controllable image animation method that
generates video from the given image using various additional controllable
signals (such as human landmarks reference, manual trajectories, and another
even provided video) or their combinations. This is different from previous
methods which only can work on a specific motion domain or show weak control
abilities with diffusion prior. To achieve our goal, we design several
domain-aware motion field adapters (\\ie, MOFA-Adapters) to control the
generated motions in the video generation pipeline. For MOFA-Adapters, we
consider the temporal motion consistency of the video and generate the dense
motion flow from the given sparse control conditions first, and then, the
multi-scale features of the given image are wrapped as a guided feature for
stable video diffusion generation. We naively train two motion adapters for the
manual trajectories and the human landmarks individually since they both
contain sparse information about the control. After training, the MOFA-Adapters
in different domains can also work together for more controllable video
generation.
`,authors:"Muyao Niu; Xiaodong Cun; Xintao Wang; Yong Zhang; Ying Shan; Yinqiang Zheng",status:0,relevancy:.36642647419455154,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20114",date:"2024-05-30",title:`Near Optimal Decentralized Optimization with Compression and Momentum
  Tracking`,abstract:`  Communication efficiency has garnered significant attention as it is
considered the main bottleneck for large-scale decentralized Machine Learning
applications in distributed and federated settings. In this regime, clients are
restricted to transmitting small amounts of quantized information to their
neighbors over a communication graph. Numerous endeavors have been made to
address this challenging problem by developing algorithms with compressed
communication for decentralized non-convex optimization problems. Despite
considerable efforts, the current results suffer from various issues such as
non-scalability with the number of clients, requirements for large batches, or
bounded gradient assumption. In this paper, we introduce MoTEF, a novel
approach that integrates communication compression with Momentum Tracking and
Error Feedback. Our analysis demonstrates that MoTEF achieves most of the
desired properties, and significantly outperforms existing methods under
arbitrary data heterogeneity. We provide numerical experiments to validate our
theoretical findings and confirm the practical superiority of MoTEF.
`,authors:"Rustem Islamov; Yuan Gao; Sebastian U. Stich",status:0,relevancy:.35704846067221097,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19757",date:"2024-05-30",title:`Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise
  Filtering`,abstract:`  Recent advances in a generative neural network model extend the development
of data augmentation methods. However, the augmentation methods based on the
modern generative models fail to achieve notable performance for class
imbalance data compared to the conventional model, the SMOTE. We investigate
the problem of the generative model for imbalanced classification and introduce
a framework to enhance the SMOTE algorithm using Variational Autoencoders
(VAE). Our approach systematically quantifies the density of data points in a
low-dimensional latent space using the VAE, simultaneously incorporating
information on class labels and classification difficulty. Then, the data
points potentially degrading the augmentation are systematically excluded, and
the neighboring observations are directly augmented on the data space.
Empirical studies on several imbalanced datasets represent that this simple
process innovatively improves the conventional SMOTE algorithm over the deep
learning models. Consequently, we conclude that the selection of minority data
and the interpolation in the data space are beneficial for imbalanced
classification problems with a relatively small number of data points.
`,authors:"Sungchul Hong; Seunghwan An; Jong-June Jeon",status:0,relevancy:.3568938759743223,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19743",date:"2024-05-30",title:"May the Dance be with You: Dance Generation Framework for Non-Humanoids",abstract:`  We hypothesize dance as a motion that forms a visual rhythm from music, where
the visual rhythm can be perceived from an optical flow. If an agent can
recognize the relationship between visual rhythm and music, it will be able to
dance by generating a motion to create a visual rhythm that matches the music.
Based on this, we propose a framework for any kind of non-humanoid agents to
learn how to dance from human videos. Our framework works in two processes: (1)
training a reward model which perceives the relationship between optical flow
(visual rhythm) and music from human dance videos, (2) training the
non-humanoid dancer based on that reward model, and reinforcement learning. Our
reward model consists of two feature encoders for optical flow and music. They
are trained based on contrastive learning which makes the higher similarity
between concurrent optical flow and music features. With this reward model, the
agent learns dancing by getting a higher reward when its action creates an
optical flow whose feature has a higher similarity with the given music
feature. Experiment results show that generated dance motion can align with the
music beat properly, and user study result indicates that our framework is more
preferred by humans compared to the baselines. To the best of our knowledge,
our work of non-humanoid agents which learn dance from human videos is
unprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.
`,authors:"Hyemin Ahn",status:0,relevancy:.35629470656024,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20231",date:"2024-05-30",title:"The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",abstract:`  Many algorithms and observed phenomena in deep learning appear to be affected
by parameter symmetries -- transformations of neural network parameters that do
not change the underlying neural network function. These include linear mode
connectivity, model merging, Bayesian neural network inference, metanetworks,
and several other characteristics of optimization or loss-landscapes. However,
theoretical analysis of the relationship between parameter space symmetries and
these phenomena is difficult. In this work, we empirically investigate the
impact of neural parameter symmetries by introducing new neural network
architectures that have reduced parameter space symmetries. We develop two
methods, with some provable guarantees, of modifying standard neural networks
to reduce parameter space symmetries. With these new methods, we conduct a
comprehensive experimental study consisting of multiple tasks aimed at
assessing the effect of removing parameter symmetries. Our experiments reveal
several interesting observations on the empirical impact of parameter
symmetries; for instance, we observe linear mode connectivity between our
networks without alignment of weight spaces, and we find that our networks
allow for faster and more effective Bayesian neural network training.
`,authors:"Derek Lim; Moe Putterman; Robin Walters; Haggai Maron; Stefanie Jegelka",status:0,relevancy:.3556465326771272,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19729",date:"2024-05-30",title:`Dynamic feature selection in medical predictive monitoring by
  reinforcement learning`,abstract:`  In this paper, we investigate dynamic feature selection within multivariate
time-series scenario, a common occurrence in clinical prediction monitoring
where each feature corresponds to a bio-test result. Many existing feature
selection methods fall short in effectively leveraging time-series information,
primarily because they are designed for static data. Our approach addresses
this limitation by enabling the selection of time-varying feature subsets for
each patient. Specifically, we employ reinforcement learning to optimize a
policy under maximum cost restrictions. The prediction model is subsequently
updated using synthetic data generated by trained policy. Our method can
seamlessly integrate with non-differentiable prediction models. We conducted
experiments on a sizable clinical dataset encompassing regression and
classification tasks. The results demonstrate that our approach outperforms
strong feature selection baselines, particularly when subjected to stringent
cost limitations. Code will be released once paper is accepted.
`,authors:"Yutong Chen; Jiandong Gao; Ji Wu",status:0,relevancy:.3519337342152189,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20287",date:"2024-05-30",title:"Flexible SE(2) graph neural networks with applications to PDE surrogates",abstract:`  This paper presents a novel approach for constructing graph neural networks
equivariant to 2D rotations and translations and leveraging them as PDE
surrogates on non-gridded domains. We show that aligning the representations
with the principal axis allows us to sidestep many constraints while preserving
SE(2) equivariance. By applying our model as a surrogate for fluid flow
simulations and conducting thorough benchmarks against non-equivariant models,
we demonstrate significant gains in terms of both data efficiency and accuracy.
`,authors:"Maria Bånkestad; Olof Mogren; Aleksis Pirinen",status:0,relevancy:.3519032565832092,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19822",date:"2024-05-30",title:`Improving Object Detector Training on Synthetic Data by Starting With a
  Strong Baseline Methodology`,abstract:`  Collecting and annotating real-world data for the development of object
detection models is a time-consuming and expensive process. In the military
domain in particular, data collection can also be dangerous or infeasible.
Training models on synthetic data may provide a solution for cases where access
to real-world training data is restricted. However, bridging the reality gap
between synthetic and real data remains a challenge. Existing methods usually
build on top of baseline Convolutional Neural Network (CNN) models that have
been shown to perform well when trained on real data, but have limited ability
to perform well when trained on synthetic data. For example, some architectures
allow for fine-tuning with the expectation of large quantities of training data
and are prone to overfitting on synthetic data. Related work usually ignores
various best practices from object detection on real data, e.g. by training on
synthetic data from a single environment with relatively little variation. In
this paper we propose a methodology for improving the performance of a
pre-trained object detector when training on synthetic data. Our approach
focuses on extracting the salient information from synthetic data without
forgetting useful features learned from pre-training on real images. Based on
the state of the art, we incorporate data augmentation methods and a
Transformer backbone. Besides reaching relatively strong performance without
any specialized synthetic data transfer methods, we show that our methods
improve the state of the art on synthetic data trained object detection for the
RarePlanes and DGTA-VisDrone datasets, and reach near-perfect performance on an
in-house vehicle detection dataset.
`,authors:"Frank A. Ruis; Alma M. Liezenga; Friso G. Heslinga; Luca Ballan; Thijs A. Eker; Richard J. M. den Hollander; Martin C. van Leeuwen; Judith Dijk; Wyke Huizinga",status:0,relevancy:.34827107155345904,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19956",date:"2024-05-30",title:"HOLMES: to Detect Adversarial Examples with Multiple Detectors",abstract:`  Deep neural networks (DNNs) can easily be cheated by some imperceptible but
purposeful noise added to images, and erroneously classify them. Previous
defensive work mostly focused on retraining the models or detecting the noise,
but has either shown limited success rates or been attacked by new adversarial
examples. Instead of focusing on adversarial images or the interior of DNN
models, we observed that adversarial examples generated by different algorithms
can be identified based on the output of DNNs (logits). Logit can serve as an
exterior feature to train detectors. Then, we propose HOLMES (Hierarchically
Organized Light-weight Multiple dEtector System) to reinforce DNNs by detecting
potential adversarial examples to minimize the threats they may bring in
practical. HOLMES is able to distinguish \\textit{unseen} adversarial examples
from multiple attacks with high accuracy and low false positive rates than
single detector systems even in an adaptive model. To ensure the diversity and
randomness of detectors in HOLMES, we use two methods: training dedicated
detectors for each label and training detectors with top-k logits. Our
effective and inexpensive strategies neither modify original DNN models nor
require its internal parameters. HOLMES is not only compatible with all kinds
of learning models (even only with external APIs), but also complementary to
other defenses to achieve higher detection rates (may also fully protect the
system against various adversarial examples).
`,authors:"Jing Wen",status:0,relevancy:.34799041283614096,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19677",date:"2024-05-30",title:"Large Language Model Watermark Stealing With Mixed Integer Programming",abstract:`  The Large Language Model (LLM) watermark is a newly emerging technique that
shows promise in addressing concerns surrounding LLM copyright, monitoring
AI-generated text, and preventing its misuse. The LLM watermark scheme commonly
includes generating secret keys to partition the vocabulary into green and red
lists, applying a perturbation to the logits of tokens in the green list to
increase their sampling likelihood, thus facilitating watermark detection to
identify AI-generated text if the proportion of green tokens exceeds a
threshold. However, recent research indicates that watermarking methods using
numerous keys are susceptible to removal attacks, such as token editing,
synonym substitution, and paraphrasing, with robustness declining as the number
of keys increases. Therefore, the state-of-the-art watermark schemes that
employ fewer or single keys have been demonstrated to be more robust against
text editing and paraphrasing. In this paper, we propose a novel green list
stealing attack against the state-of-the-art LLM watermark scheme and
systematically examine its vulnerability to this attack. We formalize the
attack as a mixed integer programming problem with constraints. We evaluate our
attack under a comprehensive threat model, including an extreme scenario where
the attacker has no prior knowledge, lacks access to the watermark detector
API, and possesses no information about the LLM's parameter settings or
watermark injection/detection scheme. Extensive experiments on LLMs, such as
OPT and LLaMA, demonstrate that our attack can successfully steal the green
list and remove the watermark across all settings.
`,authors:"Zhaoxi Zhang; Xiaomei Zhang; Yanjun Zhang; Leo Yu Zhang; Chao Chen; Shengshan Hu; Asif Gill; Shirui Pan",status:0,relevancy:.34223031569876095,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20323",date:"2024-05-30",title:`$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous
  Driving`,abstract:`  Photorealistic 3D reconstruction of street scenes is a critical technique for
developing real-world simulators for autonomous driving. Despite the efficacy
of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting
(3DGS) emerges as a promising direction due to its faster speed and more
explicit representation. However, most existing street 3DGS methods require
tracked 3D vehicle bounding boxes to decompose the static and dynamic elements
for effective reconstruction, limiting their applications for in-the-wild
scenarios. To facilitate efficient 3D scene reconstruction without costly
annotations, we propose a self-supervised street Gaussian
($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from
4D consistency. We represent each scene with 3D Gaussians to preserve the
explicitness and further accompany them with a spatial-temporal field network
to compactly model the 4D dynamics. We conduct extensive experiments on the
challenging Waymo-Open dataset to evaluate the effectiveness of our method. Our
$\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic
scenes and achieves the best performance without using 3D annotations. Code is
available at: https://github.com/nnanhuang/S3Gaussian/.
`,authors:"Nan Huang; Xiaobao Wei; Wenzhao Zheng; Pengju An; Ming Lu; Wei Zhan; Masayoshi Tomizuka; Kurt Keutzer; Shanghang Zhang",status:0,relevancy:.3400316459670276,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19701",date:"2024-05-30",title:`Significance of Chain of Thought in Gender Bias Mitigation for
  English-Dravidian Machine Translation`,abstract:`  Gender bias in machine translation (MT) systems poses a significant challenge
to achieving accurate and inclusive translations. This paper examines gender
bias in machine translation systems for languages such as Telugu and Kannada
from the Dravidian family, analyzing how gender inflections affect translation
accuracy and neutrality using Google Translate and ChatGPT. It finds that while
plural forms can reduce bias, individual-centric sentences often maintain the
bias due to historical stereotypes. The study evaluates the Chain of Thought
processing, noting significant bias mitigation from 80% to 4% in Telugu and
from 40% to 0% in Kannada. It also compares Telugu and Kannada translations,
emphasizing the need for language specific strategies to address these
challenges and suggesting directions for future research to enhance fairness in
both data preparation and prompts during inference.
`,authors:"Lavanya Prahallad; Radhika Mamidi",status:0,relevancy:.33529172424073583,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20320",date:"2024-05-30",title:"Improving the Training of Rectified Flows",abstract:`  Diffusion models have shown great promise for image and video generation, but
sampling from state-of-the-art models requires expensive numerical integration
of a generative ODE. One approach for tackling this problem is rectified flows,
which iteratively learn smooth ODE paths that are less susceptible to
truncation error. However, rectified flows still require a relatively large
number of function evaluations (NFEs). In this work, we propose improved
techniques for training rectified flows, allowing them to compete with
knowledge distillation methods even in the low NFE setting. Our main insight is
that under realistic settings, a single iteration of the Reflow algorithm for
training rectified flows is sufficient to learn nearly straight trajectories;
hence, the current practice of using multiple Reflow iterations is unnecessary.
We thus propose techniques to improve one-round training of rectified flows,
including a U-shaped timestep distribution and LPIPS-Huber premetric. With
these techniques, we improve the FID of the previous 2-rectified flow by up to
72% in the 1 NFE setting on CIFAR-10. On ImageNet 64$\\times$64, our improved
rectified flow outperforms the state-of-the-art distillation methods such as
consistency distillation and progressive distillation in both one-step and
two-step settings and rivals the performance of improved consistency training
(iCT) in FID. Code is available at https://github.com/sangyun884/rfpp.
`,authors:"Sangyun Lee; Zinan Lin; Giulia Fanti",status:0,relevancy:.3346698803247835,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19657",date:"2024-05-30",title:`Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D
  Gaussian`,abstract:`  3D Gaussian splatting has demonstrated impressive performance in real-time
novel view synthesis. However, achieving successful reconstruction from RGB
images generally requires multiple input views captured under static
conditions. To address the challenge of sparse input views, previous approaches
have incorporated depth supervision into the training of 3D Gaussians to
mitigate overfitting, using dense predictions from pretrained depth networks as
pseudo-ground truth. Nevertheless, depth predictions from monocular depth
estimation models inherently exhibit significant uncertainty in specific areas.
Relying solely on pixel-wise L2 loss may inadvertently incorporate detrimental
noise from these uncertain areas. In this work, we introduce a novel method to
supervise the depth distribution of 3D Gaussians, utilizing depth priors with
integrated uncertainty estimates. To address these localized errors in depth
predictions, we integrate a patch-wise optimal transport strategy to complement
traditional L2 loss in depth supervision. Extensive experiments conducted on
the LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT,
achieves superior novel view synthesis and consistently outperforms
state-of-the-art methods.
`,authors:"Wei Sun; Qi Zhang; Yanzhao Zhou; Qixiang Ye; Jianbin Jiao; Yuan Li",status:0,relevancy:.3276515152081394,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19650",date:"2024-05-30",title:`Few for Many: Tchebycheff Set Scalarization for Many-Objective
  Optimization`,abstract:`  Multi-objective optimization can be found in many real-world applications
where some conflicting objectives can not be optimized by a single solution.
Existing optimization methods often focus on finding a set of Pareto solutions
with different optimal trade-offs among the objectives. However, the required
number of solutions to well approximate the whole Pareto optimal set could be
exponentially large with respect to the number of objectives, which makes these
methods unsuitable for handling many optimization objectives. In this work,
instead of finding a dense set of Pareto solutions, we propose a novel
Tchebycheff set scalarization method to find a few representative solutions
(e.g., 5) to cover a large number of objectives (e.g., $>100$) in a
collaborative and complementary manner. In this way, each objective can be well
addressed by at least one solution in the small solution set. In addition, we
further develop a smooth Tchebycheff set scalarization approach for efficient
optimization with good theoretical guarantees. Experimental studies on
different problems with many optimization objectives demonstrate the
effectiveness of our proposed method.
`,authors:"Xi Lin; Yilu Liu; Xiaoyuan Zhang; Fei Liu; Zhenkun Wang; Qingfu Zhang",status:0,relevancy:.32411345654089296,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20024",date:"2024-05-30",title:`Applications of Generative AI (GAI) for Mobile and Wireless Networking:
  A Survey`,abstract:`  The success of Artificial Intelligence (AI) in multiple disciplines and
vertical domains in recent years has promoted the evolution of mobile
networking and the future Internet toward an AI-integrated Internet-of-Things
(IoT) era. Nevertheless, most AI techniques rely on data generated by physical
devices (e.g., mobile devices and network nodes) or specific applications
(e.g., fitness trackers and mobile gaming). To bypass this circumvent,
Generative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a
powerful AI paradigm; thanks to its ability to efficiently learn complex data
distributions and generate synthetic data to represent the original data in
various forms. This impressive feature is projected to transform the management
of mobile networking and diversify the current services and applications
provided. On this basis, this work presents a concise tutorial on the role of
GAIs in mobile and wireless networking. In particular, this survey first
provides the fundamentals of GAI and representative GAI models, serving as an
essential preliminary to the understanding of the applications of GAI in mobile
and wireless networking. Then, this work provides a comprehensive review of
state-of-the-art studies and GAI applications in network management, wireless
security, semantic communication, and lessons learned from the open literature.
Finally, this work summarizes the current research on GAI for mobile and
wireless networking by outlining important challenges that need to be resolved
to facilitate the development and applicability of GAI in this edge-cutting
area.
`,authors:"Thai-Hoc Vu; Senthil Kumar Jagatheesaperumal; Minh-Duong Nguyen; Nguyen Van Huynh; Sunghwan Kim; Quoc-Viet Pham",status:0,relevancy:.3177001823446892,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19730",date:"2024-05-30",title:`Research on Foundation Model for Spatial Data Intelligence: China's 2024
  White Paper on Strategic Development of Spatial Data Intelligence`,abstract:`  This report focuses on spatial data intelligent large models, delving into
the principles, methods, and cutting-edge applications of these models. It
provides an in-depth discussion on the definition, development history, current
status, and trends of spatial data intelligent large models, as well as the
challenges they face. The report systematically elucidates the key technologies
of spatial data intelligent large models and their applications in urban
environments, aerospace remote sensing, geography, transportation, and other
scenarios. Additionally, it summarizes the latest application cases of spatial
data intelligent large models in themes such as urban development, multimodal
systems, remote sensing, smart transportation, and resource environments.
Finally, the report concludes with an overview and outlook on the development
prospects of spatial data intelligent large models.
`,authors:"Shaohua Wang; Xing Xie; Yong Li; Danhuai Guo; Zhi Cai; Yu Liu; Yang Yue; Xiao Pan; Feng Lu; Huayi Wu; Zhipeng Gui; Zhiming Ding; Bolong Zheng; Fuzheng Zhang; Tao Qin; Jingyuan Wang; Chuang Tao; Zhengchao Chen; Hao Lu; Jiayi Li; Hongyang Chen; Peng Yue; Wenhao Yu; Yao Yao; Leilei Sun; Yong Zhang; Longbiao Chen; Xiaoping Du; Xiang Li; Xueying Zhang; Kun Qin; Zhaoya Gong; Weihua Dong; Xiaofeng Meng",status:0,relevancy:.3171888362381221,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19957",date:"2024-05-30",title:"PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting",abstract:`  As text-conditioned diffusion models (DMs) achieve breakthroughs in image,
video, and 3D generation, the research community's focus has shifted to the
more challenging task of text-to-4D synthesis, which introduces a temporal
dimension to generate dynamic 3D objects. In this context, we identify Score
Distillation Sampling (SDS), a widely used technique for text-to-3D synthesis,
as a significant hindrance to text-to-4D performance due to its Janus-faced and
texture-unrealistic problems coupled with high computational costs. In this
paper, we propose \\textbf{P}ixel-\\textbf{L}evel \\textbf{A}lignments for
Text-to-\\textbf{4D} Gaussian Splatting (\\textbf{PLA4D}), a novel method that
utilizes text-to-video frames as explicit pixel alignment targets to generate
static 3D objects and inject motion into them. Specifically, we introduce Focal
Alignment to calibrate camera poses for rendering and GS-Mesh Contrastive
Learning to distill geometry priors from rendered image contrasts at the pixel
level. Additionally, we develop Motion Alignment using a deformation network to
drive changes in Gaussians and implement Reference Refinement for smooth 4D
object surfaces. These techniques enable 4D Gaussian Splatting to align
geometry, texture, and motion with generated videos at the pixel level.
Compared to previous methods, PLA4D produces synthesized outputs with better
texture details in less time and effectively mitigates the Janus-faced problem.
PLA4D is fully implemented using open-source models, offering an accessible,
user-friendly, and promising direction for 4D digital content creation. Our
project page:
\\href{https://github.com/MiaoQiaowei/PLA4D.github.io}{https://github.com/MiaoQiaowei/PLA4D.github.io}.
`,authors:"Qiaowei Miao; Yawei Luo; Yi Yang",status:0,relevancy:.31119368259059876,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19970",date:"2024-05-30",title:`Strategies to Counter Artificial Intelligence in Law Enforcement:
  Cross-Country Comparison of Citizens in Greece, Italy and Spain`,abstract:`  This paper investigates citizens' counter-strategies to the use of Artificial
Intelligence (AI) by law enforcement agencies (LEAs). Based on information from
three countries (Greece, Italy and Spain) we demonstrate disparities in the
likelihood of ten specific counter-strategies. We further identified factors
that increase the propensity for counter-strategies. Our study provides an
important new perspective to societal impacts of security-focused AI
applications by illustrating the conscious, strategic choices by citizens when
confronted with AI capabilities for LEAs.
`,authors:"Petra Saskia Bayerl; Babak Akhgar; Ernesto La Mattina; Barbara Pirillo; Ioana Cotoi; Davide Ariu; Matteo Mauri; Jorge Garcia; Dimitris Kavallieros; Antonia Kardara; Konstantina Karagiorgou",status:0,relevancy:.3100039863386229,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20121",date:"2024-05-30",title:`A Structure-Aware Lane Graph Transformer Model for Vehicle Trajectory
  Prediction`,abstract:`  Accurate prediction of future trajectories for surrounding vehicles is vital
for the safe operation of autonomous vehicles. This study proposes a Lane Graph
Transformer (LGT) model with structure-aware capabilities. Its key contribution
lies in encoding the map topology structure into the attention mechanism. To
address variations in lane information from different directions, four Relative
Positional Encoding (RPE) matrices are introduced to capture the local details
of the map topology structure. Additionally, two Shortest Path Distance (SPD)
matrices are employed to capture distance information between two accessible
lanes. Numerical results indicate that the proposed LGT model achieves a
significantly higher prediction performance on the Argoverse 2 dataset.
Specifically, the minFDE$_6$ metric was decreased by 60.73% compared to the
Argoverse 2 baseline model (Nearest Neighbor) and the b-minFDE$_6$ metric was
reduced by 2.65% compared to the baseline LaneGCN model. Furthermore, ablation
experiments demonstrated that the consideration of map topology structure led
to a 4.24% drop in the b-minFDE$_6$ metric, validating the effectiveness of
this model.
`,authors:"Sun Zhanbo; Dong Caiyin; Ji Ang; Zhao Ruibin; Zhao Yu",status:0,relevancy:.3070301747801837,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19644",date:"2024-05-30",title:`EgoSurgery-Phase: A Dataset of Surgical Phase Recognition from
  Egocentric Open Surgery Videos`,abstract:`  Surgical phase recognition has gained significant attention due to its
potential to offer solutions to numerous demands of the modern operating room.
However, most existing methods concentrate on minimally invasive surgery (MIS),
leaving surgical phase recognition for open surgery understudied. This
discrepancy is primarily attributed to the scarcity of publicly available open
surgery video datasets for surgical phase recognition. To address this issue,
we introduce a new egocentric open surgery video dataset for phase recognition,
named EgoSurgery-Phase. This dataset comprises 15 hours of real open surgery
videos spanning 9 distinct surgical phases all captured using an egocentric
camera attached to the surgeon's head. In addition to video, the
EgoSurgery-Phase offers eye gaze. As far as we know, it is the first real open
surgery video dataset for surgical phase recognition publicly available.
Furthermore, inspired by the notable success of masked autoencoders (MAEs) in
video understanding tasks (e.g., action recognition), we propose a gaze-guided
masked autoencoder (GGMAE). Considering the regions where surgeons' gaze
focuses are often critical for surgical phase recognition (e.g., surgical
field), in our GGMAE, the gaze information acts as an empirical semantic
richness prior to guiding the masking process, promoting better attention to
semantically rich spatial regions. GGMAE significantly improves the previous
state-of-the-art recognition method (6.4% in Jaccard) and the masked
autoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase. The dataset
will be released at https://github.com/Fujiry0/EgoSurgery.
`,authors:"Ryo Fujii; Masashi Hatano; Hideo Saito; Hiroki Kajita",status:0,relevancy:.29810934871568195,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20278",date:"2024-05-30",title:`Length independent generalization bounds for deep SSM architectures with
  stability constraints`,abstract:`  Many state-of-the-art models trained on long-range sequences, for example S4,
S5 or LRU, are made of sequential blocks combining State-Space Models (SSMs)
with neural networks. In this paper we provide a PAC bound that holds for these
kind of architectures with stable SSM blocks and does not depend on the length
of the input sequence. Imposing stability of the SSM blocks is a standard
practice in the literature, and it is known to help performance. Our results
provide a theoretical justification for the use of stable SSM blocks as the
proposed PAC bound decreases as the degree of stability of the SSM blocks
increases.
`,authors:"Dániel Rácz; Mihály Petreczky; Bálint Daróczy",status:0,relevancy:.28922355853302295,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20142",date:"2024-05-30",title:`MSSC-BiMamba: Multimodal Sleep Stage Classification and Early Diagnosis
  of Sleep Disorders with Bidirectional Mamba`,abstract:`  Background and Objectives: Monitoring sleep states is crucial for assessing
sleep quality and diagnosing sleep disorders. Traditional manual staging
methods are not only time-consuming but also subject to subjective judgment,
leading to inconsistent results. This study developed an automated sleep
staging and sleep disorder classification model through deep learning
technology, aimed at improving diagnostic accuracy and efficiency.
  Methods: Considering the characteristics of polysomnography (PSG) multi-lead
sleep monitoring, we designed a sleep state classification model, MSSC-BiMamba,
that combines an Efficient Channel Attention (ECA) mechanism with a
Bidirectional State Space Model (BSSM). The ECA module allows for weighting
data from different sensor channels, thereby amplifying the influence of
diverse sensor inputs. Additionally, the implementation of mamba enables the
model to effectively capture the multidimensional features and long-range
dependencies of PSG data.
  Results: The developed model demonstrated impressive performance on sleep
stage classification tasks. Furthermore, the model exhibited an accuracy of
0.952 for sleep health prediction when evaluated on a combined dataset
consisting of ISRUC and Sleep-EDF.
  Conclusion: Our model is the first to apply the bidirectional Mamba to sleep
staging with complex PSG data, showing substantial gains in computational and
memory efficiency over traditional Transformer-style models. This method not
only makes health monitoring more accessible but also broadens the reach of
advanced healthcare, thereby enhancing sleep health management with innovative
technology.
`,authors:"Chao Zhanga; Weirong Cuia; Jingjing Guo",status:0,relevancy:.27258357389492793,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19915",date:"2024-05-30",title:`P$^2$-ViT: Power-of-Two Post-Training Quantization and Acceleration for
  Fully Quantized Vision Transformer`,abstract:`  Vision Transformers (ViTs) have excelled in computer vision tasks but are
memory-consuming and computation-intensive, challenging their deployment on
resource-constrained devices. To tackle this limitation, prior works have
explored ViT-tailored quantization algorithms but retained floating-point
scaling factors, which yield non-negligible re-quantization overhead, limiting
ViTs' hardware efficiency and motivating more hardware-friendly solutions. To
this end, we propose \\emph{P$^2$-ViT}, the first \\underline{P}ower-of-Two (PoT)
\\underline{p}ost-training quantization and acceleration framework to accelerate
fully quantized ViTs. Specifically, {as for quantization,} we explore a
dedicated quantization scheme to effectively quantize ViTs with PoT scaling
factors, thus minimizing the re-quantization overhead. Furthermore, we propose
coarse-to-fine automatic mixed-precision quantization to enable better
accuracy-efficiency trade-offs. {In terms of hardware,} we develop {a dedicated
chunk-based accelerator} featuring multiple tailored sub-processors to
individually handle ViTs' different types of operations, alleviating
reconfigurable overhead. Additionally, we design {a tailored row-stationary
dataflow} to seize the pipeline processing opportunity introduced by our PoT
scaling factors, thereby enhancing throughput. Extensive experiments
consistently validate P$^2$-ViT's effectiveness. {Particularly, we offer
comparable or even superior quantization performance with PoT scaling factors
when compared to the counterpart with floating-point scaling factors. Besides,
we achieve up to $\\mathbf{10.1\\times}$ speedup and $\\mathbf{36.8\\times}$ energy
saving over GPU's Turing Tensor Cores, and up to $\\mathbf{1.84\\times}$ higher
computation utilization efficiency against SOTA quantization-based ViT
accelerators. Codes are available at
\\url{https://github.com/shihuihong214/P2-ViT}.
`,authors:"Huihong Shi; Xin Cheng; Wendong Mao; Zhongfeng Wang",status:0,relevancy:.27078654329183693,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20138",date:"2024-05-30",title:`Separation and Collapse of Equilibria Inequalities on AND-OR Trees
  without Shape Constraints`,abstract:`  Herein, we investigate the randomized complexity, which is the least cost
against the worst input, of AND-OR tree computation by imposing various
restrictions on the algorithm to find the Boolean value of the root of that
tree and no restrictions on the tree shape. When a tree satisfies a certain
condition regarding its symmetry, directional algorithms proposed by Saks and
Wigderson (1986), special randomized algorithms, are known to achieve the
randomized complexity. Furthermore, there is a known example of a tree that is
so unbalanced that no directional algorithm achieves the randomized complexity
(Vereshchagin 1998). In this study, we aim to identify where deviations arise
between the general randomized Boolean decision tree and its special case,
directional algorithms. In this paper, we show that for any AND-OR tree,
randomized depth-first algorithms, which form a broader class compared with
directional algorithms, have the same equilibrium as that of the directional
algorithms. Thus, we get the collapse result on equilibria inequalities that
holds for an arbitrary AND-OR tree. This implies that there exists a case where
even depth-first algorithms cannot be the fastest, leading to the separation
result on equilibria inequality. Additionally, a new algorithm is introduced as
a key concept for proof of the separation result.
`,authors:"Fuki Ito; Toshio Suzuki",status:0,relevancy:.2657009635498164,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20237",date:"2024-05-30",title:"Training-efficient density quantum machine learning",abstract:`  Quantum machine learning requires powerful, flexible and efficiently
trainable models to be successful in solving challenging problems. In this
work, we present density quantum neural networks, a learning model
incorporating randomisation over a set of trainable unitaries. These models
generalise quantum neural networks using parameterised quantum circuits, and
allow a trade-off between expressibility and efficient trainability,
particularly on quantum hardware. We demonstrate the flexibility of the
formalism by applying it to two recently proposed model families. The first are
commuting-block quantum neural networks (QNNs) which are efficiently trainable
but may be limited in expressibility. The second are orthogonal (Hamming-weight
preserving) quantum neural networks which provide well-defined and
interpretable transformations on data but are challenging to train at scale on
quantum devices. Density commuting QNNs improve capacity with minimal gradient
complexity overhead, and density orthogonal neural networks admit a
quadratic-to-constant gradient query advantage with minimal to no performance
loss. We conduct numerical experiments on synthetic translationally invariant
data and MNIST image data with hyperparameter optimisation to support our
findings. Finally, we discuss the connection to post-variational quantum neural
networks, measurement-based quantum machine learning and the dropout mechanism.
`,authors:"Brian Coyle; El Amine Cherrat; Nishant Jain; Natansh Mathur; Snehal Raj; Skander Kazdaghli; Iordanis Kerenidis",status:0,relevancy:.2616809235221397,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19823",date:"2024-05-30",title:`Joint Selective State Space Model and Detrending for Robust Time Series
  Anomaly Detection`,abstract:`  Deep learning-based sequence models are extensively employed in Time Series
Anomaly Detection (TSAD) tasks due to their effective sequential modeling
capabilities. However, the ability of TSAD is limited by two key challenges:
(i) the ability to model long-range dependency and (ii) the generalization
issue in the presence of non-stationary data. To tackle these challenges, an
anomaly detector that leverages the selective state space model known for its
proficiency in capturing long-term dependencies across various domains is
proposed. Additionally, a multi-stage detrending mechanism is introduced to
mitigate the prominent trend component in non-stationary data to address the
generalization issue. Extensive experiments conducted on realworld public
datasets demonstrate that the proposed methods surpass all 12 compared baseline
methods.
`,authors:"Junqi Chen; Xu Tan; Sylwan Rahardja; Jiawei Yang; Susanto Rahardja",status:0,relevancy:.24535769062368173,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19864",date:"2024-05-30",title:`Out-of-distribution Reject Option Method for Dataset Shift Problem in
  Early Disease Onset Prediction`,abstract:`  Machine learning is increasingly used to predict lifestyle-related disease
onset using health and medical data. However, the prediction effectiveness is
hindered by dataset shift, which involves discrepancies in data distribution
between the training and testing datasets, misclassifying out-of-distribution
(OOD) data. To diminish dataset shift effects, this paper proposes the
out-of-distribution reject option for prediction (ODROP), which integrates OOD
detection models to preclude OOD data from the prediction phase. We
investigated the efficacy of five OOD detection methods (variational
autoencoder, neural network ensemble std, neural network ensemble epistemic,
neural network energy, and neural network gaussian mixture based energy
measurement) across two datasets, the Hirosaki and Wakayama health checkup
data, in the context of three disease onset prediction tasks: diabetes,
dyslipidemia, and hypertension. To evaluate the ODROP method, we trained
disease onset prediction models and OOD detection models on Hirosaki data and
used AUROC-rejection curve plots from Wakayama data. The variational
autoencoder method showed superior stability and magnitude of improvement in
Area Under the Receiver Operating Curve (AUROC) in five cases: AUROC in the
Wakayama data was improved from 0.80 to 0.90 at a 31.1% rejection rate for
diabetes onset and from 0.70 to 0.76 at a 34% rejection rate for dyslipidemia.
We categorized dataset shifts into two types using SHAP clustering - those that
considerably affect predictions and those that do not. We expect that this
classification will help standardize measuring instruments. This study is the
first to apply OOD detection to actual health and medical data, demonstrating
its potential to substantially improve the accuracy and reliability of disease
prediction models amidst dataset shift.
`,authors:"Taisei Tosaki; Eiichiro Uchino; Ryosuke Kojima; Yohei Mineharu; Mikio Arita; Nobuyuki Miyai; Yoshinori Tamada; Tatsuya Mikami; Koichi Murashita; Shigeyuki Nakaji; Yasushi Okuno",status:0,relevancy:.23121717909191675,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.19751",date:"2024-05-30",title:"HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization",abstract:`  Diffusion Transformers (DiTs) have recently gained substantial attention in
both industrial and academic fields for their superior visual generation
capabilities, outperforming traditional diffusion models that use U-Net.
However,the enhanced performance of DiTs also comes with high parameter counts
and implementation costs, seriously restricting their use on resource-limited
devices such as mobile phones. To address these challenges, we introduce the
Hybrid Floating-point Quantization for DiT(HQ-DiT), an efficient post-training
quantization method that utilizes 4-bit floating-point (FP) precision on both
weights and activations for DiT inference. Compared to fixed-point quantization
(e.g., INT8), FP quantization, complemented by our proposed clipping range
selection mechanism, naturally aligns with the data distribution within DiT,
resulting in a minimal quantization error. Furthermore, HQ-DiT also implements
a universal identity mathematical transform to mitigate the serious
quantization error caused by the outliers. The experimental results demonstrate
that DiT can achieve extremely low-precision quantization (i.e., 4 bits) with
negligible impact on performance. Our approach marks the first instance where
both weights and activations in DiTs are quantized to just 4 bits, with only a
0.12 increase in sFID on ImageNet.
`,authors:"Wenxuan Liu; Saiqian Zhang",status:0,relevancy:.21837478821610912,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20059",date:"2024-05-30",title:"Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation",abstract:`  Separating vocal elements from musical tracks is a longstanding challenge in
audio signal processing. This study tackles the distinct separation of vocal
components from musical spectrograms. We employ the Short Time Fourier
Transform (STFT) to extract audio waves into detailed frequency-time
spectrograms, utilizing the benchmark MUSDB18 dataset for music separation.
Subsequently, we implement a UNet neural network to segment the spectrogram
image, aiming to delineate and extract singing voice components accurately. We
achieved noteworthy results in audio source separation using of our U-Net-based
models. The combination of frequency-axis normalization with Min/Max scaling
and the Mean Absolute Error (MAE) loss function achieved the highest
Source-to-Distortion Ratio (SDR) of 7.1 dB, indicating a high level of accuracy
in preserving the quality of the original signal during separation. This setup
also recorded impressive Source-to-Interference Ratio (SIR) and
Source-to-Artifact Ratio (SAR) scores of 25.2 dB and 7.2 dB, respectively.
These values significantly outperformed other configurations, particularly
those using Quantile-based normalization or a Mean Squared Error (MSE) loss
function. Our source code, model weights, and demo material can be found at the
project's GitHub repository: https://github.com/mbrotos/SoundSeg
`,authors:"Adam Sorrenti",status:0,relevancy:.2073597021974346,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}},{id:"2405.20032",date:"2024-05-30",title:`Promptus: Can Prompts Streaming Replace Video Streaming with Stable
  Diffusion`,abstract:`  With the exponential growth of video traffic, traditional video streaming
systems are approaching their limits in compression efficiency and
communication capacity. To further reduce bitrate while maintaining quality, we
propose Promptus, a disruptive novel system that streaming prompts instead of
video content with Stable Diffusion, which converts video frames into a series
of "prompts" for delivery. To ensure pixel alignment, a gradient descent-based
prompt fitting framework is proposed. To achieve adaptive bitrate for prompts,
a low-rank decomposition-based bitrate control algorithm is introduced. For
inter-frame compression of prompts, a temporal smoothing-based prompt
interpolation algorithm is proposed. Evaluations across various video domains
and real network traces demonstrate Promptus can enhance the perceptual quality
by 0.111 and 0.092 (in LPIPS) compared to VAE and H.265, respectively, and
decreases the ratio of severely distorted frames by 89.3% and 91.7%. Moreover,
Promptus achieves real-time video generation from prompts at over 150 FPS. To
the best of our knowledge, Promptus is the first attempt to replace video
codecs with prompt inversion and the first to use prompt streaming instead of
video streaming. Our work opens up a new paradigm for efficient video
communication beyond the Shannon limit.
`,authors:"Jiangkai Wu; Liming Liu; Yunpeng Tan; Junlin Hao; Xinggong Zhang",status:0,relevancy:.1889204359426131,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:58:28.951Z",updatedAt:"2024-05-31T04:58:28.951Z",DatesTable:{value:"2024-05-30",status:"complete",count:118,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:58:28.953Z"}}]},{date:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 05:21:04.775 +00:00"},papers:[{id:"2405.18711",date:"2024-05-29",title:"Calibrating Reasoning in Language Models with Internal Consistency",abstract:`  Large language models (LLMs) have demonstrated impressive capabilities in
various reasoning tasks, aided by techniques like chain-of-thought (CoT)
prompting that elicits verbalized reasoning. However, LLMs often generate text
with obvious mistakes and contradictions, raising doubts about their ability to
robustly process and utilize generated rationales. In this work, we investigate
CoT reasoning in LLMs through the lens of internal representations, focusing on
how these representations are influenced by generated rationales. Our
preliminary analysis reveals that while generated rationales improve answer
accuracy, inconsistencies emerge between the model's internal representations
in middle layers and those in final layers, potentially undermining the
reliability of their reasoning processes. To address this, we propose internal
consistency as a measure of the model's confidence by examining the agreement
of latent predictions decoded from intermediate layers. Extensive empirical
studies across different models and datasets demonstrate that internal
consistency effectively distinguishes between correct and incorrect reasoning
paths. Motivated by this, we propose a new approach to calibrate CoT reasoning
by up-weighting reasoning paths with high internal consistency, resulting in a
significant boost in reasoning performance. Further analysis uncovers distinct
patterns in attention and feed-forward modules across layers, providing
insights into the emergence of internal inconsistency. In summary, our results
demonstrate the potential of using internal representations for self-evaluation
of LLMs.
`,authors:"Zhihui Xie; Jizhou Guo; Tong Yu; Shuai Li",status:0,relevancy:.623071608712894,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19332",date:"2024-05-29",title:`Self-Exploring Language Models: Active Preference Elicitation for Online
  Alignment`,abstract:`  Preference optimization, particularly through Reinforcement Learning from
Human Feedback (RLHF), has achieved significant success in aligning Large
Language Models (LLMs) to adhere to human intentions. Unlike offline alignment
with a fixed dataset, online feedback collection from humans or AI on model
generations typically leads to more capable reward models and better-aligned
LLMs through an iterative process. However, achieving a globally accurate
reward model requires systematic exploration to generate diverse responses that
span the vast space of natural language. Random sampling from standard
reward-maximizing LLMs alone is insufficient to fulfill this requirement. To
address this issue, we propose a bilevel objective optimistically biased
towards potentially high-reward responses to actively explore
out-of-distribution regions. By solving the inner-level problem with the
reparameterized reward function, the resulting algorithm, named Self-Exploring
Language Models (SELM), eliminates the need for a separate RM and iteratively
updates the LLM with a straightforward objective. Compared to Direct Preference
Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen
extrapolations and enhances exploration efficiency. Our experimental results
demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct
models, SELM significantly boosts the performance on instruction-following
benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard
academic benchmarks in different settings. Our code and models are available at
https://github.com/shenao-zhang/SELM.
`,authors:"Shenao Zhang; Donghan Yu; Hiteshi Sharma; Ziyi Yang; Shuohang Wang; Hany Hassan; Zhaoran Wang",status:0,relevancy:.6221507463363094,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18733",date:"2024-05-29",title:`Efficient Learning in Chinese Checkers: Comparing Parameter Sharing in
  Multi-Agent Reinforcement Learning`,abstract:`  We show that multi-agent reinforcement learning (MARL) with full parameter
sharing outperforms independent and partially shared architectures in the
competitive perfect-information homogenous game of Chinese Checkers. To run our
experiments, we develop a new MARL environment: variable-size, six-player
Chinese Checkers. This custom environment was developed in PettingZoo and
supports all traditional rules of the game including chaining jumps. This is,
to the best of our knowledge, the first implementation of Chinese Checkers that
remains faithful to the true game.
  Chinese Checkers is difficult to learn due to its large branching factor and
potentially infinite horizons. We borrow the concept of branching actions
(submoves) from complex action spaces in other RL domains, where a submove may
not end a player's turn immediately. This drastically reduces the
dimensionality of the action space. Our observation space is inspired by
AlphaGo with many binary game boards stacked in a 3D array to encode
information.
  The PettingZoo environment, training and evaluation logic, and analysis
scripts can be found on
\\href{https://github.com/noahadhikari/pettingzoo-chinese-checkers}{Github}.
`,authors:"Noah Adhikari; Allen Gu",status:0,relevancy:.6140474602633165,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19444",date:"2024-05-29",title:`MathChat: Benchmarking Mathematical Reasoning and Instruction Following
  in Multi-Turn Interactions`,abstract:`  Large language models (LLMs) have demonstrated impressive capabilities in
mathematical problem solving, particularly in single turn question answering
formats. However, real world scenarios often involve mathematical question
answering that requires multi turn or interactive information exchanges, and
the performance of LLMs on these tasks is still underexplored. This paper
introduces MathChat, a comprehensive benchmark specifically designed to
evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are
structured to assess the models' abilities in multiturn interactions and open
ended generation. We evaluate the performance of various SOTA LLMs on the
MathChat benchmark, and we observe that while these models excel in single turn
question answering, they significantly underperform in more complex scenarios
that require sustained reasoning and dialogue understanding. To address the
above limitations of existing LLMs when faced with multiturn and open ended
tasks, we develop MathChat sync, a synthetic dialogue based math dataset for
LLM finetuning, focusing on improving models' interaction and instruction
following capabilities in conversations. Experimental results emphasize the
need for training LLMs with diverse, conversational instruction tuning datasets
like MathChatsync. We believe this work outlines one promising direction for
improving the multiturn mathematical reasoning abilities of LLMs, thus pushing
forward the development of LLMs that are more adept at interactive mathematical
problem solving and real world applications.
`,authors:"Zhenwen Liang; Dian Yu; Wenhao Yu; Wenlin Yao; Zhihan Zhang; Xiangliang Zhang; Dong Yu",status:0,relevancy:.610756709252437,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18682",date:"2024-05-29",title:`Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical
  Machine Reading Comprehension`,abstract:`  Large language models (LLMs) have shown remarkable performance on many tasks
in different domains. However, their performance in closed-book biomedical
machine reading comprehension (MRC) has not been evaluated in depth. In this
work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We
experiment with different conventional prompting techniques as well as
introduce our own novel prompting method. To solve some of the retrieval
problems inherent to LLMs, we propose a prompting strategy named Implicit
Retrieval Augmented Generation (RAG) that alleviates the need for using vector
databases to retrieve important chunks in traditional RAG setups. Moreover, we
report qualitative assessments on the natural language generation outputs from
our approach. The results show that our new prompting technique is able to get
the best performance in two out of four datasets and ranks second in rest of
them. Experiments show that modern-day LLMs like GPT even in a zero-shot
setting can outperform supervised models, leading to new state-of-the-art
(SoTA) results on two of the benchmarks.
`,authors:"Shubham Vatsal; Ayush Singh",status:0,relevancy:.5998147217848517,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19262",date:"2024-05-29",title:`Weak-to-Strong Search: Align Large Language Models via Searching over
  Small Language Models`,abstract:`  Large language models are usually fine-tuned to align with human preferences.
However, fine-tuning a large language model can be challenging. In this work,
we introduce $\\textit{weak-to-strong search}$, framing the alignment of a large
language model as a test-time greedy search to maximize the log-likelihood
difference between small tuned and untuned models while sampling from the
frozen large model. This method serves both as (i) a compute-efficient model
up-scaling strategy that avoids directly tuning the large model and as (ii) an
instance of weak-to-strong generalization that enhances a strong model with
weak test-time guidance. Empirically, we demonstrate the flexibility of
weak-to-strong search across different tasks. In controlled-sentiment
generation and summarization, we use tuned and untuned $\\texttt{gpt2}$s to
effectively improve the alignment of large models without additional training.
Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0,
we show that reusing off-the-shelf small model pairs (e.g.,
$\\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve
the length-controlled win rates of both white-box and black-box large models
against $\\texttt{gpt-4-turbo}$ (e.g., $34.4 \\rightarrow 37.9$ for
$\\texttt{Llama-3-70B-Instruct}$ and $16.0 \\rightarrow 20.1$ for
$\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates
$\\approx 10.0$.
`,authors:"Zhanhui Zhou; Zhixuan Liu; Jie Liu; Zhichen Dong; Chao Yang; Yu Qiao",status:0,relevancy:.5852520404317045,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18727",date:"2024-05-29",title:"CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control",abstract:`  Retrieval-augmented generation (RAG) has emerged as a promising solution for
mitigating hallucinations of large language models (LLMs) with retrieved
external knowledge. Adaptive RAG enhances this approach by dynamically
assessing the retrieval necessity, aiming to balance external and internal
knowledge usage. However, existing adaptive RAG methods primarily realize
retrieval on demand by relying on superficially verbalize-based or
probability-based feedback of LLMs, or directly fine-tuning LLMs via carefully
crafted datasets, resulting in unreliable retrieval necessity decisions, heavy
extra costs, and sub-optimal response generation. We present the first attempts
to delve into the internal states of LLMs to mitigate such issues by
introducing an effective probe-guided adaptive RAG framework, termed CtrlA.
Specifically, CtrlA employs an honesty probe to regulate the LLM's behavior by
manipulating its representations for increased honesty, and a confidence probe
to monitor the internal states of LLM and assess confidence levels, determining
the retrieval necessity during generation. Experiments show that CtrlA is
superior to existing adaptive RAG methods on a diverse set of tasks, the
honesty control can effectively make LLMs more honest and confidence monitoring
is proven to be a promising indicator of retrieval trigger. Our codes are
available at https://github.com/HSLiu-Initial/CtrlA.git.
`,authors:"Huanshuo Liu; Hao Zhang; Zhijiang Guo; Kuicai Dong; Xiangyang Li; Yi Quan Lee; Cong Zhang; Yong Liu",status:0,relevancy:.5702015226569191,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19094",date:"2024-05-29",title:"Faithful Chart Summarization with ChaTS-Pi",abstract:`  Chart-to-summary generation can help explore data, communicate insights, and
help the visually impaired people. Multi-modal generative models have been used
to produce fluent summaries, but they can suffer from factual and perceptual
errors. In this work we present CHATS-CRITIC, a reference-free chart
summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an
image-to-text model to recover the table from a chart, and a tabular entailment
model applied to score the summary sentence by sentence. We find that
CHATS-CRITIC evaluates the summary quality according to human ratings better
than reference-based metrics, either learned or n-gram based, and can be
further used to fix candidate summaries by removing not supported sentences. We
then introduce CHATS-PI, a chart-to-summary pipeline that leverages
CHATS-CRITIC during inference to fix and rank sampled candidates from any
chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human
raters, establishing state-of-the-art results on two popular chart-to-summary
datasets.
`,authors:"Syrine Krichene; Francesco Piccinno; Fangyu Liu; Julian Martin Eisenschlos",status:0,relevancy:.568534057467102,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19080",date:"2024-05-29",title:"OMPO: A Unified Framework for RL under Policy and Dynamics Shifts",abstract:`  Training reinforcement learning policies using environment interaction data
collected from varying policies or dynamics presents a fundamental challenge.
Existing works often overlook the distribution discrepancies induced by policy
or dynamics shifts, or rely on specialized algorithms with task priors, thus
often resulting in suboptimal policy performances and high learning variances.
In this paper, we identify a unified strategy for online RL policy learning
under diverse settings of policy and dynamics shifts: transition occupancy
matching. In light of this, we introduce a surrogate policy learning objective
by considering the transition occupancy discrepancies and then cast it into a
tractable min-max optimization problem through dual reformulation. Our method,
dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized
actor-critic structure equipped with a distribution discriminator and a
small-size local buffer. We conduct extensive experiments based on the OpenAI
Gym, Meta-World, and Panda Robots environments, encompassing policy shifts
under stationary and nonstationary dynamics, as well as domain adaption. The
results demonstrate that OMPO outperforms the specialized baselines from
different categories in all settings. We also find that OMPO exhibits
particularly strong performance when combined with domain randomization,
highlighting its potential in RL-based robotics applications
`,authors:"Yu Luo; Tianying Ji; Fuchun Sun; Jianwei Zhang; Huazhe Xu; Xianyuan Zhan",status:0,relevancy:.5545371968401408,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19220",date:"2024-05-29",title:`WRDScore: New Metric for Evaluation of Natural Language Generation
  Models`,abstract:`  The problem of natural language generation, and, more specifically, method
name prediction, faces significant difficulties when proposed models need to be
evaluated on test data. Such a metric would need to consider the versatility
with which a single method can be named, with respect to both semantics and
syntax. Measuring the direct overlap between the predicted and reference (true)
sequences will not be able to capture these subtleties. Other existing
embedding based metrics either do not measure precision and recall or impose
strict unrealistic assumptions on both sequences. To address these issues, we
propose a new metric that, on the one hand, is very simple and lightweight,
and, on the other hand, is able to calculate precision and recall without
resorting to any assumptions while obtaining good performance with respect to
the human judgement.
`,authors:"Ravil Mussabayev",status:0,relevancy:.5544849074751874,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19313",date:"2024-05-29",title:`Language Models Trained to do Arithmetic Predict Human Risky and
  Intertemporal Choice`,abstract:`  The observed similarities in the behavior of humans and Large Language Models
(LLMs) have prompted researchers to consider the potential of using LLMs as
models of human cognition. However, several significant challenges must be
addressed before LLMs can be legitimately regarded as cognitive models. For
instance, LLMs are trained on far more data than humans typically encounter,
and may have been directly trained on human data in specific cognitive tasks or
aligned with human preferences. Consequently, the origins of these behavioral
similarities are not well understood. In this paper, we propose a novel way to
enhance the utility of LLMs as cognitive models. This approach involves (i)
leveraging computationally equivalent tasks that both an LLM and a rational
agent need to master for solving a cognitive problem and (ii) examining the
specific task distributions required for an LLM to exhibit human-like
behaviors. We apply this approach to decision-making -- specifically risky and
intertemporal choice -- where the key computationally equivalent task is the
arithmetic of expected value calculations. We show that an LLM pretrained on an
ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts
human behavior better than many traditional cognitive models. Pretraining LLMs
on ecologically valid arithmetic datasets is sufficient to produce a strong
correspondence between these models and human decision-making. Our results also
suggest that LLMs used as cognitive models should be carefully investigated via
ablation studies of the pretraining data.
`,authors:"Jian-Qiao Zhu; Haijiang Yan; Thomas L. Griffiths",status:0,relevancy:.5536541798065598,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18915",date:"2024-05-29",title:`Towards Faithful Chain-of-Thought: Large Language Models are Bridging
  Reasoners`,abstract:`  Large language models (LLMs) suffer from serious unfaithful chain-of-thought
(CoT) issues. Previous work attempts to measure and explain it but lacks
in-depth analysis within CoTs and does not consider the interactions among all
reasoning components jointly. In this paper, we first study the CoT
faithfulness issue at the granularity of CoT steps, identify two reasoning
paradigms: centralized reasoning and distributed reasoning, and find their
relationship with faithfulness. Subsequently, we conduct a joint analysis of
the causal relevance among the context, CoT, and answer during reasoning. The
result proves that, when the LLM predicts answers, it can recall correct
information missing in the CoT from the context, leading to unfaithfulness
issues. Finally, we propose the inferential bridging method to mitigate this
issue, in which we use the attribution method to recall information as hints
for CoT generation and filter out noisy CoTs based on their semantic
consistency and attribution scores. Extensive experiments demonstrate that our
approach effectively alleviates the unfaithful CoT problem.
`,authors:"Jiachun Li; Pengfei Cao; Yubo Chen; Kang Liu; Jun Zhao",status:0,relevancy:.5512908293507641,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18664",date:"2024-05-29",title:"Fast Explainability via Feasible Concept Sets Generator",abstract:`  A long-standing dilemma prevents the broader application of explanation
methods: general applicability and inference speed. On the one hand, existing
model-agnostic explanation methods usually make minimal pre-assumptions about
the prediction models to be explained. Still, they require additional queries
to the model through propagation or back-propagation to approximate the models'
behaviors, resulting in slow inference and hindering their use in
time-sensitive tasks. On the other hand, various model-dependent explanations
have been proposed that achieve low-cost, fast inference but at the expense of
limiting their applicability to specific model structures. In this study, we
bridge the gap between the universality of model-agnostic approaches and the
efficiency of model-specific approaches by proposing a novel framework without
assumptions on the prediction model's structures, achieving high efficiency
during inference and allowing for real-time explanations. To achieve this, we
first define explanations through a set of human-comprehensible concepts and
propose a framework to elucidate model predictions via minimal feasible concept
sets. Second, we show that a minimal feasible set generator can be learned as a
companion explainer to the prediction model, generating explanations for
predictions. Finally, we validate this framework by implementing a novel
model-agnostic method that provides robust explanations while facilitating
real-time inference. Our claims are substantiated by comprehensive experiments,
highlighting the effectiveness and efficiency of our approach.
`,authors:"Deng Pan; Nuno Moniz; Nitesh Chawla",status:0,relevancy:.5488259099104661,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19327",date:"2024-05-29",title:`MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model
  Series`,abstract:`  Large Language Models (LLMs) have made great strides in recent years to
achieve unprecedented performance across different tasks. However, due to
commercial interest, the most competitive models like GPT, Gemini, and Claude
have been gated behind proprietary interfaces without disclosing the training
details. Recently, many institutions have open-sourced several strong LLMs like
LLaMA-3, comparable to existing closed-source LLMs. However, only the model's
weights are provided with most details (e.g., intermediate checkpoints,
pre-training corpus, and training code, etc.) being undisclosed. To improve the
transparency of LLMs, the research community has formed to open-source truly
open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training
corpus and training code) are being provided. These models have greatly
advanced the scientific study of these large models including their strengths,
weaknesses, biases and risks. However, we observe that the existing truly open
LLMs on reasoning, knowledge, and coding tasks are still inferior to existing
state-of-the-art LLMs with similar model sizes. To this end, we open-source
MAP-Neo, a highly capable and transparent bilingual language model with 7B
parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the
first fully open-sourced bilingual LLM with comparable performance compared to
existing state-of-the-art LLMs. Moreover, we open-source all details to
reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning
pipeline, checkpoints, and well-optimized training/evaluation framework are
provided. Finally, we hope our MAP-Neo will enhance and strengthen the open
research community and inspire more innovations and creativities to facilitate
the further improvements of LLMs.
`,authors:"Ge Zhang; Scott Qu; Jiaheng Liu; Chenchen Zhang; Chenghua Lin; Chou Leuang Yu; Danny Pan; Esther Cheng; Jie Liu; Qunshu Lin; Raven Yuan; Tuney Zheng; Wei Pang; Xinrun Du; Yiming Liang; Yinghao Ma; Yizhi Li; Ziyang Ma; Bill Lin; Emmanouil Benetos; Huan Yang; Junting Zhou; Kaijing Ma; Minghao Liu; Morry Niu; Noah Wang; Quehry Que; Ruibo Liu; Sine Liu; Shawn Guo; Soren Gao; Wangchunshu Zhou; Xinyue Zhang; Yizhi Zhou; Yubo Wang; Yuelin Bai; Yuhan Zhang; Yuxiang Zhang; Zenith Wang; Zhenzhu Yang; Zijian Zhao; Jiajun Zhang; Wanli Ouyang; Wenhao Huang; Wenhu Chen",status:0,relevancy:.5443444264491701,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18952",date:"2024-05-29",title:`Are You Sure? Rank Them Again: Repeated Ranking For Better Preference
  Datasets`,abstract:`  Training Large Language Models (LLMs) with Reinforcement Learning from AI
Feedback (RLAIF) aligns model outputs more closely with human preferences. This
involves an evaluator model ranking multiple candidate responses to user
prompts. However, the rankings from popular evaluator models such as GPT-4 can
be inconsistent. We propose the Repeat Ranking method - where we evaluate the
same responses multiple times and train only on those responses which are
consistently ranked. Using 2,714 prompts in 62 languages, we generated
responses from 7 top multilingual LLMs and had GPT-4 rank them five times each.
Evaluating on MT-Bench chat benchmarks in six languages, our method
outperformed the standard practice of training on all available prompts. Our
work highlights the quality versus quantity trade-off in RLAIF dataset
generation and offers a stackable strategy for enhancing dataset and thus model
quality.
`,authors:"Peter Devine",status:0,relevancy:.5442714278671904,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19255",date:"2024-05-29",title:`Towards Next-Generation Urban Decision Support Systems through
  AI-Powered Generation of Scientific Ontology using Large Language Models -- A
  Case in Optimizing Intermodal Freight Transportation`,abstract:`  The incorporation of Artificial Intelligence (AI) models into various
optimization systems is on the rise. Yet, addressing complex urban and
environmental management problems normally requires in-depth domain science and
informatics expertise. This expertise is essential for deriving data and
simulation-driven for informed decision support. In this context, we
investigate the potential of leveraging the pre-trained Large Language Models
(LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated
workflow that encompasses natural language processing, methontology-based
prompt tuning, and transformers. This workflow automates the creation of
scenario-based ontology using existing research articles and technical manuals
of urban datasets and simulations. The outcomes of our methodology are
knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).
These facilitate the development of urban decision support systems by enhancing
the data and metadata modeling, the integration of complex datasets, the
coupling of multi-domain simulation models, and the formulation of
decision-making metrics and workflow. The feasibility of our methodology is
evaluated through a comparative analysis that juxtaposes our AI-generated
ontology with the well-known Pizza Ontology employed in tutorials for popular
ontology software (e.g., prot\\'eg\\'e). We close with a real-world case study of
optimizing the complex urban system of multi-modal freight transportation by
generating anthologies of various domain data and simulations to support
informed decision-making.
`,authors:"Jose Tupayachi; Haowen Xu; Olufemi A. Omitaomu; Mustafa Can Camur; Aliza Sharmin; Xueping Li",status:0,relevancy:.5437103103299856,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19107",date:"2024-05-29",title:`Offline Regularised Reinforcement Learning for Large Language Models
  Alignment`,abstract:`  The dominant framework for alignment of large language models (LLM), whether
through reinforcement learning from human feedback or direct preference
optimisation, is to learn from preference data. This involves building datasets
where each element is a quadruplet composed of a prompt, two independent
responses (completions of the prompt) and a human preference between the two
independent responses, yielding a preferred and a dis-preferred response. Such
data is typically scarce and expensive to collect. On the other hand,
\\emph{single-trajectory} datasets where each element is a triplet composed of a
prompt, a response and a human feedback is naturally more abundant. The
canonical element of such datasets is for instance an LLM's response to a
user's prompt followed by a user's feedback such as a thumbs-up/down.
Consequently, in this work, we propose DRO, or \\emph{Direct Reward
Optimisation}, as a framework and associated algorithms that do not require
pairwise preferences. DRO uses a simple mean-squared objective that can be
implemented in various ways. We validate our findings empirically, using T5
encoder-decoder language models, and show DRO's performance over selected
baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that
DRO is a simple and empirically compelling method for single-trajectory policy
optimisation.
`,authors:"Pierre Harvey Richemond; Yunhao Tang; Daniel Guo; Daniele Calandriello; Mohammad Gheshlaghi Azar; Rafael Rafailov; Bernardo Avila Pires; Eugene Tarassov; Lucas Spangher; Will Ellsworth; Aliaksei Severyn; Jonathan Mallinson; Lior Shani; Gil Shamir; Rishabh Joshi; Tianqi Liu; Remi Munos; Bilal Piot",status:0,relevancy:.5432386954237568,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19164",date:"2024-05-29",title:`Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in
  eDiscovery`,abstract:`  Electronic Discovery (eDiscovery) involves identifying relevant documents
from a vast collection based on legal production requests. The integration of
artificial intelligence (AI) and natural language processing (NLP) has
transformed this process, helping document review and enhance efficiency and
cost-effectiveness. Although traditional approaches like BM25 or fine-tuned
pre-trained models are common in eDiscovery, they face performance,
computational, and interpretability challenges. In contrast, Large Language
Model (LLM)-based methods prioritize interpretability but sacrifice performance
and throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid
approach that combines the strengths of two worlds: a heterogeneous graph-based
method for accurate document relevance prediction and subsequent LLM-driven
approach for reasoning. Graph representational learning generates embeddings
and predicts links, ranking the corpus for a given request, and the LLMs
provide reasoning for document relevance. Our approach handles datasets with
balanced and imbalanced distributions, outperforming baselines in F1-score,
precision, and recall by an average of 12%, 3%, and 16%, respectively. In an
enterprise context, our approach drastically reduces document review costs by
99.9% compared to manual processes and by 95% compared to LLM-based
classification methods
`,authors:"Sounak Lahiri; Sumit Pai; Tim Weninger; Sanmitra Bhattacharya",status:0,relevancy:.5420571473534375,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19561",date:"2024-05-29",title:"Quo Vadis ChatGPT? From Large Language Models to Large Knowledge Models",abstract:`  The startling success of ChatGPT and other large language models (LLMs) using
transformer-based generative neural network architecture in applications such
as natural language processing and image synthesis has many researchers excited
about potential opportunities in process systems engineering (PSE). The almost
human-like performance of LLMs in these areas is indeed very impressive,
surprising, and a major breakthrough. Their capabilities are very useful in
certain tasks, such as writing first drafts of documents, code writing
assistance, text summarization, etc. However, their success is limited in
highly scientific domains as they cannot yet reason, plan, or explain due to
their lack of in-depth domain knowledge. This is a problem in domains such as
chemical engineering as they are governed by fundamental laws of physics and
chemistry (and biology), constitutive relations, and highly technical knowledge
about materials, processes, and systems. Although purely data-driven machine
learning has its immediate uses, the long-term success of AI in scientific and
engineering domains would depend on developing hybrid AI systems that use first
principles and technical knowledge effectively. We call these hybrid AI systems
Large Knowledge Models (LKMs), as they will not be limited to only NLP-based
techniques or NLP-like applications. In this paper, we discuss the challenges
and opportunities in developing such systems in chemical engineering.
`,authors:"Venkat Venkatasubramanian; Arijit Chakraborty",status:0,relevancy:.5375948412157391,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19010",date:"2024-05-29",title:`Evaluating the External and Parametric Knowledge Fusion of Large
  Language Models`,abstract:`  Integrating external knowledge into large language models (LLMs) presents a
promising solution to overcome the limitations imposed by their antiquated and
static parametric memory. Prior studies, however, have tended to over-reliance
on external knowledge, underestimating the valuable contributions of an LLMs'
intrinsic parametric knowledge. The efficacy of LLMs in blending external and
parametric knowledge remains largely unexplored, especially in cases where
external knowledge is incomplete and necessitates supplementation by their
parametric knowledge. We propose to deconstruct knowledge fusion into four
distinct scenarios, offering the first thorough investigation of LLM behavior
across each. We develop a systematic pipeline for data construction and
knowledge infusion to simulate these fusion scenarios, facilitating a series of
controlled experiments. Our investigation reveals that enhancing parametric
knowledge within LLMs can significantly bolster their capability for knowledge
integration. Nonetheless, we identify persistent challenges in memorizing and
eliciting parametric knowledge, and determining parametric knowledge
boundaries. Our findings aim to steer future explorations on harmonizing
external and parametric knowledge within LLMs.
`,authors:"Hao Zhang; Yuyang Zhang; Xiaoguang Li; Wenxuan Shi; Haonan Xu; Huanshuo Liu; Yasheng Wang; Lifeng Shang; Qun Liu; Yong Liu; Ruiming Tang",status:0,relevancy:.5373678980927894,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18688",date:"2024-05-29",title:`Efficient Preference-based Reinforcement Learning via Aligned Experience
  Estimation`,abstract:`  Preference-based reinforcement learning (PbRL) has shown impressive
capabilities in training agents without reward engineering. However, a notable
limitation of PbRL is its dependency on substantial human feedback. This
dependency stems from the learning loop, which entails accurate reward learning
compounded with value/policy learning, necessitating a considerable number of
samples. To boost the learning loop, we propose SEER, an efficient PbRL method
that integrates label smoothing and policy regularization techniques. Label
smoothing reduces overfitting of the reward model by smoothing human preference
labels. Additionally, we bootstrap a conservative estimate $\\widehat{Q}$ using
well-supported state-action pairs from the current replay memory to mitigate
overestimation bias and utilize it for policy learning regularization. Our
experimental results across a variety of complex tasks, both in online and
offline settings, demonstrate that our approach improves feedback efficiency,
outperforming state-of-the-art methods by a large margin. Ablation studies
further reveal that SEER achieves a more accurate Q-function compared to prior
work.
`,authors:"Fengshuo Bai; Rui Zhao; Hongming Zhang; Sijia Cui; Ying Wen; Yaodong Yang; Bo Xu; Lei Han",status:0,relevancy:.5366568000368498,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18721",date:"2024-05-29",title:`Correctable Landmark Discovery via Large Models for Vision-Language
  Navigation`,abstract:`  Vision-Language Navigation (VLN) requires the agent to follow language
instructions to reach a target position. A key factor for successful navigation
is to align the landmarks implied in the instruction with diverse visual
observations. However, previous VLN agents fail to perform accurate modality
alignment especially in unexplored scenes, since they learn from limited
navigation data and lack sufficient open-world alignment knowledge. In this
work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via
Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential
landmark discovery problem, by introducing a novel correctable landmark
discovery scheme based on two large models ChatGPT and CLIP. Specifically, we
use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and
conduct CLIP-driven landmark discovery based on these commonsense priors. To
mitigate the noise in the priors due to the lack of visual constraints, we
introduce a learnable cooccurrence scoring module, which corrects the
importance of each cooccurrence according to actual observations for accurate
landmark discovery. We further design an observation enhancement strategy for
an elegant combination of our framework with different VLN agents, where we
utilize the corrected landmark features to obtain enhanced observation features
for action decision. Extensive experimental results on multiple popular VLN
benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE
over strong baselines. Especially, our CONSOLE establishes the new
state-of-the-art results on R2R and R4R in unseen scenarios. Code is available
at https://github.com/expectorlin/CONSOLE.
`,authors:"Bingqian Lin; Yunshuang Nie; Ziming Wei; Yi Zhu; Hang Xu; Shikui Ma; Jianzhuang Liu; Xiaodan Liang",status:0,relevancy:.5362481468861078,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19149",date:"2024-05-29",title:`CaLa: Complementary Association Learning for Augmenting Composed Image
  Retrieval`,abstract:`  Composed Image Retrieval (CIR) involves searching for target images based on
an image-text pair query. While current methods treat this as a query-target
matching problem, we argue that CIR triplets contain additional associations
beyond this primary relation. In our paper, we identify two new relations
within triplets, treating each triplet as a graph node. Firstly, we introduce
the concept of text-bridged image alignment, where the query text serves as a
bridge between the query image and the target image. We propose a hinge-based
cross-attention mechanism to incorporate this relation into network learning.
Secondly, we explore complementary text reasoning, considering CIR as a form of
cross-modal retrieval where two images compose to reason about complementary
text. To integrate these perspectives effectively, we design a twin
attention-based compositor. By combining these complementary associations with
the explicit query pair-target image relation, we establish a comprehensive set
of constraints for CIR. Our framework, CaLa (Complementary Association Learning
for Augmenting Composed Image Retrieval), leverages these insights. We evaluate
CaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating
its superiority in composed image retrieval.
`,authors:"Xintong Jiang; Yaxiong Wang; Mengjian Li; Yujiao Wu; Bingwen Hu; Xueming Qian",status:0,relevancy:.5349794440807307,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19209",date:"2024-05-29",title:`VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on
  Long Videos`,abstract:`  Video-language understanding tasks have focused on short video clips, often
struggling with long-form video understanding tasks. Recently, many long
video-language understanding approaches have leveraged the reasoning
capabilities of Large Language Models (LLMs) to perform long video QA,
transforming videos into densely sampled frame captions, and asking LLMs to
respond to text queries over captions. However, the frames used for captioning
are often redundant and contain irrelevant information, making dense sampling
inefficient, and ignoring the fact that video QA requires varying levels of
granularity, with some video segments being highly relevant to the question
(needing more fine-grained detail) while others being less relevant. Thus,
these LLM-based approaches are prone to missing information and operate on
large numbers of irrelevant captions, lowering both performance and efficiency.
To address these issues, we introduce VideoTree, a query-adaptive and
hierarchical framework for long-video understanding with LLMs. VideoTree
dynamically extracts query-related information from a video and builds a
tree-based representation for LLM reasoning. First, VideoTree adaptively
selects frames for captioning by iteratively clustering frames based on their
visual features and scoring clusters using their relevance to the query.
Second, it organizes visual clusters into a query-adaptive and hierarchical
tree structure; the tree encodes varying levels of granularity, with higher
resolution on relevant segments. Finally, VideoTree produces an answer by
traversing the tree's keyframes and passing their captions to an LLM answerer.
Our method improves both reasoning accuracy and efficiency compared to existing
methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines
on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while
reducing inference time by 40%.
`,authors:"Ziyang Wang; Shoubin Yu; Elias Stengel-Eskin; Jaehong Yoon; Feng Cheng; Gedas Bertasius; Mohit Bansal",status:0,relevancy:.5316191479269754,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19229",date:"2024-05-29",title:`On Generating Monolithic and Model Reconciling Explanations in
  Probabilistic Scenarios`,abstract:`  Explanation generation frameworks aim to make AI systems' decisions
transparent and understandable to human users. However, generating explanations
in uncertain environments characterized by incomplete information and
probabilistic models remains a significant challenge. In this paper, we propose
a novel framework for generating probabilistic monolithic explanations and
model reconciling explanations. Monolithic explanations provide self-contained
reasons for an explanandum without considering the agent receiving the
explanation, while model reconciling explanations account for the knowledge of
the agent receiving the explanation. For monolithic explanations, our approach
integrates uncertainty by utilizing probabilistic logic to increase the
probability of the explanandum. For model reconciling explanations, we propose
a framework that extends the logic-based variant of the model reconciliation
problem to account for probabilistic human models, where the goal is to find
explanations that increase the probability of the explanandum while minimizing
conflicts between the explanation and the probabilistic human model. We
introduce explanatory gain and explanatory power as quantitative metrics to
assess the quality of these explanations. Further, we present algorithms that
exploit the duality between minimal correction sets and minimal unsatisfiable
sets to efficiently compute both types of explanations in probabilistic
contexts. Extensive experimental evaluations on various benchmarks demonstrate
the effectiveness and scalability of our approach in generating explanations
under uncertainty.
`,authors:"Stylianos Loukas Vasileiou; William Yeoh; Alessandro Previti; Tran Cao Son",status:0,relevancy:.530953278153824,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19162",date:"2024-05-29",title:`Does learning the right latent variables necessarily improve in-context
  learning?`,abstract:`  Large autoregressive models like Transformers can solve tasks through
in-context learning (ICL) without learning new weights, suggesting avenues for
efficiently solving new tasks. For many tasks, e.g., linear regression, the
data factorizes: examples are independent given a task latent that generates
the data, e.g., linear coefficients. While an optimal predictor leverages this
factorization by inferring task latents, it is unclear if Transformers
implicitly do so or if they instead exploit heuristics and statistical
shortcuts enabled by attention layers. Both scenarios have inspired active
ongoing work. In this paper, we systematically investigate the effect of
explicitly inferring task latents. We minimally modify the Transformer
architecture with a bottleneck designed to prevent shortcuts in favor of more
structured solutions, and then compare performance against standard
Transformers across various ICL tasks. Contrary to intuition and some recent
works, we find little discernible difference between the two; biasing towards
task-relevant latent variables does not lead to better out-of-distribution
performance, in general. Curiously, we find that while the bottleneck
effectively learns to extract latent task variables from context, downstream
processing struggles to utilize them for robust prediction. Our study
highlights the intrinsic limitations of Transformers in achieving structured
ICL solutions that generalize, and shows that while inferring the right latents
aids interpretability, it is not sufficient to alleviate this problem.
`,authors:"Sarthak Mittal; Eric Elmoznino; Leo Gagnon; Sangnie Bhardwaj; Dhanya Sridhar; Guillaume Lajoie",status:0,relevancy:.528705347329253,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19567",date:"2024-05-29",title:"Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding",abstract:`  Vision-Language Models (VLM) can support clinicians by analyzing medical
images and engaging in natural language interactions to assist in diagnostic
and treatment tasks. However, VLMs often exhibit "hallucinogenic" behavior,
generating textual outputs not grounded in contextual multimodal information.
This challenge is particularly pronounced in the medical domain, where we do
not only require VLM outputs to be accurate in single interactions but also to
be consistent with clinical reasoning and diagnostic pathways throughout
multi-turn conversations. For this purpose, we propose a new alignment
algorithm that uses symbolic representations of clinical reasoning to ground
VLMs in medical knowledge. These representations are utilized to (i) generate
GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM
conversations with demonstrations of clinical reasoning, and (ii) create an
automatic reward function that evaluates the clinical validity of VLM
generations throughout clinician-VLM interactions. Our algorithm eliminates the
need for human involvement in training data generation or reward model
construction, reducing costs compared to standard reinforcement learning with
human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a
conversational VLM finetuned for analyzing bone marrow pathology slides,
demonstrating strong performance in multi-turn medical conversations.
`,authors:"Shenghuan Sun; Gregory M. Goldgof; Alexander Schubert; Zhiqing Sun; Thomas Hartvigsen; Atul J. Butte; Ahmed Alaa",status:0,relevancy:.524865683713274,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18870",date:"2024-05-29",title:`LLMs achieve adult human performance on higher-order theory of mind
  tasks`,abstract:`  This paper examines the extent to which large language models (LLMs) have
developed higher-order theory of mind (ToM); the human ability to reason about
multiple mental and emotional states in a recursive manner (e.g. I think that
you believe that she knows). This paper builds on prior work by introducing a
handwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to
compare the performance of five LLMs to a newly gathered adult human benchmark.
We find that GPT-4 and Flan-PaLM reach adult-level and near adult-level
performance on ToM tasks overall, and that GPT-4 exceeds adult performance on
6th order inferences. Our results suggest that there is an interplay between
model size and finetuning for the realisation of ToM abilities, and that the
best-performing LLMs have developed a generalised capacity for ToM. Given the
role that higher-order ToM plays in a wide range of cooperative and competitive
human behaviours, these findings have significant implications for user-facing
LLM applications.
`,authors:"Winnie Street; John Oliver Siy; Geoff Keeling; Adrien Baranes; Benjamin Barnett; Michael McKibben; Tatenda Kanyere; Alison Lentz; Blaise Aguera y Arcas; Robin I. M. Dunbar",status:0,relevancy:.5248202822452036,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19498",date:"2024-05-29",title:`Machine Psychology: Integrating Operant Conditioning with the
  Non-Axiomatic Reasoning System for Advancing Artificial General Intelligence
  Research`,abstract:`  This paper introduces an interdisciplinary framework called Machine
Psychology, which merges principles from operant learning psychology with a
specific Artificial Intelligence model, the Non-Axiomatic Reasoning System
(NARS), to enhance Artificial General Intelligence (AGI) research. The core
premise of this framework is that adaptation is crucial to both biological and
artificial intelligence and can be understood through operant conditioning
principles. The study assesses this approach via three operant learning tasks
using OpenNARS for Applications (ONA): simple discrimination, changing
contingencies, and conditional discrimination tasks.
  In the simple discrimination task, NARS demonstrated rapid learning,
achieving perfect accuracy during both training and testing phases. The
changing contingencies task showcased NARS's adaptability, as it successfully
adjusted its behavior when task conditions were reversed. In the conditional
discrimination task, NARS handled complex learning scenarios effectively,
achieving high accuracy by forming and utilizing intricate hypotheses based on
conditional cues.
  These findings support the application of operant conditioning as a framework
for creating adaptive AGI systems. NARS's ability to operate under conditions
of insufficient knowledge and resources, coupled with its sensorimotor
reasoning capabilities, establishes it as a robust model for AGI. The Machine
Psychology framework, by incorporating elements of natural intelligence such as
continuous learning and goal-driven behavior, offers a scalable and flexible
approach for real-world applications. Future research should investigate using
enhanced NARS systems, more advanced tasks, and applying this framework to
diverse, complex challenges to further progress the development of human-level
AI.
`,authors:"Robert Johansson",status:0,relevancy:.5217213920706358,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18751",date:"2024-05-29",title:`On the Limits of Multi-modal Meta-Learning with Auxiliary Task
  Modulation Using Conditional Batch Normalization`,abstract:`  Few-shot learning aims to learn representations that can tackle novel tasks
given a small number of examples. Recent studies show that cross-modal learning
can improve representations for few-shot classification. More specifically,
language is a rich modality that can be used to guide visual learning. In this
work, we experiment with a multi-modal architecture for few-shot learning that
consists of three components: a classifier, an auxiliary network, and a bridge
network. While the classifier performs the main classification task, the
auxiliary network learns to predict language representations from the same
input, and the bridge network transforms high-level features of the auxiliary
network into modulation parameters for layers of the few-shot classifier using
conditional batch normalization. The bridge should encourage a form of
lightweight semantic alignment between language and vision which could be
useful for the classifier. However, after evaluating the proposed approach on
two popular few-shot classification benchmarks we find that a) the improvements
do not reproduce across benchmarks, and b) when they do, the improvements are
due to the additional compute and parameters introduced by the bridge network.
We contribute insights and recommendations for future work in multi-modal
meta-learning, especially when using language representations.
`,authors:"Jordi Armengol-Estapé; Vincent Michalski; Ramnath Kumar; Pierre-Luc St-Charles; Doina Precup; Samira Ebrahimi Kahou",status:0,relevancy:.5190552499712069,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19320",date:"2024-05-29",title:`Value-Incentivized Preference Optimization: A Unified Approach to Online
  and Offline RLHF`,abstract:`  Reinforcement learning from human feedback (RLHF) has demonstrated great
promise in aligning large language models (LLMs) with human preference.
Depending on the availability of preference data, both online and offline RLHF
are active areas of investigation. A key bottleneck is understanding how to
incorporate uncertainty estimation in the reward function learned from the
preference data for RLHF, regardless of how the preference data is collected.
While the principles of optimism or pessimism under uncertainty are
well-established in standard reinforcement learning (RL), a
practically-implementable and theoretically-grounded form amenable to large
language models is not yet available, as standard techniques for constructing
confidence intervals become intractable under arbitrary policy
parameterizations.
  In this paper, we introduce a unified approach to online and offline RLHF --
value-incentivized preference optimization (VPO) -- which regularizes the
maximum-likelihood estimate of the reward function with the corresponding value
function, modulated by a $\\textit{sign}$ to indicate whether the optimism or
pessimism is chosen. VPO also directly optimizes the policy with implicit
reward modeling, and therefore shares a simpler RLHF pipeline similar to direct
preference optimization. Theoretical guarantees of VPO are provided for both
online and offline settings, matching the rates of their standard RL
counterparts. Moreover, experiments on text summarization and dialog verify the
practicality and effectiveness of VPO.
`,authors:"Shicong Cen; Jincheng Mei; Katayoon Goshvadi; Hanjun Dai; Tong Yang; Sherry Yang; Dale Schuurmans; Yuejie Chi; Bo Dai",status:0,relevancy:.5152347105726177,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19334",date:"2024-05-29",title:"LLMs Meet Multimodal Generation and Editing: A Survey",abstract:`  With the recent advancement in large language models (LLMs), there is a
growing interest in combining LLMs with multimodal learning. Previous surveys
of multimodal large language models (MLLMs) mainly focus on understanding. This
survey elaborates on multimodal generation across different domains, including
image, video, 3D, and audio, where we highlight the notable advancements with
milestone works in these fields. Specifically, we exhaustively investigate the
key technical components behind methods and multimodal datasets utilized in
these studies. Moreover, we dig into tool-augmented multimodal agents that can
use existing generative models for human-computer interaction. Lastly, we also
comprehensively discuss the advancement in AI safety and investigate emerging
applications as well as future prospects. Our work provides a systematic and
insightful overview of multimodal generation, which is expected to advance the
development of Artificial Intelligence for Generative Content (AIGC) and world
models. A curated list of all related papers can be found at
https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
`,authors:"Yingqing He; Zhaoyang Liu; Jingye Chen; Zeyue Tian; Hongyu Liu; Xiaowei Chi; Runtao Liu; Ruibin Yuan; Yazhou Xing; Wenhai Wang; Jifeng Dai; Yong Zhang; Wei Xue; Qifeng Liu; Yike Guo; Qifeng Chen",status:0,relevancy:.5151688872910891,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19062",date:"2024-05-29",title:`SIG: Efficient Self-Interpretable Graph Neural Network for
  Continuous-time Dynamic Graphs`,abstract:`  While dynamic graph neural networks have shown promise in various
applications, explaining their predictions on continuous-time dynamic graphs
(CTDGs) is difficult. This paper investigates a new research task:
self-interpretable GNNs for CTDGs. We aim to predict future links within the
dynamic graph while simultaneously providing causal explanations for these
predictions. There are two key challenges: (1) capturing the underlying
structural and temporal information that remains consistent across both
independent and identically distributed (IID) and out-of-distribution (OOD)
data, and (2) efficiently generating high-quality link prediction results and
explanations. To tackle these challenges, we propose a novel causal inference
model, namely the Independent and Confounded Causal Model (ICCM). ICCM is then
integrated into a deep learning architecture that considers both effectiveness
and efficiency. Extensive experiments demonstrate that our proposed model
significantly outperforms existing methods across link prediction accuracy,
explanation quality, and robustness to shortcut features. Our code and datasets
are anonymously released at https://github.com/2024SIG/SIG.
`,authors:"Lanting Fang; Yulian Yang; Kai Wang; Shanshan Feng; Kaiyu Feng; Jie Gui; Shuliang Wang; Yew-Soon Ong",status:0,relevancy:.5146677004232605,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19207",date:"2024-05-29",title:"A Multi-Source Retrieval Question Answering Framework Based on RAG",abstract:`  With the rapid development of large-scale language models,
Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing
RAG paradigms are inevitably influenced by erroneous retrieval information,
thereby reducing the reliability and correctness of generated results.
Therefore, to improve the relevance of retrieval information, this study
proposes a method that replaces traditional retrievers with GPT-3.5, leveraging
its vast corpus knowledge to generate retrieval information. We also propose a
web retrieval based method to implement fine-grained knowledge retrieval,
Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic
partitioning of problem.In order to mitigate the illusion of GPT retrieval and
reduce noise in Web retrieval,we proposes a multi-source retrieval framework,
named MSRAG, which combines GPT retrieval with web retrieval. Experiments on
multiple knowledge-intensive QA datasets demonstrate that the proposed
framework in this study performs better than existing RAG framework in
enhancing the overall efficiency and accuracy of QA systems.
`,authors:"Ridong Wu; Shuhong Chen; Xiangbiao Su; Yuankai Zhu; Yifei Liao; Jianming Wu",status:0,relevancy:.5088269168654478,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18917",date:"2024-05-29",title:"Causal Action Influence Aware Counterfactual Data Augmentation",abstract:`  Offline data are both valuable and practical resources for teaching robots
complex behaviors. Ideally, learning agents should not be constrained by the
scarcity of available demonstrations, but rather generalize beyond the training
distribution. However, the complexity of real-world scenarios typically
requires huge amounts of data to prevent neural network policies from picking
up on spurious correlations and learning non-causal relationships. We propose
CAIAC, a data augmentation method that can create feasible synthetic
transitions from a fixed dataset without having access to online environment
interactions. By utilizing principled methods for quantifying causal influence,
we are able to perform counterfactual reasoning by swapping
$\\it{action}$-unaffected parts of the state-space between independent
trajectories in the dataset. We empirically show that this leads to a
substantial increase in robustness of offline learning algorithms against
distributional shift.
`,authors:"Núria Armengol Urpí; Marco Bagatella; Marin Vlastelica; Georg Martius",status:0,relevancy:.5087991529676509,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19024",date:"2024-05-29",title:"Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory",abstract:`  We consider inverse reinforcement learning problems with concave utilities.
Concave Utility Reinforcement Learning (CURL) is a generalisation of the
standard RL objective, which employs a concave function of the state occupancy
measure, rather than a linear function. CURL has garnered recent attention for
its ability to represent instances of many important applications including the
standard RL such as imitation learning, pure exploration, constrained MDPs,
offline RL, human-regularized RL, and others. Inverse reinforcement learning is
a powerful paradigm that focuses on recovering an unknown reward function that
can rationalize the observed behaviour of an agent. There has been recent
theoretical advances in inverse RL where the problem is formulated as
identifying the set of feasible reward functions. However, inverse RL for CURL
problems has not been considered previously. In this paper we show that most of
the standard IRL results do not apply to CURL in general, since CURL
invalidates the classical Bellman equations. This calls for a new theoretical
framework for the inverse CURL problem. Using a recent equivalence result
between CURL and Mean-field Games, we propose a new definition for the feasible
rewards for I-CURL by proving that this problem is equivalent to an inverse
game theory problem in a subclass of mean-field games. We present initial query
and sample complexity results for the I-CURL problem under assumptions such as
Lipschitz-continuity. Finally, we outline future directions and applications in
human--AI collaboration enabled by our results.
`,authors:"Mustafa Mert Çelikok; Frans A. Oliehoek; Jan-Willem van de Meent",status:0,relevancy:.5083688591509455,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18741",date:"2024-05-29",title:`Genshin: General Shield for Natural Language Processing with Large
  Language Models`,abstract:`  Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been
trending recently, demonstrating considerable advancement and generalizability
power in countless domains. However, LLMs create an even bigger black box
exacerbating opacity, with interpretability limited to few approaches. The
uncertainty and opacity embedded in LLMs' nature restrict their application in
high-stakes domains like financial fraud, phishing, etc. Current approaches
mainly rely on traditional textual classification with posterior interpretable
algorithms, suffering from attackers who may create versatile adversarial
samples to break the system's defense, forcing users to make trade-offs between
efficiency and robustness. To address this issue, we propose a novel cascading
framework called Genshin (General Shield for Natural Language Processing with
Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike
most applications of LLMs that try to transform text into something new or
structural, Genshin uses LLMs to recover text to its original state. Genshin
aims to combine the generalizability of the LLM, the discrimination of the
median model, and the interpretability of the simple model. Our experiments on
the task of sentimental analysis and spam detection have shown fatal flaws of
the current median models and exhilarating results on LLMs' recovery ability,
demonstrating that Genshin is both effective and efficient. In our ablation
study, we unearth several intriguing observations. Utilizing the LLM defender,
a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal
mask rate results in the 3rd paradigm of NLP. Additionally, when employing the
LLM as a potential adversarial tool, attackers are capable of executing
effective attacks that are nearly semantically lossless.
`,authors:"Xiao Peng; Tao Liu; Ying Wang",status:0,relevancy:.5078287158605328,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19139",date:"2024-05-29",title:`DGRC: An Effective Fine-tuning Framework for Distractor Generation in
  Chinese Multi-choice Reading Comprehension`,abstract:`  When evaluating a learner's knowledge proficiency, the multiple-choice
question is an efficient and widely used format in standardized tests.
Nevertheless, generating these questions, particularly plausible distractors
(incorrect options), poses a considerable challenge. Generally, the distractor
generation can be classified into cloze-style distractor generation (CDG) and
natural questions distractor generation (NQDG). In contrast to the CDG,
utilizing pre-trained language models (PLMs) for NQDG presents three primary
challenges: (1) PLMs are typically trained to generate \`\`correct'' content,
like answers, while rarely trained to generate \`\`plausible" content, like
distractors; (2) PLMs often struggle to produce content that aligns well with
specific knowledge and the style of exams; (3) NQDG necessitates the model to
produce longer, context-sensitive, and question-relevant distractors. In this
study, we introduce a fine-tuning framework named DGRC for NQDG in Chinese
multi-choice reading comprehension from authentic examinations. DGRC comprises
three major components: hard chain-of-thought, multi-task learning, and
generation mask patterns. The experiment results demonstrate that DGRC
significantly enhances generation performance, achieving a more than 2.5-fold
improvement in BLEU scores.
`,authors:"Runfeng Lin; Dacheng Xu; Huijiang Wang; Zebiao Chen; Yating Wang; Shouqiang Liu",status:0,relevancy:.5060433283611545,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18669",date:"2024-05-29",title:"Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities",abstract:`  Integrating multiple generative foundation models, especially those trained
on different modalities, into something greater than the sum of its parts poses
significant challenges. Two key hurdles are the availability of aligned data
(concepts that contain similar meaning but is expressed differently in
different modalities), and effectively leveraging unimodal representations in
cross-domain generative tasks, without compromising their original unimodal
capabilities.
  We propose Zipper, a multi-tower decoder architecture that addresses these
concerns by using cross-attention to flexibly compose multimodal generative
models from independently pre-trained unimodal decoders. In our experiments
fusing speech and text modalities, we show the proposed architecture performs
very competitively in scenarios with limited aligned text-speech data. We also
showcase the flexibility of our model to selectively maintain unimodal (e.g.,
text-to-text generation) generation performance by freezing the corresponding
modal tower (e.g. text). In cross-modal tasks such as automatic speech
recognition (ASR) where the output modality is text, we show that freezing the
text backbone results in negligible performance degradation. In cross-modal
tasks such as text-to-speech generation (TTS) where the output modality is
speech, we show that using a pre-trained speech backbone results in superior
performance to the baseline.
`,authors:"Vicky Zayats; Peter Chen; Melissa Merrari; Dirk Padfield",status:0,relevancy:.5045697980937968,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18708",date:"2024-05-29",title:`Cognitive Evolutionary Learning to Select Feature Interactions for
  Recommender Systems`,abstract:`  Feature interaction selection is a fundamental problem in commercial
recommender systems. Most approaches equally enumerate all features and
interactions by the same pre-defined operation under expert guidance. Their
recommendation is unsatisfactory sometimes due to the following issues:
(1)~They cannot ensure the learning abilities of models because their
architectures are poorly adaptable to tasks and data; (2)~Useless features and
interactions can bring unnecessary noise and complicate the training process.
In this paper, we aim to adaptively evolve the model to select appropriate
operations, features, and interactions under task guidance. Inspired by the
evolution and functioning of natural organisms, we propose a novel
\\textsl{Cognitive EvoLutionary Learning (CELL)} framework, where cognitive
ability refers to a property of organisms that allows them to react and survive
in diverse environments. It consists of three stages, i.e., DNA search, genome
search, and model functioning. Specifically, if we regard the relationship
between models and tasks as the relationship between organisms and natural
environments, interactions of feature pairs can be analogous to double-stranded
DNA, of which relevant features and interactions can be analogous to genomes.
Along this line, we diagnose the fitness of the model on operations, features,
and interactions to simulate the survival rates of organisms for natural
selection. We show that CELL can adaptively evolve into different models for
different tasks and data, which enables practitioners to access off-the-shelf
models. Extensive experiments on four real-world datasets demonstrate that CELL
significantly outperforms state-of-the-art baselines. Also, we conduct
synthetic experiments to ascertain that CELL can consistently discover the
pre-defined interaction patterns for feature pairs.
`,authors:"Runlong Yu; Qixiang Shao; Qi Liu; Huan Liu; Enhong Chen",status:0,relevancy:.502107526650884,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19033",date:"2024-05-29",title:`CiliaGraph: Enabling Expression-enhanced Hyper-Dimensional Computation
  in Ultra-Lightweight and One-Shot Graph Classification on Edge`,abstract:`  Graph Neural Networks (GNNs) are computationally demanding and inefficient
when applied to graph classification tasks in resource-constrained edge
scenarios due to their inherent process, involving multiple rounds of forward
and backward propagation. As a lightweight alternative, Hyper-Dimensional
Computing (HDC), which leverages high-dimensional vectors for data encoding and
processing, offers a more efficient solution by addressing computational
bottleneck. However, current HDC methods primarily focus on static graphs and
neglect to effectively capture node attributes and structural information,
which leads to poor accuracy. In this work, we propose CiliaGraph, an enhanced
expressive yet ultra-lightweight HDC model for graph classification. This model
introduces a novel node encoding strategy that preserves relative distance
isomorphism for accurate node connection representation. In addition, node
distances are utilized as edge weights for information aggregation, and the
encoded node attributes and structural information are concatenated to obtain a
comprehensive graph representation. Furthermore, we explore the relationship
between orthogonality and dimensionality to reduce the dimensions, thereby
further enhancing computational efficiency. Compared to the SOTA GNNs,
extensive experiments show that CiliaGraph reduces memory usage and accelerates
training speed by an average of 292 times(up to 2341 times) and 103 times(up to
313 times) respectively while maintaining comparable accuracy.
`,authors:"Yuxi Han; Jihe Wang; Danghui Wang",status:0,relevancy:.49969572401070184,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18823",date:"2024-05-29",title:"Why Reinforcement Learning in Energy Systems Needs Explanations",abstract:`  With economic development, the complexity of infrastructure has increased
drastically. Similarly, with the shift from fossil fuels to renewable sources
of energy, there is a dire need for such systems that not only predict and
forecast with accuracy but also help in understanding the process of
predictions. Artificial intelligence and machine learning techniques have
helped in finding out wellperforming solutions to different problems in the
energy sector. However, the usage of state-of-the-art techniques like
reinforcement learning is not surprisingly convincing. This paper discusses the
application of reinforcement techniques in energy systems and how explanations
of these models can be helpful
`,authors:"Hallah Shahid Butt; Benjamin Schäfer",status:0,relevancy:.4986922627845327,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18729",date:"2024-05-29",title:`Preferred-Action-Optimized Diffusion Policies for Offline Reinforcement
  Learning`,abstract:`  Offline reinforcement learning (RL) aims to learn optimal policies from
previously collected datasets. Recently, due to their powerful representational
capabilities, diffusion models have shown significant potential as policy
models for offline RL issues. However, previous offline RL algorithms based on
diffusion policies generally adopt weighted regression to improve the policy.
This approach optimizes the policy only using the collected actions and is
sensitive to Q-values, which limits the potential for further performance
enhancement. To this end, we propose a novel preferred-action-optimized
diffusion policy for offline RL. In particular, an expressive conditional
diffusion model is utilized to represent the diverse distribution of a behavior
policy. Meanwhile, based on the diffusion model, preferred actions within the
same behavior distribution are automatically generated through the critic
function. Moreover, an anti-noise preference optimization is designed to
achieve policy improvement by using the preferred actions, which can adapt to
noise-preferred actions for stable training. Extensive experiments demonstrate
that the proposed method provides competitive or superior performance compared
to previous state-of-the-art offline RL methods, particularly in sparse reward
tasks such as Kitchen and AntMaze. Additionally, we empirically prove the
effectiveness of anti-noise preference optimization.
`,authors:"Tianle Zhang; Jiayi Guan; Lin Zhao; Yihang Li; Dongjiang Li; Zecui Zeng; Lei Sun; Yue Chen; Xuelong Wei; Lusong Li; Xiaodong He",status:0,relevancy:.4974785576771925,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18867",date:"2024-05-29",title:"Topological Perspectives on Optimal Multimodal Embedding Spaces",abstract:`  Recent strides in multimodal model development have ignited a paradigm shift
in the realm of text-to-image generation. Among these advancements, CLIP stands
out as a remarkable achievement which is a sophisticated autoencoder adept at
encoding both textual and visual information within a unified latent space.
This paper delves into a comparative analysis between CLIP and its recent
counterpart, CLOOB. To unravel the intricate distinctions within the embedding
spaces crafted by these models, we employ topological data analysis. Our
approach encompasses a comprehensive examination of the modality gap drivers,
the clustering structures existing across both high and low dimensions, and the
pivotal role that dimension collapse plays in shaping their respective
embedding spaces. Empirical experiments substantiate the implications of our
analyses on downstream performance across various contextual scenarios. Through
this investigation, we aim to shed light on the nuanced intricacies that
underlie the comparative efficacy of CLIP and CLOOB, offering insights into
their respective strengths and weaknesses, and providing a foundation for
further refinement and advancement in multimodal model research.
`,authors:"Abdul Aziz A. B; A. B Abdul Rahim",status:0,relevancy:.4972126720138256,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19519",date:"2024-05-29",title:`Two-layer retrieval augmented generation framework for low-resource
  medical question-answering: proof of concept using Reddit data`,abstract:`  Retrieval augmented generation (RAG) provides the capability to constrain
generative model outputs, and mitigate the possibility of hallucination, by
providing relevant in-context text. The number of tokens a generative large
language model (LLM) can incorporate as context is finite, thus limiting the
volume of knowledge from which to generate an answer. We propose a two-layer
RAG framework for query-focused answer generation and evaluate a
proof-of-concept for this framework in the context of query-focused summary
generation from social media forums, focusing on emerging drug-related
information. The evaluations demonstrate the effectiveness of the two-layer
framework in resource constrained settings to enable researchers in obtaining
near real-time data from users.
`,authors:"Sudeshna Das; Yao Ge; Yuting Guo; Swati Rajwal; JaMor Hairston; Jeanne Powell; Drew Walker; Snigdha Peddireddy; Sahithi Lakamana; Selen Bozkurt; Matthew Reyna; Reza Sameni; Yunyu Xiao; Sangmi Kim; Rasheeta Chandler; Natalie Hernandez; Danielle Mowery; Rachel Wightman; Jennifer Love; Anthony Spadaro; Jeanmarie Perrone; Abeed Sarker",status:0,relevancy:.4936405808677772,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19261",date:"2024-05-29",title:"Faster Cascades via Speculative Decoding",abstract:`  Cascades and speculative decoding are two common approaches to improving
language models' inference efficiency. Both approaches involve interleaving
models of different sizes, but via fundamentally distinct mechanisms: cascades
employ a deferral rule that invokes the larger model only for "hard" inputs,
while speculative decoding uses speculative execution to primarily invoke the
larger model in parallel verification mode. These mechanisms offer different
benefits: empirically, cascades are often capable of yielding better quality
than even the larger model, while theoretically, speculative decoding offers a
guarantee of quality-neutrality. In this paper, we leverage the best of both
these approaches by designing new speculative cascading techniques that
implement their deferral rule through speculative execution. We characterize
the optimal deferral rule for our speculative cascades, and employ a plug-in
approximation to the optimal rule. Through experiments with T5 models on
benchmark language tasks, we show that the proposed approach yields better
cost-quality trade-offs than cascading and speculative decoding baselines.
`,authors:"Harikrishna Narasimhan; Wittawat Jitkrittum; Ankit Singh Rawat; Seungyeon Kim; Neha Gupta; Aditya Krishna Menon; Sanjiv Kumar",status:0,relevancy:.49014235973475473,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19047",date:"2024-05-29",title:"Statistical Context Detection for Deep Lifelong Reinforcement Learning",abstract:`  Context detection involves labeling segments of an online stream of data as
belonging to different tasks. Task labels are used in lifelong learning
algorithms to perform consolidation or other procedures that prevent
catastrophic forgetting. Inferring task labels from online experiences remains
a challenging problem. Most approaches assume finite and low-dimension
observation spaces or a preliminary training phase during which task labels are
learned. Moreover, changes in the transition or reward functions can be
detected only in combination with a policy, and therefore are more difficult to
detect than changes in the input distribution. This paper presents an approach
to learning both policies and labels in an online deep reinforcement learning
setting. The key idea is to use distance metrics, obtained via optimal
transport methods, i.e., Wasserstein distance, on suitable latent action-reward
spaces to measure distances between sets of data points from past and current
streams. Such distances can then be used for statistical tests based on an
adapted Kolmogorov-Smirnov calculation to assign labels to sequences of
experiences. A rollback procedure is introduced to learn multiple policies by
ensuring that only the appropriate data is used to train the corresponding
policy. The combination of task detection and policy deployment allows for the
optimization of lifelong reinforcement learning agents without an oracle that
provides task labels. The approach is tested using two benchmarks and the
results show promising performance when compared with related context detection
algorithms. The results suggest that optimal transport statistical methods
provide an explainable and justifiable procedure for online context detection
and reward optimization in lifelong reinforcement learning.
`,authors:"Jeffery Dick; Saptarshi Nath; Christos Peridis; Eseoghene Benjamin; Soheil Kolouri; Andrea Soltoggio",status:0,relevancy:.48755249325037087,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19153",date:"2024-05-29",title:"A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning",abstract:`  Continual learning with deep neural networks presents challenges distinct
from both the fixed-dataset and convex continual learning regimes. One such
challenge is plasticity loss, wherein a neural network trained in an online
fashion displays a degraded ability to fit new tasks. This problem has been
extensively studied in both supervised learning and off-policy reinforcement
learning (RL), where a number of remedies have been proposed. Still, plasticity
loss has received less attention in the on-policy deep RL setting. Here we
perform an extensive set of experiments examining plasticity loss and a variety
of mitigation methods in on-policy deep RL. We demonstrate that plasticity loss
is pervasive under domain shift in this regime, and that a number of methods
developed to resolve it in other settings fail, sometimes even resulting in
performance that is worse than performing no intervention at all. In contrast,
we find that a class of \`\`regenerative'' methods are able to consistently
mitigate plasticity loss in a variety of contexts, including in gridworld tasks
and more challenging environments like Montezuma's Revenge and ProcGen.
`,authors:"Arthur Juliani; Jordan T. Ash",status:0,relevancy:.4851474332669473,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18792",date:"2024-05-29",title:`Kernel Metric Learning for In-Sample Off-Policy Evaluation of
  Deterministic RL Policies`,abstract:`  We consider off-policy evaluation (OPE) of deterministic target policies for
reinforcement learning (RL) in environments with continuous action spaces.
While it is common to use importance sampling for OPE, it suffers from high
variance when the behavior policy deviates significantly from the target
policy. In order to address this issue, some recent works on OPE proposed
in-sample learning with importance resampling. Yet, these approaches are not
applicable to deterministic target policies for continuous action spaces. To
address this limitation, we propose to relax the deterministic target policy
using a kernel and learn the kernel metrics that minimize the overall mean
squared error of the estimated temporal difference update vector of an action
value function, where the action value function is used for policy evaluation.
We derive the bias and variance of the estimation error due to this relaxation
and provide analytic solutions for the optimal kernel metric. In empirical
studies using various test domains, we show that the OPE with in-sample
learning using the kernel with optimized metric achieves significantly improved
accuracy than other baselines.
`,authors:"Haanvid Lee; Tri Wahyu Guntara; Jongmin Lee; Yung-Kyun Noh; Kee-Eung Kim",status:0,relevancy:.4828688850997147,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18780",date:"2024-05-29",title:"Quantitative Certification of Bias in Large Language Models",abstract:`  Large Language Models (LLMs) can produce responses that exhibit social biases
and support stereotypes. However, conventional benchmarking is insufficient to
thoroughly evaluate LLM bias, as it can not scale to large sets of prompts and
provides no guarantees. Therefore, we propose a novel certification framework
QuaCer-B (Quantitative Certification of Bias) that provides formal guarantees
on obtaining unbiased responses from target LLMs under large sets of prompts. A
certificate consists of high-confidence bounds on the probability of obtaining
biased responses from the LLM for any set of prompts containing sensitive
attributes, sampled from a distribution. We illustrate the bias certification
in LLMs for prompts with various prefixes drawn from given distributions. We
consider distributions of random token sequences, mixtures of manual
jailbreaks, and jailbreaks in the LLM's embedding space to certify its bias. We
certify popular LLMs with QuaCer-B and present novel insights into their
biases.
`,authors:"Isha Chaudhary; Qian Hu; Manoj Kumar; Morteza Ziyadi; Rahul Gupta; Gagandeep Singh",status:0,relevancy:.482455834813305,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19544",date:"2024-05-29",title:`One-Shot Safety Alignment for Large Language Models via Optimal
  Dualization`,abstract:`  The growing safety concerns surrounding Large Language Models (LLMs) raise an
urgent need to align them with diverse human preferences to simultaneously
enhance their helpfulness and safety. A promising approach is to enforce safety
constraints through Reinforcement Learning from Human Feedback (RLHF). For such
constrained RLHF, common Lagrangian-based primal-dual policy optimization
methods are computationally expensive and often unstable. This paper presents a
dualization perspective that reduces constrained alignment to an equivalent
unconstrained alignment problem. We do so by pre-optimizing a smooth and convex
dual function that has a closed form. This shortcut eliminates the need for
cumbersome primal-dual policy iterations, thus greatly reducing the
computational burden and improving training stability. Our strategy leads to
two practical algorithms in model-based and preference-based scenarios (MoCAN
and PeCAN, respectively). A broad range of experiments demonstrate the
effectiveness of our methods.
`,authors:"Xinmeng Huang; Shuo Li; Edgar Dobriban; Osbert Bastani; Hamed Hassani; Dongsheng Ding",status:0,relevancy:.4803656048969728,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19534",date:"2024-05-29",title:"Preference Learning Algorithms Do Not Learn Preference Rankings",abstract:`  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to
steer LLMs to produce generations that are more preferred by humans, but our
understanding of their inner workings is still limited. In this work, we study
the conventional wisdom that preference learning trains models to assign higher
likelihoods to more preferred outputs than less preferred outputs, measured via
$\\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art
preference-tuned models achieve a ranking accuracy of less than 60% on common
preference datasets. We furthermore derive the $\\textit{idealized ranking
accuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or
RLHF objective perfectly. We demonstrate that existing models exhibit a
significant $\\textit{alignment gap}$ -- $\\textit{i.e.}$, a gap between the
observed and idealized ranking accuracies. We attribute this discrepancy to the
DPO objective, which is empirically and theoretically ill-suited to fix even
mild ranking errors in the reference model, and derive a simple and efficient
formula for quantifying the difficulty of learning a given preference
datapoint. Finally, we demonstrate that ranking accuracy strongly correlates
with the empirically popular win rate metric when the model is close to the
reference model used in the objective, shedding further light on the
differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO)
preference learning algorithms.
`,authors:"Angelica Chen; Sadhika Malladi; Lily H. Zhang; Xinyi Chen; Qiuyi Zhang; Rajesh Ranganath; Kyunghyun Cho",status:0,relevancy:.4732342244423171,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19323",date:"2024-05-29",title:"Are Large Language Models Chameleons?",abstract:`  Do large language models (LLMs) have their own worldviews and personality
tendencies? Simulations in which an LLM was asked to answer subjective
questions were conducted more than 1 million times. Comparison of the responses
from different LLMs with real data from the European Social Survey (ESS)
suggests that the effect of prompts on bias and variability is fundamental,
highlighting major cultural, age, and gender biases. Methods for measuring the
difference between LLMs and survey data are discussed, such as calculating
weighted means and a new proposed measure inspired by Jaccard similarity. We
conclude that it is important to analyze the robustness and variability of
prompts before using LLMs to model individual decisions or collective behavior,
as their imitation abilities are approximate at best.
`,authors:"Mingmeng Geng; Sihong He; Roberto Trotta",status:0,relevancy:.4695448291346914,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18719",date:"2024-05-29",title:"Contextual Position Encoding: Learning to Count What's Important",abstract:`  The attention mechanism is a critical component of Large Language Models
(LLMs) that allows tokens in a sequence to interact with each other, but is
order-invariant. Incorporating position encoding (PE) makes it possible to
address by position, such as attending to the i-th token. However, current PE
methods use token counts to derive position, and thus cannot generalize to
higher levels of abstraction, such as attending to the i-th sentence. In this
paper, we propose a new position encoding method, Contextual Position Encoding
(CoPE), that allows positions to be conditioned on context by incrementing
position only on certain tokens determined by the model. This allows more
general position addressing such as attending to the $i$-th particular word,
noun, or sentence. We show that CoPE can solve the selective copy, counting and
Flip-Flop tasks where popular position embeddings fail, and improves perplexity
on language modeling and coding tasks.
`,authors:"Olga Golovneva; Tianlu Wang; Jason Weston; Sainbayar Sukhbaatar",status:0,relevancy:.46879091917938376,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19458",date:"2024-05-29",title:`MemControl: Mitigating Memorization in Medical Diffusion Models via
  Automated Parameter Selection`,abstract:`  Diffusion models show a remarkable ability in generating images that closely
mirror the training distribution. However, these models are prone to training
data memorization, leading to significant privacy, ethical, and legal concerns,
particularly in sensitive fields such as medical imaging. We hypothesize that
memorization is driven by the overparameterization of deep models, suggesting
that regularizing model capacity during fine-tuning could be an effective
mitigation strategy. Parameter-efficient fine-tuning (PEFT) methods offer a
promising approach to capacity control by selectively updating specific
parameters. However, finding the optimal subset of learnable parameters that
balances generation quality and memorization remains elusive. To address this
challenge, we propose a bi-level optimization framework that guides automated
parameter selection by utilizing memorization and generation quality metrics as
rewards. Our framework successfully identifies the optimal parameter set to be
updated to satisfy the generation-memorization tradeoff. We perform our
experiments for the specific task of medical image generation and outperform
existing state-of-the-art training-time mitigation strategies by fine-tuning as
few as 0.019% of model parameters. Furthermore, we show that the strategies
learned through our framework are transferable across different datasets and
domains. Our proposed framework is scalable to large datasets and agnostic to
the choice of reward functions. Finally, we show that our framework can be
combined with existing approaches for further memorization mitigation.
`,authors:"Raman Dutt; Pedro Sanchez; Ondrej Bohdal; Sotirios A. Tsaftaris; Timothy Hospedales",status:0,relevancy:.46231410262382755,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19032",date:"2024-05-29",title:"Large Language Models for Code Summarization",abstract:`  Recently, there has been increasing activity in using deep learning for
software engineering, including tasks like code generation and summarization.
In particular, the most recent coding Large Language Models seem to perform
well on these problems. In this technical report, we aim to review how these
models perform in code explanation/summarization, while also investigating
their code generation capabilities (based on natural language descriptions).
`,authors:"Balázs Szalontai; Gergő Szalay; Tamás Márton; Anna Sike; Balázs Pintér; Tibor Gregorics",status:0,relevancy:.4602678976928025,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18693",date:"2024-05-29",title:`DeepHGNN: Study of Graph Neural Network based Forecasting Methods for
  Hierarchically Related Multivariate Time Series`,abstract:`  Graph Neural Networks (GNN) have gained significant traction in the
forecasting domain, especially for their capacity to simultaneously account for
intra-series temporal correlations and inter-series relationships. This paper
introduces a novel Hierarchical GNN (DeepHGNN) framework, explicitly designed
for forecasting in complex hierarchical structures. The uniqueness of DeepHGNN
lies in its innovative graph-based hierarchical interpolation and an end-to-end
reconciliation mechanism. This approach ensures forecast accuracy and coherence
across various hierarchical levels while sharing signals across them,
addressing a key challenge in hierarchical forecasting. A critical insight in
hierarchical time series is the variance in forecastability across levels, with
upper levels typically presenting more predictable components. DeepHGNN
capitalizes on this insight by pooling and leveraging knowledge from all
hierarchy levels, thereby enhancing the overall forecast accuracy. Our
comprehensive evaluation set against several state-of-the-art models confirm
the superior performance of DeepHGNN. This research not only demonstrates
DeepHGNN's effectiveness in achieving significantly improved forecast accuracy
but also contributes to the understanding of graph-based methods in
hierarchical time series forecasting.
`,authors:"Abishek Sriramulu; Nicolas Fourrier; Christoph Bergmeir",status:0,relevancy:.4588965895437087,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19423",date:"2024-05-29",title:"Evaluating Vision-Language Models on Bistable Images",abstract:`  Bistable images, also known as ambiguous or reversible images, present visual
stimuli that can be seen in two distinct interpretations, though not
simultaneously by the observer. In this study, we conduct the most extensive
examination of vision-language models using bistable images to date. We
manually gathered a dataset of 29 bistable images, along with their associated
labels, and subjected them to 116 different manipulations in brightness, tint,
and rotation. We evaluated twelve different models in both classification and
generative tasks across six model architectures. Our findings reveal that, with
the exception of models from the Idefics family and LLaVA1.5-13b, there is a
pronounced preference for one interpretation over another among the models, and
minimal variance under image manipulations, with few exceptions on image
rotations. Additionally, we compared the model preferences with humans, noting
that the models do not exhibit the same continuity biases as humans and often
diverge from human initial interpretations. We also investigated the influence
of variations in prompts and the use of synonymous labels, discovering that
these factors significantly affect model interpretations more than image
manipulations showing a higher influence of the language priors on bistable
image interpretations compared to image-text training data. All code and data
is open sourced.
`,authors:"Artemis Panagopoulou; Coby Melkin; Chris Callison-Burch",status:0,relevancy:.4585280500520289,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19250",date:"2024-05-29",title:"Kotlin ML Pack: Technical Report",abstract:`  In this technical report, we present three novel datasets of Kotlin code:
KStack, KStack-clean, and KExercises. We also describe the results of
fine-tuning CodeLlama and DeepSeek models on this data. Additionally, we
present a version of the HumanEval benchmark rewritten by human experts into
Kotlin - both the solutions and the tests. Our results demonstrate that small,
high-quality datasets (KStack-clean and KExercises) can significantly improve
model performance on code generation tasks, achieving up to a 16-point increase
in pass rate on the HumanEval benchmark. Lastly, we discuss potential future
work in the field of improving language modeling for Kotlin, including the use
of static analysis tools in the learning process and the introduction of more
intricate and realistic benchmarks.
`,authors:"Sergey Titov; Mikhail Evtikhiev; Anton Shapkin; Oleg Smirnov; Sergei Boytsov; Sergei Boytsov; Dariia Karaeva; Maksim Sheptyakov; Mikhail Arkhipov; Timofey Bryksin; Egor Bogomolov",status:0,relevancy:.4559003778105136,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18875",date:"2024-05-29",title:"Counterfactual Metarules for Local and Global Recourse",abstract:`  We introduce T-CREx, a novel model-agnostic method for local and global
counterfactual explanation (CE), which summarises recourse options for both
individuals and groups in the form of human-readable rules. It leverages
tree-based surrogate models to learn the counterfactual rules, alongside
'metarules' denoting their regions of optimality, providing both a global
analysis of model behaviour and diverse recourse options for users. Experiments
indicate that T-CREx achieves superior aggregate performance over existing
rule-based baselines on a range of CE desiderata, while being orders of
magnitude faster to run.
`,authors:"Tom Bewley; Salim I. Amoukou; Saumitra Mishra; Daniele Magazzeni; Manuela Veloso",status:0,relevancy:.45396323771383096,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19074",date:"2024-05-29",title:`Resurrecting Old Classes with New Data for Exemplar-Free Continual
  Learning`,abstract:`  Continual learning methods are known to suffer from catastrophic forgetting,
a phenomenon that is particularly hard to counter for methods that do not store
exemplars of previous tasks. Therefore, to reduce potential drift in the
feature extractor, existing exemplar-free methods are typically evaluated in
settings where the first task is significantly larger than subsequent tasks.
Their performance drops drastically in more challenging settings starting with
a smaller first task. To address this problem of feature drift estimation for
exemplar-free methods, we propose to adversarially perturb the current samples
such that their embeddings are close to the old class prototypes in the old
model embedding space. We then estimate the drift in the embedding space from
the old to the new model using the perturbed images and compensate the
prototypes accordingly. We exploit the fact that adversarial samples are
transferable from the old to the new feature space in a continual learning
setting. The generation of these images is simple and computationally cheap. We
demonstrate in our experiments that the proposed approach better tracks the
movement of prototypes in embedding space and outperforms existing methods on
several standard continual learning benchmarks as well as on fine-grained
datasets. Code is available at https://github.com/dipamgoswami/ADC.
`,authors:"Dipam Goswami; Albin Soutif--Cormerais; Yuyang Liu; Sandesh Kamath; Bartłomiej Twardowski; Joost van de Weijer",status:0,relevancy:.45395732423525503,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19575",date:"2024-05-29",title:`A Deep Convolutional Neural Network-based Model for Aspect and Polarity
  Classification in Hausa Movie Reviews`,abstract:`  Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment
nuances in text, especially across diverse languages and cultures. This paper
introduces a novel Deep Convolutional Neural Network (CNN)-based model tailored
for aspect and polarity classification in Hausa movie reviews, an
underrepresented language in sentiment analysis research. A comprehensive Hausa
ABSA dataset is created, filling a significant gap in resource availability.
The dataset, preprocessed using sci-kit-learn for TF-IDF transformation,
includes manually annotated aspect-level feature ontology words and sentiment
polarity assignments. The proposed model combines CNNs with attention
mechanisms for aspect-word prediction, leveraging contextual information and
sentiment polarities. With 91% accuracy on aspect term extraction and 92% on
sentiment polarity classification, the model outperforms traditional machine
models, offering insights into specific aspects and sentiments. This study
advances ABSA research, particularly in underrepresented languages, with
implications for cross-cultural linguistic research.
`,authors:"Umar Ibrahim; Abubakar Yakubu Zandam; Fatima Muhammad Adam; Aminu Musa",status:0,relevancy:.45257678300257476,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19420",date:"2024-05-29",title:`Using Contrastive Learning with Generative Similarity to Learn Spaces
  that Capture Human Inductive Biases`,abstract:`  Humans rely on strong inductive biases to learn from few examples and
abstract useful information from sensory data. Instilling such biases in
machine learning models has been shown to improve their performance on various
benchmarks including few-shot learning, robustness, and alignment. However,
finding effective training procedures to achieve that goal can be challenging
as psychologically-rich training data such as human similarity judgments are
expensive to scale, and Bayesian models of human inductive biases are often
intractable for complex, realistic domains. Here, we address this challenge by
introducing a Bayesian notion of generative similarity whereby two datapoints
are considered similar if they are likely to have been sampled from the same
distribution. This measure can be applied to complex generative processes,
including probabilistic programs. We show that generative similarity can be
used to define a contrastive learning objective even when its exact form is
intractable, enabling learning of spatial embeddings that express specific
inductive biases. We demonstrate the utility of our approach by showing how it
can be used to capture human inductive biases for geometric shapes, and to
better distinguish different abstract drawing styles that are parameterized by
probabilistic programs.
`,authors:"Raja Marjieh; Sreejan Kumar; Declan Campbell; Liyi Zhang; Gianluca Bencomo; Jake Snell; Thomas L. Griffiths",status:0,relevancy:.45076182816763066,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18724",date:"2024-05-29",title:`Adapting Differential Molecular Representation with Hierarchical Prompts
  for Multi-label Property Prediction`,abstract:`  Accurate prediction of molecular properties is critical in the field of drug
discovery. However, existing methods do not fully consider the fact that
molecules in the real world usually possess multiple property labels, and
complex high-order relationships may exist among these labels. Therefore,
molecular representation learning models should generate differential molecular
representations that consider multi-granularity correlation information among
tasks. To this end, our research introduces a Hierarchical Prompted Molecular
Representation Learning Framework (HiPM), which enhances the differential
expression of tasks in molecular representations through task-aware prompts,
and utilizes shared information among labels to mitigate negative transfer
between different tasks. HiPM primarily consists of two core components: the
Molecular Representation Encoder (MRE) and the Task-Aware Prompter (TAP). The
MRE employs a hierarchical message-passing network architecture to capture
molecular features at both the atomic and motif levels, while the TAP uses
agglomerative hierarchical clustering to build a prompt tree that reflects the
affinity and distinctiveness of tasks, enabling the model to effectively handle
the complexity of multi-label property predictions. Extensive experiments
demonstrate that HiPM achieves state-of-the-art performance across various
multi-label datasets, offering a new perspective on multi-label molecular
representation learning.
`,authors:"Linjia Kang; Songhua Zhou; Shuyan Fang; Shichao Liu; Wen Zhang",status:0,relevancy:.44709170221701766,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18810",date:"2024-05-29",title:"UniPTS: A Unified Framework for Proficient Post-Training Sparsity",abstract:`  Post-training Sparsity (PTS) is a recently emerged avenue that chases
efficient network sparsity with limited data in need. Existing PTS methods,
however, undergo significant performance degradation compared with traditional
methods that retrain the sparse networks via the whole dataset, especially at
high sparsity ratios. In this paper, we attempt to reconcile this disparity by
transposing three cardinal factors that profoundly alter the performance of
conventional sparsity into the context of PTS. Our endeavors particularly
comprise (1) A base-decayed sparsity objective that promotes efficient
knowledge transferring from dense network to the sparse counterpart. (2) A
reducing-regrowing search algorithm designed to ascertain the optimal sparsity
distribution while circumventing overfitting to the small calibration set in
PTS. (3) The employment of dynamic sparse training predicated on the preceding
aspects, aimed at comprehensively optimizing the sparsity structure while
ensuring training stability. Our proposed framework, termed UniPTS, is
validated to be much superior to existing PTS methods across extensive
benchmarks. As an illustration, it amplifies the performance of POT, a recently
proposed recipe, from 3.9% to 68.6% when pruning ResNet-50 at 90% sparsity
ratio on ImageNet. We release the code of our paper at
https://github.com/xjjxmu/UniPTS.
`,authors:"Jingjing Xie; Yuxin Zhang; Mingbao Lin; Zhihang Lin; Liujuan Cao; Rongrong Ji",status:0,relevancy:.447029926017696,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19026",date:"2024-05-29",title:"DiveR-CT: Diversity-enhanced Red Teaming with Relaxing Constraints",abstract:`  Recent advances in large language models (LLMs) have made them indispensable,
raising significant concerns over managing their safety. Automated red teaming
offers a promising alternative to the labor-intensive and error-prone manual
probing for vulnerabilities, providing more consistent and scalable safety
evaluations. However, existing approaches often compromise diversity by
focusing on maximizing attack success rate. Additionally, methods that decrease
the cosine similarity from historical embeddings with semantic diversity
rewards lead to novelty stagnation as history grows. To address these issues,
we introduce DiveR-CT, which relaxes conventional constraints on the objective
and semantic reward, granting greater freedom for the policy to enhance
diversity. Our experiments demonstrate DiveR-CT's marked superiority over
baselines by 1) generating data that perform better in various diversity
metrics across different attack success rate levels, 2) better-enhancing
resiliency in blue team models through safety tuning based on collected data,
3) allowing dynamic control of objective weights for reliable and controllable
attack success rates, and 4) reducing susceptibility to reward
overoptimization. Project details and code can be found at
https://andrewzh112.github.io/#diverct.
`,authors:"Andrew Zhao; Quentin Xu; Matthieu Lin; Shenzhi Wang; Yong-jin Liu; Zilong Zheng; Gao Huang",status:0,relevancy:.4464088830842974,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18758",date:"2024-05-29",title:"Learning to Continually Learn with the Bayesian Principle",abstract:`  In the present era of deep learning, continual learning research is mainly
focused on mitigating forgetting when training a neural network with stochastic
gradient descent on a non-stationary stream of data. On the other hand, in the
more classical literature of statistical machine learning, many models have
sequential Bayesian update rules that yield the same learning outcome as the
batch training, i.e., they are completely immune to catastrophic forgetting.
However, they are often overly simple to model complex real-world data. In this
work, we adopt the meta-learning paradigm to combine the strong
representational power of neural networks and simple statistical models'
robustness to forgetting. In our novel meta-continual learning framework,
continual learning takes place only in statistical models via ideal sequential
Bayesian update rules, while neural networks are meta-learned to bridge the raw
data and the statistical models. Since the neural networks remain fixed during
continual learning, they are protected from catastrophic forgetting. This
approach not only achieves significantly improved performance but also exhibits
excellent scalability. Since our approach is domain-agnostic and
model-agnostic, it can be applied to a wide range of problems and easily
integrated with existing model architectures.
`,authors:"Soochan Lee; Hyeonseong Jeon; Jaehyeon Son; Gunhee Kim",status:0,relevancy:.445099674002285,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18770",date:"2024-05-29",title:`Leveraging Many-To-Many Relationships for Defending Against
  Visual-Language Adversarial Attacks`,abstract:`  Recent studies have revealed that vision-language (VL) models are vulnerable
to adversarial attacks for image-text retrieval (ITR). However, existing
defense strategies for VL models primarily focus on zero-shot image
classification, which do not consider the simultaneous manipulation of image
and text, as well as the inherent many-to-many (N:N) nature of ITR, where a
single image can be described in numerous ways, and vice versa. To this end,
this paper studies defense strategies against adversarial attacks on VL models
for ITR for the first time. Particularly, we focus on how to leverage the N:N
relationship in ITR to enhance adversarial robustness. We found that, although
adversarial training easily overfits to specific one-to-one (1:1) image-text
pairs in the train data, diverse augmentation techniques to create one-to-many
(1:N) / many-to-one (N:1) image-text pairs can significantly improve
adversarial robustness in VL models. Additionally, we show that the alignment
of the augmented image-text pairs is crucial for the effectiveness of the
defense strategy, and that inappropriate augmentations can even degrade the
model's performance. Based on these findings, we propose a novel defense
strategy that leverages the N:N relationship in ITR, which effectively
generates diverse yet highly-aligned N:N pairs using basic augmentations and
generative model-based augmentations. This work provides a novel perspective on
defending against adversarial attacks in VL tasks and opens up new research
directions for future work.
`,authors:"Futa Waseda; Antonio Tejero-de-Pablos",status:0,relevancy:.44258475393659946,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18710",date:"2024-05-29",title:`To FP8 and Back Again: Quantifying the Effects of Reducing Precision on
  LLM Training Stability`,abstract:`  The massive computational costs associated with large language model (LLM)
pretraining have spurred great interest in reduced-precision floating-point
representations to accelerate the process. As a result, the BrainFloat16 (BF16)
precision has become the de facto standard for LLM training, with hardware
support included in recent accelerators. This trend has gone even further in
the latest processors, where FP8 has recently been introduced. However, prior
experience with FP16, which was found to be less stable than BF16, raises
concerns as to whether FP8, with even fewer bits than FP16, can be a
cost-effective option for LLM training. We argue that reduced-precision
training schemes must have similar training stability and hyperparameter
sensitivities to their higher-precision counterparts in order to be
cost-effective. However, we find that currently available methods for FP8
training are not robust enough to allow their use as economical replacements.
This prompts us to investigate the stability of reduced-precision LLM training
in terms of robustness across random seeds and learning rates. To this end, we
propose new evaluation techniques and a new metric for quantifying loss
landscape sharpness in autoregressive language models. By simulating
incremental bit reductions in floating-point representations, we analyze the
relationship between representational power and training stability with the
intent of aiding future research into the field.
`,authors:"Joonhyung Lee; Jeongin Bae; Byeongwook Kim; Se Jung Kwon; Dongsoo Lee",status:0,relevancy:.4413392843518984,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18881",date:"2024-05-29",title:"Tuning-Free Alignment of Diffusion Models with Direct Noise Optimization",abstract:`  In this work, we focus on the alignment problem of diffusion models with a
continuous reward function, which represents specific objectives for downstream
tasks, such as improving human preference. The central goal of the alignment
problem is to adjust the distribution learned by diffusion models such that the
generated samples maximize the target reward function. We propose a novel
alignment approach, named Direct Noise Optimization (DNO), that optimizes the
injected noise during the sampling process of diffusion models. By design, DNO
is tuning-free and prompt-agnostic, as the alignment occurs in an online
fashion during generation. We rigorously study the theoretical properties of
DNO and also propose variants to deal with non-differentiable reward functions.
Furthermore, we identify that naive implementation of DNO occasionally suffers
from the out-of-distribution reward hacking problem, where optimized samples
have high rewards but are no longer in the support of the pretrained
distribution. To remedy this issue, we leverage classical high-dimensional
statistics theory and propose to augment the DNO loss with certain probability
regularization. We conduct extensive experiments on several popular reward
functions trained on human feedback data and demonstrate that the proposed DNO
approach achieves state-of-the-art reward scores as well as high image quality,
all within a reasonable time budget for generation.
`,authors:"Zhiwei Tang; Jiangweizhi Peng; Jiasheng Tang; Mingyi Hong; Fan Wang; Tsung-Hui Chang",status:0,relevancy:.4371682751270487,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18832",date:"2024-05-29",title:"MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models",abstract:`  Mixture-of-Experts (MoE) large language models (LLM) have memory requirements
that often exceed the GPU memory capacity, requiring costly parameter movement
from secondary memories to the GPU for expert computation. In this work, we
present Mixture of Near-Data Experts (MoNDE), a near-data computing solution
that efficiently enables MoE LLM inference. MoNDE reduces the volume of MoE
parameter movement by transferring only the $\\textit{hot}$ experts to the GPU,
while computing the remaining $\\textit{cold}$ experts inside the host memory
device. By replacing the transfers of massive expert parameters with the ones
of small activations, MoNDE enables far more communication-efficient MoE
inference, thereby resulting in substantial speedups over the existing
parameter offloading frameworks for both encoder and decoder operations.
`,authors:"Taehyun Kim; Kwanseok Choi; Youngmock Cho; Jaehoon Cho; Hyuk-Jae Lee; Jaewoong Sim",status:0,relevancy:.43558057637047565,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19132",date:"2024-05-29",title:`Analyzing Chat Protocols of Novice Programmers Solving Introductory
  Programming Tasks with ChatGPT`,abstract:`  Large Language Models (LLMs) have taken the world by storm, and students are
assumed to use related tools at a great scale. In this research paper we aim to
gain an understanding of how introductory programming students chat with LLMs
and related tools, e.g., ChatGPT-3.5. To address this goal, computing students
at a large German university were motivated to solve programming exercises with
the assistance of ChatGPT as part of their weekly introductory course
exercises. Then students (n=213) submitted their chat protocols (with 2335
prompts in sum) as data basis for this analysis. The data was analyzed w.r.t.
the prompts, frequencies, the chats' progress, contents, and other use pattern,
which revealed a great variety of interactions, both potentially supportive and
concerning. Learning about students' interactions with ChatGPT will help inform
and align teaching practices and instructions for future introductory
programming courses in higher education.
`,authors:"Andreas Scholl; Daniel Schiffner; Natalie Kiesler",status:0,relevancy:.4319753617315164,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19121",date:"2024-05-29",title:"Spatio-Spectral Graph Neural Networks",abstract:`  Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for
learning on graph-structured data. However, key limitations of l-step MPGNNs
are that their "receptive field" is typically limited to the l-hop neighborhood
of a node and that information exchange between distant nodes is limited by
over-squashing. Motivated by these limitations, we propose Spatio-Spectral
Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural
Networks (GNNs) that synergistically combines spatially and spectrally
parametrized graph filters. Parameterizing filters partially in the frequency
domain enables global yet efficient information propagation. We show that
S$^2$GNNs vanquish over-squashing and yield strictly tighter
approximation-theoretic error bounds than MPGNNs. Further, rethinking graph
convolutions at a fundamental level unlocks new design spaces. For example,
S$^2$GNNs allow for free positional encodings that make them strictly more
expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain
general-purpose S$^2$GNNs, we propose spectrally parametrized filters for
directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and
graph rewirings, e.g., on the peptide long-range benchmark tasks, and are
competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs
scale to millions of nodes.
`,authors:"Simon Geisler; Arthur Kosmala; Daniel Herbst; Stephan Günnemann",status:0,relevancy:.430269302084484,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19098",date:"2024-05-29",title:`Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided
  by a Function Prior`,abstract:`  This paper studies the challenging black-box adversarial attack that aims to
generate adversarial examples against a black-box model by only using output
feedback of the model to input queries. Some previous methods improve the query
efficiency by incorporating the gradient of a surrogate white-box model into
query-based attacks due to the adversarial transferability. However, the
localized gradient is not informative enough, making these methods still
query-intensive. In this paper, we propose a Prior-guided Bayesian Optimization
(P-BO) algorithm that leverages the surrogate model as a global function prior
in black-box adversarial attacks. As the surrogate model contains rich prior
information of the black-box one, P-BO models the attack objective with a
Gaussian process whose mean function is initialized as the surrogate model's
loss. Our theoretical analysis on the regret bound indicates that the
performance of P-BO may be affected by a bad prior. Therefore, we further
propose an adaptive integration strategy to automatically adjust a coefficient
on the function prior by minimizing the regret bound. Extensive experiments on
image classifiers and large vision-language models demonstrate the superiority
of the proposed algorithm in reducing queries and improving attack success
rates compared with the state-of-the-art black-box attacks. Code is available
at https://github.com/yibo-miao/PBO-Attack.
`,authors:"Shuyu Cheng; Yibo Miao; Yinpeng Dong; Xiao Yang; Xiao-Shan Gao; Jun Zhu",status:0,relevancy:.4225890656422713,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18732",date:"2024-05-29",title:`Gemini & Physical World: Large Language Models Can Estimate the
  Intensity of Earthquake Shaking from Multi-Modal Social Media Posts`,abstract:`  This paper presents a novel approach for estimating the ground shaking
intensity using social media data and CCTV footage. Employing the Gemini Pro
(Reid et al. 2024) model, a multi-modal language model, we demonstrate the
ability to extract relevant information from unstructured data utilizing
generative AI and natural language processing. The model output, in the form of
Modified Mercalli Intensity (MMI) values, align well with independent
observational data. Furthermore, our results suggest that beyond its advanced
visual and auditory understanding abilities, Gemini appears to utilize
additional sources of knowledge, including a simplified understanding of the
general relationship between earthquake magnitude, distance, and MMI intensity,
which it presumably acquired during its training, in its reasoning and
decision-making processes. These findings raise intriguing questions about the
extent of Gemini's general understanding of the physical world and its
phenomena. The ability of Gemini to generate results consistent with
established scientific knowledge highlights the potential of LLMs like Gemini
in augmenting our understanding of complex physical phenomena such as
earthquakes. More specifically, the results of this study highlight the
potential of LLMs like Gemini to revolutionize citizen seismology by enabling
rapid, effective, and flexible analysis of crowdsourced data from eyewitness
accounts for assessing earthquake impact and providing crisis situational
awareness. This approach holds great promise for improving early warning
systems, disaster response, and overall resilience in earthquake-prone regions.
This study provides a significant step toward harnessing the power of social
media and AI for earthquake disaster mitigation.
`,authors:"S. Mostafa Mousavi; Marc Stogaitis; Tajinder Gadh; Richard M Allen; Alexei Barski; Robert Bosch; Patrick Robertson; Nivetha Thiruverahan; Youngmin Cho",status:0,relevancy:.4221081965668697,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18902",date:"2024-05-29",title:"A Causal Framework for Evaluating Deferring Systems",abstract:`  Deferring systems extend supervised Machine Learning (ML) models with the
possibility to defer predictions to human experts. However, evaluating the
impact of a deferring strategy on system accuracy is still an overlooked area.
This paper fills this gap by evaluating deferring systems through a causal
lens. We link the potential outcomes framework for causal inference with
deferring systems. This allows us to identify the causal impact of the
deferring strategy on predictive accuracy. We distinguish two scenarios. In the
first one, we can access both the human and the ML model predictions for the
deferred instances. In such a case, we can identify the individual causal
effects for deferred instances and aggregates of them. In the second scenario,
only human predictions are available for the deferred instances. In this case,
we can resort to regression discontinuity design to estimate a local causal
effect. We empirically evaluate our approach on synthetic and real datasets for
seven deferring systems from the literature.
`,authors:"Filippo Palomba; Andrea Pugnana; José Manuel Alvarez; Salvatore Ruggieri",status:0,relevancy:.42099516588555763,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18852",date:"2024-05-29",title:"LetsMap: Unsupervised Representation Learning for Semantic BEV Mapping",abstract:`  Semantic Bird's Eye View (BEV) maps offer a rich representation with strong
occlusion reasoning for various decision making tasks in autonomous driving.
However, most BEV mapping approaches employ a fully supervised learning
paradigm that relies on large amounts of human-annotated BEV ground truth data.
In this work, we address this limitation by proposing the first unsupervised
representation learning approach to generate semantic BEV maps from a monocular
frontal view (FV) image in a label-efficient manner. Our approach pretrains the
network to independently reason about scene geometry and scene semantics using
two disjoint neural pathways in an unsupervised manner and then finetunes it
for the task of semantic BEV mapping using only a small fraction of labels in
the BEV. We achieve label-free pretraining by exploiting spatial and temporal
consistency of FV images to learn scene geometry while relying on a novel
temporal masked autoencoder formulation to encode the scene representation.
Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that
our approach performs on par with the existing state-of-the-art approaches
while using only 1% of BEV labels and no additional labeled data.
`,authors:"Nikhil Gosala; Kürsat Petek; B Ravi Kiran; Senthil Yogamani; Paulo Drews-Jr; Wolfram Burgard; Abhinav Valada",status:0,relevancy:.41065188419580223,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18931",date:"2024-05-29",title:"EntProp: High Entropy Propagation for Improving Accuracy and Robustness",abstract:`  Deep neural networks (DNNs) struggle to generalize to out-of-distribution
domains that are different from those in training despite their impressive
performance. In practical applications, it is important for DNNs to have both
high standard accuracy and robustness against out-of-distribution domains. One
technique that achieves both of these improvements is disentangled learning
with mixture distribution via auxiliary batch normalization layers (ABNs). This
technique treats clean and transformed samples as different domains, allowing a
DNN to learn better features from mixed domains. However, if we distinguish the
domains of the samples based on entropy, we find that some transformed samples
are drawn from the same domain as clean samples, and these samples are not
completely different domains. To generate samples drawn from a completely
different domain than clean samples, we hypothesize that transforming clean
high-entropy samples to further increase the entropy generates
out-of-distribution samples that are much further away from the in-distribution
domain. On the basis of the hypothesis, we propose high entropy
propagation~(EntProp), which feeds high-entropy samples to the network that
uses ABNs. We introduce two techniques, data augmentation and free adversarial
training, that increase entropy and bring the sample further away from the
in-distribution domain. These techniques do not require additional training
costs. Our experimental results show that EntProp achieves higher standard
accuracy and robustness with a lower training cost than the baseline methods.
In particular, EntProp is highly effective at training on small datasets.
`,authors:"Shohei Enomoto",status:0,relevancy:.4100677003887617,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19237",date:"2024-05-29",title:`ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron
  Pruning`,abstract:`  While large-scale text-to-image diffusion models have demonstrated impressive
image-generation capabilities, there are significant concerns about their
potential misuse for generating unsafe content, violating copyright, and
perpetuating societal biases. Recently, the text-to-image generation community
has begun addressing these concerns by editing or unlearning undesired concepts
from pre-trained models. However, these methods often involve data-intensive
and inefficient fine-tuning or utilize various forms of token remapping,
rendering them susceptible to adversarial jailbreaks. In this paper, we present
a simple and effective training-free approach, ConceptPrune, wherein we first
identify critical regions within pre-trained models responsible for generating
undesirable concepts, thereby facilitating straightforward concept unlearning
via weight pruning. Experiments across a range of concepts including artistic
styles, nudity, object erasure, and gender debiasing demonstrate that target
concepts can be efficiently erased by pruning a tiny fraction, approximately
0.12% of total weights, enabling multi-concept erasure and robustness against
various white-box and black-box adversarial attacks.
`,authors:"Ruchika Chavhan; Da Li; Timothy Hospedales",status:0,relevancy:.40664847564041173,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19210",date:"2024-05-29",title:`Gradient Guided Hypotheses: A unified solution to enable machine
  learning models on scarce and noisy data regimes`,abstract:`  Ensuring high-quality data is paramount for maximizing the performance of
machine learning models and business intelligence systems. However, challenges
in data quality, including noise in data capture, missing records, limited data
production, and confounding variables, significantly constrain the potential
performance of these systems. In this study, we propose an
architecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed to
address these challenges. GGH analyses gradients from hypotheses as a proxy of
distinct and possibly contradictory patterns in the data. This framework
entails an additional step in machine learning training, where gradients can be
included or excluded from backpropagation. In this manner, missing and noisy
data are addressed through a unified solution that perceives both challenges as
facets of the same overarching issue: the propagation of erroneous information.
Experimental validation of GGH is conducted using real-world open-source
datasets, where records with missing rates of up to 98.5% are simulated.
Comparative analysis with state-of-the-art imputation methods demonstrates a
substantial improvement in model performance achieved by GGH. Specifically in
very high scarcity regimes, GGH was found to be the only viable solution.
Additionally, GGH's noise detection capabilities are showcased by introducing
simulated noise into the datasets and observing enhanced model performance
after filtering out the noisy data. This study presents GGH as a promising
solution for improving data quality and model performance in various
applications.
`,authors:"Paulo Neves; Joerg K. Wegner; Philippe Schwaller",status:0,relevancy:.40663266762608186,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19238",date:"2024-05-29",title:`Explanation-based Belief Revision: Moving Beyond Minimalism to
  Explanatory Understanding`,abstract:`  In belief revision, agents typically modify their beliefs when they receive
some new piece of information that is in conflict with them. The guiding
principle behind most belief revision frameworks is that of minimalism, which
advocates minimal changes to existing beliefs. However, minimalism may not
necessarily capture the nuanced ways in which human agents reevaluate and
modify their beliefs. In contrast, the explanatory hypothesis indicates that
people are inherently driven to seek explanations for inconsistencies, thereby
striving for explanatory coherence rather than minimal changes when revising
beliefs. Our contribution in this paper is two-fold. Motivated by the
explanatory hypothesis, we first present a novel, yet simple belief revision
operator that, given a belief base and an explanation for an explanandum, it
revises the belief bases in a manner that preserves the explanandum and is not
necessarily minimal. We call this operator explanation-based belief revision.
Second, we conduct two human-subject studies to empirically validate our
approach and investigate belief revision behavior in real-world scenarios. Our
findings support the explanatory hypothesis and provide insights into the
strategies people employ when resolving inconsistencies.
`,authors:"Stylianos Loukas Vasileiou; William Yeoh",status:0,relevancy:.4043755067873307,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18942",date:"2024-05-29",title:"Verifiably Robust Conformal Prediction",abstract:`  Conformal Prediction (CP) is a popular uncertainty quantification method that
provides distribution-free, statistically valid prediction sets, assuming that
training and test data are exchangeable. In such a case, CP's prediction sets
are guaranteed to cover the (unknown) true test output with a user-specified
probability. Nevertheless, this guarantee is violated when the data is
subjected to adversarial attacks, which often result in a significant loss of
coverage. Recently, several approaches have been put forward to recover CP
guarantees in this setting. These approaches leverage variations of randomised
smoothing to produce conservative sets which account for the effect of the
adversarial perturbations. They are, however, limited in that they only support
$\\ell^2$-bounded perturbations and classification tasks. This paper introduces
\\emph{VRCP (Verifiably Robust Conformal Prediction)}, a new framework that
leverages recent neural network verification methods to recover coverage
guarantees under adversarial attacks. Our VRCP method is the first to support
perturbations bounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and
$\\ell^\\infty$, as well as regression tasks. We evaluate and compare our
approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet)
and regression tasks for deep reinforcement learning environments. In every
case, VRCP achieves above nominal coverage and yields significantly more
efficient and informative prediction regions than the SotA.
`,authors:"Linus Jeary; Tom Kuipers; Mehran Hosseini; Nicola Paoletti",status:0,relevancy:.4022948920414964,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19501",date:"2024-05-29",title:`MDS-ViTNet: Improving saliency prediction for Eye-Tracking with Vision
  Transformer`,abstract:`  In this paper, we present a novel methodology we call MDS-ViTNet (Multi
Decoder Saliency by Vision Transformer Network) for enhancing visual saliency
prediction or eye-tracking. This approach holds significant potential for
diverse fields, including marketing, medicine, robotics, and retail. We propose
a network architecture that leverages the Vision Transformer, moving beyond the
conventional ImageNet backbone. The framework adopts an encoder-decoder
structure, with the encoder utilizing a Swin transformer to efficiently embed
most important features. This process involves a Transfer Learning method,
wherein layers from the Vision Transformer are converted by the Encoder
Transformer and seamlessly integrated into a CNN Decoder. This methodology
ensures minimal information loss from the original input image. The decoder
employs a multi-decoding technique, utilizing dual decoders to generate two
distinct attention maps. These maps are subsequently combined into a singular
output via an additional CNN model. Our trained model MDS-ViTNet achieves
state-of-the-art results across several benchmarks. Committed to fostering
further collaboration, we intend to make our code, models, and datasets
accessible to the public.
`,authors:"Polezhaev Ignat; Goncharenko Igor; Iurina Natalya",status:0,relevancy:.40135242063673715,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18762",date:"2024-05-29",title:"Inpaint Biases: A Pathway to Accurate and Unbiased Image Generation",abstract:`  This paper examines the limitations of advanced text-to-image models in
accurately rendering unconventional concepts which are scarcely represented or
absent in their training datasets. We identify how these limitations not only
confine the creative potential of these models but also pose risks of
reinforcing stereotypes. To address these challenges, we introduce the Inpaint
Biases framework, which employs user-defined masks and inpainting techniques to
enhance the accuracy of image generation, particularly for novel or
inaccurately rendered objects. Through experimental validation, we demonstrate
how this framework significantly improves the fidelity of generated images to
the user's intent, thereby expanding the models' creative capabilities and
mitigating the risk of perpetuating biases. Our study contributes to the
advancement of text-to-image models as unbiased, versatile tools for creative
expression.
`,authors:"Jiyoon Myung; Jihyeon Park",status:0,relevancy:.4006091752668067,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18698",date:"2024-05-29",title:"Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees",abstract:`  The field of risk-constrained reinforcement learning (RCRL) has been
developed to effectively reduce the likelihood of worst-case scenarios by
explicitly handling risk-measure-based constraints. However, the nonlinearity
of risk measures makes it challenging to achieve convergence and optimality. To
overcome the difficulties posed by the nonlinearity, we propose a spectral risk
measure-constrained RL algorithm, spectral-risk-constrained policy optimization
(SRCPO), a bilevel optimization approach that utilizes the duality of spectral
risk measures. In the bilevel optimization structure, the outer problem
involves optimizing dual variables derived from the risk measures, while the
inner problem involves finding an optimal policy given these dual variables.
The proposed method, to the best of our knowledge, is the first to guarantee
convergence to an optimum in the tabular setting. Furthermore, the proposed
method has been evaluated on continuous control tasks and showed the best
performance among other RCRL algorithms satisfying the constraints.
`,authors:"Dohyeong Kim; Taehyun Cho; Seungyub Han; Hojun Chung; Kyungjae Lee; Songhwai Oh",status:0,relevancy:.3956661895746282,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18848",date:"2024-05-29",title:"Anomaly Detection by Context Contrasting",abstract:`  Anomaly Detection focuses on identifying samples that deviate from the norm.
When working with high-dimensional data such as images, a crucial requirement
for detecting anomalous patterns is learning lower-dimensional representations
that capture normal concepts seen during training. Recent advances in
self-supervised learning have shown great promise in this regard. However, many
of the most successful self-supervised anomaly detection methods assume prior
knowledge about the structure of anomalies and leverage synthetic anomalies
during training. Yet, in many real-world applications, we do not know what to
expect from unseen data, and we can solely leverage knowledge about normal
data. In this work, we propose Con2, which addresses this problem by setting
normal training data into distinct contexts while preserving its normal
properties, letting us observe the data from different perspectives. Unseen
normal data consequently adheres to learned context representations while
anomalies fail to do so, letting us detect them without any knowledge about
anomalies during training. Our experiments demonstrate that our approach
achieves state-of-the-art performance on various benchmarks while exhibiting
superior performance in a more realistic healthcare setting, where knowledge
about potential anomalies is often scarce.
`,authors:"Alain Ryser; Thomas M. Sutter; Alexander Marx; Julia E. Vogt",status:0,relevancy:.39473001712551403,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19456",date:"2024-05-29",title:`An Automated Startup Evaluation Pipeline: Startup Success Forecasting
  Framework (SSFF)`,abstract:`  Evaluating startups in their early stages is a complex task that requires
detailed analysis by experts. While automating this process on a large scale
can significantly impact businesses, the inherent complexity poses challenges.
This paper addresses this challenge by introducing the Startup Success
Forecasting Framework (SSFF), a new automated system that combines traditional
machine learning with advanced language models. This intelligent agent-based
architecture is designed to reason, act, synthesize, and decide like a venture
capitalist to perform the analysis end-to-end. The SSFF is made up of three
main parts: - Prediction Block: Uses random forests and neural networks to make
predictions. - Analyst Block: Simulates VC analysis scenario and uses SOTA
prompting techniques - External Knowledge Block: Gathers real-time information
from external sources. This framework requires minimal input data about the
founder and startup description, enhances it with additional data from external
resources, and performs a detailed analysis with high accuracy, all in an
automated manner
`,authors:"Xisen Wang; Yigit Ihlamur",status:0,relevancy:.39271458087525835,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19522",date:"2024-05-29",title:"Artificial Intelligence Index Report 2024",abstract:`  The 2024 Index is our most comprehensive to date and arrives at an important
moment when AI's influence on society has never been more pronounced. This
year, we have broadened our scope to more extensively cover essential trends
such as technical advancements in AI, public perceptions of the technology, and
the geopolitical dynamics surrounding its development. Featuring more original
data than ever before, this edition introduces new estimates on AI training
costs, detailed analyses of the responsible AI landscape, and an entirely new
chapter dedicated to AI's impact on science and medicine. The AI Index report
tracks, collates, distills, and visualizes data related to artificial
intelligence (AI). Our mission is to provide unbiased, rigorously vetted,
broadly sourced data in order for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The AI Index is recognized globally
as one of the most credible and authoritative sources for data and insights on
artificial intelligence. Previous editions have been cited in major newspapers,
including the The New York Times, Bloomberg, and The Guardian, have amassed
hundreds of academic citations, and been referenced by high-level policymakers
in the United States, the United Kingdom, and the European Union, among other
places. This year's edition surpasses all previous ones in size, scale, and
scope, reflecting the growing significance that AI is coming to hold in all of
our lives.
`,authors:"Nestor Maslej; Loredana Fattorini; Raymond Perrault; Vanessa Parli; Anka Reuel; Erik Brynjolfsson; John Etchemendy; Katrina Ligett; Terah Lyons; James Manyika; Juan Carlos Niebles; Yoav Shoham; Russell Wald; Jack Clark",status:0,relevancy:.3904021519416203,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19524",date:"2024-05-29",title:"AI Risk Management Should Incorporate Both Safety and Security",abstract:`  The exposure of security vulnerabilities in safety-aligned language models,
e.g., susceptibility to adversarial attacks, has shed light on the intricate
interplay between AI safety and AI security. Although the two disciplines now
come together under the overarching goal of AI risk management, they have
historically evolved separately, giving rise to differing perspectives.
Therefore, in this paper, we advocate that stakeholders in AI risk management
should be aware of the nuances, synergies, and interplay between safety and
security, and unambiguously take into account the perspectives of both
disciplines in order to devise mostly effective and holistic risk mitigation
approaches. Unfortunately, this vision is often obfuscated, as the definitions
of the basic concepts of "safety" and "security" themselves are often
inconsistent and lack consensus across communities. With AI risk management
being increasingly cross-disciplinary, this issue is particularly salient. In
light of this conceptual challenge, we introduce a unified reference framework
to clarify the differences and interplay between AI safety and AI security,
aiming to facilitate a shared understanding and effective collaboration across
communities.
`,authors:"Xiangyu Qi; Yangsibo Huang; Yi Zeng; Edoardo Debenedetti; Jonas Geiping; Luxi He; Kaixuan Huang; Udari Madhushani; Vikash Sehwag; Weijia Shi; Boyi Wei; Tinghao Xie; Danqi Chen; Pin-Yu Chen; Jeffrey Ding; Ruoxi Jia; Jiaqi Ma; Arvind Narayanan; Weijie J Su; Mengdi Wang; Chaowei Xiao; Bo Li; Dawn Song; Peter Henderson; Prateek Mittal",status:0,relevancy:.3873078006450864,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19296",date:"2024-05-29",title:"Neural Isometries: Taming Transformations for Equivariant ML",abstract:`  Real-world geometry and 3D vision tasks are replete with challenging
symmetries that defy tractable analytical expression. In this paper, we
introduce Neural Isometries, an autoencoder framework which learns to map the
observation space to a general-purpose latent space wherein encodings are
related by isometries whenever their corresponding observations are
geometrically related in world space. Specifically, we regularize the latent
space such that maps between encodings preserve a learned inner product and
commute with a learned functional operator, in the same manner as rigid-body
transformations commute with the Laplacian. This approach forms an effective
backbone for self-supervised representation learning, and we demonstrate that a
simple off-the-shelf equivariant network operating in the pre-trained latent
space can achieve results on par with meticulously-engineered, handcrafted
networks designed to handle complex, nonlinear symmetries. Furthermore,
isometric maps capture information about the respective transformations in
world space, and we show that this allows us to regress camera poses directly
from the coefficients of the maps between encodings of adjacent views of a
scene.
`,authors:"Thomas W. Mitchel; Michael Taylor; Vincent Sitzmann",status:0,relevancy:.3838432644456644,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18910",date:"2024-05-29",title:`Predicting Parking Availability in Singapore with Cross-Domain Data: A
  New Dataset and A Data-Driven Approach`,abstract:`  The increasing number of vehicles highlights the need for efficient parking
space management. Predicting real-time Parking Availability (PA) can help
mitigate traffic congestion and the corresponding social problems, which is a
pressing issue in densely populated cities like Singapore. In this study, we
aim to collectively predict future PA across Singapore with complex factors
from various domains. The contributions in this paper are listed as follows:
(1) A New Dataset: We introduce the \\texttt{SINPA} dataset, containing a year's
worth of PA data from 1,687 parking lots in Singapore, enriched with various
spatial and temporal factors. (2) A Data-Driven Approach: We present DeepPA, a
novel deep-learning framework, to collectively and efficiently predict future
PA across thousands of parking lots. (3) Extensive Experiments and Deployment:
DeepPA demonstrates a 9.2% reduction in prediction error for up to 3-hour
forecasts compared to existing advanced models. Furthermore, we implement
DeepPA in a practical web-based platform to provide real-time PA predictions to
aid drivers and inform urban planning for the governors in Singapore. We
release the dataset and source code at https://github.com/yoshall/SINPA.
`,authors:"Huaiwu Zhang; Yutong Xia; Siru Zhong; Kun Wang; Zekun Tong; Qingsong Wen; Roger Zimmermann; Yuxuan Liang",status:0,relevancy:.38236117955777016,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18756",date:"2024-05-29",title:"Provable Contrastive Continual Learning",abstract:`  Continual learning requires learning incremental tasks with dynamic data
distributions. So far, it has been observed that employing a combination of
contrastive loss and distillation loss for training in continual learning
yields strong performance. To the best of our knowledge, however, this
contrastive continual learning framework lacks convincing theoretical
explanations. In this work, we fill this gap by establishing theoretical
performance guarantees, which reveal how the performance of the model is
bounded by training losses of previous tasks in the contrastive continual
learning framework. Our theoretical explanations further support the idea that
pre-training can benefit continual learning. Inspired by our theoretical
analysis of these guarantees, we propose a novel contrastive continual learning
algorithm called CILA, which uses adaptive distillation coefficients for
different tasks. These distillation coefficients are easily computed by the
ratio between average distillation losses and average contrastive losses from
previous tasks. Our method shows great improvement on standard benchmarks and
achieves new state-of-the-art performance.
`,authors:"Yichen Wen; Zhiquan Tan; Kaipeng Zheng; Chuanlong Xie; Weiran Huang",status:0,relevancy:.38203618582803467,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19538",date:"2024-05-29",title:`CheXpert Plus: Hundreds of Thousands of Aligned Radiology Texts, Images
  and Patients`,abstract:`  Since the release of the original CheXpert paper five years ago, CheXpert has
become one of the most widely used and cited clinical AI datasets. The
emergence of vision language models has sparked an increase in demands for
sharing reports linked to CheXpert images, along with a growing interest among
AI fairness researchers in obtaining demographic data. To address this,
CheXpert Plus serves as a new collection of radiology data sources, made
publicly available to enhance the scaling, performance, robustness, and
fairness of models for all subsequent machine learning tasks in the field of
radiology. CheXpert Plus is the largest text dataset publicly released in
radiology, with a total of 36 million text tokens, including 13 million
impression tokens. To the best of our knowledge, it represents the largest text
de-identification effort in radiology, with almost 1 million PHI spans
anonymized. It is only the second time that a large-scale English paired
dataset has been released in radiology, thereby enabling, for the first time,
cross-institution training at scale. All reports are paired with high-quality
images in DICOM format, along with numerous image and patient metadata covering
various clinical and socio-economic groups, as well as many pathology labels
and RadGraph annotations. We hope this dataset will boost research for AI
models that can further assist radiologists and help improve medical care. Data
is available at the following URL:
https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1
Models are available at the following URL:
https://github.com/Stanford-AIMI/chexpert-plus
`,authors:"Pierre Chambon; Jean-Benoit Delbrouck; Thomas Sounack; Shih-Cheng Huang; Zhihong Chen; Maya Varma; Steven QH Truong; Chu The Chuong; Curtis P. Langlotz",status:0,relevancy:.37835273214981524,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18845",date:"2024-05-29",title:`Simulation, Modelling and Classification of Wiki Contributors: Spotting
  The Good, The Bad, and The Ugly`,abstract:`  Data crowdsourcing is a data acquisition process where groups of voluntary
contributors feed platforms with highly relevant data ranging from news,
comments, and media to knowledge and classifications. It typically processes
user-generated data streams to provide and refine popular services such as
wikis, collaborative maps, e-commerce sites, and social networks. Nevertheless,
this modus operandi raises severe concerns regarding ill-intentioned data
manipulation in adversarial environments. This paper presents a simulation,
modelling, and classification approach to automatically identify human and
non-human (bots) as well as benign and malign contributors by using data
fabrication to balance classes within experimental data sets, data stream
modelling to build and update contributor profiles and, finally, autonomic data
stream classification. By employing WikiVoyage - a free worldwide wiki travel
guide open to contribution from the general public - as a testbed, our approach
proves to significantly boost the confidence and quality of the classifier by
using a class-balanced data stream, comprising both real and synthetic data.
Our empirical results show that the proposed method distinguishes between
benign and malign bots as well as human contributors with a classification
accuracy of up to 92 %.
`,authors:"Silvia García Méndez; Fátima Leal; Benedita Malheiro; Juan Carlos Burguillo Rial; Bruno Veloso; Adriana E. Chis; Horacio González Vélez",status:0,relevancy:.3771270497721225,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19464",date:"2024-05-29",title:`Leveraging Generative AI for Smart City Digital Twins: A Survey on the
  Autonomous Generation of Data, Scenarios, 3D City Models, and Urban Designs`,abstract:`  The digital transformation of modern cities by integrating advanced
information, communication, and computing technologies has marked the epoch of
data-driven smart city applications for efficient and sustainable urban
management. Despite their effectiveness, these applications often rely on
massive amounts of high-dimensional and multi-domain data for monitoring and
characterizing different urban sub-systems, presenting challenges in
application areas that are limited by data quality and availability, as well as
costly efforts for generating urban scenarios and design alternatives. As an
emerging research area in deep learning, Generative Artificial Intelligence
(AI) models have demonstrated their unique values in data and code generation.
This survey paper aims to explore the innovative integration of generative AI
techniques and urban digital twins to address challenges in the realm of smart
cities in various urban sectors, such as transportation and mobility
management, energy system operations, building and infrastructure management,
and urban design. The survey starts with the introduction of popular generative
AI models with their application areas, followed by a structured review of the
existing urban science applications that leverage the autonomous capability of
the generative AI techniques to facilitate (a) data augmentation for promoting
urban monitoring and predictive analytics, (b) synthetic data and scenario
generation, (c) automated 3D city modeling, and (d) generative urban design and
optimization. Based on the review, this survey discusses potential
opportunities and technical strategies that integrate generative AI models into
the next-generation urban digital twins for more reliable, scalable, and
automated management of smart cities.
`,authors:"Haowen Xu; Femi Omitaomu; Soheil Sabri; Xiao Li; Yongze Song",status:0,relevancy:.3769032798310912,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19201",date:"2024-05-29",title:`Going beyond compositional generalization, DDPMs can produce zero-shot
  interpolation`,abstract:`  Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkable
capabilities in image generation, with studies suggesting that they can
generalize by composing latent factors learned from the training data. In this
work, we go further and study DDPMs trained on strictly separate subsets of the
data distribution with large gaps on the support of the latent factors. We show
that such a model can effectively generate images in the unexplored,
intermediate regions of the distribution. For instance, when trained on clearly
smiling and non-smiling faces, we demonstrate a sampling procedure which can
generate slightly smiling faces without reference images (zero-shot
interpolation). We replicate these findings for other attributes as well as
other datasets.
$\\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\\text{Our
code is available on GitHub.}}$
`,authors:"Justin Deschenaux; Igor Krawczuk; Grigorios Chrysos; Volkan Cevher",status:0,relevancy:.37528768841224847,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18968",date:"2024-05-29",title:"UniIF: Unified Molecule Inverse Folding",abstract:`  Molecule inverse folding has been a long-standing challenge in chemistry and
biology, with the potential to revolutionize drug discovery and material
science. Despite specified models have been proposed for different small- or
macro-molecules, few have attempted to unify the learning process, resulting in
redundant efforts. Complementary to recent advancements in molecular structure
prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified
model UniIF for the inverse folding of all molecules. We do such unification in
two levels: 1) Data-Level: We propose a unified block graph data form for all
molecules, including the local frame building and geometric feature
initialization. 2) Model-Level: We introduce a geometric block attention
network, comprising a geometric interaction, interactive attention and virtual
long-term dependency modules, to capture the 3D interactions of all molecules.
Through comprehensive evaluations across various tasks such as protein design,
RNA design, and material design, we demonstrate that our proposed method
surpasses state-of-the-art methods on all tasks. UniIF offers a versatile and
effective solution for general molecule inverse folding.
`,authors:"Zhangyang Gao; Jue Wang; Cheng Tan; Lirong Wu; Yufei Huang; Siyuan Li; Zhirui Ye; Stan Z. Li",status:0,relevancy:.37069833497965055,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19331",date:"2024-05-29",title:"NPGA: Neural Parametric Gaussian Avatars",abstract:`  The creation of high-fidelity, digital versions of human heads is an
important stepping stone in the process of further integrating virtual
components into our everyday lives. Constructing such avatars is a challenging
research problem, due to a high demand for photo-realism and real-time
rendering performance. In this work, we propose Neural Parametric Gaussian
Avatars (NPGA), a data-driven approach to create high-fidelity, controllable
avatars from multi-view video recordings. We build our method around 3D
Gaussian Splatting for its highly efficient rendering and to inherit the
topological flexibility of point clouds. In contrast to previous work, we
condition our avatars' dynamics on the rich expression space of neural
parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we
distill the backward deformation field of our underlying NPHM into forward
deformations which are compatible with rasterization-based rendering. All
remaining fine-scale, expression-dependent details are learned from the
multi-view videos. To increase the representational capacity of our avatars, we
augment the canonical Gaussian point cloud using per-primitive latent features
which govern its dynamic behavior. To regularize this increased dynamic
expressivity, we propose Laplacian terms on the latent features and predicted
dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating
that NPGA significantly outperforms the previous state-of-the-art avatars on
the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate
animation capabilities from real-world monocular videos.
`,authors:"Simon Giebenhain; Tobias Kirschstein; Martin Rünz; Lourdes Agapito; Matthias Nießner",status:0,relevancy:.3698521178439419,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19317",date:"2024-05-29",title:`Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal
  Best Arm Identification`,abstract:`  This study investigates a local asymptotic minimax optimal strategy for
fixed-budget best arm identification (BAI). We propose the Adaptive Generalized
Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of
the probability of misidentifying the best arm aligns with the worst-case lower
bound under the small-gap regime, where the gap between the expected outcomes
of the best and suboptimal arms is small. Our strategy corresponds to a
generalization of the Neyman allocation for two-armed bandits (Neyman, 1934;
Kaufmann et al., 2016) and a refinement of existing strategies such as the ones
proposed by Glynn & Juneja (2004) and Shin et al. (2018). Compared to Komiyama
et al. (2022), which proposes a minimax rate-optimal strategy, our proposed
strategy has a tighter upper bound that exactly matches the lower bound,
including the constant terms, by restricting the class of distributions to the
class of small-gap distributions. Our result contributes to the longstanding
open issue about the existence of asymptotically optimal strategies in
fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.
`,authors:"Masahiro Kato",status:0,relevancy:.3631625035025057,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19176",date:"2024-05-29",title:"The ethical situation of DALL-E 2",abstract:`  A hot topic of Artificial Intelligence right now is image generation from
prompts. DALL-E 2 is one of the biggest names in this domain, as it allows
people to create images from simple text inputs, to even more complicated ones.
The company that made this possible, OpenAI, has assured everyone that visited
their website that their mission is to ensure that artificial general
intelligence benefits all humanity. A noble idea in our opinion, that also
stood as the motive behind us choosing this subject. This paper analyzes the
ethical implications of an AI image generative system, with an emphasis on how
society is responding to it, how it probably will and how it should if all the
right measures are taken.
`,authors:"Eduard Hogea; Josem Rocafortf",status:0,relevancy:.35870291508335583,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19284",date:"2024-05-29",title:`Optimizing Foundation Model Inference on a Many-tiny-core Open-source
  RISC-V Platform`,abstract:`  Transformer-based foundation models have become crucial for various domains,
most notably natural language processing (NLP) or computer vision (CV). These
models are predominantly deployed on high-performance GPUs or hardwired
accelerators with highly customized, proprietary instruction sets. Until now,
limited attention has been given to RISC-V-based general-purpose platforms. In
our work, we present the first end-to-end inference results of transformer
models on an open-source many-tiny-core RISC-V platform implementing
distributed Softmax primitives and leveraging ISA extensions for SIMD
floating-point operand streaming and instruction repetition, as well as
specialized DMA engines to minimize costly main memory accesses and to tolerate
their latency. We focus on two foundational transformer topologies,
encoder-only and decoder-only models. For encoder-only models, we demonstrate a
speedup of up to 12.8x between the most optimized implementation and the
baseline version. We reach over 79% FPU utilization and 294 GFLOPS/W,
outperforming State-of-the-Art (SoA) accelerators by more than 2x utilizing the
HW platform while achieving comparable throughput per computational unit. For
decoder-only topologies, we achieve 16.1x speedup in the Non-Autoregressive
(NAR) mode and up to 35.6x speedup in the Autoregressive (AR) mode compared to
the baseline implementation. Compared to the best SoA dedicated accelerator, we
achieve 2.04x higher FPU utilization.
`,authors:"Viviane Potocnik; Luca Colagrande; Tim Fischer; Luca Bertaccini; Daniele Jahier Pagliari; Alessio Burrello; Luca Benini",status:0,relevancy:.3535956017607178,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18707",date:"2024-05-29",title:`Adaptive and Parallel Split Federated Learning in Vehicular Edge
  Computing`,abstract:`  Vehicular edge intelligence (VEI) is a promising paradigm for enabling future
intelligent transportation systems by accommodating artificial intelligence
(AI) at the vehicular edge computing (VEC) system. Federated learning (FL)
stands as one of the fundamental technologies facilitating collaborative model
training locally and aggregation, while safeguarding the privacy of vehicle
data in VEI. However, traditional FL faces challenges in adapting to vehicle
heterogeneity, training large models on resource-constrained vehicles, and
remaining susceptible to model weight privacy leakage. Meanwhile, split
learning (SL) is proposed as a promising collaborative learning framework which
can mitigate the risk of model wights leakage, and release the training
workload on vehicles. SL sequentially trains a model between a vehicle and an
edge cloud (EC) by dividing the entire model into a vehicle-side model and an
EC-side model at a given cut layer. In this work, we combine the advantages of
SL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular
Edge Computing (ASFV). The ASFV scheme adaptively splits the model and
parallelizes the training process, taking into account mobile vehicle selection
and resource allocation. Our extensive simulations, conducted on
non-independent and identically distributed data, demonstrate that the proposed
ASFV solution significantly reduces training latency compared to existing
benchmarks, while adapting to network dynamics and vehicles' mobility.
`,authors:"Xianke Qiang; Zheng Chang; Yun Hu; Lei Liu; Timo Hamalainen",status:0,relevancy:.35243586623531886,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19243",date:"2024-05-29",title:`Challenge-Device-Synthesis: A multi-disciplinary approach for the
  development of social innovation competences for students of Artificial
  Intelligence`,abstract:`  The advent of Artificial Intelligence is expected to imply profound changes
in the short-term. It is therefore imperative for Academia, and particularly
for the Computer Science scope, to develop cross-disciplinary tools that bond
AI developments to their social dimension. To this aim, we introduce the
Challenge-Device-Synthesis methodology (CDS), in which a specific challenge is
presented to the students of AI, who are required to develop a device as a
solution for the challenge. The device becomes the object of study for the
different dimensions of social transformation, and the conclusions addressed by
the students during the discussion around the device are presented in a
synthesis piece in the shape of a 10-page scientific paper. The latter is
evaluated taking into account both the depth of analysis and the level to which
it genuinely reflects the social transformations associated with the proposed
AI-based device. We provide data obtained during the pilot for the
implementation phase of CDS within the subject of Social Innovation, a 6-ECTS
subject from the 6th semester of the Degree of Artificial Intelligence,
UAB-Barcelona. We provide details on temporalisation, task distribution,
methodological tools used and assessment delivery procedure, as well as
qualitative analysis of the results obtained.
`,authors:"Matías Bilkis; Joan Moya Kohler; Fernando Vilariño",status:0,relevancy:.35169060506191663,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18820",date:"2024-05-29",title:`Diffeomorphic interpolation for efficient persistence-based topological
  optimization`,abstract:`  Topological Data Analysis (TDA) provides a pipeline to extract quantitative
topological descriptors from structured objects. This enables the definition of
topological loss functions, which assert to what extent a given object exhibits
some topological properties. These losses can then be used to perform
topological optimizationvia gradient descent routines. While theoretically
sounded, topological optimization faces an important challenge: gradients tend
to be extremely sparse, in the sense that the loss function typically depends
on only very few coordinates of the input object, yielding dramatically slow
optimization schemes in practice.Focusing on the central case of topological
optimization for point clouds, we propose in this work to overcome this
limitation using diffeomorphic interpolation, turning sparse gradients into
smooth vector fields defined on the whole space, with quantifiable Lipschitz
constants. In particular, we show that our approach combines efficiently with
subsampling techniques routinely used in TDA, as the diffeomorphism derived
from the gradient computed on a subsample can be used to update the coordinates
of the full input object, allowing us to perform topological optimization on
point clouds at an unprecedented scale. Finally, we also showcase the relevance
of our approach for black-box autoencoder (AE) regularization, where we aim at
enforcing topological priors on the latent spaces associated to fixed,
pre-trained, black-box AE models, and where we show thatlearning a
diffeomorphic flow can be done once and then re-applied to new data in linear
time (while vanilla topological optimization has to be re-run from scratch).
Moreover, reverting the flow allows us to generate data by sampling the
topologically-optimized latent space directly, yielding better interpretability
of the model.
`,authors:"Mathieu Carriere; Marc Theveneau; Théo Lacombe",status:0,relevancy:.34661584343309726,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18723",date:"2024-05-29",title:"Conformal Depression Prediction",abstract:`  While existing depression recognition methods based on deep learning show
promise, their practical application is hindered by the lack of
trustworthiness, as these deep models are often deployed as \\textit{black box}
models, leaving us uncertain about the confidence of the model predictions. For
high-risk clinical applications like depression recognition, uncertainty
quantification is essential in decision-making. In this paper, we introduce
conformal depression prediction (CDP), a depression recognition method with
uncertainty quantification based on conformal prediction (CP), giving valid
confidence intervals with theoretical coverage guarantees for the model
predictions. CDP is a plug-and-play module that requires neither model
retraining nor an assumption about the depression data distribution. As CDP
provides only an average performance guarantee across all inputs rather than
per-input performance guarantee, we propose CDP-ACC, an improved conformal
prediction with approximate conditional coverage. CDP-ACC firstly estimates the
prediction distribution through neighborhood relaxation, and then introduces a
conformal score function by constructing nested sequences, so as to provide
tighter prediction interval for each specific input. We empirically demonstrate
the application of uncertainty quantification in depression recognition, and
the effectiveness and superiority of CDP and CDP-ACC on the AVEC 2013 and AVEC
2014 datasets
`,authors:"Yonghong Li; Shan Qu; Xiuzhuang Zhou",status:0,relevancy:.340765280481393,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18877",date:"2024-05-29",title:"Continuous Product Graph Neural Networks",abstract:`  Processing multidomain data defined on multiple graphs holds significant
potential in various practical applications in computer science. However,
current methods are mostly limited to discrete graph filtering operations.
Tensorial partial differential equations on graphs (TPDEGs) provide a
principled framework for modeling structured data across multiple interacting
graphs, addressing the limitations of the existing discrete methodologies. In
this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that
emerge as a natural solution to the TPDEG. CITRUS leverages the separability of
continuous heat kernels from Cartesian graph products to efficiently implement
graph spectral decomposition. We conduct thorough theoretical analyses of the
stability and over-smoothing properties of CITRUS in response to
domain-specific graph perturbations and graph spectra effects on the
performance. We evaluate CITRUS on well-known traffic and weather
spatiotemporal forecasting datasets, demonstrating superior performance over
existing approaches.
`,authors:"Aref Einizade; Fragkiskos D. Malliaros; Jhony H. Giraldo",status:0,relevancy:.33932337406366275,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19495",date:"2024-05-29",title:`Qiskit Code Assistant: Training LLMs for generating Quantum Computing
  Code`,abstract:`  Code Large Language Models (Code LLMs) have emerged as powerful tools,
revolutionizing the software development landscape by automating the coding
process and reducing time and effort required to build applications. This paper
focuses on training Code LLMs to specialize in the field of quantum computing.
We begin by discussing the unique needs of quantum computing programming, which
differ significantly from classical programming approaches or languages. A Code
LLM specializing in quantum computing requires a foundational understanding of
quantum computing and quantum information theory. However, the scarcity of
available quantum code examples and the rapidly evolving field, which
necessitates continuous dataset updates, present significant challenges.
Moreover, we discuss our work on training Code LLMs to produce high-quality
quantum code using the Qiskit library. This work includes an examination of the
various aspects of the LLMs used for training and the specific training
conditions, as well as the results obtained with our current models. To
evaluate our models, we have developed a custom benchmark, similar to
HumanEval, which includes a set of tests specifically designed for the field of
quantum computing programming using Qiskit. Our findings indicate that our
model outperforms existing state-of-the-art models in quantum computing tasks.
We also provide examples of code suggestions, comparing our model to other
relevant code LLMs. Finally, we introduce a discussion on the potential
benefits of Code LLMs for quantum computing computational scientists,
researchers, and practitioners. We also explore various features and future
work that could be relevant in this context.
`,authors:"Nicolas Dupuis; Luca Buratti; Sanjay Vishwakarma; Aitana Viudes Forrat; David Kremer; Ismael Faro; Ruchir Puri; Juan Cruz-Benito",status:0,relevancy:.33826390050958177,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19053",date:"2024-05-29",title:`Multiscale Spatio-Temporal Enhanced Short-term Load Forecasting of
  Electric Vehicle Charging Stations`,abstract:`  The rapid expansion of electric vehicles (EVs) has rendered the load
forecasting of electric vehicle charging stations (EVCS) increasingly critical.
The primary challenge in achieving precise load forecasting for EVCS lies in
accounting for the nonlinear of charging behaviors, the spatial interactions
among different stations, and the intricate temporal variations in usage
patterns. To address these challenges, we propose a Multiscale Spatio-Temporal
Enhanced Model (MSTEM) for effective load forecasting at EVCS. MSTEM
incorporates a multiscale graph neural network to discern hierarchical
nonlinear temporal dependencies across various time scales. Besides, it also
integrates a recurrent learning component and a residual fusion mechanism,
enhancing its capability to accurately capture spatial and temporal variations
in charging patterns. The effectiveness of the proposed MSTEM has been
validated through comparative analysis with six baseline models using three
evaluation metrics. The case studies utilize real-world datasets for both fast
and slow charging loads at EVCS in Perth, UK. The experimental results
demonstrate the superiority of MSTEM in short-term continuous load forecasting
for EVCS.
`,authors:"Zongbao Zhang; Jiao Hao; Wenmeng Zhao; Yan Liu; Yaohui Huang; Xinhang Luo",status:0,relevancy:.3376575572574454,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18984",date:"2024-05-29",title:`Optimizing Vehicular Networks with Variational Quantum Circuits-based
  Reinforcement Learning`,abstract:`  In vehicular networks (VNets), ensuring both road safety and dependable
network connectivity is of utmost importance. Achieving this necessitates the
creation of resilient and efficient decision-making policies that prioritize
multiple objectives. In this paper, we develop a Variational Quantum Circuit
(VQC)-based multi-objective reinforcement learning (MORL) framework to
characterize efficient network selection and autonomous driving policies in a
vehicular network (VNet). Numerical results showcase notable enhancements in
both convergence rates and rewards when compared to conventional deep-Q
networks (DQNs), validating the efficacy of the VQC-MORL solution.
`,authors:"Zijiang Yan; Ramsundar Tanikella; Hina Tabassum",status:0,relevancy:.3370702903778152,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18681",date:"2024-05-29",title:"A random-key GRASP for combinatorial optimization",abstract:`  This paper proposes a problem-independent GRASP metaheuristic using the
random-key optimizer (RKO) paradigm. GRASP (greedy randomized adaptive search
procedure) is a metaheuristic for combinatorial optimization that repeatedly
applies a semi-greedy construction procedure followed by a local search
procedure. The best solution found over all iterations is returned as the
solution of the GRASP. Continuous GRASP (C-GRASP) is an extension of GRASP for
continuous optimization in the unit hypercube. A random-key optimizer (RKO)
uses a vector of random keys to encode a solution to a combinatorial
optimization problem. It uses a decoder to evaluate a solution encoded by the
vector of random keys. A random-key GRASP is a C-GRASP where points in the unit
hypercube are evaluated employing a decoder. We describe random key GRASP
consisting of a problem-independent component and a problem-dependent decoder.
As a proof of concept, the random-key GRASP is tested on five NP-hard
combinatorial optimization problems: traveling salesman problem, tree of hubs
location problem, Steiner triple covering problem, node capacitated graph
partitioning problem, and job sequencing and tool switching problem.
`,authors:"Antonio A. Chaves; Mauricio G. C. Resende; Ricardo M. A. Silva",status:0,relevancy:.33583722498065316,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19202",date:"2024-05-29",title:`Vulnerable Road User Detection and Safety Enhancement: A Comprehensive
  Survey`,abstract:`  Traffic incidents involving vulnerable road users (VRUs) constitute a
significant proportion of global road accidents. Advances in traffic
communication ecosystems, coupled with sophisticated signal processing and
machine learning techniques, have facilitated the utilization of data from
diverse sensors. Despite these advancements and the availability of extensive
datasets, substantial progress is required to mitigate traffic casualties. This
paper provides a comprehensive survey of state-of-the-art technologies and
methodologies to enhance the safety of VRUs. The study delves into the
communication networks between vehicles and VRUs, emphasizing the integration
of advanced sensors and the availability of relevant datasets. It explores
preprocessing techniques and data fusion methods to enhance sensor data
quality. Furthermore, our study assesses critical simulation environments
essential for developing and testing VRU safety systems. Our research also
highlights recent advances in VRU detection and classification algorithms,
addressing challenges such as variable environmental conditions. Additionally,
we cover cutting-edge research in predicting VRU intentions and behaviors,
which is crucial for proactive collision avoidance strategies. Through this
survey, we aim to provide a comprehensive understanding of the current
landscape of VRU safety technologies, identifying areas of progress and areas
needing further research and development.
`,authors:"Renato M. Silva; Gregório F. Azevedo; Matheus V. V. Berto; Jean R. Rocha; Eduardo C. Fidelis; Matheus V. Nogueira; Pedro H. Lisboa; Tiago A. Almeida",status:0,relevancy:.33476239158703225,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18742",date:"2024-05-29",title:"Musical Phrase Segmentation via Grammatical Induction",abstract:`  We outline a solution to the challenge of musical phrase segmentation that
uses grammatical induction algorithms, a class of algorithms which infer a
context-free grammar from an input sequence. We analyze the performance of five
grammatical induction algorithms on three datasets using various musical
viewpoint combinations. Our experiments show that the LONGESTFIRST algorithm
achieves the best F1 scores across all three datasets and that input encodings
that include the duration viewpoint result in the best performance.
`,authors:"Reed Perkins; Dan Ventura",status:0,relevancy:.3340359364976775,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19213",date:"2024-05-29",title:"HawkVision: Low-Latency Modeless Edge AI Serving",abstract:`  The trend of modeless ML inference is increasingly growing in popularity as
it hides the complexity of model inference from users and caters to diverse
user and application accuracy requirements. Previous work mostly focuses on
modeless inference in data centers. To provide low-latency inference, in this
paper, we promote modeless inference at the edge. The edge environment
introduces additional challenges related to low power consumption, limited
device memory, and volatile network environments.
  To address these challenges, we propose HawkVision, which provides
low-latency modeless serving of vision DNNs. HawkVision leverages a two-layer
edge-DC architecture that employs confidence scaling to reduce the number of
model options while meeting diverse accuracy requirements. It also supports
lossy inference under volatile network environments. Our experimental results
show that HawkVision outperforms current serving systems by up to 1.6X in P99
latency for providing modeless service. Our FPGA prototype demonstrates similar
performance at certain accuracy levels with up to a 3.34X reduction in power
consumption.
`,authors:"ChonLam Lao; Jiaqi Gao; Ganesh Ananthanarayanan; Aditya Akella; Minlan Yu",status:0,relevancy:.3337253868241944,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18802",date:"2024-05-29",title:`Enhancing Security and Privacy in Federated Learning using Update
  Digests and Voting-Based Defense`,abstract:`  Federated Learning (FL) is a promising privacy-preserving machine learning
paradigm that allows data owners to collaboratively train models while keeping
their data localized. Despite its potential, FL faces challenges related to the
trustworthiness of both clients and servers, especially in the presence of
curious or malicious adversaries. In this paper, we introduce a novel framework
named \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with
\\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses
the critical issues of privacy preservation and resistance to Byzantine attacks
within distributed learning environments. FLUD utilizes an innovative approach,
the $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$
norm across sliding windows of updates as an update digest. This digest enables
the server to calculate a shared distance matrix, significantly reducing the
overhead associated with Secure Multi-Party Computation (SMPC) by three orders
of magnitude while effectively distinguishing between benign and malicious
updates. Additionally, FLUD integrates a privacy-preserving, voting-based
defense mechanism that employs optimized SMPC protocols to minimize
communication rounds. Our comprehensive experiments demonstrate FLUD's
effectiveness in countering Byzantine adversaries while incurring low
communication and runtime overhead. FLUD offers a scalable framework for secure
and reliable FL in distributed environments, facilitating its application in
scenarios requiring robust data management and security.
`,authors:"Wenjie Li; Kai Fan; Jingyuan Zhang; Hui Li; Wei Yang Bryan Lim; Qiang Yang",status:0,relevancy:.3293751801649848,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18929",date:"2024-05-29",title:`Deep Positive-Unlabeled Anomaly Detection for Contaminated Unlabeled
  Data`,abstract:`  Semi-supervised anomaly detection, which aims to improve the performance of
the anomaly detector by using a small amount of anomaly data in addition to
unlabeled data, has attracted attention. Existing semi-supervised approaches
assume that unlabeled data are mostly normal. They train the anomaly detector
to minimize the anomaly scores for the unlabeled data, and to maximize those
for the anomaly data. However, in practice, the unlabeled data are often
contaminated with anomalies. This weakens the effect of maximizing the anomaly
scores for anomalies, and prevents us from improving the detection performance.
To solve this problem, we propose the positive-unlabeled autoencoder, which is
based on positive-unlabeled learning and the anomaly detector such as the
autoencoder. With our approach, we can approximate the anomaly scores for
normal data using the unlabeled and anomaly data. Therefore, without the
labeled normal data, we can train the anomaly detector to minimize the anomaly
scores for normal data, and to maximize those for the anomaly data. In
addition, our approach is applicable to various anomaly detectors such as the
DeepSVDD. Experiments on various datasets show that our approach achieves
better detection performance than existing approaches.
`,authors:"Hiroshi Takahashi; Tomoharu Iwata; Atsutoshi Kumagai; Yuuki Yamanaka",status:0,relevancy:.3222058537326241,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19236",date:"2024-05-29",title:`Exploring the impact of traffic signal control and connected and
  automated vehicles on intersections safety: A deep reinforcement learning
  approach`,abstract:`  In transportation networks, intersections pose significant risks of
collisions due to conflicting movements of vehicles approaching from different
directions. To address this issue, various tools can exert influence on traffic
safety both directly and indirectly. This study focuses on investigating the
impact of adaptive signal control and connected and automated vehicles (CAVs)
on intersection safety using a deep reinforcement learning approach. The
objective is to assess the individual and combined effects of CAVs and adaptive
traffic signal control on traffic safety, considering rear-end and crossing
conflicts. The study employs a Deep Q Network (DQN) to regulate traffic signals
and driving behaviors of both CAVs and Human Drive Vehicles (HDVs), and uses
Time To Collision (TTC) metric to evaluate safety. The findings demonstrate a
significant reduction in rear-end and crossing conflicts through the combined
implementation of CAVs and DQNs-based traffic signal control. Additionally, the
long-term positive effects of CAVs on safety are similar to the short-term
effects of combined CAVs and DQNs-based traffic signal control. Overall, the
study emphasizes the potential benefits of integrating CAVs and adaptive
traffic signal control approaches in order to enhance traffic safety. The
findings of this study could provide valuable insights for city officials and
transportation authorities in developing effective strategies to improve safety
at signalized intersections.
`,authors:"Amir Hossein Karbasi; Hao Yang; Saiedeh Razavi",status:0,relevancy:.32070303691652524,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19453",date:"2024-05-29",title:"Optimizing Split Points for Error-Resilient SplitFed Learning",abstract:`  Recent advancements in decentralized learning, such as Federated Learning
(FL), Split Learning (SL), and Split Federated Learning (SplitFed), have
expanded the potentials of machine learning. SplitFed aims to minimize the
computational burden on individual clients in FL and parallelize SL while
maintaining privacy. This study investigates the resilience of SplitFed to
packet loss at model split points. It explores various parameter aggregation
strategies of SplitFed by examining the impact of splitting the model at
different points-either shallow split or deep split-on the final global model
performance. The experiments, conducted on a human embryo image segmentation
task, reveal a statistically significant advantage of a deeper split point.
`,authors:"Chamani Shiranthika; Parvaneh Saeedi; Ivan V. Bajić",status:0,relevancy:.3192607615605515,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19300",date:"2024-05-29",title:`Measuring and Mitigating Bias for Tabular Datasets with Multiple
  Protected Attributes`,abstract:`  Motivated by the recital (67) of the current corrigendum of the AI Act in the
European Union, we propose and present measures and mitigation strategies for
discrimination in tabular datasets. We specifically focus on datasets that
contain multiple protected attributes, such as nationality, age, and sex. This
makes measuring and mitigating bias more challenging, as many existing methods
are designed for a single protected attribute. This paper comes with a twofold
contribution: Firstly, new discrimination measures are introduced. These
measures are categorized in our framework along with existing ones, guiding
researchers and practitioners in choosing the right measure to assess the
fairness of the underlying dataset. Secondly, a novel application of an
existing bias mitigation method, FairDo, is presented. We show that this
strategy can mitigate any type of discrimination, including intersectional
discrimination, by transforming the dataset. By conducting experiments on
real-world datasets (Adult, Bank, Compas), we demonstrate that de-biasing
datasets with multiple protected attributes is achievable. Further, the
transformed fair datasets do not compromise any of the tested machine learning
models' performances significantly when trained on these datasets compared to
the original datasets. Discrimination was reduced by up to 83% in our
experimentation. For most experiments, the disparity between protected groups
was reduced by at least 7% and 27% on average. Generally, the findings show
that the mitigation strategy used is effective, and this study contributes to
the ongoing discussion on the implementation of the European Union's AI Act.
`,authors:"Manh Khoi Duong; Stefan Conrad",status:0,relevancy:.3177973673221842,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18843",date:"2024-05-29",title:"Data-driven Machinery Fault Detection: A Comprehensive Review",abstract:`  In this era of advanced manufacturing, it's now more crucial than ever to
diagnose machine faults as early as possible to guarantee their safe and
efficient operation. With the massive surge in industrial big data and
advancement in sensing and computational technologies, data-driven Machinery
Fault Diagnosis (MFD) solutions based on machine/deep learning approaches have
been used ubiquitously in manufacturing. Timely and accurately identifying
faulty machine signals is vital in industrial applications for which many
relevant solutions have been proposed and are reviewed in many articles.
Despite the availability of numerous solutions and reviews on MFD, existing
works often lack several aspects. Most of the available literature has limited
applicability in a wide range of manufacturing settings due to their
concentration on a particular type of equipment or method of analysis.
Additionally, discussions regarding the challenges associated with implementing
data-driven approaches, such as dealing with noisy data, selecting appropriate
features, and adapting models to accommodate new or unforeseen faults, are
often superficial or completely overlooked. Thus, this survey provides a
comprehensive review of the articles using different types of machine learning
approaches for the detection and diagnosis of various types of machinery
faults, highlights their strengths and limitations, provides a review of the
methods used for condition-based analyses, comprehensively discusses the
available machinery fault datasets, introduces future researchers to the
possible challenges they have to encounter while using these approaches for MFD
and recommends the probable solutions to mitigate those problems. The future
research prospects are also pointed out for a better understanding of the
field. We believe this article will help researchers and contribute to the
further development of the field.
`,authors:"Dhiraj Neupane; Mohamed Reda Bouadjenek; Richard Dazeley; Sunil Aryal",status:0,relevancy:.31612336870644375,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19166",date:"2024-05-29",title:`Transformers as Neural Operators for Solutions of Differential Equations
  with Finite Regularity`,abstract:`  Neural operator learning models have emerged as very effective surrogates in
data-driven methods for partial differential equations (PDEs) across different
applications from computational science and engineering. Such operator learning
models not only predict particular instances of a physical or biological system
in real-time but also forecast classes of solutions corresponding to a
distribution of initial and boundary conditions or forcing terms. % DeepONet is
the first neural operator model and has been tested extensively for a broad
class of solutions, including Riemann problems. Transformers have not been used
in that capacity, and specifically, they have not been tested for solutions of
PDEs with low regularity. %
  In this work, we first establish the theoretical groundwork that transformers
possess the universal approximation property as operator learning models.
  We then apply transformers to forecast solutions of diverse dynamical systems
with solutions of finite regularity for a plurality of initial conditions and
forcing terms. In particular, we consider three examples: the Izhikevich neuron
model, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and
the one-dimensional Euler equation Riemann problem. For the latter problem, we
also compare with variants of DeepONet, and we find that transformers
outperform DeepONet in accuracy but they are computationally more expensive.
`,authors:"Benjamin Shih; Ahmad Peyvan; Zhongqiang Zhang; George Em Karniadakis",status:0,relevancy:.31378908612081013,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19029",date:"2024-05-29",title:"Convex neural network synthesis for robustness in the 1-norm",abstract:`  With neural networks being used to control safety-critical systems, they
increasingly have to be both accurate (in the sense of matching inputs to
outputs) and robust. However, these two properties are often at odds with each
other and a trade-off has to be navigated. To address this issue, this paper
proposes a method to generate an approximation of a neural network which is
certifiably more robust. Crucially, the method is fully convex and posed as a
semi-definite programme. An application to robustifying model predictive
control is used to demonstrate the results. The aim of this work is to
introduce a method to navigate the neural network robustness/accuracy
trade-off.
`,authors:"Ross Drummond; Chris Guiver; Matthew C. Turner",status:0,relevancy:.3043396034245549,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18886",date:"2024-05-29",title:`Compressing Large Language Models using Low Rank and Low Precision
  Decomposition`,abstract:`  The prohibitive sizes of Large Language Models (LLMs) today make it difficult
to deploy them on memory-constrained edge devices. This work introduces $\\rm
CALDERA$ -- a new post-training LLM compression algorithm that harnesses the
inherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it
via a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} +
\\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank
factors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are
quantized. The model is compressed by substituting each layer with its
$\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot
performance of the compressed model is evaluated. Additionally, $\\mathbf{L}$
and $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently
enhancing the zero-shot performance. $\\rm CALDERA$ obtains this decomposition
by formulating it as an optimization problem
$\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} +
\\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where
$\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$
are constrained to be representable using low-precision formats. Theoretical
upper bounds on the approximation error of $\\rm CALDERA$ are established using
a rank-constrained regression framework, and the tradeoff between compression
ratio and model performance is studied by analyzing the impact of target rank
and quantization bit budget. Results illustrate that compressing LlaMa-$2$
$7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms
existing post-training LLM compression techniques in the regime of less than
$2.5$ bits per parameter. The implementation is available at:
\\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}.
`,authors:"Rajarshi Saha; Naomi Sagan; Varun Srivastava; Andrea J. Goldsmith; Mert Pilanci",status:0,relevancy:.2960778544824819,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19479",date:"2024-05-29",title:"Participation in the age of foundation models",abstract:`  Growing interest and investment in the capabilities of foundation models has
positioned such systems to impact a wide array of public services. Alongside
these opportunities is the risk that these systems reify existing power
imbalances and cause disproportionate harm to marginalized communities.
Participatory approaches hold promise to instead lend agency and
decision-making power to marginalized stakeholders. But existing approaches in
participatory AI/ML are typically deeply grounded in context - how do we apply
these approaches to foundation models, which are, by design, disconnected from
context? Our paper interrogates this question.
  First, we examine existing attempts at incorporating participation into
foundation models. We highlight the tension between participation and scale,
demonstrating that it is intractable for impacted communities to meaningfully
shape a foundation model that is intended to be universally applicable. In
response, we develop a blueprint for participatory foundation models that
identifies more local, application-oriented opportunities for meaningful
participation. In addition to the "foundation" layer, our framework proposes
the "subfloor'' layer, in which stakeholders develop shared technical
infrastructure, norms and governance for a grounded domain, and the "surface''
layer, in which affected communities shape the use of a foundation model for a
specific downstream task. The intermediate "subfloor'' layer scopes the range
of potential harms to consider, and affords communities more concrete avenues
for deliberation and intervention. At the same time, it avoids duplicative
effort by scaling input across relevant use cases. Through three case studies
in clinical care, financial services, and journalism, we illustrate how this
multi-layer model can create more meaningful opportunities for participation
than solely intervening at the foundation layer.
`,authors:"Harini Suresh; Emily Tseng; Meg Young; Mary L. Gray; Emma Pierson; Karen Levy",status:0,relevancy:.2838741916613018,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19212",date:"2024-05-29",title:`Partial Information Decomposition for Data Interpretability and Feature
  Selection`,abstract:`  In this paper, we introduce Partial Information Decomposition of Features
(PIDF), a new paradigm for simultaneous data interpretability and feature
selection. Contrary to traditional methods that assign a single importance
value, our approach is based on three metrics per feature: the mutual
information shared with the target variable, the feature's contribution to
synergistic information, and the amount of this information that is redundant.
In particular, we develop a novel procedure based on these three metrics, which
reveals not only how features are correlated with the target but also the
additional and overlapping information provided by considering them in
combination with other features. We extensively evaluate PIDF using both
synthetic and real-world data, demonstrating its potential applications and
effectiveness, by considering case studies from genetics and neuroscience.
`,authors:"Charles Westphal; Stephen Hailes; Mirco Musolesi",status:0,relevancy:.28308530850550995,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18894",date:"2024-05-29",title:`Few-Shot Testing: Estimating Uncertainty of Memristive Deep Neural
  Networks Using One Bayesian Test Vector`,abstract:`  The performance of deep learning algorithms such as neural networks (NNs) has
increased tremendously recently, and they can achieve state-of-the-art
performance in many domains. However, due to memory and computation resource
constraints, implementing NNs on edge devices is a challenging task. Therefore,
hardware accelerators such as computation-in-memory (CIM) with memristive
devices have been developed to accelerate the most common operations, i.e.,
matrix-vector multiplication. However, due to inherent device properties,
external environmental factors such as temperature, and an immature fabrication
process, memristors suffer from various non-idealities, including defects and
variations occurring during manufacturing and runtime. Consequently, there is a
lack of complete confidence in the predictions made by the model. To improve
confidence in NN predictions made by hardware accelerators in the presence of
device non-idealities, in this paper, we propose a Bayesian test vector
generation framework that can estimate the model uncertainty of NNs implemented
on memristor-based CIM hardware. Compared to the conventional point estimate
test vector generation method, our method is more generalizable across
different model dimensions and requires storing only one test Bayesian vector
in the hardware. Our method is evaluated on different model dimensions, tasks,
fault rates, and variation noise to show that it can consistently achieve
$100\\%$ coverage with only $0.024$ MB of memory overhead.
`,authors:"Soyed Tuhin Ahmed; Mehdi Tahoori",status:0,relevancy:.2822313482469797,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19471",date:"2024-05-29",title:"The Data Minimization Principle in Machine Learning",abstract:`  The principle of data minimization aims to reduce the amount of data
collected, processed or retained to minimize the potential for misuse,
unauthorized access, or data breaches. Rooted in privacy-by-design principles,
data minimization has been endorsed by various global data protection
regulations. However, its practical implementation remains a challenge due to
the lack of a rigorous formulation. This paper addresses this gap and
introduces an optimization framework for data minimization based on its legal
definitions. It then adapts several optimization algorithms to perform data
minimization and conducts a comprehensive evaluation in terms of their
compliance with minimization objectives as well as their impact on user
privacy. Our analysis underscores the mismatch between the privacy expectations
of data minimization and the actual privacy benefits, emphasizing the need for
approaches that account for multiple facets of real-world privacy risks.
`,authors:"Prakhar Ganesh; Cuong Tran; Reza Shokri; Ferdinando Fioretto",status:0,relevancy:.24154683442099745,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18999",date:"2024-05-29",title:`Continuously Optimizing Radar Placement with Model Predictive Path
  Integrals`,abstract:`  Continuously optimizing sensor placement is essential for precise target
localization in various military and civilian applications. While information
theory has shown promise in optimizing sensor placement, many studies
oversimplify sensor measurement models or neglect dynamic constraints of mobile
sensors. To address these challenges, we employ a range measurement model that
incorporates radar parameters and radar-target distance, coupled with Model
Predictive Path Integral (MPPI) control to manage complex environmental
obstacles and dynamic constraints. We compare the proposed approach against
stationary radars or simplified range measurement models based on the root mean
squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the
targets' state. Additionally, we visualize the evolving geometry of radars and
targets over time, highlighting areas of highest measurement information gain,
demonstrating the strengths of the approach. The proposed strategy outperforms
stationary radars and simplified range measurement models in target
localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction
in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl
(MC) trials across all time steps.
  Code will be made publicly available upon acceptance.
`,authors:"Michael Potter; Shuo Tang; Paul Ghanem; Milica Stojanovic; Pau Closas; Murat Akcakaya; Ben Wright; Marius Necsoiu; Deniz Erdogmus; Michael Everett; Tales Imbiriba",status:0,relevancy:.23836429077249166,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19184",date:"2024-05-29",title:"Promoting Two-sided Fairness in Dynamic Vehicle Routing Problem",abstract:`  Dynamic Vehicle Routing Problem (DVRP), is an extension of the classic
Vehicle Routing Problem (VRP), which is a fundamental problem in logistics and
transportation. Typically, DVRPs involve two stakeholders: service providers
that deliver services to customers and customers who raise requests from
different locations. Many real-world applications can be formulated as DVRP
such as ridesharing and non-compliance capture. Apart from original objectives
like optimising total utility or efficiency, DVRP should also consider fairness
for all parties. Unfairness can induce service providers and customers to give
up on the systems, leading to negative financial and social impacts. However,
most existing DVRP-related applications focus on improving fairness from a
single side, and there have been few works considering two-sided fairness and
utility optimisation concurrently. To this end, we propose a novel framework, a
Two-sided Fairness-aware Genetic Algorithm (named 2FairGA), which expands the
genetic algorithm from the original objective solely focusing on utility to
multi-objectives that incorporate two-sided fairness. Subsequently, the impact
of injecting two fairness definitions into the utility-focused model and the
correlation between any pair of the three objectives are explored. Extensive
experiments demonstrate the superiority of our proposed framework compared to
the state-of-the-art.
`,authors:"Yufan Kang; Rongsheng Zhang; Wei Shao; Flora D. Salim; Jeffrey Chan",status:0,relevancy:.22907416774040323,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18731",date:"2024-05-29",title:`VBIM-Net: Variational Born Iterative Network for Inverse Scattering
  Problems`,abstract:`  Recently, studies have shown the potential of integrating field-type
iterative methods with deep learning (DL) techniques in solving inverse
scattering problems (ISPs). In this article, we propose a novel Variational
Born Iterative Network, namely, VBIM-Net, to solve the full-wave ISPs with
significantly improved flexibility and inversion quality. The proposed VBIM-Net
emulates the alternating updates of the total electric field and the contrast
in the variational Born iterative method (VBIM) by multiple layers of
subnetworks. We embed the calculation of the contrast variation into each of
the subnetworks, converting the scattered field residual into an approximate
contrast variation and then enhancing it by a U-Net, thus avoiding the
requirement of matched measurement dimension and grid resolution as in existing
approaches. The total field and contrast of each layer's output is supervised
in the loss function of VBIM-Net, which guarantees the physical
interpretability of variables of the subnetworks. In addition, we design a
training scheme with extra noise to enhance the model's stability. Extensive
numerical results on synthetic and experimental data both verify the inversion
quality, generalization ability, and robustness of the proposed VBIM-Net. This
work may provide some new inspiration for the design of efficient field-type DL
schemes.
`,authors:"Ziqing Xing; Zhaoyang Zhang; Zirui Chen; Yusong Wang; Haoran Ma; Zhun Wei; Gang Bao",status:0,relevancy:.22405573259782163,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19012",date:"2024-05-29",title:"Implicit Neural Image Field for Biological Microscopy Image Compression",abstract:`  The rapid pace of innovation in biological microscopy imaging has led to
large images, putting pressure on data storage and impeding efficient sharing,
management, and visualization. This necessitates the development of efficient
compression solutions. Traditional CODEC methods struggle to adapt to the
diverse bioimaging data and often suffer from sub-optimal compression. In this
study, we propose an adaptive compression workflow based on Implicit Neural
Representation (INR). This approach permits application-specific compression
objectives, capable of compressing images of any shape and arbitrary pixel-wise
decompression. We demonstrated on a wide range of microscopy images from real
applications that our workflow not only achieved high, controllable compression
ratios (e.g., 512x) but also preserved detailed information critical for
downstream analysis.
`,authors:"Gaole Dai; Cheng-Ching Tseng; Qingpo Wuwu; Rongyu Zhang; Shaokang Wang; Ming Lu; Tiejun Huang; Yu Zhou; Ali Ata Tuz; Matthias Gunzer; Jianxu Chen; Shanghang Zhang",status:0,relevancy:.22247836646075891,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19413",date:"2024-05-29",title:`VisTA-SR: Improving the Accuracy and Resolution of Low-Cost Thermal
  Imaging Cameras for Agriculture`,abstract:`  Thermal cameras are an important tool for agricultural research because they
allow for non-invasive measurement of plant temperature, which relates to
important photochemical, hydraulic, and agronomic traits. Utilizing low-cost
thermal cameras can lower the barrier to introducing thermal imaging in
agricultural research and production. This paper presents an approach to
improve the temperature accuracy and image quality of low-cost thermal imaging
cameras for agricultural applications. Leveraging advancements in computer
vision techniques, particularly deep learning networks, we propose a method,
called $\\textbf{VisTA-SR}$ ($\\textbf{Vis}$ual \\& $\\textbf{T}$hermal
$\\textbf{A}$lignment and $\\textbf{S}$uper-$\\textbf{R}$esolution Enhancement)
that combines RGB and thermal images to enhance the capabilities of
low-resolution thermal cameras. The research includes calibration and
validation of temperature measurements, acquisition of paired image datasets,
and the development of a deep learning network tailored for agricultural
thermal imaging. Our study addresses the challenges of image enhancement in the
agricultural domain and explores the potential of low-cost thermal cameras to
replace high-resolution industrial cameras. Experimental results demonstrate
the effectiveness of our approach in enhancing temperature accuracy and image
sharpness, paving the way for more accessible and efficient thermal imaging
solutions in agriculture.
`,authors:"Heesup Yun; Sassoum Lo; Christine H. Diepenbrock; Brian N. Bailey; J. Mason Earles",status:0,relevancy:.2125032929335794,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.18889",date:"2024-05-29",title:"On Perception of Prevalence of Cheating and Usage of Generative AI",abstract:`  This report investigates the perceptions of teaching staff on the prevalence
of student cheating and the impact of Generative AI on academic integrity. Data
was collected via an anonymous survey of teachers at the Department of
Information Technology at Uppsala University and analyzed alongside
institutional statistics on cheating investigations from 2004 to 2023. The
results indicate that while teachers generally do not view cheating as highly
prevalent, there is a strong belief that its incidence is increasing,
potentially due to the accessibility of Generative AI. Most teachers do not
equate AI usage with cheating but acknowledge its widespread use among
students. Furthermore, teachers' perceptions align with objective data on
cheating trends, highlighting their awareness of the evolving landscape of
academic dishonesty.
`,authors:"Roman Denkin",status:0,relevancy:.2014656036339435,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}},{id:"2405.19085",date:"2024-05-29",title:"Patch-enhanced Mask Encoder Prompt Image Generation",abstract:`  Artificial Intelligence Generated Content(AIGC), known for its superior
visual results, represents a promising mitigation method for high-cost
advertising applications. Numerous approaches have been developed to manipulate
generated content under different conditions. However, a crucial limitation
lies in the accurate description of products in advertising applications.
Applying previous methods directly may lead to considerable distortion and
deformation of advertised products, primarily due to oversimplified content
control conditions. Hence, in this work, we propose a patch-enhanced mask
encoder approach to ensure accurate product descriptions while preserving
diverse backgrounds. Our approach consists of three components Patch Flexible
Visibility, Mask Encoder Prompt Adapter and an image Foundation Model. Patch
Flexible Visibility is used for generating a more reasonable background image.
Mask Encoder Prompt Adapter enables region-controlled fusion. We also conduct
an analysis of the structure and operational mechanisms of the Generation
Module. Experimental results show our method can achieve the highest visual
results and FID scores compared with other methods.
`,authors:"Shusong Xu; Peiye Liu",status:0,relevancy:.19786574989094752,isStarred:!1,keywords:null,createdAt:"2024-05-31T05:21:04.773Z",updatedAt:"2024-05-31T05:21:04.773Z",DatesTable:{value:"2024-05-29",status:"complete",count:133,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T05:21:04.775Z"}}]},{date:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:51:45.389 +00:00"},papers:[{id:"2405.17950",date:"2024-05-28",title:"Self-Guiding Exploration for Combinatorial Problems",abstract:`  Large Language Models (LLMs) have become pivotal in addressing reasoning
tasks across diverse domains, including arithmetic, commonsense, and symbolic
reasoning. They utilize prompting techniques such as Exploration-of-Thought,
Decomposition, and Refinement to effectively navigate and solve intricate
tasks. Despite these advancements, the application of LLMs to Combinatorial
Problems (CPs), known for their NP-hardness and critical roles in logistics and
resource management remains underexplored. To address this gap, we introduce a
novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance
the performance of solving CPs. SGE operates autonomously, generating multiple
thought trajectories for each CP task. It then breaks these trajectories down
into actionable subtasks, executes them sequentially, and refines the results
to ensure optimal outcomes. We present our research as the first to apply LLMs
to a broad range of CPs and demonstrate that SGE outperforms existing prompting
strategies by over 27.84% in CP optimization performance. Additionally, SGE
achieves a 2.46% higher accuracy over the best existing results in other
reasoning tasks (arithmetic, commonsense, and symbolic).
`,authors:"Zangir Iklassov; Yali Du; Farkhad Akimov; Martin Takac",status:0,relevancy:.6772564044041381,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18208",date:"2024-05-28",title:`A Human-Like Reasoning Framework for Multi-Phases Planning Task with
  Large Language Models`,abstract:`  Recent studies have highlighted their proficiency in some simple tasks like
writing and coding through various reasoning strategies. However, LLM agents
still struggle with tasks that require comprehensive planning, a process that
challenges current models and remains a critical research issue. In this study,
we concentrate on travel planning, a Multi-Phases planning problem, that
involves multiple interconnected stages, such as outlining, information
gathering, and planning, often characterized by the need to manage various
constraints and uncertainties. Existing reasoning approaches have struggled to
effectively address this complex task. Our research aims to address this
challenge by developing a human-like planning framework for LLM agents, i.e.,
guiding the LLM agent to simulate various steps that humans take when solving
Multi-Phases problems. Specifically, we implement several strategies to enable
LLM agents to generate a coherent outline for each travel query, mirroring
human planning patterns. Additionally, we integrate Strategy Block and
Knowledge Block into our framework: Strategy Block facilitates information
collection, while Knowledge Block provides essential information for detailed
planning. Through our extensive experiments, we demonstrate that our framework
significantly improves the planning capabilities of LLM agents, enabling them
to tackle the travel planning task with improved efficiency and effectiveness.
Our experimental results showcase the exceptional performance of the proposed
framework; when combined with GPT-4-Turbo, it attains $10\\times$ the
performance gains in comparison to the baseline framework deployed on
GPT-4-Turbo.
`,authors:"Chengxing Xie; Difan Zou",status:0,relevancy:.6687598875397698,isStarred:!0,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:52:20.960Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17974",date:"2024-05-28",title:`Recent Trends in Personalized Dialogue Generation: A Review of Datasets,
  Methodologies, and Evaluations`,abstract:`  Enhancing user engagement through personalization in conversational agents
has gained significance, especially with the advent of large language models
that generate fluent responses. Personalized dialogue generation, however, is
multifaceted and varies in its definition -- ranging from instilling a persona
in the agent to capturing users' explicit and implicit cues. This paper seeks
to systemically survey the recent landscape of personalized dialogue
generation, including the datasets employed, methodologies developed, and
evaluation metrics applied. Covering 22 datasets, we highlight benchmark
datasets and newer ones enriched with additional features. We further analyze
17 seminal works from top conferences between 2021-2023 and identify five
distinct types of problems. We also shed light on recent progress by LLMs in
personalized dialogue generation. Our evaluation section offers a comprehensive
summary of assessment facets and metrics utilized in these works. In
conclusion, we discuss prevailing challenges and envision prospect directions
for future research in personalized dialogue generation.
`,authors:"Yi-Pei Chen; Noriki Nishida; Hideki Nakayama; Yuji Matsumoto",status:0,relevancy:.658808673281338,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:52:09.160Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18581",date:"2024-05-28",title:`Unleashing the Potential of Text-attributed Graphs: Automatic Relation
  Decomposition via Large Language Models`,abstract:`  Recent advancements in text-attributed graphs (TAGs) have significantly
improved the quality of node features by using the textual modeling
capabilities of language models. Despite this success, utilizing text
attributes to enhance the predefined graph structure remains largely
unexplored. Our extensive analysis reveals that conventional edges on TAGs,
treated as a single relation (e.g., hyperlinks) in previous literature,
actually encompass mixed semantics (e.g., "advised by" and "participates in").
This simplification hinders the representation learning process of Graph Neural
Networks (GNNs) on downstream tasks, even when integrated with advanced node
features. In contrast, we discover that decomposing these edges into distinct
semantic relations significantly enhances the performance of GNNs. Despite
this, manually identifying and labeling of edges to corresponding semantic
relations is labor-intensive, often requiring domain expertise. To this end, we
introduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel
framework that leverages the capability of Large Language Models (LLMs) to
decompose the graph structure by analyzing raw text attributes - in a fully
automated manner. RoSE operates in two stages: (1) identifying meaningful
relations using an LLM-based generator and discriminator, and (2) categorizing
each edge into corresponding relations by analyzing textual contents associated
with connected nodes via an LLM-based decomposer. Extensive experiments
demonstrate that our model-agnostic framework significantly enhances node
classification performance across various datasets, with improvements of up to
16% on the Wisconsin dataset.
`,authors:"Hyunjin Seo; Taewon Kim; June Yong Yang; Eunho Yang",status:0,relevancy:.625058710296175,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18369",date:"2024-05-28",title:"PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework",abstract:`  Large language models (LLMs) have revolutionized AI across diverse domains,
showcasing remarkable capabilities. Central to their success is the concept of
prompting, which guides model output generation. However, manual prompt
engineering is labor-intensive and domain-specific, necessitating automated
solutions. This paper introduces PromptWizard, a novel framework leveraging
LLMs to iteratively synthesize and refine prompts tailored to specific tasks.
Unlike existing approaches, PromptWizard optimizes both prompt instructions and
in-context examples, maximizing model performance. The framework iteratively
refines prompts by mutating instructions and incorporating negative examples to
deepen understanding and ensure diversity. It further enhances both
instructions and examples with the aid of a critic, synthesizing new
instructions and examples enriched with detailed reasoning steps for optimal
performance. PromptWizard offers several key features and capabilities,
including computational efficiency compared to state-of-the-art approaches,
adaptability to scenarios with varying amounts of training data, and
effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8
datasets demonstrates PromptWizard's superiority over existing prompt
strategies, showcasing its efficacy and scalability in prompt optimization.
`,authors:"Eshaan Agarwal; Vivek Dani; Tanuja Ganu; Akshay Nambi",status:0,relevancy:.6224390267317457,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18110",date:"2024-05-28",title:`Individual Contributions as Intrinsic Exploration Scaffolds for
  Multi-agent Reinforcement Learning`,abstract:`  In multi-agent reinforcement learning (MARL), effective exploration is
critical, especially in sparse reward environments. Although introducing global
intrinsic rewards can foster exploration in such settings, it often complicates
credit assignment among agents. To address this difficulty, we propose
Individual Contributions as intrinsic Exploration Scaffolds (ICES), a novel
approach to motivate exploration by assessing each agent's contribution from a
global view. In particular, ICES constructs exploration scaffolds with Bayesian
surprise, leveraging global transition information during centralized training.
These scaffolds, used only in training, help to guide individual agents towards
actions that significantly impact the global latent state transitions.
Additionally, ICES separates exploration policies from exploitation policies,
enabling the former to utilize privileged global information during training.
Extensive experiments on cooperative benchmark tasks with sparse rewards,
including Google Research Football (GRF) and StarCraft Multi-agent Challenge
(SMAC), demonstrate that ICES exhibits superior exploration capabilities
compared with baselines. The code is publicly available at
https://github.com/LXXXXR/ICES.
`,authors:"Xinran Li; Zifan Liu; Shibo Chen; Jun Zhang",status:0,relevancy:.610816543969512,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18118",date:"2024-05-28",title:`An approach to improve agent learning via guaranteeing goal reaching in
  all episodes`,abstract:`  Reinforcement learning is commonly concerned with problems of maximizing
accumulated rewards in Markov decision processes. Oftentimes, a certain goal
state or a subset of the state space attain maximal reward. In such a case, the
environment may be considered solved when the goal is reached. Whereas numerous
techniques, learning or non-learning based, exist for solving environments,
doing so optimally is the biggest challenge. Say, one may choose a reward rate
which penalizes the action effort. Reinforcement learning is currently among
the most actively developed frameworks for solving environments optimally by
virtue of maximizing accumulated reward, in other words, returns. Yet, tuning
agents is a notoriously hard task as reported in a series of works. Our aim
here is to help the agent learn a near-optimal policy efficiently while
ensuring a goal reaching property of some basis policy that merely solves the
environment. We suggest an algorithm, which is fairly flexible, and can be used
to augment practically any agent as long as it comprises of a critic. A formal
proof of a goal reaching property is provided. Simulation experiments on six
problems under five agents, including the benchmarked one, provided an
empirical evidence that the learning can indeed be boosted while ensuring goal
reaching property.
`,authors:"Pavel Osinenko; Grigory Yaremenko; Georgiy Malaniya; Anton Bolychev",status:0,relevancy:.599505902476585,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18359",date:"2024-05-28",title:`Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual
  Performance in LLMs`,abstract:`  Large language models (LLMs) are at the forefront of transforming numerous
domains globally. However, their inclusivity and effectiveness remain limited
for non-Latin scripts and low-resource languages. This paper tackles the
imperative challenge of enhancing the multilingual performance of LLMs without
extensive training or fine-tuning. Through systematic investigation and
evaluation of diverse languages using popular question-answering (QA) datasets,
we present novel techniques that unlock the true potential of LLMs in a
polyglot landscape. Our approach encompasses three key strategies that yield
significant improvements in multilingual proficiency. First, by meticulously
optimizing prompts tailored for polyglot LLMs, we unlock their latent
capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes LLM Retrieval
Augmented Generation (RAG) with multilingual embeddings and achieves improved
multilingual task performance. Finally, we introduce a novel learning approach
that dynamically selects the optimal prompt strategy, LLM model, and embedding
model per query at run-time. This dynamic adaptation maximizes the efficacy of
LLMs across languages, outperforming best static and random strategies.
Additionally, our approach adapts configurations in both offline and online
settings, and can seamlessly adapt to new languages and datasets, leading to
substantial advancements in multilingual understanding and generation across
diverse languages.
`,authors:"Somnath Kumar; Vaibhav Balloli; Mercy Ranjit; Kabir Ahuja; Tanuja Ganu; Sunayana Sitaram; Kalika Bali; Akshay Nambi",status:0,relevancy:.591437679750576,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17822",date:"2024-05-28",title:`Conv-CoA: Improving Open-domain Question Answering in Large Language
  Models via Conversational Chain-of-Action`,abstract:`  We present a Conversational Chain-of-Action (Conv-CoA) framework for
Open-domain Conversational Question Answering (OCQA). Compared with literature,
Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is
inconsistent with real-time or domain facts, (ii) weak reasoning performance in
conversational scenarios, and (iii) unsatisfying performance in conversational
information retrieval. Our key contribution is a dynamic reasoning-retrieval
mechanism that extracts the intent of the question and decomposes it into a
reasoning chain to be solved via systematic prompting, pre-designed actions,
updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based
retriever. Methodologically, we propose a resource-efficiency Hopfield
retriever to enhance the efficiency and accuracy of conversational information
retrieval within our actions. Additionally, we propose a
conversational-multi-reference faith score (Conv-MRFS) to verify and resolve
conflicts between retrieved knowledge and answers in conversations.
Empirically, we conduct comparisons between our framework and 23
state-of-the-art methods across five different research directions and two
public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms
other methods in both the accuracy and efficiency dimensions.
`,authors:"Zhenyu Pan; Haozheng Luo; Manling Li; Han Liu",status:0,relevancy:.5879060542498787,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18414",date:"2024-05-28",title:"Don't Forget to Connect! Improving RAG with Graph-based Reranking",abstract:`  Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.
`,authors:"Jialin Dong; Bahare Fatemi; Bryan Perozzi; Lin F. Yang; Anton Tsitsulin",status:0,relevancy:.5847993182247199,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17888",date:"2024-05-28",title:`Getting More Juice Out of the SFT Data: Reward Learning from Human
  Demonstration Improves SFT for LLM Alignment`,abstract:`  Aligning human preference and value is an important requirement for
contemporary foundation models. State-of-the-art techniques such as
Reinforcement Learning from Human Feedback (RLHF) often consist of two stages:
1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from
human demonstration data; 2) Preference learning, where preference data is used
to learn a reward model, which is in turn used by a reinforcement learning (RL)
step to fine-tune the model. Such reward model serves as a proxy to human
preference, and it is critical to guide the RL step towards improving the model
quality. In this work, we argue that the SFT stage significantly benefits from
learning a reward model as well. Instead of using the human demonstration data
directly via supervised learning, we propose to leverage an Inverse
Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an
reward model, while learning the policy model. This approach leads to new SFT
algorithms that are not only efficient to implement, but also promote the
ability to distinguish between the preferred and non-preferred continuations.
Moreover, we identify a connection between the proposed IRL based approach, and
certain self-play approach proposed recently, and showed that self-play is a
special case of modeling a reward-learning agent. Theoretically, we show that
the proposed algorithms converge to the stationary solutions of the IRL
problem. Empirically, we align 1B and 7B models using proposed methods and
evaluate them on a reward benchmark model and the HuggingFace Open LLM
Leaderboard. The proposed methods show significant performance improvement over
existing SFT approaches. Our results indicate that it is beneficial to
explicitly or implicitly leverage reward learning throughout the entire
alignment process.
`,authors:"Jiaxiang Li; Siliang Zeng; Hoi-To Wai; Chenliang Li; Alfredo Garcia; Mingyi Hong",status:0,relevancy:.5838876694831042,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18289",date:"2024-05-28",title:"Highway Reinforcement Learning",abstract:`  Learning from multi-step off-policy data collected by a set of policies is a
core problem of reinforcement learning (RL). Approaches based on importance
sampling (IS) often suffer from large variances due to products of IS ratios.
Typical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time
steps along the trajectory of actions (where $n$ is called the lookahead depth)
and utilize off-policy data directly without any additional adjustment. They
work well for proper choices of $n$. We show, however, that such IS-free
methods underestimate the optimal value function (VF), especially for large
$n$, restricting their capacity to efficiently utilize information from distant
future time steps. To overcome this problem, we introduce a novel, IS-free,
multi-step off-policy method that avoids the underestimation issue and
converges to the optimal VF. At its core lies a simple but non-trivial
\\emph{highway gate}, which controls the information flow from the distant
future by comparing it to a threshold. The highway gate guarantees convergence
to the optimal VF for arbitrary $n$ and arbitrary behavioral policies. It gives
rise to a novel family of off-policy RL algorithms that safely learn even when
$n$ is very large, facilitating rapid credit assignment from the far future to
the past. On tasks with greatly delayed rewards, including video games where
the reward is given only at the end of the game, our new methods outperform
many existing multi-step off-policy algorithms.
`,authors:"Yuhui Wang; Miroslav Strupl; Francesco Faccio; Qingyuan Wu; Haozhe Liu; Michał Grudzień; Xiaoyang Tan; Jürgen Schmidhuber",status:0,relevancy:.5819882330995535,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18344",date:"2024-05-28",title:"The Battle of LLMs: A Comparative Study in Conversational QA Tasks",abstract:`  Large language models have gained considerable interest for their impressive
performance on various tasks. Within this domain, ChatGPT and GPT-4, developed
by OpenAI, and the Gemini, developed by Google, have emerged as particularly
popular among early adopters. Additionally, Mixtral by Mistral AI and Claude by
Anthropic are newly released, further expanding the landscape of advanced
language models. These models are viewed as disruptive technologies with
applications spanning customer service, education, healthcare, and finance.
More recently, Mistral has entered the scene, captivating users with its unique
ability to generate creative content. Understanding the perspectives of these
users is crucial, as they can offer valuable insights into the potential
strengths, weaknesses, and overall success or failure of these technologies in
various domains. This research delves into the responses generated by ChatGPT,
GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.
Evaluation scores were meticulously computed and subsequently compared to
ascertain the overall performance of these models. Our study pinpointed
instances where these models provided inaccurate answers to questions, offering
insights into potential areas where they might be susceptible to errors. In
essence, this research provides a comprehensive comparison and evaluation of
these state of-the-art language models, shedding light on their capabilities
while also highlighting potential areas for improvement
`,authors:"Aryan Rangapur; Aman Rangapur",status:0,relevancy:.5813454744354037,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17935",date:"2024-05-28",title:"Tool Learning with Large Language Models: A Survey",abstract:`  Recently, tool learning with large language models (LLMs) has emerged as a
promising paradigm for augmenting the capabilities of LLMs to tackle highly
complex problems. Despite growing attention and rapid advancements in this
field, the existing literature remains fragmented and lacks systematic
organization, posing barriers to entry for newcomers. This gap motivates us to
conduct a comprehensive survey of existing works on tool learning with LLMs. In
this survey, we focus on reviewing existing literature from the two primary
aspects (1) why tool learning is beneficial and (2) how tool learning is
implemented, enabling a comprehensive understanding of tool learning with LLMs.
We first explore the "why" by reviewing both the benefits of tool integration
and the inherent benefits of the tool learning paradigm from six specific
aspects. In terms of "how", we systematically review the literature according
to a taxonomy of four key stages in the tool learning workflow: task planning,
tool selection, tool calling, and response generation. Additionally, we provide
a detailed summary of existing benchmarks and evaluation methods, categorizing
them according to their relevance to different stages. Finally, we discuss
current challenges and outline potential future directions, aiming to inspire
both researchers and industrial developers to further explore this emerging and
promising area. We also maintain a GitHub repository to continually keep track
of the relevant papers and resources in this rising area at
\\url{https://github.com/quchangle1/LLM-Tool-Survey}.
`,authors:"Changle Qu; Sunhao Dai; Xiaochi Wei; Hengyi Cai; Shuaiqiang Wang; Dawei Yin; Jun Xu; Ji-Rong Wen",status:0,relevancy:.5772621332788771,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18358",date:"2024-05-28",title:`MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex
  Visual Reasoning`,abstract:`  Recent advancements in Multi-modal Large Language Models (MLLMs) have
significantly improved their performance in tasks combining vision and
language. However, challenges persist in detailed multi-modal understanding,
comprehension of complex tasks, and reasoning over multi-modal information.
This paper introduces MMCTAgent, a novel multi-modal critical thinking agent
framework designed to address the inherent limitations of current MLLMs in
complex visual reasoning tasks. Inspired by human cognitive processes and
critical thinking, MMCTAgent iteratively analyzes multi-modal information,
decomposes queries, plans strategies, and dynamically evolves its reasoning.
Additionally, MMCTAgent incorporates critical thinking elements such as
verification of final answers and self-reflection through a novel approach that
defines a vision-based critic and identifies task-specific evaluation criteria,
thereby enhancing its decision-making abilities. Through rigorous evaluations
across various image and video understanding benchmarks, we demonstrate that
MMCTAgent (with and without the critic) outperforms both foundational MLLMs and
other tool-augmented pipelines.
`,authors:"Somnath Kumar; Yash Gadhia; Tanuja Ganu; Akshay Nambi",status:0,relevancy:.5743416288656753,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18520",date:"2024-05-28",title:`Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical
  Behaviors in Deep Off-Policy RL`,abstract:`  Off-policy reinforcement learning (RL) has achieved notable success in
tackling many complex real-world tasks, by leveraging previously collected data
for policy learning. However, most existing off-policy RL algorithms fail to
maximally exploit the information in the replay buffer, limiting sample
efficiency and policy performance. In this work, we discover that concurrently
training an offline RL policy based on the shared online replay buffer can
sometimes outperform the original online learning policy, though the occurrence
of such performance gains remains uncertain. This motivates a new possibility
of harnessing the emergent outperforming offline optimal policy to improve
online policy learning. Based on this insight, we present Offline-Boosted
Actor-Critic (OBAC), a model-free online RL framework that elegantly identifies
the outperforming offline policy through value comparison, and uses it as an
adaptive constraint to guarantee stronger policy learning performance. Our
experiments demonstrate that OBAC outperforms other popular model-free RL
baselines and rivals advanced model-based RL methods in terms of sample
efficiency and asymptotic performance across 53 tasks spanning 6 task suites.
`,authors:"Yu Luo; Tianying Ji; Fuchun Sun; Jianwei Zhang; Huazhe Xu; Xianyuan Zhan",status:0,relevancy:.5705726677815043,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18123",date:"2024-05-28",title:"PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning",abstract:`  Modern Tabletop Games present various interesting challenges for Multi-agent
Reinforcement Learning. In this paper, we introduce PyTAG, a new framework that
supports interacting with a large collection of games implemented in the
Tabletop Games framework. In this work we highlight the challenges tabletop
games provide, from a game-playing agent perspective, along with the
opportunities they provide for future research. Additionally, we highlight the
technical challenges that involve training Reinforcement Learning agents on
these games. To explore the Multi-agent setting provided by PyTAG we train the
popular Proximal Policy Optimisation Reinforcement Learning algorithm using
self-play on a subset of games and evaluate the trained policies against some
simple agents and Monte-Carlo Tree Search implemented in the Tabletop Games
framework.
`,authors:"Martin Balla; George E. M. Long; James Goodman; Raluca D. Gaina; Diego Perez-Liebana",status:0,relevancy:.5615817257458973,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18380",date:"2024-05-28",title:`OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for
  Memory-Efficient LLM Fine-tuning`,abstract:`  The rapid advancements in Large Language Models (LLMs) have revolutionized
various natural language processing tasks. However, the substantial size of
LLMs presents significant challenges in training or fine-tuning. While
parameter-efficient approaches such as low-rank adaptation (LoRA) have gained
popularity, they often compromise performance compared to full-rank
fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled
Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach,
inspired by the layerwise outlier distribution of LLMs, which dynamically
samples pre-trained layers to fine-tune instead of adding additional adaptors.
We first interpret the outlier phenomenon through the lens of Heavy-Tailed
Self-Regularization theory (HT-SR), discovering that layers with more outliers
tend to be more heavy-tailed and consequently better trained. Inspired by this
finding, OwLore strategically assigns higher sampling probabilities to layers
with more outliers to better leverage the knowledge stored in pre-trained LLMs.
To further mitigate the memory demands of fine-tuning, we integrate gradient
low-rank projection into our approach, which facilitates each layer to be
efficiently trained in a low-rank manner. By incorporating the efficient
characteristics of low-rank and optimal layerwise sampling, OwLore
significantly improves the memory-performance trade-off in LLM pruning. Our
extensive experiments across various architectures, including LLaMa2, LLaMa3,
and Mistral, demonstrate that OwLore consistently outperforms baseline
approaches, including full fine-tuning. Specifically, it achieves up to a 1.1%
average accuracy gain on the Commonsense Reasoning benchmark, a 3.0%
improvement on MMLU, and a notable 10% boost on MT-Bench, while being more
memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of
memory.
`,authors:"Pengxiang Li; Lu Yin; Xiaowei Gao; Shiwei Liu",status:0,relevancy:.5533503896343944,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18248",date:"2024-05-28",title:"Extreme Value Monte Carlo Tree Search",abstract:`  Despite being successful in board games and reinforcement learning (RL), UCT,
a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB),
has had limited success in domain-independent planning until recently. Previous
work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate
for estimating the distance-to-go which are potentially unbounded in
$\\mathbb{R}$, such as heuristic functions used in classical planning, then
proposed combining MCTS with MABs designed for Gaussian reward distributions
and successfully improved the performance. In this paper, we further sharpen
our understanding of ideal bandits for planning tasks. Existing work has two
issues: First, while Gaussian MABs no longer over-specify the distances as
$h\\in [0,1]$, they under-specify them as $h\\in [-\\infty,\\infty]$ while they are
non-negative and can be further bounded in some cases. Second, there is no
theoretical justifications for Full-Bellman backup (Schulte & Keller, 2014)
that backpropagates minimum/maximum of samples. We identified \\emph{extreme
value} statistics as a theoretical framework that resolves both issues at once
and propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for
classical planning. We formally prove their regret bounds and empirically
demonstrate their performance in classical planning.
`,authors:"Masataro Asai; Stephen Wissow",status:0,relevancy:.5518046770472743,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18028",date:"2024-05-28",title:`Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language
  Models with Hints`,abstract:`  The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language
Models (LLMs) to identify and correct medical errors in clinical notes. In this
study, we evaluate the capability of general LLMs, specifically GPT-3.5 and
GPT-4, to identify and correct medical errors with multiple prompting
strategies. Recognising the limitation of LLMs in generating accurate
corrections only via prompting strategies, we propose incorporating error-span
predictions from a smaller, fine-tuned model in two ways: 1) by presenting it
as a hint in the prompt and 2) by framing it as multiple-choice questions from
which the LLM can choose the best correction. We found that our proposed
prompting strategies significantly improve the LLM's ability to generate
corrections. Our best-performing solution with 8-shot + CoT + hints ranked
sixth in the shared task leaderboard. Additionally, our comprehensive analyses
show the impact of the location of the error sentence, the prompted role, and
the position of the multiple-choice option on the accuracy of the LLM. This
prompts further questions about the readiness of LLM to be implemented in
real-world clinical settings.
`,authors:"Aryo Pradipta Gema; Chaeeun Lee; Pasquale Minervini; Luke Daines; T. Ian Simpson; Beatrice Alex",status:0,relevancy:.5509592100176375,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18649",date:"2024-05-28",title:"Training LLMs to Better Self-Debug and Explain Code",abstract:`  In the domain of code generation, self-debugging is crucial. It allows LLMs
to refine their generated code based on execution feedback. This is
particularly important because generating correct solutions in one attempt
proves challenging for complex tasks. Prior works on self-debugging mostly
focus on prompting methods by providing LLMs with few-shot examples, which work
poorly on small open-sourced LLMs. In this work, we propose a training
framework that significantly improves self-debugging capability of LLMs.
Intuitively, we observe that a chain of explanations on the wrong code followed
by code refinement helps LLMs better analyze the wrong code and do refinement.
We thus propose an automated pipeline to collect a high-quality dataset for
code explanation and refinement by generating a number of explanations and
refinement trajectories and filtering via execution verification. We perform
supervised fine-tuning (SFT) and further reinforcement learning (RL) on both
success and failure trajectories with a novel reward design considering code
explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and
pass@10 by 9.30% over four benchmarks. RL training brings additional up to
3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs
show iterative refinement ability, and can keep refining code continuously.
Lastly, our human evaluation shows that the LLMs trained with our framework
generate more useful code explanations and help developers better understand
bugs in source code.
`,authors:"Nan Jiang; Xiaopeng Li; Shiqi Wang; Qiang Zhou; Soneya Binta Hossain; Baishakhi Ray; Varun Kumar; Xiaofei Ma; Anoop Deoras",status:0,relevancy:.5490521632445885,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18650",date:"2024-05-28",title:"Approximating Human Models During Argumentation-based Dialogues",abstract:`  Explainable AI Planning (XAIP) aims to develop AI agents that can effectively
explain their decisions and actions to human users, fostering trust and
facilitating human-AI collaboration. A key challenge in XAIP is model
reconciliation, which seeks to align the mental models of AI agents and humans.
While existing approaches often assume a known and deterministic human model,
this simplification may not capture the complexities and uncertainties of
real-world interactions. In this paper, we propose a novel framework that
enables AI agents to learn and update a probabilistic human model through
argumentation-based dialogues. Our approach incorporates trust-based and
certainty-based update mechanisms, allowing the agent to refine its
understanding of the human's mental state based on the human's expressed trust
in the agent's arguments and certainty in their own arguments. We employ a
probability weighting function inspired by prospect theory to capture the
relationship between trust and perceived probability, and use a Bayesian
approach to update the agent's probability distribution over possible human
models. We conduct a human-subject study to empirically evaluate the
effectiveness of our approach in an argumentation scenario, demonstrating its
ability to capture the dynamics of human belief formation and adaptation.
`,authors:"Yinxu Tang; Stylianos Loukas Vasileiou; William Yeoh",status:0,relevancy:.5392468235525845,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18512",date:"2024-05-28",title:"Understanding Transformer Reasoning Capabilities via Graph Algorithms",abstract:`  Which transformer scaling regimes are able to perfectly solve different
classes of algorithmic problems? While tremendous empirical advances have been
attained by transformer-based neural networks, a theoretical understanding of
their algorithmic reasoning capabilities in realistic parameter regimes is
lacking. We investigate this question in terms of the network's depth, width,
and number of extra tokens for algorithm execution. Our novel representational
hierarchy separates 9 algorithmic reasoning problems into classes solvable by
transformers in different realistic parameter scaling regimes. We prove that
logarithmic depth is necessary and sufficient for tasks like graph
connectivity, while single-layer transformers with small embedding dimensions
can solve contextual retrieval tasks. We also support our theoretical analysis
with ample empirical evidence using the GraphQA benchmark. These results show
that transformers excel at many graph reasoning tasks, even outperforming
specialized graph neural networks.
`,authors:"Clayton Sanford; Bahare Fatemi; Ethan Hall; Anton Tsitsulin; Mehran Kazemi; Jonathan Halcrow; Bryan Perozzi; Vahab Mirrokni",status:0,relevancy:.5381364633905632,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18073",date:"2024-05-28",title:"Towards Dialogues for Joint Human-AI Reasoning and Value Alignment",abstract:`  We argue that enabling human-AI dialogue, purposed to support joint reasoning
(i.e., 'inquiry'), is important for ensuring that AI decision making is aligned
with human values and preferences. In particular, we point to logic-based
models of argumentation and dialogue, and suggest that the traditional focus on
persuasion dialogues be replaced by a focus on inquiry dialogues, and the
distinct challenges that joint inquiry raises. Given recent dramatic advances
in the performance of large language models (LLMs), and the anticipated
increase in their use for decision making, we provide a roadmap for research
into inquiry dialogues for supporting joint human-LLM reasoning tasks that are
ethically salient, and that thereby require that decisions are value aligned.
`,authors:"Elfia Bezou-Vrakatseli; Oana Cocarascu; Sanjay Modgil",status:0,relevancy:.537977847914832,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18092",date:"2024-05-28",title:`LLM experiments with simulation: Large Language Model Multi-Agent System
  for Process Simulation Parametrization in Digital Twins`,abstract:`  This paper presents a novel design of a multi-agent system framework that
applies a large language model (LLM) to automate the parametrization of process
simulations in digital twins. We propose a multi-agent framework that includes
four types of agents: observation, reasoning, decision and summarization. By
enabling dynamic interaction between LLM agents and simulation model, the
developed system can automatically explore the parametrization of the
simulation and use heuristic reasoning to determine a set of parameters to
control the simulation to achieve an objective. The proposed approach enhances
the simulation model by infusing it with heuristics from LLM and enables
autonomous search for feasible parametrization to solve a user task.
Furthermore, the system has the potential to increase user-friendliness and
reduce the cognitive load on human users by assisting in complex
decision-making processes. The effectiveness and functionality of the system
are demonstrated through a case study, and the visualized demos are available
at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation
`,authors:"Yuchen Xia; Daniel Dittler; Nasser Jazdi; Haonan Chen; Michael Weyrich",status:0,relevancy:.5365061176244996,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17846",date:"2024-05-28",title:"Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs",abstract:`  Safety limitations in service robotics across various industries have raised
significant concerns about the need for robust mechanisms ensuring that robots
adhere to safe practices, thereby preventing actions that might harm humans or
cause property damage. Despite advances, including the integration of Knowledge
Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring
consistent safety in autonomous robot actions persist. In this paper, we
propose a novel integration of Large Language Models with Embodied Robotic
Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the
safety framework for service robots. ERCPs are designed as predefined
instructions that ensure LLMs generate safe and precise responses. These
responses are subsequently validated by EKGs, which provide a comprehensive
knowledge base ensuring that the actions of the robot are continuously aligned
with safety protocols, thereby promoting safer operational practices in varied
contexts. Our experimental setup involved diverse real-world tasks, where
robots equipped with our framework demonstrated significantly higher compliance
with safety standards compared to traditional methods. This integration fosters
secure human-robot interactions and positions our methodology at the forefront
of AI-driven safety innovations in service robotics.
`,authors:"Yong Qi; Gabriel Kyebambo; Siyuan Xie; Wei Shen; Shenghui Wang; Bitao Xie; Bin He; Zhipeng Wang; Shuo Jiang",status:0,relevancy:.5361101468889371,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17956",date:"2024-05-28",title:`Hybrid Preference Optimization: Augmenting Direct Preference
  Optimization with Auxiliary Objectives`,abstract:`  For aligning large language models (LLMs), prior work has leveraged
reinforcement learning via human feedback (RLHF) or variations of direct
preference optimization (DPO). While DPO offers a simpler framework based on
maximum likelihood estimation, it compromises on the ability to tune language
models to easily maximize non-differentiable and non-binary objectives
according to the LLM designer's preferences (e.g., using simpler language or
minimizing specific kinds of harmful content). These may neither align with
user preferences nor even be able to be captured tractably by binary preference
data. To leverage the simplicity and performance of DPO with the
generalizability of RL, we propose a hybrid approach between DPO and RLHF. With
a simple augmentation to the implicit reward decomposition of DPO, we allow for
tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL.
The proposed method, Hybrid Preference Optimization (HPO), shows the ability to
effectively generalize to both user preferences and auxiliary designer
objectives, while preserving alignment performance across a range of
challenging benchmarks and model sizes.
`,authors:"Anirudhan Badrinath; Prabhat Agarwal; Jiajing Xu",status:0,relevancy:.5283808724778053,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17755",date:"2024-05-28",title:`XL3M: A Training-free Framework for LLM Length Extension Based on
  Segment-wise Inference`,abstract:`  Length generalization failure problem, namely the large language model (LLM)
fails to generalize to texts longer than its maximum training length, greatly
restricts the application of LLM in the scenarios with streaming long inputs.
To address this problem, the existing methods either require substantial costs
or introduce precision loss. In this paper, we empirically find that the
accuracy of the LLM's prediction is highly correlated to its certainty. Based
on this, we propose an efficient training free framework, named XL3M (it means
extra-long large language model), which enables the LLMs trained on short
sequences to reason extremely long sequence without any further training or
fine-tuning. Under the XL3M framework, the input context will be firstly
decomposed into multiple short sub-contexts, where each sub-context contains an
independent segment and a common \`\`question'' which is a few tokens from the
end of the original context. Then XL3M gives a method to measure the relevance
between each segment and the \`\`question'', and constructs a concise key context
by splicing all the relevant segments in chronological order. The key context
is further used instead of the original context to complete the inference task.
Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our
framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card
Huawei Ascend 910B NPU machine with 64GB memory per card.
`,authors:"Shengnan Wang; Youhui Bai; Lin Zhang; Pingyi Zhou; Shixiong Zhao; Gong Zhang; Sen Wang; Renhai Chen; Hua Xu; Hongwei Sun",status:0,relevancy:.5271678320189086,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17743",date:"2024-05-28",title:"ORLM: Training Large Language Models for Optimization Modeling",abstract:`  Large Language Models (LLMs) have emerged as powerful tools for tackling
complex Operations Research (OR) problem by providing the capacity in
automating optimization modeling. However, current methodologies heavily rely
on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs,
raising data privacy concerns that could be prohibitive in industry
applications. To tackle this issue, we propose training open-source LLMs for
optimization modeling. We identify four critical requirements for the training
dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process
for creating synthetic data tailored to specific requirements. We also
introduce the IndustryOR benchmark, the first industrial benchmark for testing
LLMs on solving real-world OR problems. We apply the data from OR-Instruct to
various open-source LLMs of 7b size (termed as ORLMs), resulting in a
significantly improved capability for optimization modeling. Our
best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO,
and IndustryOR benchmarks. Our code and data are available at
\\url{https://github.com/Cardinal-Operations/ORLM}.
`,authors:"Zhengyang Tang; Chenyu Huang; Xin Zheng; Shixi Hu; Zizhuo Wang; Dongdong Ge; Benyou Wang",status:0,relevancy:.5236272328810484,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17893",date:"2024-05-28",title:"Arithmetic Reasoning with LLM: Prolog Generation & Permutation",abstract:`  Instructing large language models (LLMs) to solve elementary school math
problems has shown great success using Chain of Thought (CoT). However, the CoT
approach relies on an LLM to generate a sequence of arithmetic calculations
which can be prone to cascaded calculation errors. We hypothesize that an LLM
should focus on extracting predicates and generating symbolic formulas from the
math problem description so that the underlying calculation can be done via an
external code interpreter. We investigate using LLM to generate Prolog programs
to solve mathematical questions. Experimental results show that our
Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K
benchmark across three distinct LLMs. In addition, given the insensitive
ordering of predicates and symbolic formulas in Prolog, we propose to permute
the ground truth predicates for more robust LLM training via data augmentation.
`,authors:"Xiaocheng Yang; Bingsen Chen; Yik-Cheung Tam",status:0,relevancy:.5213901825067223,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18542",date:"2024-05-28",title:`Automatic detection of cognitive impairment in elderly people using an
  entertainment chatbot with Natural Language Processing capabilities`,abstract:`  Previous researchers have proposed intelligent systems for therapeutic
monitoring of cognitive impairments. However, most existing practical
approaches for this purpose are based on manual tests. This raises issues such
as excessive caretaking effort and the white-coat effect. To avoid these
issues, we present an intelligent conversational system for entertaining
elderly people with news of their interest that monitors cognitive impairment
transparently. Automatic chatbot dialogue stages allow assessing content
description skills and detecting cognitive impairment with Machine Learning
algorithms. We create these dialogue flows automatically from updated news
items using Natural Language Generation techniques. The system also infers the
gold standard of the answers to the questions, so it can assess cognitive
capabilities automatically by comparing these answers with the user responses.
It employs a similarity metric with values in [0, 1], in increasing level of
similarity. To evaluate the performance and usability of our approach, we have
conducted field tests with a test group of 30 elderly people in the earliest
stages of dementia, under the supervision of gerontologists. In the
experiments, we have analysed the effect of stress and concentration in these
users. Those without cognitive impairment performed up to five times better. In
particular, the similarity metric varied between 0.03, for stressed and
unfocused participants, and 0.36, for relaxed and focused users. Finally, we
developed a Machine Learning algorithm based on textual analysis features for
automatic cognitive impairment detection, which attained accuracy, F-measure
and recall levels above 80%. We have thus validated the automatic approach to
detect cognitive impairment in elderly people based on entertainment content.
`,authors:"Francisco de Arriba-Pérez; Silvia García-Méndez; Francisco J. González-Castaño; Enrique Costa-Montenegro",status:0,relevancy:.5193799491103603,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18377",date:"2024-05-28",title:`LLaMA-NAS: Efficient Neural Architecture Search for Large Language
  Models`,abstract:`  The abilities of modern large language models (LLMs) in solving natural
language processing, complex reasoning, sentiment analysis and other tasks have
been extraordinary which has prompted their extensive adoption. Unfortunately,
these abilities come with very high memory and computational costs which
precludes the use of LLMs on most hardware platforms. To mitigate this, we
propose an effective method of finding Pareto-optimal network architectures
based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B
only once and then apply genetic algorithm-based search to find smaller, less
computationally complex network architectures. We show that, for certain
standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily
large and complex. More specifically, we demonstrate a 1.5x reduction in model
size and 1.3x speedup in throughput for certain tasks with negligible drop in
accuracy. In addition to finding smaller, higher-performing network
architectures, our method does so more effectively and efficiently than certain
pruning or sparsification techniques. Finally, we demonstrate how quantization
is complementary to our method and that the size and complexity of the networks
we find can be further decreased using quantization. We believe that our work
provides a way to automatically create LLMs which can be used on less expensive
and more readily available hardware platforms.
`,authors:"Anthony Sarah; Sharath Nittur Sridhar; Maciej Szankin; Sairam Sundaresan",status:0,relevancy:.5188820875193044,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17764",date:"2024-05-28",title:"On the Sequence Evaluation based on Stochastic Processes",abstract:`  Modeling and analyzing long sequences of text is an essential task for
Natural Language Processing. Success in capturing long text dynamics using
neural language models will facilitate many downstream tasks such as coherence
evaluation, text generation, machine translation and so on. This paper presents
a novel approach to model sequences through a stochastic process. We introduce
a likelihood-based training objective for the text encoder and design a more
thorough measurement (score) for long text evaluation compared to the previous
approach. The proposed training objective effectively preserves the sequence
coherence, while the new score comprehensively captures both temporal and
spatial dependencies. Theoretical properties of our new score show its
advantages in sequence evaluation. Experimental results show superior
performance in various sequence evaluation tasks, including global and local
discrimination within and between documents of different lengths. We also
demonstrate the encoder achieves competitive results on discriminating human
and AI written text.
`,authors:"Tianhao Zhang; Zhexiao Lin; Zhecheng Sheng; Chen Jiang; Dongyeop Kang",status:0,relevancy:.5167412359666395,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17739",date:"2024-05-28",title:`The Widening Gap: The Benefits and Harms of Generative AI for Novice
  Programmers`,abstract:`  Novice programmers often struggle through programming problem solving due to
a lack of metacognitive awareness and strategies. Previous research has shown
that novices can encounter multiple metacognitive difficulties while
programming. Novices are typically unaware of how these difficulties are
hindering their progress. Meanwhile, many novices are now programming with
generative AI (GenAI), which can provide complete solutions to most
introductory programming problems, code suggestions, hints for next steps when
stuck, and explain cryptic error messages. Its impact on novice metacognition
has only started to be explored. Here we replicate a previous study that
examined novice programming problem solving behavior and extend it by
incorporating GenAI tools. Through 21 lab sessions consisting of participant
observation, interview, and eye tracking, we explore how novices are coding
with GenAI tools. Although 20 of 21 students completed the assigned programming
problem, our findings show an unfortunate divide in the use of GenAI tools
between students who accelerated and students who struggled. Students who
accelerated were able to use GenAI to create code they already intended to make
and were able to ignore unhelpful or incorrect inline code suggestions. But for
students who struggled, our findings indicate that previously known
metacognitive difficulties persist, and that GenAI unfortunately can compound
them and even introduce new metacognitive difficulties. Furthermore, struggling
students often expressed cognitive dissonance about their problem solving
ability, thought they performed better than they did, and finished with an
illusion of competence. Based on our observations from both groups, we propose
ways to scaffold the novice GenAI experience and make suggestions for future
work.
`,authors:"James Prather; Brent Reeves; Juho Leinonen; Stephen MacNeil; Arisoa S. Randrianasolo; Brett Becker; Bailey Kimmel; Jared Wright; Ben Briggs",status:0,relevancy:.5159764894068817,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17879",date:"2024-05-28",title:`Resisting Stochastic Risks in Diffusion Planners with the Trajectory
  Aggregation Tree`,abstract:`  Diffusion planners have shown promise in handling long-horizon and
sparse-reward tasks due to the non-autoregressive plan generation. However,
their inherent stochastic risk of generating infeasible trajectories presents
significant challenges to their reliability and stability. We introduce a novel
approach, the Trajectory Aggregation Tree (TAT), to address this issue in
diffusion planners. Compared to prior methods that rely solely on raw
trajectory predictions, TAT aggregates information from both historical and
current trajectories, forming a dynamic tree-like structure. Each trajectory is
conceptualized as a branch and individual states as nodes. As the structure
evolves with the integration of new trajectories, unreliable states are
marginalized, and the most impactful nodes are prioritized for decision-making.
TAT can be deployed without modifying the original training and sampling
pipelines of diffusion planners, making it a training-free, ready-to-deploy
solution. We provide both theoretical analysis and empirical evidence to
support TAT's effectiveness. Our results highlight its remarkable ability to
resist the risk from unreliable trajectories, guarantee the performance
boosting of diffusion planners in $100\\%$ of tasks, and exhibit an appreciable
tolerance margin for sample quality, thereby enabling planning with a more than
$3\\times$ acceleration.
`,authors:"Lang Feng; Pengjie Gu; Bo An; Gang Pan",status:0,relevancy:.5152195033933369,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18553",date:"2024-05-28",title:`The FAIIR Tool: A Conversational AI Agent Assistant for Youth Mental
  Health Service Provision`,abstract:`  World's healthcare systems and mental health agencies face both a growing
demand for youth mental health services, alongside a simultaneous challenge of
limited resources. Given these constraints, this work presents our experience
in the creation and evaluation of the FAIIR (Frontline Assistant: Issue
Identification and Recommendation) tool, an ensemble of domain-adapted and
fine-tuned transformer models, leveraging natural language processing to
identify issues that youth may be experiencing. We explore the technical
development, performance, and validation processes leveraged for the FAIIR tool
in application to situations of frontline crisis response via Kids Help Phone.
Frontline Crisis Responders assign an issue tag from a defined list following
each conversation. Assisting with the identification of issues of relevance
helps reduce the burden on CRs, ensuring that appropriate resources can be
provided and that active rescues and mandatory reporting can take place in
critical situations requiring immediate de-escalation.
`,authors:"Stephen Obadinma; Alia Lachana; Maia Norman; Jocelyn Rankin; Joanna Yu; Xiaodan Zhu; Darren Mastropaolo; Deval Pandya; Roxana Sultan; Elham Dolatabadi",status:0,relevancy:.5151707121878215,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18626",date:"2024-05-28",title:"Causal Contextual Bandits with Adaptive Context",abstract:`  We study a variant of causal contextual bandits where the context is chosen
based on an initial intervention chosen by the learner. At the beginning of
each round, the learner selects an initial action, depending on which a
stochastic context is revealed by the environment. Following this, the learner
then selects a final action and receives a reward. Given $T$ rounds of
interactions with the environment, the objective of the learner is to learn a
policy (of selecting the initial and the final action) with maximum expected
reward. In this paper we study the specific situation where every action
corresponds to intervening on a node in some known causal graph. We extend
prior work from the deterministic context setting to obtain simple regret
minimization guarantees. This is achieved through an instance-dependent causal
parameter, $\\lambda$, which characterizes our upper bound. Furthermore, we
prove that our simple regret is essentially tight for a large class of
instances. A key feature of our work is that we use convex optimization to
address the bandit exploration problem. We also conduct experiments to validate
our theoretical results, and release our code at our project GitHub repository:
https://github.com/adaptiveContextualCausalBandits/aCCB.
`,authors:"Rahul Madhavan; Aurghya Maiti; Gaurav Sinha; Siddharth Barman",status:0,relevancy:.5139517434237361,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17991",date:"2024-05-28",title:"VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections",abstract:`  Large language models (LLMs) have recently emerged as powerful tools for
tackling many language-processing tasks. Despite their success, training and
fine-tuning these models is still far too computationally and memory intensive.
In this paper, we identify and characterise the important components needed for
effective model convergence using gradient descent. In doing so we find that
the intermediate activations used to implement backpropagation can be
excessively compressed without incurring any degradation in performance. This
result leads us to a cheap and memory-efficient algorithm for both fine-tuning
and pre-training LLMs. The proposed algorithm simply divides the tokens up into
smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace
during the forward pass. These features are then coarsely reconstructed during
the backward pass to implement the update rules. We confirm the effectiveness
of our algorithm as being complimentary to many state-of-the-art PEFT methods
on the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for
fine-tuning LLaMA and show competitive performance against other
memory-efficient pre-training methods on the large-scale C4 dataset.
`,authors:"Roy Miles; Pradyumna Reddy; Ismail Elezi; Jiankang Deng",status:0,relevancy:.5129212230701997,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18272",date:"2024-05-28",title:`Metaheuristics and Large Language Models Join Forces: Towards an
  Integrated Optimization Approach`,abstract:`  Since the rise of Large Language Models (LLMs) a couple of years ago,
researchers in metaheuristics (MHs) have wondered how to use their power in a
beneficial way within their algorithms. This paper introduces a novel approach
that leverages LLMs as pattern recognition tools to improve MHs. The resulting
hybrid method, tested in the context of a social network-based combinatorial
optimization problem, outperforms existing state-of-the-art approaches that
combine machine learning with MHs regarding the obtained solution quality. By
carefully designing prompts, we demonstrate that the output obtained from LLMs
can be used as problem knowledge, leading to improved results. Lastly, we
acknowledge LLMs' potential drawbacks and limitations and consider it essential
to examine them to advance this type of research further.
`,authors:"Camilo Chacón Sartori; Christian Blum; Filippo Bistaffa; Guillem Rodríguez Corominas",status:0,relevancy:.5074855790680858,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17998",date:"2024-05-28",title:`Source Echo Chamber: Exploring the Escalation of Source Bias in User,
  Data, and Recommender System Feedback Loop`,abstract:`  Recently, researchers have uncovered that neural retrieval models prefer
AI-generated content (AIGC), called source bias. Compared to active search
behavior, recommendation represents another important means of information
acquisition, where users are more prone to source bias. Furthermore, delving
into the recommendation scenario, as AIGC becomes integrated within the
feedback loop involving users, data, and the recommender system, it
progressively contaminates the candidate items, the user interaction history,
and ultimately, the data used to train the recommendation models. How and to
what extent the source bias affects the neural recommendation models within
feedback loop remains unknown. In this study, we extend the investigation of
source bias into the realm of recommender systems, specifically examining its
impact across different phases of the feedback loop. We conceptualize the
progression of AIGC integration into the recommendation content ecosystem in
three distinct phases-HGC dominate, HGC-AIGC coexist, and AIGC dominance-each
representing past, present, and future states, respectively. Through extensive
experiments across three datasets from diverse domains, we demonstrate the
prevalence of source bias and reveal a potential digital echo chamber with
source bias amplification throughout the feedback loop. This trend risks
creating a recommender ecosystem with limited information source, such as AIGC,
being disproportionately recommended. To counteract this bias and prevent its
escalation in the feedback loop, we introduce a black-box debiasing method that
maintains model impartiality towards both HGC and AIGC. Our experimental
results validate the effectiveness of the proposed debiasing method, confirming
its potential to disrupt the feedback loop.
`,authors:"Yuqi Zhou; Sunhao Dai; Liang Pang; Gang Wang; Zhenhua Dong; Jun Xu; Ji-Rong Wen",status:0,relevancy:.5068231599869906,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18180",date:"2024-05-28",title:`Safe Reinforcement Learning in Black-Box Environments via Adaptive
  Shielding`,abstract:`  Empowering safe exploration of reinforcement learning (RL) agents during
training is a critical impediment towards deploying RL agents in many
real-world scenarios. Training RL agents in unknown, black-box environments
poses an even greater safety risk when prior knowledge of the domain/task is
unavailable. We introduce ADVICE (Adaptive Shielding with a Contrastive
Autoencoder), a novel post-shielding technique that distinguishes safe and
unsafe features of state-action pairs during training, thus protecting the RL
agent from executing actions that yield potentially hazardous outcomes. Our
comprehensive experimental evaluation against state-of-the-art safe RL
exploration techniques demonstrates how ADVICE can significantly reduce safety
violations during training while maintaining a competitive outcome reward.
`,authors:"Daniel Bethell; Simos Gerasimou; Radu Calinescu; Calum Imrie",status:0,relevancy:.5066017536340098,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17969",date:"2024-05-28",title:"Knowledge Circuits in Pretrained Transformers",abstract:`  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, has allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuit holds
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
`,authors:"Yunzhi Yao; Ningyu Zhang; Zekun Xi; Mengru Wang; Ziwen Xu; Shumin Deng; Huajun Chen",status:0,relevancy:.4996402280743558,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18241",date:"2024-05-28",title:`Active Use of Latent Constituency Representation in both Humans and
  Large Language Models`,abstract:`  Understanding how sentences are internally represented in the human brain, as
well as in large language models (LLMs) such as ChatGPT, is a major challenge
for cognitive science. Classic linguistic theories propose that the brain
represents a sentence by parsing it into hierarchically organized constituents.
In contrast, LLMs do not explicitly parse linguistic constituents and their
latent representations remains poorly explained. Here, we demonstrate that
humans and LLMs construct similar latent representations of hierarchical
linguistic constituents by analyzing their behaviors during a novel one-shot
learning task, in which they infer which words should be deleted from a
sentence. Both humans and LLMs tend to delete a constituent, instead of a
nonconstituent word string. In contrast, a naive sequence processing model that
has access to word properties and ordinal positions does not show this
property. Based on the word deletion behaviors, we can reconstruct the latent
constituency tree representation of a sentence for both humans and LLMs. These
results demonstrate that a latent tree-structured constituency representation
can emerge in both the human brain and LLMs.
`,authors:"Wei Liu; Ming Xiang; Nai Ding",status:0,relevancy:.4970011354378443,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17820",date:"2024-05-28",title:`Don't Miss the Forest for the Trees: Attentional Vision Calibration for
  Large Vision Language Models`,abstract:`  This study addresses the issue observed in Large Vision Language Models
(LVLMs), where excessive attention on a few image tokens, referred to as blind
tokens, leads to hallucinatory responses in tasks requiring fine-grained
understanding of visual objects. We found that tokens receiving lower attention
weights often hold essential information for identifying nuanced object details
-- ranging from merely recognizing object existence to identifying their
attributes (color, position, etc.) and understanding their relationships. To
counteract the over-emphasis on blind tokens and to accurately respond to user
queries, we introduce a technique called Attentional Vision Calibration (AVC).
During the decoding phase, AVC identifies blind tokens by analyzing the
image-related attention distribution. It then dynamically adjusts the logits
for the next token prediction by contrasting the logits conditioned on the
original visual tokens with those conditioned on the blind tokens. This
effectively lowers the dependency on blind tokens and promotes a more balanced
consideration of all tokens. We validate AVC on benchmarks such as POPE, MME,
and AMBER, where it consistently outperforms existing decoding techniques in
mitigating object hallucinations in LVLMs.
`,authors:"Sangmin Woo; Donguk Kim; Jaehyuk Jang; Yubin Choi; Changick Kim",status:0,relevancy:.4958776841847703,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18572",date:"2024-05-28",title:"Low-rank finetuning for LLMs: A fairness perspective",abstract:`  Low-rank approximation techniques have become the de facto standard for
fine-tuning Large Language Models (LLMs) due to their reduced computational and
memory requirements. This paper investigates the effectiveness of these methods
in capturing the shift of fine-tuning datasets from the initial pre-trained
data distribution. Our findings reveal that there are cases in which low-rank
fine-tuning falls short in learning such shifts. This, in turn, produces
non-negligible side effects, especially when fine-tuning is adopted for
toxicity mitigation in pre-trained models, or in scenarios where it is
important to provide fair models. Through comprehensive empirical evidence on
several models, datasets, and tasks, we show that low-rank fine-tuning
inadvertently preserves undesirable biases and toxic behaviors. We also show
that this extends to sequential decision-making tasks, emphasizing the need for
careful evaluation to promote responsible LLMs development.
`,authors:"Saswat Das; Marco Romanelli; Cuong Tran; Zarreen Reza; Bhavya Kailkhura; Ferdinando Fioretto",status:0,relevancy:.4944283926549484,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17809",date:"2024-05-28",title:`TransVIP: Speech to Speech Translation System with Voice and Isochrony
  Preservation`,abstract:`  There is a rising interest and trend in research towards directly translating
speech from one language to another, known as end-to-end speech-to-speech
translation. However, most end-to-end models struggle to outperform cascade
models, i.e., a pipeline framework by concatenating speech recognition, machine
translation and text-to-speech models. The primary challenges stem from the
inherent complexities involved in direct translation tasks and the scarcity of
data. In this study, we introduce a novel model framework TransVIP that
leverages diverse datasets in a cascade fashion yet facilitates end-to-end
inference through joint probability. Furthermore, we propose two separated
encoders to preserve the speaker's voice characteristics and isochrony from the
source speech during the translation process, making it highly suitable for
scenarios such as video dubbing. Our experiments on the French-English language
pair demonstrate that our model outperforms the current state-of-the-art
speech-to-speech translation model.
`,authors:"Chenyang Le; Yao Qian; Dongmei Wang; Long Zhou; Shujie Liu; Xiaofei Wang; Midia Yousefi; Yanmin Qian; Jinyu Li; Sheng Zhao; Michael Zeng",status:0,relevancy:.4942942352267037,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18346",date:"2024-05-28",title:`Intelligent Clinical Documentation: Harnessing Generative AI for
  Patient-Centric Clinical Note Generation`,abstract:`  Comprehensive clinical documentation is crucial for effective healthcare
delivery, yet it poses a significant burden on healthcare professionals,
leading to burnout, increased medical errors, and compromised patient safety.
This paper explores the potential of generative AI (Artificial Intelligence) to
streamline the clinical documentation process, specifically focusing on
generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior,
Intervention, Response, Plan) notes. We present a case study demonstrating the
application of natural language processing (NLP) and automatic speech
recognition (ASR) technologies to transcribe patient-clinician interactions,
coupled with advanced prompting techniques to generate draft clinical notes
using large language models (LLMs). The study highlights the benefits of this
approach, including time savings, improved documentation quality, and enhanced
patient-centered care. Additionally, we discuss ethical considerations, such as
maintaining patient confidentiality and addressing model biases, underscoring
the need for responsible deployment of generative AI in healthcare settings.
The findings suggest that generative AI has the potential to revolutionize
clinical documentation practices, alleviating administrative burdens and
enabling healthcare professionals to focus more on direct patient care.
`,authors:"Anjanava Biswas; Wrick Talukdar",status:0,relevancy:.4910412988786512,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17959",date:"2024-05-28",title:"Attention-based sequential recommendation system using multimodal data",abstract:`  Sequential recommendation systems that model dynamic preferences based on a
use's past behavior are crucial to e-commerce. Recent studies on these systems
have considered various types of information such as images and texts. However,
multimodal data have not yet been utilized directly to recommend products to
users. In this study, we propose an attention-based sequential recommendation
method that employs multimodal data of items such as images, texts, and
categories. First, we extract image and text features from pre-trained VGG and
BERT and convert categories into multi-labeled forms. Subsequently, attention
operations are performed independent of the item sequence and multimodal
representations. Finally, the individual attention information is integrated
through an attention fusion function. In addition, we apply multitask learning
loss for each modality to improve the generalization performance. The
experimental results obtained from the Amazon datasets show that the proposed
method outperforms those of conventional sequential recommendation systems.
`,authors:"Hyungtaik Oh; Wonkeun Jo; Dongil Kim",status:0,relevancy:.4898352172466597,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18638",date:"2024-05-28",title:`ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation
  for Generative Large Language Models`,abstract:`  In this position paper, we argue that human evaluation of generative large
language models (LLMs) should be a multidisciplinary undertaking that draws
upon insights from disciplines such as user experience research and human
behavioral psychology to ensure that the experimental design and results are
reliable. The conclusions from these evaluations, thus, must consider factors
such as usability, aesthetics, and cognitive biases. We highlight how cognitive
biases can conflate fluent information and truthfulness, and how cognitive
uncertainty affects the reliability of rating scores such as Likert.
Furthermore, the evaluation should differentiate the capabilities and
weaknesses of increasingly powerful large language models -- which requires
effective test sets. The scalability of human evaluation is also crucial to
wider adoption. Hence, to design an effective human evaluation system in the
age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework
consisting of 6 pillars --Consistency, Scoring Critera, Differentiating, User
Experience, Responsible, and Scalability.
`,authors:"Aparna Elangovan; Ling Liu; Lei Xu; Sravan Bodapati; Dan Roth",status:0,relevancy:.4888812049931227,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18258",date:"2024-05-28",title:"Text-only Synthesis for Image Captioning",abstract:`  From paired image-text training to text-only training for image captioning,
the pursuit of relaxing the requirements for high-cost and large-scale
annotation of good quality data remains consistent. In this paper, we propose
Text-only Synthesis for Image Captioning (ToCa), which further advances this
relaxation with fewer human labor and less computing time. Specifically, we
deconstruct caption text into structures and lexical words, which serve as the
fundamental components of the caption. By combining different structures and
lexical words as inputs to the large language model, massive captions that
contain various patterns of lexical words are generated. This method not only
approaches the target domain but also surpasses it by generating new captions,
thereby enhancing the zero-shot generalization ability of the model.
Considering the different levels of data access in the real world, we define
three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and
data-efficient synthesis. Experiments in these scenarios demonstrate the
generalizability, transferability and practicability of ToCa with a nearly 5
CIDEr improvement for zero-shot cross-domain captioning and a maximum increase
of over 20 CIDEr for data-efficient captioning.
`,authors:"Qing Zhou; Junlin Huang; Qiang Li; Junyu Gao; Qi Wang",status:0,relevancy:.48785918089931724,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18106",date:"2024-05-28",title:`A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation
  and Extrapolation`,abstract:`  Temporal knowledge graph (TKG) reasoning has two settings: interpolation
reasoning and extrapolation reasoning. Both of them draw plenty of research
interest and have great significance. Methods of the former de-emphasize the
temporal correlations among facts sequences, while methods of the latter
require strict chronological order of knowledge and ignore inferring clues
provided by missing facts of the past. These limit the practicability of TKG
applications as almost all of the existing TKG reasoning methods are designed
specifically to address either one setting. To this end, this paper proposes an
original Temporal PAth-based Reasoning (TPAR) model for both the interpolation
and extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning
fashion that is robust to ambiguous and noisy temporal data and with fine
interpretability as well. Comprehensive experiments show that TPAR outperforms
SOTA methods on the link prediction task for both the interpolation and the
extrapolation settings. A novel pipeline experimental setting is designed to
evaluate the performances of SOTA combinations and the proposed TPAR towards
interpolation and extrapolation reasoning. More diverse experiments are
conducted to show the robustness and interpretability of TPAR.
`,authors:"Kai Chen; Ye Wang; Yitong Li; Aiping Li; Han Yu; Xin Song",status:0,relevancy:.48525179207716307,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17730",date:"2024-05-28",title:"MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance",abstract:`  Multimodal learning methods with targeted unimodal learning objectives have
exhibited their superior efficacy in alleviating the imbalanced multimodal
learning problem. However, in this paper, we identify the previously ignored
gradient conflict between multimodal and unimodal learning objectives,
potentially misleading the unimodal encoder optimization. To well diminish
these conflicts, we observe the discrepancy between multimodal loss and
unimodal loss, where both gradient magnitude and covariance of the
easier-to-learn multimodal loss are smaller than the unimodal one. With this
property, we analyze Pareto integration under our multimodal scenario and
propose MMPareto algorithm, which could ensure a final gradient with direction
that is common to all learning objectives and enhanced magnitude to improve
generalization, providing innocent unimodal assistance. Finally, experiments
across multiple types of modalities and frameworks with dense cross-modal
interaction indicate our superior and extendable method performance. Our method
is also expected to facilitate multi-task cases with a clear discrepancy in
task difficulty, demonstrating its ideal scalability. The source code and
dataset are available at https://github.com/GeWu-Lab/MMPareto_ICML2024.
`,authors:"Yake Wei; Di Hu",status:0,relevancy:.48349556893841217,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18068",date:"2024-05-28",title:"A Survey of Latent Factor Models in Recommender Systems",abstract:`  Recommender systems are essential tools in the digital era, providing
personalized content to users in areas like e-commerce, entertainment, and
social media. Among the many approaches developed to create these systems,
latent factor models have proven particularly effective. This survey
systematically reviews latent factor models in recommender systems, focusing on
their core principles, methodologies, and recent advancements. The literature
is examined through a structured framework covering learning data, model
architecture, learning strategies, and optimization techniques. The analysis
includes a taxonomy of contributions and detailed discussions on the types of
learning data used, such as implicit feedback, trust, and content data, various
models such as probabilistic, nonlinear, and neural models, and an exploration
of diverse learning strategies like online learning, transfer learning, and
active learning. Furthermore, the survey addresses the optimization strategies
used to train latent factor models, improving their performance and
scalability. By identifying trends, gaps, and potential research directions,
this survey aims to provide valuable insights for researchers and practitioners
looking to advance the field of recommender systems.
`,authors:"Hind I. Alshbanat; Hafida Benhidour; Said Kerrache",status:0,relevancy:.48248742690915747,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18642",date:"2024-05-28",title:`JADS: A Framework for Self-supervised Joint Aspect Discovery and
  Summarization`,abstract:`  To generate summaries that include multiple aspects or topics for text
documents, most approaches use clustering or topic modeling to group relevant
sentences and then generate a summary for each group. These approaches struggle
to optimize the summarization and clustering algorithms jointly. On the other
hand, aspect-based summarization requires known aspects. Our solution
integrates topic discovery and summarization into a single step. Given text
data, our Joint Aspect Discovery and Summarization algorithm (JADS) discovers
aspects from the input and generates a summary of the topics, in one step. We
propose a self-supervised framework that creates a labeled dataset by first
mixing sentences from multiple documents (e.g., CNN/DailyMail articles) as the
input and then uses the article summaries from the mixture as the labels. The
JADS model outperforms the two-step baselines. With pretraining, the model
achieves better performance and stability. Furthermore, embeddings derived from
JADS exhibit superior clustering capabilities. Our proposed method achieves
higher semantic alignment with ground truth and is factual.
`,authors:"Xiaobo Guo; Jay Desai; Srinivasan H. Sengamedu",status:0,relevancy:.4766947271742752,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18161",date:"2024-05-28",title:"Back to the Drawing Board for Fair Representation Learning",abstract:`  The goal of Fair Representation Learning (FRL) is to mitigate biases in
machine learning models by learning data representations that enable high
accuracy on downstream tasks while minimizing discrimination based on sensitive
attributes. The evaluation of FRL methods in many recent works primarily
focuses on the tradeoff between downstream fairness and accuracy with respect
to a single task that was used to approximate the utility of representations
during training (proxy task). This incentivizes retaining only features
relevant to the proxy task while discarding all other information. In extreme
cases, this can cause the learned representations to collapse to a trivial,
binary value, rendering them unusable in transfer settings. In this work, we
argue that this approach is fundamentally mismatched with the original
motivation of FRL, which arises from settings with many downstream tasks
unknown at training time (transfer tasks). To remedy this, we propose to
refocus the evaluation protocol of FRL methods primarily around the performance
on transfer tasks. A key challenge when conducting such an evaluation is the
lack of adequate benchmarks. We address this by formulating four criteria that
a suitable evaluation procedure should fulfill. Based on these, we propose
TransFair, a benchmark that satisfies these criteria, consisting of novel
variations of popular FRL datasets with carefully calibrated transfer tasks. In
this setting, we reevaluate state-of-the-art FRL methods, observing that they
often overfit to the proxy task, which causes them to underperform on certain
transfer tasks. We further highlight the importance of task-agnostic learning
signals for FRL methods, as they can lead to more transferrable
representations.
`,authors:"Angéline Pouget; Nikola Jovanović; Mark Vero; Robin Staab; Martin Vechev",status:0,relevancy:.476540623175971,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17978",date:"2024-05-28",title:`FASTopic: A Fast, Adaptive, Stable, and Transferable Topic Modeling
  Paradigm`,abstract:`  Topic models have been evolving rapidly over the years, from conventional to
recent neural models. However, existing topic models generally struggle with
either effectiveness, efficiency, or stability, highly impeding their practical
applications. In this paper, we propose FASTopic, a fast, adaptive, stable, and
transferable topic model. FASTopic follows a new paradigm: Dual
Semantic-relation Reconstruction (DSR). Instead of previous conventional,
neural VAE-based or clustering-based methods, DSR discovers latent topics by
reconstruction through modeling the semantic relations among document, topic,
and word embeddings. This brings about a neat and efficient topic modeling
framework. We further propose a novel Embedding Transport Plan (ETP) method.
Rather than early straightforward approaches, ETP explicitly regularizes the
semantic relations as optimal transport plans. This addresses the relation bias
issue and thus leads to effective topic modeling. Extensive experiments on
benchmark datasets demonstrate that our FASTopic shows superior effectiveness,
efficiency, adaptivity, stability, and transferability, compared to
state-of-the-art baselines across various scenarios. Our code is available at
https://github.com/bobxwu/FASTopic .
`,authors:"Xiaobao Wu; Thong Nguyen; Delvin Ce Zhang; William Yang Wang; Anh Tuan Luu",status:0,relevancy:.4762399963696454,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18330",date:"2024-05-28",title:"Frustratingly Easy Test-Time Adaptation of Vision-Language Models",abstract:`  Vision-Language Models seamlessly discriminate among arbitrary semantic
categories, yet they still suffer from poor generalization when presented with
challenging examples. For this reason, Episodic Test-Time Adaptation (TTA)
strategies have recently emerged as powerful techniques to adapt VLMs in the
presence of a single unlabeled image. The recent literature on TTA is dominated
by the paradigm of prompt tuning by Marginal Entropy Minimization, which,
relying on online backpropagation, inevitably slows down inference while
increasing memory. In this work, we theoretically investigate the properties of
this approach and unveil that a surprisingly strong TTA method lies dormant and
hidden within it. We term this approach ZERO (TTA with "zero" temperature),
whose design is both incredibly effective and frustratingly simple: augment N
times, predict, retain the most confident predictions, and marginalize after
setting the Softmax temperature to zero. Remarkably, ZERO requires a single
batched forward pass through the vision encoder only and no backward passes. We
thoroughly evaluate our approach following the experimental protocol
established in the literature and show that ZERO largely surpasses or compares
favorably w.r.t. the state-of-the-art while being almost 10x faster and 13x
more memory-friendly than standard Test-Time Prompt Tuning. Thanks to its
simplicity and comparatively negligible computation, ZERO can serve as a strong
baseline for future work in this field. The code is available at
https://github.com/FarinaMatteo/zero.
`,authors:"Matteo Farina; Gianni Franchi; Giovanni Iacca; Massimiliano Mancini; Elisa Ricci",status:0,relevancy:.4753889689512445,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18523",date:"2024-05-28",title:"TripletMix: Triplet Data Augmentation for 3D Understanding",abstract:`  Data augmentation has proven to be a vital tool for enhancing the
generalization capabilities of deep learning models, especially in the context
of 3D vision where traditional datasets are often limited. Despite previous
advancements, existing methods primarily cater to unimodal data scenarios,
leaving a gap in the augmentation of multimodal triplet data, which integrates
text, images, and point clouds. Simultaneously augmenting all three modalities
enhances diversity and improves alignment across modalities, resulting in more
comprehensive and robust 3D representations. To address this gap, we propose
TripletMix, a novel approach to address the previously unexplored issue of
multimodal data augmentation in 3D understanding. TripletMix innovatively
applies the principles of mixed-based augmentation to multimodal triplet data,
allowing for the preservation and optimization of cross-modal connections. Our
proposed TripletMix combines feature-level and input-level augmentations to
achieve dual enhancement between raw data and latent features, significantly
improving the model's cross-modal understanding and generalization capabilities
by ensuring feature consistency and providing diverse and realistic training
samples. We demonstrate that TripletMix not only improves the baseline
performance of models in various learning scenarios including zero-shot and
linear probing classification but also significantly enhances model
generalizability. Notably, we improved the zero-shot classification accuracy on
ScanObjectNN from 51.3 percent to 61.9 percent, and on Objaverse-LVIS from 46.8
percent to 51.4 percent. Our findings highlight the potential of multimodal
data augmentation to significantly advance 3D object recognition and
understanding.
`,authors:"Jiaze Wang; Yi Wang; Ziyu Guo; Renrui Zhang; Donghao Zhou; Guangyong Chen; Anfeng Liu; Pheng-Ann Heng",status:0,relevancy:.47330038070319025,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17832",date:"2024-05-28",title:"Mollification Effects of Policy Gradient Methods",abstract:`  Policy gradient methods have enabled deep reinforcement learning (RL) to
approach challenging continuous control problems, even when the underlying
systems involve highly nonlinear dynamics that generate complex non-smooth
optimization landscapes. We develop a rigorous framework for understanding how
policy gradient methods mollify non-smooth optimization landscapes to enable
effective policy search, as well as the downside of it: while making the
objective function smoother and easier to optimize, the stochastic objective
deviates further from the original problem. We demonstrate the equivalence
between policy gradient methods and solving backward heat equations. Following
the ill-posedness of backward heat equations from PDE theory, we present a
fundamental challenge to the use of policy gradient under stochasticity.
Moreover, we make the connection between this limitation and the uncertainty
principle in harmonic analysis to understand the effects of exploration with
stochastic policies in RL. We also provide experimental results to illustrate
both the positive and negative aspects of mollification effects in practice.
`,authors:"Tao Wang; Sylvia Herbert; Sicun Gao",status:0,relevancy:.4731394987063351,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18139",date:"2024-05-28",title:`Unlocking Futures: A Natural Language Driven Career Prediction System
  for Computer Science and Software Engineering Students`,abstract:`  A career is a crucial aspect for any person to fulfill their desires through
hard work. During their studies, students cannot find the best career
suggestions unless they receive meaningful guidance tailored to their skills.
Therefore, we developed an AI-assisted model for early prediction to provide
better career suggestions. Although the task is difficult, proper guidance can
make it easier. Effective career guidance requires understanding a student's
academic skills, interests, and skill-related activities. In this research, we
collected essential information from Computer Science (CS) and Software
Engineering (SWE) students to train a machine learning (ML) model that predicts
career paths based on students' career-related information. To adequately train
the models, we applied Natural Language Processing (NLP) techniques and
completed dataset pre-processing. For comparative analysis, we utilized
multiple classification ML algorithms and deep learning (DL) algorithms. This
study contributes valuable insights to educational advising by providing
specific career suggestions based on the unique features of CS and SWE
students. Additionally, the research helps individual CS and SWE students find
suitable jobs that match their skills, interests, and skill-related activities.
`,authors:"Sakir Hossain Faruque; Sharun Akter Khushbu; Sharmin Akter",status:0,relevancy:.4721310304102726,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17995",date:"2024-05-28",title:`DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive
  Architecture`,abstract:`  The joint-embedding predictive architecture (JEPA) recently has shown
impressive results in extracting visual representations from unlabeled imagery
under a masking strategy. However, we reveal its disadvantages, notably its
insufficient understanding of local semantics. This deficiency originates from
masked modeling in the embedding space, resulting in a reduction of
discriminative power and can even lead to the neglect of critical local
semantics. To bridge this gap, we introduce DMT-JEPA, a novel masked modeling
objective rooted in JEPA, specifically designed to generate discriminative
latent targets from neighboring information. Our key idea is simple: we
consider a set of semantically similar neighboring patches as a target of a
masked patch. To be specific, the proposed DMT-JEPA (a) computes feature
similarities between each masked patch and its corresponding neighboring
patches to select patches having semantically meaningful relations, and (b)
employs lightweight cross-attention heads to aggregate features of neighboring
patches as the masked targets. Consequently, DMT-JEPA demonstrates strong
discriminative power, offering benefits across a diverse spectrum of downstream
tasks. Through extensive experiments, we demonstrate our effectiveness across
various visual benchmarks, including ImageNet-1K image classification, ADE20K
semantic segmentation, and COCO object detection tasks. Code is available at:
\\url{https://github.com/DMTJEPA/DMTJEPA}.
`,authors:"Shentong Mo; Sukmin Yun",status:0,relevancy:.4692978572429557,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17913",date:"2024-05-28",title:`OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and
  Open-World Unknown Objects Supervision`,abstract:`  Open-Vocabulary Detection (OVD) aims to detect objects from novel categories
beyond the base categories on which the detector is trained. However, existing
open-vocabulary detectors trained on known category data tend to assign higher
confidence to trained categories and confuse novel categories with background.
To resolve this, we propose OV-DQUO, an \\textbf{O}pen-\\textbf{V}ocabulary DETR
with \\textbf{D}enoising text \\textbf{Q}uery training and open-world
\\textbf{U}nknown \\textbf{O}bjects supervision. Specifically, we introduce a
wildcard matching method that enables the detector to learn from pairs of
unknown objects recognized by the open-world detector and text embeddings with
general semantics, mitigating the confidence bias between base and novel
categories. Additionally, we propose a denoising text query training strategy
that synthesizes additional noisy query-box pairs from open-world unknown
objects to trains the detector through contrastive learning, enhancing its
ability to distinguish novel objects from the background. We conducted
extensive experiments on the challenging OV-COCO and OV-LVIS benchmarks,
achieving new state-of-the-art results of 45.6 AP50 and 39.3 mAP on novel
categories respectively, without the need for additional training data. Models
and code are released at https://github.com/xiaomoguhz/OV-DQUO
`,authors:"Junjie Wang; Bin Chen; Bin Kang; Yulin Li; YiChi Chen; Weizhi Xian; Huifeng Chang",status:0,relevancy:.4671651657710123,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18113",date:"2024-05-28",title:`Facilitating Multi-Role and Multi-Behavior Collaboration of Large
  Language Models for Online Job Seeking and Recruiting`,abstract:`  The emergence of online recruitment services has revolutionized the
traditional landscape of job seeking and recruitment, necessitating the
development of high-quality industrial applications to improve person-job
fitting. Existing methods generally rely on modeling the latent semantics of
resumes and job descriptions and learning a matching function between them.
Inspired by the powerful role-playing capabilities of Large Language Models
(LLMs), we propose to introduce a mock interview process between LLM-played
interviewers and candidates. The mock interview conversations can provide
additional evidence for candidate evaluation, thereby augmenting traditional
person-job fitting based solely on resumes and job descriptions. However,
characterizing these two roles in online recruitment still presents several
challenges, such as developing the skills to raise interview questions,
formulating appropriate answers, and evaluating two-sided fitness. To this end,
we propose MockLLM, a novel applicable framework that divides the person-job
matching process into two modules: mock interview generation and two-sided
evaluation in handshake protocol, jointly enhancing their performance through
collaborative behaviors between interviewers and candidates. We design a
role-playing framework as a multi-role and multi-behavior paradigm to enable a
single LLM agent to effectively behave with multiple functions for both
parties. Moreover, we propose reflection memory generation and dynamic prompt
modification techniques to refine the behaviors of both sides, enabling
continuous optimization of the augmented additional evidence. Extensive
experimental results show that MockLLM can achieve the best performance on
person-job matching accompanied by high mock interview quality, envisioning its
emerging application in real online recruitment in the future.
`,authors:"Hongda Sun; Hongzhan Lin; Haiyu Yan; Chen Zhu; Yang Song; Xin Gao; Shuo Shang; Rui Yan",status:0,relevancy:.4671475609579421,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18050",date:"2024-05-28",title:"Learning-Based Link Anomaly Detection in Continuous-Time Dynamic Graphs",abstract:`  Anomaly detection in continuous-time dynamic graphs is an emerging field yet
under-explored in the context of learning-based approaches. In this paper, we
pioneer structured analyses of link-level anomalies and graph representation
learning for identifying anomalous links in these graphs. First, we introduce a
fine-grain taxonomy for edge-level anomalies leveraging structural, temporal,
and contextual graph properties. We present a method for generating and
injecting such typed anomalies into graphs. Next, we introduce a novel method
to generate continuous-time dynamic graphs with consistent patterns across
time, structure, and context. To allow temporal graph methods to learn the link
anomaly detection task, we extend the generic link prediction setting by: (1)
conditioning link existence on contextual edge attributes; and (2) refining the
training regime to accommodate diverse perturbations in the negative edge
sampler. Building on this, we benchmark methods for anomaly detection.
Comprehensive experiments on synthetic and real-world datasets -- featuring
synthetic and labeled organic anomalies and employing six state-of-the-art
learning methods -- validate our taxonomy and generation processes for
anomalies and benign graphs, as well as our approach to adapting link
prediction methods for anomaly detection. Our results further reveal that
different learning methods excel in capturing different aspects of graph
normality and detecting different types of anomalies. We conclude with a
comprehensive list of findings highlighting opportunities for future research.
`,authors:"Tim Poštuvan; Claas Grohnfeldt; Michele Russo; Giulio Lovisotto",status:0,relevancy:.4669409562930932,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18044",date:"2024-05-28",title:`Cognitive Insights and Stable Coalition Matching for Fostering
  Multi-Agent Cooperation`,abstract:`  Cognitive abilities, such as Theory of Mind (ToM), play a vital role in
facilitating cooperation in human social interactions. However, our study
reveals that agents with higher ToM abilities may not necessarily exhibit
better cooperative behavior compared to those with lower ToM abilities. To
address this challenge, we propose a novel matching coalition mechanism that
leverages the strengths of agents with different ToM levels by explicitly
considering belief alignment and specialized abilities when forming coalitions.
Our proposed matching algorithm seeks to find stable coalitions that maximize
the potential for cooperative behavior and ensure long-term viability. By
incorporating cognitive insights into the design of multi-agent systems, our
work demonstrates the potential of leveraging ToM to create more sophisticated
and human-like coordination strategies that foster cooperation and improve
overall system performance.
`,authors:"Jiaqi Shao; Tianjun Yuan; Tao Lin; Xuanyu Cao; Bing Luo",status:0,relevancy:.46599837101190855,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17713",date:"2024-05-28",title:"AI Alignment with Changing and Influenceable Reward Functions",abstract:`  Existing AI alignment approaches assume that preferences are static, which is
unrealistic: our preferences change, and may even be influenced by our
interactions with AI systems themselves. To clarify the consequences of
incorrectly assuming static preferences, we introduce Dynamic Reward Markov
Decision Processes (DR-MDPs), which explicitly model preference changes and the
AI's influence on them. We show that despite its convenience, the
static-preference assumption may undermine the soundness of existing alignment
techniques, leading them to implicitly reward AI systems for influencing user
preferences in ways users may not truly want. We then explore potential
solutions. First, we offer a unifying perspective on how an agent's
optimization horizon may partially help reduce undesirable AI influence. Then,
we formalize different notions of AI alignment that account for preference
change from the outset. Comparing the strengths and limitations of 8 such
notions of alignment, we find that they all either err towards causing
undesirable AI influence, or are overly risk-averse, suggesting that a
straightforward solution to the problems of changing preferences may not exist.
As there is no avoiding grappling with changing preferences in real-world
settings, this makes it all the more important to handle these issues with
care, balancing risks and capabilities. We hope our work can provide conceptual
clarity and constitute a first step towards AI alignment practices which
explicitly account for (and contend with) the changing and influenceable nature
of human preferences.
`,authors:"Micah Carroll; Davis Foote; Anand Siththaranjan; Stuart Russell; Anca Dragan",status:0,relevancy:.4646108229816245,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18405",date:"2024-05-28",title:`WIDIn: Wording Image for Domain-Invariant Representation in
  Single-Source Domain Generalization`,abstract:`  Language has been useful in extending the vision encoder to data from diverse
distributions without empirical discovery in training domains. However, as the
image description is mostly at coarse-grained level and ignores visual details,
the resulted embeddings are still ineffective in overcoming complexity of
domains at inference time. We present a self-supervision framework WIDIn,
Wording Images for Domain-Invariant representation, to disentangle
discriminative visual representation, by only leveraging data in a single
domain and without any test prior. Specifically, for each image, we first
estimate the language embedding with fine-grained alignment, which can be
consequently used to adaptively identify and then remove domain-specific
counterpart from the raw visual embedding. WIDIn can be applied to both
pretrained vision-language models like CLIP, and separately trained uni-modal
models like MoCo and BERT. Experimental studies on three domain generalization
datasets demonstrate the effectiveness of our approach.
`,authors:"Jiawei Ma; Yulei Niu; Shiyuan Huang; Guangxing Han; Shih-Fu Chang",status:0,relevancy:.4629400786649298,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17902",date:"2024-05-28",title:"Boosting Protein Language Models with Negative Sample Mining",abstract:`  We introduce a pioneering methodology for boosting large language models in
the domain of protein representation learning. Our primary contribution lies in
the refinement process for correlating the over-reliance on co-evolution
knowledge, in a way that networks are trained to distill invaluable insights
from negative samples, constituted by protein pairs sourced from disparate
categories. By capitalizing on this novel approach, our technique steers the
training of transformer-based models within the attention score space. This
advanced strategy not only amplifies performance but also reflects the nuanced
biological behaviors exhibited by proteins, offering aligned evidence with
traditional biological mechanisms such as protein-protein interaction. We
experimentally observed improved performance on various tasks over datasets, on
top of several well-established large protein models. This innovative paradigm
opens up promising horizons for further progress in the realms of protein
research and computational biology.
`,authors:"Yaoyao Xu; Xinjian Zhao; Xiaozhuang Song; Benyou Wang; Tianshu Yu",status:0,relevancy:.4612553321497934,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18196",date:"2024-05-28",title:`Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based
  Behaviour Cloning`,abstract:`  In the field of Robot Learning, the complex mapping between high-dimensional
observations such as RGB images and low-level robotic actions, two inherently
very different spaces, constitutes a complex learning problem, especially with
limited amounts of data. In this work, we introduce Render and Diffuse (R&D) a
method that unifies low-level robot actions and RGB observations within the
image space using virtual renders of the 3D model of the robot. Using this
joint observation-action representation it computes low-level robot actions
using a learnt diffusion process that iteratively updates the virtual renders
of the robot. This space unification simplifies the learning problem and
introduces inductive biases that are crucial for sample efficiency and spatial
generalisation. We thoroughly evaluate several variants of R&D in simulation
and showcase their applicability on six everyday tasks in the real world. Our
results show that R&D exhibits strong spatial generalisation capabilities and
is more sample efficient than more common image-to-action methods.
`,authors:"Vitalis Vosylius; Younggyo Seo; Jafar Uruç; Stephen James",status:0,relevancy:.4608678641104317,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17821",date:"2024-05-28",title:`RITUAL: Random Image Transformations as a Universal Anti-hallucination
  Lever in LVLMs`,abstract:`  Recent advancements in Large Vision Language Models (LVLMs) have
revolutionized how machines understand and generate textual responses based on
visual inputs. Despite their impressive capabilities, they often produce
"hallucinatory" outputs that do not accurately reflect the visual information,
posing challenges in reliability and trustworthiness. Current methods such as
contrastive decoding have made strides in addressing these issues by
contrasting the original probability distribution of generated tokens with
distorted counterparts; yet, generating visually-faithful outputs remains a
challenge. In this work, we shift our focus to the opposite: What could serve
as a complementary enhancement to the original probability distribution? We
propose a simple, training-free method termed RITUAL to enhance robustness
against hallucinations in LVLMs. Our approach employs random image
transformations as complements to the original probability distribution, aiming
to mitigate the likelihood of hallucinatory visual explanations by enriching
the model's exposure to varied visual scenarios. Our empirical results show
that while the isolated use of transformed images initially degrades
performance, strategic implementation of these transformations can indeed serve
as effective complements. Notably, our method is compatible with current
contrastive decoding methods and does not require external models or costly
self-feedback mechanisms, making it a practical addition. In experiments,
RITUAL significantly outperforms existing contrastive decoding methods across
several object hallucination benchmarks, including POPE, CHAIR, and MME.
`,authors:"Sangmin Woo; Jaehyuk Jang; Donguk Kim; Yubin Choi; Changick Kim",status:0,relevancy:.45838152955094225,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18281",date:"2024-05-28",title:"MODL: Multilearner Online Deep Learning",abstract:`  Online deep learning solves the problem of learning from streams of data,
reconciling two opposing objectives: learn fast and learn deep. Existing work
focuses almost exclusively on exploring pure deep learning solutions, which are
much better suited to handle the "deep" than the "fast" part of the online
learning equation. In our work, we propose a different paradigm, based on a
hybrid multilearner approach. First, we develop a fast online logistic
regression learner. This learner does not rely on backpropagation. Instead, it
uses closed form recursive updates of model parameters, handling the fast
learning part of the online learning problem. We then analyze the existing
online deep learning theory and show that the widespread ODL approach,
currently operating at complexity $O(L^2)$ in terms of the number of layers
$L$, can be equivalently implemented in $O(L)$ complexity. This further leads
us to the cascaded multilearner design, in which multiple shallow and deep
learners are co-trained to solve the online learning problem in a cooperative,
synergistic fashion. We show that this approach achieves state-of-the-art
results on common online learning datasets, while also being able to handle
missing features gracefully. Our code is publicly available at
https://github.com/AntonValk/MODL.
`,authors:"Antonios Valkanas; Boris N. Oreshkin; Mark Coates",status:0,relevancy:.4573266022847642,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18350",date:"2024-05-28",title:"A System for Automatic English Text Expansion",abstract:`  We present an automatic text expansion system to generate English sentences,
which performs automatic Natural Language Generation (NLG) by combining
linguistic rules with statistical approaches. Here, "automatic" means that the
system can generate coherent and correct sentences from a minimum set of words.
From its inception, the design is modular and adaptable to other languages.
This adaptability is one of its greatest advantages. For English, we have
created the highly precise aLexiE lexicon with wide coverage, which represents
a contribution on its own. We have evaluated the resulting NLG library in an
Augmentative and Alternative Communication (AAC) proof of concept, both
directly (by regenerating corpus sentences) and manually (from annotations)
using a popular corpus in the NLG field. We performed a second analysis by
comparing the quality of text expansion in English to Spanish, using an ad-hoc
Spanish-English parallel corpus. The system might also be applied to other
domains such as report and news generation.
`,authors:"Silvia García Méndez; Milagros Fernández Gavilanes; Enrique Costa Montenegro; Jonathan Juncal Martínez; Francisco Javier González Castaño; Ehud Reiter",status:0,relevancy:.4530532782297587,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18315",date:"2024-05-28",title:`DSDL: Data Set Description Language for Bridging Modalities and Tasks in
  AI Data`,abstract:`  In the era of artificial intelligence, the diversity of data modalities and
annotation formats often renders data unusable directly, requiring
understanding and format conversion before it can be used by researchers or
developers with different needs. To tackle this problem, this article
introduces a framework called Dataset Description Language (DSDL) that aims to
simplify dataset processing by providing a unified standard for AI datasets.
DSDL adheres to the three basic practical principles of generic, portable, and
extensible, using a unified standard to express data of different modalities
and structures, facilitating the dissemination of AI data, and easily extending
to new modalities and tasks. The standardized specifications of DSDL reduce the
workload for users in data dissemination, processing, and usage. To further
improve user convenience, we provide predefined DSDL templates for various
tasks, convert mainstream datasets to comply with DSDL specifications, and
provide comprehensive documentation and DSDL tools. These efforts aim to
simplify the use of AI data, thereby improving the efficiency of AI
development.
`,authors:"Bin Wang; Linke Ouyang; Fan Wu; Wenchang Ning; Xiao Han; Zhiyuan Zhao; Jiahui Peng; Yiying Jiang; Dahua Lin; Conghui He",status:0,relevancy:.4509819666706769,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18560",date:"2024-05-28",title:"Potential Field Based Deep Metric Learning",abstract:`  Deep metric learning (DML) involves training a network to learn a
semantically meaningful representation space. Many current approaches mine
n-tuples of examples and model interactions within each tuplets. We present a
novel, compositional DML model, inspired by electrostatic fields in physics
that, instead of in tuples, represents the influence of each example
(embedding) by a continuous potential field, and superposes the fields to
obtain their combined global potential field. We use attractive/repulsive
potential fields to represent interactions among embeddings from images of the
same/different classes. Contrary to typical learning methods, where mutual
influence of samples is proportional to their distance, we enforce reduction in
such influence with distance, leading to a decaying field. We show that such
decay helps improve performance on real world datasets with large intra-class
variations and label noise. Like other proxy-based methods, we also use proxies
to succinctly represent sub-populations of examples. We evaluate our method on
three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where
it outperforms state-of-the-art baselines.
`,authors:"Shubhang Bhatnagar; Narendra Ahuja",status:0,relevancy:.4499224271124421,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17825",date:"2024-05-28",title:"Diffusion Model Patching via Mixture-of-Prompts",abstract:`  We present Diffusion Model Patching (DMP), a simple method to boost the
performance of pre-trained diffusion models that have already reached
convergence, with a negligible increase in parameters. DMP inserts a small,
learnable set of prompts into the model's input space while keeping the
original model frozen. The effectiveness of DMP is not merely due to the
addition of parameters but stems from its dynamic gating mechanism, which
selects and combines a subset of learnable prompts at every step of the
generative process (e.g., reverse denoising steps). This strategy, which we
term "mixture-of-prompts", enables the model to draw on the distinct expertise
of each prompt, essentially "patching" the model's functionality at every step
with minimal yet specialized parameters. Uniquely, DMP enhances the model by
further training on the same dataset on which it was originally trained, even
in a scenario where significant improvements are typically not expected due to
model convergence. Experiments show that DMP significantly enhances the
converged FID of DiT-L/2 on FFHQ 256x256 by 10.38%, achieved with only a 1.43%
parameter increase and 50K additional training iterations.
`,authors:"Seokil Ham; Sangmin Woo; Jin-Young Kim; Hyojun Go; Byeongjun Park; Changick Kim",status:0,relevancy:.44975087428677474,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17784",date:"2024-05-28",title:`Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich
  Differentiable Simulation`,abstract:`  Model-Free Reinforcement Learning~(MFRL), leveraging the policy gradient
theorem, has demonstrated considerable success in continuous control tasks.
However, these approaches are plagued by high gradient variance due to
zeroth-order gradient estimation, resulting in suboptimal policies. Conversely,
First-Order Model-Based Reinforcement Learning~(FO-MBRL) methods, employing
differentiable simulation, provide gradients with reduced variance but are
susceptible to sampling error in scenarios involving stiff dynamics, such as
physical contact. This paper investigates the source of this error and
introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that
reduces gradient error by adapting the model-based horizon to avoid stiff
dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines,
attaining 40\\% more reward across a set of locomotion tasks, and efficiently
scaling to high-dimensional control environments with improved wall-clock-time
efficiency.
`,authors:"Ignat Georgiev; Krishnan Srinivasan; Jie Xu; Eric Heiden; Animesh Garg",status:0,relevancy:.4491718861360514,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18406",date:"2024-05-28",title:`RACCooN: Remove, Add, and Change Video Content with Auto-Generated
  Narratives`,abstract:`  Recent video generative models primarily rely on carefully written text
prompts for specific tasks, like inpainting or style editing. They require
labor-intensive textual descriptions for input videos, hindering their
flexibility to adapt personal/raw videos to user specifications. This paper
proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video
generative framework that supports multiple video editing capabilities such as
removal, addition, and modification, through a unified pipeline. RACCooN
consists of two principal stages: Video-to-Paragraph (V2P) and
Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video
scenes in well-structured natural language, capturing both the holistic context
and focused object details. Subsequently, in the P2V stage, users can
optionally refine these descriptions to guide the video diffusion model,
enabling various modifications to the input video, such as removing, changing
subjects, and/or adding new objects. The proposed approach stands out from
other methods through several significant contributions: (1) RACCooN suggests a
multi-granular spatiotemporal pooling strategy to generate well-structured
video descriptions, capturing both the broad context and object details without
requiring complex human annotations, simplifying precise video content editing
based on text for users. (2) Our video generative model incorporates
auto-generated narratives or instructions to enhance the quality and accuracy
of the generated content. It supports the addition of video objects,
inpainting, and attribute modification within a unified framework, surpassing
existing video editing and inpainting benchmarks. The proposed framework
demonstrates impressive versatile capabilities in video-to-paragraph
generation, video content editing, and can be incorporated into other SoTA
video generative models for further enhancement.
`,authors:"Jaehong Yoon; Shoubin Yu; Mohit Bansal",status:0,relevancy:.44877581009015877,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18610",date:"2024-05-28",title:`DTR-Bench: An in silico Environment and Benchmark Platform for
  Reinforcement Learning Based Dynamic Treatment Regime`,abstract:`  Reinforcement learning (RL) has garnered increasing recognition for its
potential to optimise dynamic treatment regimes (DTRs) in personalised
medicine, particularly for drug dosage prescriptions and medication
recommendations. However, a significant challenge persists: the absence of a
unified framework for simulating diverse healthcare scenarios and a
comprehensive analysis to benchmark the effectiveness of RL algorithms within
these contexts. To address this gap, we introduce \\textit{DTR-Bench}, a
benchmarking platform comprising four distinct simulation environments tailored
to common DTR applications, including cancer chemotherapy, radiotherapy,
glucose management in diabetes, and sepsis treatment. We evaluate various
state-of-the-art RL algorithms across these settings, particularly highlighting
their performance amidst real-world challenges such as
pharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data.
Our experiments reveal varying degrees of performance degradation among RL
algorithms in the presence of noise and patient variability, with some
algorithms failing to converge. Additionally, we observe that using temporal
observation representations does not consistently lead to improved performance
in DTR settings. Our findings underscore the necessity of developing robust,
adaptive RL algorithms capable of effectively managing these complexities to
enhance patient-specific healthcare. We have open-sourced our benchmark and
code at https://github.com/GilesLuo/DTR-Bench.
`,authors:"Zhiyao Luo; Mingcheng Zhu; Fenglin Liu; Jiali Li; Yangchen Pan; Jiandong Zhou; Tingting Zhu",status:0,relevancy:.448666604338693,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17927",date:"2024-05-28",title:"The Evolution of Multimodal Model Architectures",abstract:`  This work uniquely identifies and characterizes four prevalent multimodal
model architectural patterns in the contemporary multimodal landscape.
Systematically categorizing models by architecture type facilitates monitoring
of developments in the multimodal domain. Distinct from recent survey papers
that present general information on multimodal architectures, this research
conducts a comprehensive exploration of architectural details and identifies
four specific architectural types. The types are distinguished by their
respective methodologies for integrating multimodal inputs into the deep neural
network model. The first two types (Type A and B) deeply fuses multimodal
inputs within the internal layers of the model, whereas the following two types
(Type C and D) facilitate early fusion at the input stage. Type-A employs
standard cross-attention, whereas Type-B utilizes custom-designed layers for
modality fusion within the internal layers. On the other hand, Type-C utilizes
modality-specific encoders, while Type-D leverages tokenizers to process the
modalities at the model's input stage. The identified architecture types aid
the monitoring of any-to-any multimodal model development. Notably, Type-C and
Type-D are currently favored in the construction of any-to-any multimodal
models. Type-C, distinguished by its non-tokenizing multimodal model
architecture, is emerging as a viable alternative to Type-D, which utilizes
input-tokenizing techniques. To assist in model selection, this work highlights
the advantages and disadvantages of each architecture type based on data and
compute requirements, architecture complexity, scalability, simplification of
adding modalities, training objectives, and any-to-any multimodal generation
capability.
`,authors:"Shakti N. Wadekar; Abhishek Chaurasia; Aman Chadha; Eugenio Culurciello",status:0,relevancy:.44795431593343815,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17724",date:"2024-05-28",title:`ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion
  Models`,abstract:`  Recent research in tabular data synthesis has focused on single tables,
whereas real-world applications often involve complex data with tens or
hundreds of interconnected tables. Previous approaches to synthesizing
multi-relational (multi-table) data fall short in two key aspects: scalability
for larger datasets and capturing long-range dependencies, such as correlations
between attributes spread across different tables. Inspired by the success of
diffusion models in tabular data modeling, we introduce
  $\\textbf{C}luster$ $\\textbf{La}tent$ $\\textbf{Va}riable$ $guided$
$\\textbf{D}enoising$ $\\textbf{D}iffusion$ $\\textbf{P}robabilistic$
$\\textbf{M}odels$ (ClavaDDPM). This novel approach leverages clustering labels
as intermediaries to model relationships between tables, specifically focusing
on foreign key constraints. ClavaDDPM leverages the robust generation
capabilities of diffusion models while incorporating efficient algorithms to
propagate the learned latent variables across tables. This enables ClavaDDPM to
capture long-range dependencies effectively.
  Extensive evaluations on multi-table datasets of varying sizes show that
ClavaDDPM significantly outperforms existing methods for these long-range
dependencies while remaining competitive on utility metrics for single-table
data.
`,authors:"Wei Pang; Masoumeh Shafieinejad; Lucy Liu; Xi He",status:0,relevancy:.44540486209067287,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18025",date:"2024-05-28",title:`Unveiling the Power of Diffusion Features For Personalized Segmentation
  and Retrieval`,abstract:`  Personalized retrieval and segmentation aim to locate specific instances
within a dataset based on an input image and a short description of the
reference instance. While supervised methods are effective, they require
extensive labeled data for training. Recently, self-supervised foundation
models have been introduced to these tasks showing comparable results to
supervised methods. However, a significant flaw in these models is evident:
they struggle to locate a desired instance when other instances within the same
class are presented. In this paper, we explore text-to-image diffusion models
for these tasks. Specifically, we propose a novel approach called PDM for
Personalized Features Diffusion Matching, that leverages intermediate features
of pre-trained text-to-image models for personalization tasks without any
additional training. PDM demonstrates superior performance on popular retrieval
and segmentation benchmarks, outperforming even supervised methods. We also
highlight notable shortcomings in current instance and segmentation datasets
and propose new benchmarks for these tasks.
`,authors:"Dvir Samuel; Rami Ben-Ari; Matan Levy; Nir Darshan; Gal Chechik",status:0,relevancy:.444737569088822,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18415",date:"2024-05-28",title:"Why are Visually-Grounded Language Models Bad at Image Classification?",abstract:`  Image classification is one of the most fundamental capabilities of machine
vision intelligence. In this work, we revisit the image classification task
using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We
find that existing proprietary and public VLMs, despite often using CLIP as a
vision encoder and having many more parameters, significantly underperform CLIP
on standard image classification benchmarks like ImageNet. To understand the
reason, we explore several hypotheses concerning the inference algorithms,
training objectives, and data processing in VLMs. Our analysis reveals that the
primary cause is data-related: critical information for image classification is
encoded in the VLM's latent space but can only be effectively decoded with
enough training data. Specifically, there is a strong correlation between the
frequency of class exposure during VLM training and instruction-tuning and the
VLM's performance in those classes; when trained with sufficient data, VLMs can
match the accuracy of state-of-the-art classification models. Based on these
findings, we enhance a VLM by integrating classification-focused datasets into
its training, and demonstrate that the enhanced classification performance of
the VLM transfers to its general capabilities, resulting in an improvement of
11.8% on the newly collected ImageWikiQA dataset.
`,authors:"Yuhui Zhang; Alyssa Unell; Xiaohan Wang; Dhruba Ghosh; Yuchang Su; Ludwig Schmidt; Serena Yeung-Levy",status:0,relevancy:.43976592749373355,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18556",date:"2024-05-28",title:`Reinforcement Learning in Dynamic Treatment Regimes Needs Critical
  Reexamination`,abstract:`  In the rapidly changing healthcare landscape, the implementation of offline
reinforcement learning (RL) in dynamic treatment regimes (DTRs) presents a mix
of unprecedented opportunities and challenges. This position paper offers a
critical examination of the current status of offline RL in the context of
DTRs. We argue for a reassessment of applying RL in DTRs, citing concerns such
as inconsistent and potentially inconclusive evaluation metrics, the absence of
naive and supervised learning baselines, and the diverse choice of RL
formulation in existing research. Through a case study with more than 17,000
evaluation experiments using a publicly available Sepsis dataset, we
demonstrate that the performance of RL algorithms can significantly vary with
changes in evaluation metrics and Markov Decision Process (MDP) formulations.
Surprisingly, it is observed that in some instances, RL algorithms can be
surpassed by random baselines subjected to policy evaluation methods and reward
design. This calls for more careful policy evaluation and algorithm development
in future DTR works. Additionally, we discussed potential enhancements toward
more reliable development of RL-based dynamic treatment regimes and invited
further discussion within the community. Code is available at
https://github.com/GilesLuo/ReassessDTR.
`,authors:"Zhiyao Luo; Yangchen Pan; Peter Watkinson; Tingting Zhu",status:0,relevancy:.4370009931794995,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17871",date:"2024-05-28",title:`Seeing the Image: Prioritizing Visual Correlation by Contrastive
  Alignment`,abstract:`  Existing image-text modality alignment in Vision Language Models (VLMs)
treats each text token equally in an autoregressive manner. Despite being
simple and effective, this method results in sub-optimal cross-modal alignment
by over-emphasizing the text tokens that are less correlated with or even
contradictory with the input images. In this paper, we advocate for assigning
distinct contributions for each text token based on its visual correlation.
Specifically, we present by contrasting image inputs, the difference in
prediction logits on each text token provides strong guidance of visual
correlation. We therefore introduce Contrastive ALignment (CAL), a simple yet
effective re-weighting strategy that prioritizes training visually correlated
tokens. Our experimental results demonstrate that CAL consistently improves
different types of VLMs across different resolutions and model sizes on various
benchmark datasets. Importantly, our method incurs minimal additional
computational overhead, rendering it highly efficient compared to alternative
data scaling strategies. Codes are available at
https://github.com/foundation-multimodal-models/CAL.
`,authors:"Xin Xiao; Bohong Wu; Jiacong Wang; Chunyuan Li; Xun Zhou; Haoyuan Guo",status:0,relevancy:.43597643397719865,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17878",date:"2024-05-28",title:"An Information Theoretic Metric for Evaluating Unlearning Models",abstract:`  Machine unlearning (MU) addresses privacy concerns by removing information of
\`forgetting data' samples from trained models. Typically, evaluating MU methods
involves comparing unlearned models to those retrained from scratch without
forgetting data, using metrics such as membership inference attacks (MIA) and
accuracy measurements. These evaluations implicitly assume that if the output
logits of the unlearned and retrained models are similar, the unlearned model
has successfully forgotten the data. Here, we challenge if this assumption is
valid. In particular, we conduct a simple experiment of training only the last
layer of a given original model using a novel masked-distillation technique
while keeping the rest fixed. Surprisingly, simply altering the last layer
yields favorable outcomes in the existing evaluation metrics, while the model
does not successfully unlearn the samples or classes. For better evaluating the
MU methods, we propose a metric that quantifies the residual information about
forgetting data samples in intermediate features using mutual information,
called information difference index or IDI for short. The IDI provides a
comprehensive evaluation of MU methods by efficiently analyzing the internal
structure of DNNs. Our metric is scalable to large datasets and adaptable to
various model architectures. Additionally, we present COLapse-and-Align (COLA),
a simple contrastive-based method that effectively unlearns intermediate
features.
`,authors:"Dongjae Jeon; Wonje Jeung; Taeheon Kim; Albert No; Jonghyun Choi",status:0,relevancy:.4325258785364253,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17829",date:"2024-05-28",title:`LDMol: Text-Conditioned Molecule Diffusion Model Leveraging Chemically
  Informative Latent Space`,abstract:`  With the emergence of diffusion models as the frontline of generative models,
many researchers have proposed molecule generation techniques using conditional
diffusion models. However, due to the fundamental nature of a molecule, which
carries highly entangled correlations within a small number of atoms and bonds,
it becomes difficult for a model to connect raw data with the conditions when
the conditions become more complex as natural language. To address this, here
we present a novel latent diffusion model dubbed LDMol, which enables a natural
text-conditioned molecule generation. Specifically, LDMol is composed of three
building blocks: a molecule encoder that produces a chemically informative
feature space, a natural language-conditioned latent diffusion model using a
Diffusion Transformer (DiT), and an autoregressive decoder for molecule re. In
particular, recognizing that multiple SMILES notations can represent the same
molecule, we employ a contrastive learning strategy to extract the chemical
informative feature space. LDMol not only beats the existing baselines on the
text-to-molecule generation benchmark but is also capable of zero-shot
inference with unseen scenarios. Furthermore, we show that LDMol can be applied
to downstream tasks such as molecule-to-text retrieval and text-driven molecule
editing, demonstrating its versatility as a diffusion model.
`,authors:"Jinho Chang; Jong Chul Ye",status:0,relevancy:.43248361742376906,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18510",date:"2024-05-28",title:`Improved Emotional Alignment of AI and Humans: Human Ratings of Emotions
  Expressed by Stable Diffusion v1, DALL-E 2, and DALL-E 3`,abstract:`  Generative AI systems are increasingly capable of expressing emotions via
text and imagery. Effective emotional expression will likely play a major role
in the efficacy of AI systems -- particularly those designed to support human
mental health and wellbeing. This motivates our present research to better
understand the alignment of AI expressed emotions with the human perception of
emotions. When AI tries to express a particular emotion, how might we assess
whether they are successful? To answer this question, we designed a survey to
measure the alignment between emotions expressed by generative AI and human
perceptions. Three generative image models (DALL-E 2, DALL-E 3 and Stable
Diffusion v1) were used to generate 240 examples of images, each of which was
based on a prompt designed to express five positive and five negative emotions
across both humans and robots. 24 participants recruited from the Prolific
website rated the alignment of AI-generated emotional expressions with a text
prompt used to generate the emotion (i.e., "A robot expressing the emotion
amusement"). The results of our evaluation suggest that generative AI models
are indeed capable of producing emotional expressions that are well-aligned
with a range of human emotions; however, we show that the alignment
significantly depends upon the AI model used and the emotion itself. We analyze
variations in the performance of these systems to identify gaps for future
improvement. We conclude with a discussion of the implications for future AI
systems designed to support mental health and wellbeing.
`,authors:"James Derek Lomas; Willem van der Maden; Sohhom Bandyopadhyay; Giovanni Lion; Nirmal Patel; Gyanesh Jain; Yanna Litowsky; Haian Xue; Pieter Desmet",status:0,relevancy:.432431378752716,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18166",date:"2024-05-28",title:`Defending Large Language Models Against Jailbreak Attacks via
  Layer-specific Editing`,abstract:`  Large language models (LLMs) are increasingly being adopted in a wide range
of real-world applications. Despite their impressive performance, recent
studies have shown that LLMs are vulnerable to deliberately crafted adversarial
prompts even when aligned via Reinforcement Learning from Human Feedback or
supervised fine-tuning. While existing defense methods focus on either
detecting harmful prompts or reducing the likelihood of harmful responses
through various means, defending LLMs against jailbreak attacks based on the
inner mechanisms of LLMs remains largely unexplored. In this work, we
investigate how LLMs response to harmful prompts and propose a novel defense
method termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the
resilience of LLMs against jailbreak attacks. Through LED, we reveal that
several critical \\textit{safety layers} exist among the early layers of LLMs.
We then show that realigning these safety layers (and some selected additional
layers) with the decoded safe response from selected target layers can
significantly improve the alignment of LLMs against jailbreak attacks.
Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the
effectiveness of LED, which effectively defends against jailbreak attacks while
maintaining performance on benign prompts. Our code is available at
\\url{https://github.com/ledllm/ledllm}.
`,authors:"Wei Zhao; Zhe Li; Yige Li; Ye Zhang; Jun Sun",status:0,relevancy:.43063009002870933,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17741",date:"2024-05-28",title:`LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via
  System-Algorithm Co-design`,abstract:`  Recent literature has found that an effective method to customize or further
improve large language models (LLMs) is to add dynamic adapters, such as
low-rank adapters (LoRA) with Mixture-of-Experts (MoE) structures. Though such
dynamic adapters incur modest computational complexity, they surprisingly lead
to huge inference latency overhead, slowing down the decoding speed by 2.5+
times. In this paper, we analyze the fine-grained costs of the dynamic adapters
and find that the fragmented CUDA kernel calls are the root cause. Therefore,
we propose LoRA-Switch, a system-algorithm co-designed architecture for
efficient dynamic adapters. Unlike most existing dynamic structures that adopt
layer-wise or block-wise dynamic routing, LoRA-Switch introduces a token-wise
routing mechanism. It switches the LoRA adapters and weights for each token and
merges them into the backbone for inference. For efficiency, this switching is
implemented with an optimized CUDA kernel, which fuses the merging operations
for all LoRA adapters at once. Based on experiments with popular open-source
LLMs on common benchmarks, our approach has demonstrated similar accuracy
improvement as existing dynamic adapters, while reducing the decoding latency
by more than 2.4 times.
`,authors:"Rui Kong; Qiyang Li; Xinyu Fang; Qingtian Feng; Qingfeng He; Yazhu Dong; Weijun Wang; Yuanchun Li; Linghe Kong; Yunxin Liu",status:0,relevancy:.4301885317883706,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17728",date:"2024-05-28",title:`Facilitating Holistic Evaluations with LLMs: Insights from
  Scenario-Based Experiments`,abstract:`  Workshop courses designed to foster creativity are gaining popularity.
However, achieving a holistic evaluation that accommodates diverse perspectives
is challenging, even for experienced faculty teams. Adequate discussion is
essential to integrate varied assessments, but faculty often lack the time for
such deliberations. Deriving an average score without discussion undermines the
purpose of a holistic evaluation. This paper explores the use of a Large
Language Model (LLM) as a facilitator to integrate diverse faculty assessments.
Scenario-based experiments were conducted to determine if the LLM could
synthesize diverse evaluations and explain the underlying theories to faculty.
The results were noteworthy, showing that the LLM effectively facilitated
faculty discussions. Additionally, the LLM demonstrated the capability to
generalize and create evaluation criteria from a single scenario based on its
learned domain knowledge.
`,authors:"Toru Ishida",status:0,relevancy:.42996538011129903,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18291",date:"2024-05-28",title:`FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in
  Federated Learning`,abstract:`  Collaborative fairness stands as an essential element in federated learning
to encourage client participation by equitably distributing rewards based on
individual contributions. Existing methods primarily focus on adjusting
gradient allocations among clients to achieve collaborative fairness. However,
they frequently overlook crucial factors such as maintaining consistency across
local models and catering to the diverse requirements of high-contributing
clients. This oversight inevitably decreases both fairness and model accuracy
in practice. To address these issues, we propose FedSAC, a novel Federated
learning framework with dynamic Submodel Allocation for Collaborative fairness,
backed by a theoretical convergence guarantee. First, we present the concept of
"bounded collaborative fairness (BCF)", which ensures fairness by tailoring
rewards to individual clients based on their contributions. Second, to
implement the BCF, we design a submodel allocation module with a theoretical
guarantee of fairness. This module incentivizes high-contributing clients with
high-performance submodels containing a diverse range of crucial neurons,
thereby preserving consistency across local models. Third, we further develop a
dynamic aggregation module to adaptively aggregate submodels, ensuring the
equitable treatment of low-frequency neurons and consequently enhancing overall
model accuracy. Extensive experiments conducted on three public benchmarks
demonstrate that FedSAC outperforms all baseline methods in both fairness and
model accuracy. We see this work as a significant step towards incentivizing
broader client participation in federated learning. The source code is
available at https://github.com/wangzihuixmu/FedSAC.
`,authors:"Zihui Wang; Zheng Wang; Lingjuan Lyu; Zhaopeng Peng; Zhicheng Yang; Chenglu Wen; Rongshan Yu; Cheng Wang; Xiaoliang Fan",status:0,relevancy:.4289100644033852,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18620",date:"2024-05-28",title:`RealitySummary: On-Demand Mixed Reality Document Enhancement using Large
  Language Models`,abstract:`  We introduce RealitySummary, a mixed reality reading assistant that can
enhance any printed or digital document using on-demand text extraction,
summarization, and augmentation. While augmented reading tools promise to
enhance physical reading experiences with overlaid digital content, prior
systems have typically required pre-processed documents, which limits their
generalizability and real-world use cases. In this paper, we explore on-demand
document augmentation by leveraging large language models. To understand
generalizable techniques for diverse documents, we first conducted an
exploratory design study which identified five categories of document
enhancements (summarization, augmentation, navigation, comparison, and
extraction). Based on this, we developed a proof-of-concept system that can
automatically extract and summarize text using Google Cloud OCR and GPT-4, then
embed information around documents using a Microsoft Hololens 2 and Apple
Vision Pro. We demonstrate real-time examples of six specific document
augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword
lists, 5) summary highlighting, and 6) information cards. Results from a
usability study (N=12) and in-the-wild study (N=11) highlight the potential
benefits of on-demand MR document enhancement and opportunities for future
research.
`,authors:"Aditya Gunturu; Shivesh Jadon; Nandi Zhang; Jarin Thundathil; Wesley Willett; Ryo Suzuki",status:0,relevancy:.42877027919261557,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18386",date:"2024-05-28",title:`Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language
  Models via Instruction Tuning`,abstract:`  Recent advances in text-to-music editing, which employ text queries to modify
music (e.g.\\ by changing its style or adjusting instrumental components),
present unique challenges and opportunities for AI-assisted music creation.
Previous approaches in this domain have been constrained by the necessity to
train specific editing models from scratch, which is both resource-intensive
and inefficient; other research uses large language models to predict edited
music, resulting in imprecise audio reconstruction. To Combine the strengths
and address these limitations, we introduce Instruct-MusicGen, a novel approach
that finetunes a pretrained MusicGen model to efficiently follow editing
instructions such as adding, removing, or separating stems. Our approach
involves a modification of the original MusicGen architecture by incorporating
a text fusion module and an audio fusion module, which allow the model to
process instruction texts and audio inputs concurrently and yield the desired
edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters
to the original MusicGen model and only trains for 5K steps, yet it achieves
superior performance across all tasks compared to existing baselines, and
demonstrates performance comparable to the models trained for specific tasks.
This advancement not only enhances the efficiency of text-to-music editing but
also broadens the applicability of music language models in dynamic music
production environments.
`,authors:"Yixiao Zhang; Yukara Ikemiya; Woosung Choi; Naoki Murata; Marco A. Martínez-Ramírez; Liwei Lin; Gus Xia; Wei-Hsiang Liao; Yuki Mitsufuji; Simon Dixon",status:0,relevancy:.4275335455986071,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18065",date:"2024-05-28",title:`EffoVPR: Effective Foundation Model Utilization for Visual Place
  Recognition`,abstract:`  The task of Visual Place Recognition (VPR) is to predict the location of a
query image from a database of geo-tagged images. Recent studies in VPR have
highlighted the significant advantage of employing pre-trained foundation
models like DINOv2 for the VPR task. However, these models are often deemed
inadequate for VPR without further fine-tuning on task-specific data. In this
paper, we propose a simple yet powerful approach to better exploit the
potential of a foundation model for VPR. We first demonstrate that features
extracted from self-attention layers can serve as a powerful re-ranker for VPR.
Utilizing these features in a zero-shot manner, our method surpasses previous
zero-shot methods and achieves competitive results compared to supervised
methods across multiple datasets. Subsequently, we demonstrate that a
single-stage method leveraging internal ViT layers for pooling can generate
global features that achieve state-of-the-art results, even when reduced to a
dimensionality as low as 128D. Nevertheless, incorporating our local foundation
features for re-ranking, expands this gap. Our approach further demonstrates
remarkable robustness and generalization, achieving state-of-the-art results,
with a significant gap, in challenging scenarios, involving occlusion,
day-night variations, and seasonal changes.
`,authors:"Issar Tzachor; Boaz Lerner; Matan Levy; Michael Green; Tal Berkovitz Shalev; Gavriel Habib; Dvir Samuel; Noam Korngut Zailer; Or Shimshi; Nir Darshan; Rami Ben-Ari",status:0,relevancy:.4274677593901701,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18507",date:"2024-05-28",title:`Injecting Hierarchical Biological Priors into Graph Neural Networks for
  Flow Cytometry Prediction`,abstract:`  In the complex landscape of hematologic samples such as peripheral blood or
bone marrow derived from flow cytometry (FC) data, cell-level prediction
presents profound challenges. This work explores injecting hierarchical prior
knowledge into graph neural networks (GNNs) for single-cell multi-class
classification of tabular cellular data. By representing the data as graphs and
encoding hierarchical relationships between classes, we propose our
hierarchical plug-in method to be applied to several GNN models, namely,
FCHC-GNN, and effectively designed to capture neighborhood information crucial
for single-cell FC domain. Extensive experiments on our cohort of 19 distinct
patients, demonstrate that incorporating hierarchical biological constraints
boosts performance significantly across multiple metrics compared to baseline
GNNs without such priors. The proposed approach highlights the importance of
structured inductive biases for gaining improved generalization in complex
biological prediction tasks.
`,authors:"Fatemeh Nassajian Mojarrad; Lorenzo Bini; Thomas Matthes; Stéphane Marchand-Maillet",status:0,relevancy:.426226112603169,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18632",date:"2024-05-28",title:"Large Language Models as Partners in Student Essay Evaluation",abstract:`  As the importance of comprehensive evaluation in workshop courses increases,
there is a growing demand for efficient and fair assessment methods that reduce
the workload for faculty members. This paper presents an evaluation conducted
with Large Language Models (LLMs) using actual student essays in three
scenarios: 1) without providing guidance such as rubrics, 2) with pre-specified
rubrics, and 3) through pairwise comparison of essays. Quantitative analysis of
the results revealed a strong correlation between LLM and faculty member
assessments in the pairwise comparison scenario with pre-specified rubrics,
although concerns about the quality and stability of evaluations remained.
Therefore, we conducted a qualitative analysis of LLM assessment comments,
showing that: 1) LLMs can match the assessment capabilities of faculty members,
2) variations in LLM assessments should be interpreted as diversity rather than
confusion, and 3) assessments by humans and LLMs can differ and complement each
other. In conclusion, this paper suggests that LLMs should not be seen merely
as assistants to faculty members but as partners in evaluation committees and
outlines directions for further research.
`,authors:"Toru Ishida; Tongxi Liu; Hailong Wang; William K. Cheung",status:0,relevancy:.4238266887285501,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18165",date:"2024-05-28",title:"Time Series Representation Models",abstract:`  Time series analysis remains a major challenge due to its sparse
characteristics, high dimensionality, and inconsistent data quality. Recent
advancements in transformer-based techniques have enhanced capabilities in
forecasting and imputation; however, these methods are still resource-heavy,
lack adaptability, and face difficulties in integrating both local and global
attributes of time series. To tackle these challenges, we propose a new
architectural concept for time series analysis based on introspection. Central
to this concept is the self-supervised pretraining of Time Series
Representation Models (TSRMs), which once learned can be easily tailored and
fine-tuned for specific tasks, such as forecasting and imputation, in an
automated and resource-efficient manner. Our architecture is equipped with a
flexible and hierarchical representation learning process, which is robust
against missing data and outliers. It can capture and learn both local and
global features of the structure, semantics, and crucial patterns of a given
time series category, such as heart rate data. Our learned time series
representation models can be efficiently adapted to a specific task, such as
forecasting or imputation, without manual intervention. Furthermore, our
architecture's design supports explainability by highlighting the significance
of each input value for the task at hand. Our empirical study using four
benchmark datasets shows that, compared to investigated state-of-the-art
baseline methods, our architecture improves imputation and forecasting errors
by up to 90.34% and 71.54%, respectively, while reducing the required trainable
parameters by up to 92.43%. The source code is available at
https://github.com/RobertLeppich/TSRM.
`,authors:"Robert Leppich; Vanessa Borst; Veronika Lesch; Samuel Kounev",status:0,relevancy:.41983046121433154,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17918",date:"2024-05-28",title:`Cost-Sensitive Multi-Fidelity Bayesian Optimization with Transfer of
  Learning Curve Extrapolation`,abstract:`  In this paper, we address the problem of cost-sensitive multi-fidelity
Bayesian Optimization (BO) for efficient hyperparameter optimization (HPO).
Specifically, we assume a scenario where users want to early-stop the BO when
the performance improvement is not satisfactory with respect to the required
computational cost. Motivated by this scenario, we introduce utility, which is
a function predefined by each user and describes the trade-off between cost and
performance of BO. This utility function, combined with our novel acquisition
function and stopping criterion, allows us to dynamically choose for each BO
step the best configuration that we expect to maximally improve the utility in
future, and also automatically stop the BO around the maximum utility. Further,
we improve the sample efficiency of existing learning curve (LC) extrapolation
methods with transfer learning, while successfully capturing the correlations
between different configurations to develop a sensible surrogate function for
multi-fidelity BO. We validate our algorithm on various LC datasets and found
it outperform all the previous multi-fidelity BO and transfer-BO baselines we
consider, achieving significantly better trade-off between cost and performance
of BO.
`,authors:"Dong Bok Lee; Aoxuan Silvia Zhang; Byungjoo Kim; Junhyeon Park; Juho Lee; Sung Ju Hwang; Hae Beom Lee",status:0,relevancy:.4133124845818694,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17924",date:"2024-05-28",title:`Generative AI Enhances Team Performance and Reduces Need for Traditional
  Teams`,abstract:`  Recent advancements in generative artificial intelligence (AI) have
transformed collaborative work processes, yet the impact on team performance
remains underexplored. Here we examine the role of generative AI in enhancing
or replacing traditional team dynamics using a randomized controlled experiment
with 435 participants across 122 teams. We show that teams augmented with
generative AI significantly outperformed those relying solely on human
collaboration across various performance measures. Interestingly, teams with
multiple AIs did not exhibit further gains, indicating diminishing returns with
increased AI integration. Our analysis suggests that centralized AI usage by a
few team members is more effective than distributed engagement. Additionally,
individual-AI pairs matched the performance of conventional teams, suggesting a
reduced need for traditional team structures in some contexts. However, despite
this capability, individual-AI pairs still fell short of the performance levels
achieved by AI-assisted teams. These findings underscore that while generative
AI can replace some traditional team functions, more comprehensively
integrating AI within team structures provides superior benefits, enhancing
overall effectiveness beyond individual efforts.
`,authors:"Ning Li; Huaikang Zhou; Kris Mikel-Hong",status:0,relevancy:.4115967548791559,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18040",date:"2024-05-28",title:`Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew
  Resilience`,abstract:`  Federated learning (FL) has recently emerged as a compelling machine learning
paradigm, prioritizing the protection of privacy for training data. The
increasing demand to address issues such as \`\`the right to be forgotten'' and
combat data poisoning attacks highlights the importance of techniques, known as
\\textit{unlearning}, which facilitate the removal of specific training data
from trained FL models. Despite numerous unlearning methods proposed for
centralized learning, they often prove inapplicable to FL due to fundamental
differences in the operation of the two learning paradigms. Consequently,
unlearning in FL remains in its early stages, presenting several challenges.
Many existing unlearning solutions in FL require a costly retraining process,
which can be burdensome for clients. Moreover, these methods are primarily
validated through experiments, lacking theoretical assurances. In this study,
we introduce Fast-FedUL, a tailored unlearning method for FL, which eliminates
the need for retraining entirely. Through meticulous analysis of the target
client's influence on the global model in each round, we develop an algorithm
to systematically remove the impact of the target client from the trained
model. In addition to presenting empirical findings, we offer a theoretical
analysis delineating the upper bound of our unlearned model and the exact
retrained model (the one obtained through retraining using untargeted clients).
Experimental results with backdoor attack scenarios indicate that Fast-FedUL
effectively removes almost all traces of the target client, while retaining the
knowledge of untargeted clients (obtaining a high accuracy of up to 98\\% on the
main task). Significantly, Fast-FedUL attains the lowest time complexity,
providing a speed that is 1000 times faster than retraining. Our source code is
publicly available at \\url{https://github.com/thanhtrunghuynh93/fastFedUL}.
`,authors:"Thanh Trung Huynh; Trong Bang Nguyen; Phi Le Nguyen; Thanh Tam Nguyen; Matthias Weidlich; Quoc Viet Hung Nguyen; Karl Aberer",status:0,relevancy:.40989976161506747,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17814",date:"2024-05-28",title:`FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in
  Text-to-Image Models`,abstract:`  The rapid development and reduced barriers to entry for Text-to-Image (T2I)
models have raised concerns about the biases in their outputs, but existing
research lacks a holistic definition and evaluation framework of biases,
limiting the enhancement of debiasing techniques. To address this issue, we
introduce FAIntbench, a holistic and precise benchmark for biases in T2I
models. In contrast to existing benchmarks that evaluate bias in limited
aspects, FAIntbench evaluate biases from four dimensions: manifestation of
bias, visibility of bias, acquired attributes, and protected attributes. We
applied FAIntbench to evaluate seven recent large-scale T2I models and
conducted human evaluation, whose results demonstrated the effectiveness of
FAIntbench in identifying various biases. Our study also revealed new research
questions about biases, including the side-effect of distillation. The findings
presented here are preliminary, highlighting the potential of FAIntbench to
advance future research aimed at mitigating the biases in T2I models. Our
benchmark is publicly available to ensure the reproducibility.
`,authors:"Hanjun Luo; Ziye Deng; Ruizhe Chen; Zuozhu Liu",status:0,relevancy:.40739744638283404,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18327",date:"2024-05-28",title:`Histopathology Based AI Model Predicts Anti-Angiogenic Therapy Response
  in Renal Cancer Clinical Trial`,abstract:`  Predictive biomarkers of treatment response are lacking for metastatic clear
cell renal cell carcinoma (ccRCC), a tumor type that is treated with
angiogenesis inhibitors, immune checkpoint inhibitors, mTOR inhibitors and a
HIF2 inhibitor. The Angioscore, an RNA-based quantification of angiogenesis, is
arguably the best candidate to predict anti-angiogenic (AA) response. However,
the clinical adoption of transcriptomic assays faces several challenges
including standardization, time delay, and high cost. Further, ccRCC tumors are
highly heterogenous, and sampling multiple areas for sequencing is impractical.
Here we present a novel deep learning (DL) approach to predict the Angioscore
from ubiquitous histopathology slides. To overcome the lack of
interpretability, one of the biggest limitations of typical DL models, our
model produces a visual vascular network which is the basis of the model's
prediction. To test its reliability, we applied this model to multiple cohorts
including a clinical trial dataset. Our model accurately predicts the RNA-based
Angioscore on multiple independent cohorts (spearman correlations of 0.77 and
0.73). Further, the predictions help unravel meaningful biology such as
association of angiogenesis with grade, stage, and driver mutation status.
Finally, we find our model can predict response to AA therapy, in both a
real-world cohort and the IMmotion150 clinical trial. The predictive power of
our model vastly exceeds that of CD31, a marker of vasculature, and nearly
rivals the performance (c-index 0.66 vs 0.67) of the ground truth RNA-based
Angioscore at a fraction of the cost. By providing a robust yet interpretable
prediction of the Angioscore from histopathology slides alone, our approach
offers insights into angiogenesis biology and AA treatment response.
`,authors:"Jay Jasti; Hua Zhong; Vandana Panwar; Vipul Jarmale; Jeffrey Miyata; Deyssy Carrillo; Alana Christie; Dinesh Rakheja; Zora Modrusan; Edward Ernest Kadel III; Niha Beig; Mahrukh Huseni; James Brugarolas; Payal Kapur; Satwik Rajaram",status:0,relevancy:.40642693768662375,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18148",date:"2024-05-28",title:`Learning to Detour: Shortcut Mitigating Augmentation for Weakly
  Supervised Semantic Segmentation`,abstract:`  Weakly supervised semantic segmentation (WSSS) employing weak forms of labels
has been actively studied to alleviate the annotation cost of acquiring
pixel-level labels. However, classifiers trained on biased datasets tend to
exploit shortcut features and make predictions based on spurious correlations
between certain backgrounds and objects, leading to a poor generalization
performance. In this paper, we propose shortcut mitigating augmentation (SMA)
for WSSS, which generates synthetic representations of object-background
combinations not seen in the training data to reduce the use of shortcut
features. Our approach disentangles the object-relevant and background
features. We then shuffle and combine the disentangled representations to
create synthetic features of diverse object-background combinations.
SMA-trained classifier depends less on contexts and focuses more on the target
object when making predictions. In addition, we analyzed the behavior of the
classifier on shortcut usage after applying our augmentation using an
attribution method-based metric. The proposed method achieved the improved
performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014
datasets.
`,authors:"JuneHyoung Kwon; Eunju Lee; Yunsung Cho; YoungBin Kim",status:0,relevancy:.40573900463812274,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18322",date:"2024-05-28",title:`SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder
  for Self-Supervised Landmark Estimation`,abstract:`  Self-supervised landmark estimation is a challenging task that demands the
formation of locally distinct feature representations to identify sparse facial
landmarks in the absence of annotated data. To tackle this task, existing
state-of-the-art (SOTA) methods (1) extract coarse features from backbones that
are trained with instance-level self-supervised learning (SSL) paradigms, which
neglect the dense prediction nature of the task, (2) aggregate them into
memory-intensive hypercolumn formations, and (3) supervise lightweight
projector networks to naively establish full local correspondences among all
pairs of spatial features. In this paper, we introduce SCE-MAE, a framework
that (1) leverages the MAE, a region-level SSL method that naturally better
suits the landmark prediction task, (2) operates on the vanilla feature map
instead of on expensive hypercolumns, and (3) employs a Correspondence
Approximation and Refinement Block (CARB) that utilizes a simple density peak
clustering algorithm and our proposed Locality-Constrained Repellence Loss to
directly hone only select local correspondences. We demonstrate through
extensive experiments that SCE-MAE is highly effective and robust,
outperforming existing SOTA methods by large margins of approximately 20%-44%
on the landmark matching and approximately 9%-15% on the landmark detection
tasks.
`,authors:"Kejia Yin; Varshanth R. Rao; Ruowei Jiang; Xudong Liu; Parham Aarabi; David B. Lindell",status:0,relevancy:.40572639872939775,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18425",date:"2024-05-28",title:`ViG: Linear-complexity Visual Sequence Learning with Gated Linear
  Attention`,abstract:`  Recently, linear complexity sequence modeling networks have achieved modeling
capabilities similar to Vision Transformers on a variety of computer vision
tasks, while using fewer FLOPs and less memory. However, their advantage in
terms of actual runtime speed is not significant. To address this issue, we
introduce Gated Linear Attention (GLA) for vision, leveraging its superior
hardware-awareness and efficiency. We propose direction-wise gating to capture
1D global context through bidirectional modeling and a 2D gating locality
injection to adaptively inject 2D local details into 1D global context. Our
hardware-aware implementation further merges forward and backward scanning into
a single kernel, enhancing parallelism and reducing memory cost and latency.
The proposed model, ViG, offers a favorable trade-off in accuracy, parameters,
and FLOPs on ImageNet and downstream tasks, outperforming popular Transformer
and CNN-based models. Notably, ViG-S matches DeiT-B's accuracy while using only
27% of the parameters and 20% of the FLOPs, running 2$\\times$ faster on
$224\\times224$ images. At $1024\\times1024$ resolution, ViG-T uses 5.2$\\times$
fewer FLOPs, saves 90% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7%
higher top-1 accuracy than DeiT-T. These results position ViG as an efficient
and scalable solution for visual representation learning. Code is available at
\\url{https://github.com/hustvl/ViG}.
`,authors:"Bencheng Liao; Xinggang Wang; Lianghui Zhu; Qian Zhang; Chang Huang",status:0,relevancy:.40567791159791344,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17894",date:"2024-05-28",title:"White-box Multimodal Jailbreaks Against Large Vision-Language Models",abstract:`  Recent advancements in Large Vision-Language Models (VLMs) have underscored
their superiority in various multimodal tasks. However, the adversarial
robustness of VLMs has not been fully explored. Existing methods mainly assess
robustness through unimodal adversarial attacks that perturb images, while
assuming inherent resilience against text-based attacks. Different from
existing attacks, in this work we propose a more comprehensive strategy that
jointly attacks both text and image modalities to exploit a broader spectrum of
vulnerability within VLMs. Specifically, we propose a dual optimization
objective aimed at guiding the model to generate affirmative responses with
high toxicity. Our attack method begins by optimizing an adversarial image
prefix from random noise to generate diverse harmful responses in the absence
of text input, thus imbuing the image with toxic semantics. Subsequently, an
adversarial text suffix is integrated and co-optimized with the adversarial
image prefix to maximize the probability of eliciting affirmative responses to
various harmful instructions. The discovered adversarial image prefix and text
suffix are collectively denoted as a Universal Master Key (UMK). When
integrated into various malicious queries, UMK can circumvent the alignment
defenses of VLMs and lead to the generation of objectionable content, known as
jailbreaks. The experimental results demonstrate that our universal attack
strategy can effectively jailbreak MiniGPT-4 with a 96% success rate,
highlighting the vulnerability of VLMs and the urgent need for new alignment
strategies.
`,authors:"Ruofan Wang; Xingjun Ma; Hanxu Zhou; Chuanjun Ji; Guangnan Ye; Yu-Gang Jiang",status:0,relevancy:.40417267133145773,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18335",date:"2024-05-28",title:"Interpretable classification of wiki-review streams",abstract:`  Wiki articles are created and maintained by a crowd of editors, producing a
continuous stream of reviews. Reviews can take the form of additions, reverts,
or both. This crowdsourcing model is exposed to manipulation since neither
reviews nor editors are automatically screened and purged. To protect articles
against vandalism or damage, the stream of reviews can be mined to classify
reviews and profile editors in real-time. The goal of this work is to
anticipate and explain which reviews to revert. This way, editors are informed
why their edits will be reverted. The proposed method employs stream-based
processing, updating the profiling and classification models on each incoming
event. The profiling uses side and content-based features employing Natural
Language Processing, and editor profiles are incrementally updated based on
their reviews. Since the proposed method relies on self-explainable
classification algorithms, it is possible to understand why a review has been
classified as a revert or a non-revert. In addition, this work contributes an
algorithm for generating synthetic data for class balancing, making the final
classification fairer. The proposed online method was tested with a real data
set from Wikivoyage, which was balanced through the aforementioned synthetic
data generation. The results attained near-90 % values for all evaluation
metrics (accuracy, precision, recall, and F-measure).
`,authors:"Silvia García Méndez; Fátima Leal; Benedita Malheiro; Juan Carlos Burguillo Rial",status:0,relevancy:.4038534301778801,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17957",date:"2024-05-28",title:`Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking
  Contrastive Learning and Unassociated Word Exclusion`,abstract:`  Dynamic topic models track the evolution of topics in sequential documents,
which have derived various applications like trend analysis and opinion mining.
However, existing models suffer from repetitive topic and unassociated topic
issues, failing to reveal the evolution and hindering further applications. To
address these issues, we break the tradition of simply chaining topics in
existing work and propose a novel neural \\modelfullname. We introduce a new
evolution-tracking contrastive learning method that builds the similarity
relations among dynamic topics. This not only tracks topic evolution but also
maintains topic diversity, mitigating the repetitive topic issue. To avoid
unassociated topics, we further present an unassociated word exclusion method
that consistently excludes unassociated words from discovered topics. Extensive
experiments demonstrate our model significantly outperforms state-of-the-art
baselines, tracking topic evolution with high-quality topics, showing better
performance on downstream tasks, and remaining robust to the hyperparameter for
evolution intensities. Our code is available at https://github.com/bobxwu/CFDTM .
`,authors:"Xiaobao Wu; Xinshuai Dong; Liangming Pan; Thong Nguyen; Anh Tuan Luu",status:0,relevancy:.40346648234587934,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18003",date:"2024-05-28",title:`MAVIN: Multi-Action Video Generation with Diffusion Models via
  Transition Video Infilling`,abstract:`  Diffusion-based video generation has achieved significant progress, yet
generating multiple actions that occur sequentially remains a formidable task.
Directly generating a video with sequential actions can be extremely
challenging due to the scarcity of fine-grained action annotations and the
difficulty in establishing temporal semantic correspondences and maintaining
long-term consistency. To tackle this, we propose an intuitive and
straightforward solution: splicing multiple single-action video segments
sequentially. The core challenge lies in generating smooth and natural
transitions between these segments given the inherent complexity and
variability of action transitions. We introduce MAVIN (Multi-Action Video
INfilling model), designed to generate transition videos that seamlessly
connect two given videos, forming a cohesive integrated sequence. MAVIN
incorporates several innovative techniques to address challenges in the
transition video infilling task. Firstly, a consecutive noising strategy
coupled with variable-length sampling is employed to handle large infilling
gaps and varied generation lengths. Secondly, boundary frame guidance (BFG) is
proposed to address the lack of semantic guidance during transition generation.
Lastly, a Gaussian filter mixer (GFM) dynamically manages noise initialization
during inference, mitigating train-test discrepancy while preserving generation
flexibility. Additionally, we introduce a new metric, CLIP-RS (CLIP Relative
Smoothness), to evaluate temporal coherence and smoothness, complementing
traditional quality-based metrics. Experimental results on horse and tiger
scenarios demonstrate MAVIN's superior performance in generating smooth and
coherent video transitions compared to existing methods.
`,authors:"Bowen Zhang; Xiaofei Xie; Haotian Lu; Na Ma; Tianlin Li; Qing Guo",status:0,relevancy:.4032169511470619,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17874",date:"2024-05-28",title:"NUTS, NARS, and Speech",abstract:`  To investigate whether "Intelligence is the capacity of an
information-processing system to adapt to its environment while operating with
insufficient knowledge and resources", we look at utilising the non axiomatic
reasoning system (NARS) for speech recognition. This article presents NUTS:
raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for
perception. NUTS consists of naive dimensionality reduction, some
pre-processing, and then non axiomatic reasoning (NARS). With only 2 training
examples NUTS performs similarly to the Whisper Tiny model for discrete word
identification.
`,authors:"D. van der Sluis",status:0,relevancy:.4030182594399745,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17940",date:"2024-05-28",title:"World Models for General Surgical Grasping",abstract:`  Intelligent vision control systems for surgical robots should adapt to
unknown and diverse objects while being robust to system disturbances. Previous
methods did not meet these requirements due to mainly relying on pose
estimation and feature tracking. We propose a world-model-based deep
reinforcement learning framework "Grasp Anything for Surgery" (GAS), that
learns a pixel-level visuomotor policy for surgical grasping, enhancing both
generality and robustness. In particular, a novel method is proposed to
estimate the values and uncertainties of depth pixels for a rigid-link object's
inaccurate region based on the empirical prior of the object's size; both depth
and mask images of task objects are encoded to a single compact 3-channel image
(size: 64x64x3) by dynamically zooming in the mask regions, minimizing the
information loss. The learned controller's effectiveness is extensively
evaluated in simulation and in a real robot. Our learned visuomotor policy
handles: i) unseen objects, including 5 types of target grasping objects and a
robot gripper, in unstructured real-world surgery environments, and ii)
disturbances in perception and control. Note that we are the first work to
achieve a unified surgical control system that grasps diverse surgical objects
using different robot grippers on real robots in complex surgery scenes
(average success rate: 69%). Our system also demonstrates significant
robustness across 6 conditions including background variation, target
disturbance, camera pose variation, kinematic control error, image noise, and
re-grasping after the gripped target object drops from the gripper. Videos and
codes can be found on our project page: https://linhongbin.github.io/gas/.
`,authors:"Hongbin Lin; Bin Li; Chun Wai Wong; Juan Rojas; Xiangyu Chu; Kwok Wai Samuel Au",status:0,relevancy:.4002606149039689,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18663",date:"2024-05-28",title:"Lifelong Learning and Selective Forgetting via Contrastive Strategy",abstract:`  Lifelong learning aims to train a model with good performance for new tasks
while retaining the capacity of previous tasks. However, some practical
scenarios require the system to forget undesirable knowledge due to privacy
issues, which is called selective forgetting. The joint task of the two is
dubbed Learning with Selective Forgetting (LSF). In this paper, we propose a
new framework based on contrastive strategy for LSF. Specifically, for the
preserved classes (tasks), we make features extracted from different samples
within a same class compacted. And for the deleted classes, we make the
features from different samples of a same class dispersed and irregular, i.e.,
the network does not have any regular response to samples from a specific
deleted class as if the network has no training at all. Through maintaining or
disturbing the feature distribution, the forgetting and memory of different
classes can be or independent of each other. Experiments are conducted on four
benchmark datasets, and our method acieves new state-of-the-art.
`,authors:"Lianlei Shan; Wenzhang Zhou; Wei Li; Xingyu Ding",status:0,relevancy:.3979575634270527,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17942",date:"2024-05-28",title:"Self-supervised Pre-training for Transferable Multi-modal Perception",abstract:`  In autonomous driving, multi-modal perception models leveraging inputs from
multiple sensors exhibit strong robustness in degraded environments. However,
these models face challenges in efficiently and effectively transferring
learned representations across different modalities and tasks. This paper
presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised
pre-training paradigm for transferable multi-modal representation learning.
NS-MAE is designed to provide pre-trained model initializations for efficient
and high-performance fine-tuning. Our approach uses masked multi-modal
reconstruction in neural radiance fields (NeRF), training the model to
reconstruct missing or corrupted input data across multiple modalities.
Specifically, multi-modal embeddings are extracted from corrupted LiDAR point
clouds and images, conditioned on specific view directions and locations. These
embeddings are then rendered into projected multi-modal feature maps using
neural rendering techniques. The original multi-modal signals serve as
reconstruction targets for the rendered feature maps, facilitating
self-supervised representation learning. Extensive experiments demonstrate the
promising transferability of NS-MAE representations across diverse multi-modal
and single-modal perception models. This transferability is evaluated on
various 3D perception downstream tasks, such as 3D object detection and BEV map
segmentation, using different amounts of fine-tuning labeled data. Our code
will be released to support the community.
`,authors:"Xiaohao Xu; Tianyi Zhang; Jinrong Yang; Matthew Johnson-Roberson; Xiaonan Huang",status:0,relevancy:.3967230227177043,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18014",date:"2024-05-28",title:`Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space
  Model`,abstract:`  The essence of multi-modal fusion lies in exploiting the complementary
information inherent in diverse modalities. However, prevalent fusion methods
rely on traditional neural architectures and are inadequately equipped to
capture the dynamics of interactions across modalities, particularly in
presence of complex intra- and inter-modality correlations. Recent advancements
in State Space Models (SSMs), notably exemplified by the Mamba model, have
emerged as promising contenders. Particularly, its state evolving process
implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an
appealing direction. However, fusing multiple modalities is challenging for
SSMs due to its hardware-aware parallelism designs. To this end, this paper
proposes the Coupled SSM model, for coupling state chains of multiple
modalities while maintaining independence of intra-modality state processes.
Specifically, in our coupled scheme, we devise an inter-modal hidden states
transition scheme, in which the current state is dependent on the states of its
own chain and that of the neighbouring chains at the previous time-step. To
fully comply with the hardware-aware parallelism, we devise an expedite coupled
state transition scheme and derive its corresponding global convolution kernel
for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through
multi-domain input verify the effectiveness of our model compared to current
state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the
three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save.
The results demonstrate that Coupled Mamba model is capable of enhanced
multi-modal fusion.
`,authors:"Wenbing Li; Hang Zhou; Junqing Yu; Zikai Song; Wei Yang",status:0,relevancy:.39009738529761795,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17766",date:"2024-05-28",title:`SleepFM: Multi-modal Representation Learning for Sleep Across Brain
  Activity, ECG and Respiratory Signals`,abstract:`  Sleep is a complex physiological process evaluated through various modalities
recording electrical brain, cardiac, and respiratory activities. We curate a
large polysomnography dataset from over 14,000 participants comprising over
100,000 hours of multi-modal sleep recordings. Leveraging this extensive
dataset, we developed SleepFM, the first multi-modal foundation model for sleep
analysis. We show that a novel leave-one-out approach for contrastive learning
significantly improves downstream task performance compared to representations
from standard pairwise contrastive learning. A logistic regression model
trained on SleepFM's learned embeddings outperforms an end-to-end trained
convolutional neural network (CNN) on sleep stage classification (macro AUROC
0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing
detection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61). Notably, the learned
embeddings achieve 48% top-1 average accuracy in retrieving the corresponding
recording clips of other modalities from 90,000 candidates. This work
demonstrates the value of holistic multi-modal sleep modeling to fully capture
the richness of sleep recordings. SleepFM is open source and available at
https://github.com/rthapa84/sleepfm-codebase.
`,authors:"Rahul Thapa; Bryan He; Magnus Ruud Kjaer; Hyatt Moore; Gauri Ganjoo; Emmanuel Mignot; James Zou",status:0,relevancy:.3898255435339463,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17838",date:"2024-05-28",title:`Trust and Terror: Hazards in Text Reveal Negatively Biased Credulity and
  Partisan Negativity Bias`,abstract:`  Socio-linguistic indicators of text, such as emotion or sentiment, are often
extracted using neural networks in order to better understand features of
social media. One indicator that is often overlooked, however, is the presence
of hazards within text. Recent psychological research suggests that statements
about hazards are more believable than statements about benefits (a property
known as negatively biased credulity), and that political liberals and
conservatives differ in how often they share hazards. Here, we develop a new
model to detect information concerning hazards, trained on a new collection of
annotated X posts, as well as urban legends annotated in previous work. We show
that not only does this model perform well (outperforming, e.g., zero-shot
human annotator proxies, such as GPT-4) but that the hazard information it
extracts is not strongly correlated with other indicators, namely moral
outrage, sentiment, emotions, and threat words. (That said, consonant with
expectations, hazard information does correlate positively with such emotions
as fear, and negatively with emotions like joy.) We then apply this model to
three datasets: X posts about COVID-19, X posts about the 2023 Hamas-Israel
war, and a new expanded collection of urban legends. From these data, we
uncover words associated with hazards unique to each dataset as well as
differences in this language between groups of users, such as conservatives and
liberals, which informs what these groups perceive as hazards. We further show
that information about hazards peaks in frequency after major hazard events,
and therefore acts as an automated indicator of such events. Finally, we find
that information about hazards is especially prevalent in urban legends, which
is consistent with previous work that finds that reports of hazards are more
likely to be both believed and transmitted.
`,authors:"Keith Burghardt; Daniel M. T. Fessler; Chyna Tang; Anne Pisor; Kristina Lerman",status:0,relevancy:.3885782353072148,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17934",date:"2024-05-28",title:`Proof of Quality: A Costless Paradigm for Trustless Generative AI Model
  Inference on Blockchains`,abstract:`  Generative AI models, such as GPT-4 and Stable Diffusion, have demonstrated
powerful and disruptive capabilities in natural language and image tasks.
However, deploying these models in decentralized environments remains
challenging. Unlike traditional centralized deployment, systematically
guaranteeing the integrity of AI model services in fully decentralized
environments, particularly on trustless blockchains, is both crucial and
difficult. In this paper, we present a new inference paradigm called
\\emph{proof of quality} (PoQ) to enable the deployment of arbitrarily large
generative models on blockchain architecture. Unlike traditional approaches
based on validating inference procedures, such as ZKML or OPML, our PoQ
paradigm focuses on the outcome quality of model inference. Using lightweight
BERT-based cross-encoders as our underlying quality evaluation model, we design
and implement PQML, the first practical protocol for real-world NLP generative
model inference on blockchains, tailored for popular open-source models such as
Llama 3 and Mixtral. Our analysis demonstrates that our protocol is robust
against adversarial but rational participants in ecosystems, where lazy or
dishonest behavior results in fewer benefits compared to well-behaving
participants. The computational overhead of validating the quality evaluation
is minimal, allowing quality validators to complete the quality check within a
second, even using only a CPU. Preliminary simulation results show that PoQ
consensus is generated in milliseconds, 1,000 times faster than any existing
scheme.
`,authors:"Zhenjie Zhang; Yuyang Rao; Hao Xiao; Xiaokui Xiao; Yin Yang",status:0,relevancy:.38675751504487776,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17746",date:"2024-05-28",title:"Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective",abstract:`  Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks,
posing concerning threats to their reliable deployment. Recent research reveals
that backdoors can be erased from infected DNNs by pruning a specific group of
neurons, while how to effectively identify and remove these backdoor-associated
neurons remains an open challenge. Most of the existing defense methods rely on
defined rules and focus on neuron's local properties, ignoring the exploration
and optimization of pruning policies. To address this gap, we propose an
Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN)
and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP
first models the target DNN as graphs based on neuron connectivity, and then
uses GNN-based RL agents to learn graph embeddings and find a suitable pruning
policy. To the best of our knowledge, this is the first attempt to employ GNN
and RL for optimizing pruning policies in the field of backdoor defense.
Experiments show, with a small amount of clean data, ONP can effectively prune
the backdoor neurons implanted by a set of backdoor attacks at the cost of
negligible performance degradation, achieving a new state-of-the-art
performance for backdoor mitigation.
`,authors:"Nan Li; Haiyang Yu; Ping Yi",status:0,relevancy:.3860347237446391,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17992",date:"2024-05-28",title:`fMRI predictors based on language models of increasing complexity
  recover brain left lateralization`,abstract:`  Over the past decade, studies of naturalistic language processing where
participants are scanned while listening to continuous text have flourished.
Using word embeddings at first, then large language models, researchers have
created encoding models to analyze the brain signals. Presenting these models
with the same text as the participants allows to identify brain areas where
there is a significant correlation between the functional magnetic resonance
imaging (fMRI) time series and the ones predicted by the models' artificial
neurons. One intriguing finding from these studies is that they have revealed
highly symmetric bilateral activation patterns, somewhat at odds with the
well-known left lateralization of language processing. Here, we report analyses
of an fMRI dataset where we manipulate the complexity of large language models,
testing 28 pretrained models from 8 different families, ranging from 124M to
14.2B parameters. First, we observe that the performance of models in
predicting brain responses follows a scaling law, where the fit with brain
activity increases linearly with the logarithm of the number of parameters of
the model (and its performance on natural language processing tasks). Second,
we show that a left-right asymmetry gradually appears as model size increases,
and that the difference in left-right brain correlations also follows a scaling
law. Whereas the smallest models show no asymmetry, larger models fit better
and better left hemispheric activations than right hemispheric ones. This
finding reconciles computational analyses of brain activity using large
language models with the classic observation from aphasic patients showing left
hemisphere dominance for language.
`,authors:"Laurent Bonnasse-Gahot; Christophe Pallier",status:0,relevancy:.380465478549635,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18636",date:"2024-05-28",title:`ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of
  AI Content Governance?`,abstract:`  As one of the most enduring metaphors within legal discourse, the marketplace
of ideas has wielded considerable influence over the jurisprudential landscape
for decades. A century after the inception of this theory, ChatGPT emerged as a
revolutionary technological advancement in the twenty-first century. This
research finds that ChatGPT effectively manifests the marketplace metaphor. It
not only instantiates the promises envisaged by generations of legal scholars
but also lays bare the perils discerned through sustained academic critique.
Specifically, the workings of ChatGPT and the marketplace of ideas theory
exhibit at least four common features: arena, means, objectives, and flaws.
These shared attributes are sufficient to render ChatGPT historically the most
qualified engine for actualizing the marketplace of ideas theory.
  The comparison of the marketplace theory and ChatGPT merely marks a starting
point. A more meaningful undertaking entails reevaluating and reframing both
internal and external AI policies by referring to the accumulated experience,
insights, and suggestions researchers have raised to fix the marketplace
theory. Here, a pivotal issue is: should truth-seeking be set as the goal of AI
content governance? Given the unattainability of the absolute truth-seeking
goal, I argue against adopting zero-risk policies. Instead, a more judicious
approach would be to embrace a knowledge-based alternative wherein large
language models (LLMs) are trained to generate competing and divergent
viewpoints based on sufficient justifications. This research also argues that
so-called AI content risks are not created by AI companies but are inherent in
the entire information ecosystem. Thus, the burden of managing these risks
should be distributed among different social actors, rather than being solely
shouldered by chatbot companies.
`,authors:"Jiawei Zhang",status:0,relevancy:.3804020367454448,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18428",date:"2024-05-28",title:"DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention",abstract:`  Diffusion models with large-scale pre-training have achieved significant
success in the field of visual content generation, particularly exemplified by
Diffusion Transformers (DiT). However, DiT models have faced challenges with
scalability and quadratic complexity efficiency. In this paper, we aim to
leverage the long sequence modeling capability of Gated Linear Attention (GLA)
Transformers, expanding its applicability to diffusion models. We introduce
Diffusion Gated Linear Attention Transformers (DiG), a simple, adoptable
solution with minimal parameter overhead, following the DiT design, but
offering superior efficiency and effectiveness. In addition to better
performance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than
DiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.
Moreover, we analyze the scalability of DiG across a variety of computational
complexity. DiG models, with increased depth/width or augmentation of input
tokens, consistently exhibit decreasing FID. We further compare DiG with other
subquadratic-time diffusion models. With the same model size, DiG-XL/2 is
$4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$
resolution, and is $1.8\\times$ faster than DiT with CUDA-optimized
FlashAttention-2 under the $2048$ resolution. All these results demonstrate its
superior efficiency among the latest diffusion models. Code is released at
https://github.com/hustvl/DiG.
`,authors:"Lianghui Zhu; Zilong Huang; Bencheng Liao; Jun Hao Liew; Hanshu Yan; Jiashi Feng; Xinggang Wang",status:0,relevancy:.378326993971983,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18602",date:"2024-05-28",title:`SST-GCN: The Sequential based Spatio-Temporal Graph Convolutional
  networks for Minute-level and Road-level Traffic Accident Risk Predictio`,abstract:`  Traffic accidents are recognized as a major social issue worldwide, causing
numerous injuries and significant costs annually. Consequently, methods for
predicting and preventing traffic accidents have been researched for many
years. With advancements in the field of artificial intelligence, various
studies have applied Machine Learning and Deep Learning techniques to traffic
accident prediction. Modern traffic conditions change rapidly by the minute,
and these changes vary significantly across different roads. In other words,
the risk of traffic accidents changes minute by minute in various patterns for
each road. Therefore, it is desirable to predict traffic accident risk at the
Minute-Level and Road-Level. However, because roads have close and complex
relationships with adjacent roads, research on predicting traffic accidents at
the Minute-Level and Road-Level is challenging. Thus, it is essential to build
a model that can reflect the spatial and temporal characteristics of roads for
traffic accident prediction. Consequently, recent attempts have been made to
use Graph Convolutional Networks to capture the spatial characteristics of
roads and Recurrent Neural Networks to capture their temporal characteristics
for predicting traffic accident risk. This paper proposes the Sequential based
Spatio-Temporal Graph Convolutional Networks (SST-GCN), which combines GCN and
LSTM, to predict traffic accidents at the Minute-Level and Road-Level using a
road dataset constructed in Seoul, the capital of South Korea. Experiments have
demonstrated that SST-GCN outperforms other state-of-the-art models in
Minute-Level predictions.
`,authors:"Tae-wook Kim; Han-jin Lee; Hyeon-Jin Jung; Ji-Woong Yang; Ellen J. Hong",status:0,relevancy:.3735551232509764,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18300",date:"2024-05-28",title:"CompetEvo: Towards Morphological Evolution from Competition",abstract:`  Training an agent to adapt to specific tasks through co-optimization of
morphology and control has widely attracted attention. However, whether there
exists an optimal configuration and tactics for agents in a multiagent
competition scenario is still an issue that is challenging to definitively
conclude. In this context, we propose competitive evolution (CompetEvo), which
co-evolves agents' designs and tactics in confrontation. We build arenas
consisting of three animals and their evolved derivatives, placing agents with
different morphologies in direct competition with each other. The results
reveal that our method enables agents to evolve a more suitable design and
strategy for fighting compared to fixed-morph agents, allowing them to obtain
advantages in combat scenarios. Moreover, we demonstrate the amazing and
impressive behaviors that emerge when confrontations are conducted under
asymmetrical morphs.
`,authors:"Kangyao Huang; Di Guo; Xinyu Zhang; Xiangyang Ji; Huaping Liu",status:0,relevancy:.3731645532957174,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18064",date:"2024-05-28",title:`Automated Real-World Sustainability Data Generation from Images of
  Buildings`,abstract:`  When data on building features is unavailable, the task of determining how to
improve that building in terms of carbon emissions becomes infeasible. We show
that from only a set of images, a Large Language Model with appropriate prompt
engineering and domain knowledge can successfully estimate a range of building
features relevant for sustainability calculations. We compare our novel
image-to-data method with a ground truth comprising real building data for 47
apartments and achieve accuracy better than a human performing the same task.
We also demonstrate that the method can generate tailored recommendations to
the owner on how best to improve their properties and discuss methods to scale
the approach.
`,authors:"Peter J Bentley; Soo Ling Lim; Rajat Mathur; Sid Narang",status:0,relevancy:.37260089535019003,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18137",date:"2024-05-28",title:"Exploiting LLM Quantization",abstract:`  Quantization leverages lower-precision weights to reduce the memory usage of
large language models (LLMs) and is a key technique for enabling their
deployment on commodity hardware. While LLM quantization's impact on utility
has been extensively explored, this work for the first time studies its adverse
effects from a security perspective. We reveal that widely used quantization
methods can be exploited to produce a harmful quantized LLM, even though the
full-precision counterpart appears benign, potentially tricking users into
deploying the malicious quantized model. We demonstrate this threat using a
three-staged attack framework: (i) first, we obtain a malicious LLM through
fine-tuning on an adversarial task; (ii) next, we quantize the malicious model
and calculate constraints that characterize all full-precision models that map
to the same quantized model; (iii) finally, using projected gradient descent,
we tune out the poisoned behavior from the full-precision model while ensuring
that its weights satisfy the constraints computed in step (ii). This procedure
results in an LLM that exhibits benign behavior in full precision but when
quantized, it follows the adversarial behavior injected in step (i). We
experimentally demonstrate the feasibility and severity of such an attack
across three diverse scenarios: vulnerable code generation, content injection,
and over-refusal attack. In practice, the adversary could host the resulting
full-precision model on an LLM community hub such as Hugging Face, exposing
millions of users to the threat of deploying its malicious quantized version on
their devices.
`,authors:"Kazuki Egashira; Mark Vero; Robin Staab; Jingxuan He; Martin Vechev",status:0,relevancy:.3688298202796041,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17898",date:"2024-05-28",title:`FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic
  Prediction`,abstract:`  The objective of traffic prediction is to accurately forecast and analyze the
dynamics of transportation patterns, considering both space and time. However,
the presence of distribution shift poses a significant challenge in this field,
as existing models struggle to generalize well when faced with test data that
significantly differs from the training distribution. To tackle this issue,
this paper introduces a simple and universal spatio-temporal prompt-tuning
framework-FlashST, which adapts pre-trained models to the specific
characteristics of diverse downstream datasets, improving generalization in
diverse traffic prediction scenarios. Specifically, the FlashST framework
employs a lightweight spatio-temporal prompt network for in-context learning,
capturing spatio-temporal invariant knowledge and facilitating effective
adaptation to diverse scenarios. Additionally, we incorporate a distribution
mapping mechanism to align the data distributions of pre-training and
downstream data, facilitating effective knowledge transfer in spatio-temporal
forecasting. Empirical evaluations demonstrate the effectiveness of our FlashST
across different spatio-temporal prediction tasks using diverse urban datasets.
Code is available at https://github.com/HKUDS/FlashST.
`,authors:"Zhonghang Li; Lianghao Xia; Yong Xu; Chao Huang",status:0,relevancy:.3686578294078132,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17910",date:"2024-05-28",title:`Human-Cobot collaboration's impact on success, time completion, errors,
  workload, gestures and acceptability during an assembly task`,abstract:`  The 5.0 industry promotes collaborative robots (cobots). This research
studies the impacts of cobot collaboration using an experimental setup. 120
participants realized a simple and a complex assembly task. 50% collaborated
with another human (H/H) and 50% with a cobot (H/C). The workload and the
acceptability of the cobotic collaboration were measured. Working with a cobot
decreases the effect of the task complexity on the human workload and on the
output quality. However, it increases the time completion and the number of
gestures (while decreasing their frequency). The H/C couples have a higher
chance of success but they take more time and more gestures to realize the
task. The results of this research could help developers and stakeholders to
understand the impacts of implementing a cobot in production chains.
`,authors:"Étienne Fournier; Christine Jeoffrion; Belal Hmedan; Damien Pellier; Humbert Fiorino; Aurélie Landry",status:0,relevancy:.364853025509027,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17720",date:"2024-05-28",title:`MindFormer: A Transformer Architecture for Multi-Subject Brain Decoding
  via fMRI`,abstract:`  Research efforts to understand neural signals have been ongoing for many
years, with visual decoding from fMRI signals attracting considerable
attention. Particularly, the advent of image diffusion models has advanced the
reconstruction of images from fMRI data significantly. However, existing
approaches often introduce inter- and intra- subject variations in the
reconstructed images, which can compromise accuracy. To address current
limitations in multi-subject brain decoding, we introduce a new Transformer
architecture called MindFormer. This model is specifically designed to generate
fMRI-conditioned feature vectors that can be used for conditioning Stable
Diffusion model. More specifically, MindFormer incorporates two key
innovations: 1) a novel training strategy based on the IP-Adapter to extract
semantically meaningful features from fMRI signals, and 2) a subject specific
token and linear layer that effectively capture individual differences in fMRI
signals while synergistically combines multi subject fMRI data for training.
Our experimental results demonstrate that Stable Diffusion, when integrated
with MindFormer, produces semantically consistent images across different
subjects. This capability significantly surpasses existing models in
multi-subject brain decoding. Such advancements not only improve the accuracy
of our reconstructions but also deepen our understanding of neural processing
variations among individuals.
`,authors:"Inhwa Han; Jaayeon Lee; Jong Chul Ye",status:0,relevancy:.36230292825655397,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17849",date:"2024-05-28",title:`I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
  Large Language Models`,abstract:`  Post-training quantization (PTQ) serves as a potent technique to accelerate
the inference of large language models (LLMs). Nonetheless, existing works
still necessitate a considerable number of floating-point (FP) operations
during inference, including additional quantization and de-quantization, as
well as non-linear operators such as RMSNorm and Softmax. This limitation
hinders the deployment of LLMs on the edge and cloud devices. In this paper, we
identify the primary obstacle to integer-only quantization for LLMs lies in the
large fluctuation of activations across channels and tokens in both linear and
non-linear operations. To address this issue, we propose I-LLM, a novel
integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1)
we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth
inter-channel variations of all activations and weights. (2) to alleviate
degradation caused by inter-token variations, we introduce a novel approach
called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic
quantization in full-integer matrix multiplication by dynamically quantizing
the input and outputs with integer-only operations. (3) we design
DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to
execute non-linear operators efficiently while maintaining accuracy. The
experiment shows that our I-LLM achieves comparable accuracy to the FP baseline
and outperforms non-integer quantization methods. For example, I-LLM can
operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the
first to bridge the gap between integer-only quantization and LLMs. We've
published our code on anonymous.4open.science, aiming to contribute to the
advancement of this field.
`,authors:"Xing Hu; Yuan Chen; Dawei Yang; Sifan Zhou; Zhihang Yuan; Jiangyong Yu; Chen Xu",status:0,relevancy:.3600921885200329,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18119",date:"2024-05-28",title:`Low-Resource Crop Classification from Multi-Spectral Time Series Using
  Lossless Compressors`,abstract:`  Deep learning has significantly improved the accuracy of crop classification
using multispectral temporal data. However, these models have complex
structures with numerous parameters, requiring large amounts of data and costly
training. In low-resource situations with fewer labeled samples, deep learning
models perform poorly due to insufficient data. Conversely, compressors are
data-type agnostic, and non-parametric methods do not bring underlying
assumptions. Inspired by this insight, we propose a non-training alternative to
deep learning models, aiming to address these situations. Specifically, the
Symbolic Representation Module is proposed to convert the reflectivity into
symbolic representations. The symbolic representations are then
cross-transformed in both the channel and time dimensions to generate symbolic
embeddings. Next, the Multi-scale Normalised Compression Distance (MNCD) is
designed to measure the correlation between any two symbolic embeddings.
Finally, based on the MNCDs, high quality crop classification can be achieved
using only a k-nearest-neighbor classifier kNN. The entire framework is
ready-to-use and lightweight. Without any training, it outperformed, on
average, 7 advanced deep learning models trained at scale on three benchmark
datasets. It also outperforms more than half of these models in the few-shot
setting with sparse crop labels. Therefore, the high performance and robustness
of our non-training framework makes it truly applicable to real-world crop
mapping. Codes are available at:
https://github.com/qinfengsama/Compressor-Based-Crop-Mapping.
`,authors:"Wei Cheng; Hongrui Ye; Xiao Wen; Jiachen Zhang; Jiping Xu; Feifan Zhang",status:0,relevancy:.3599758879307282,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17976",date:"2024-05-28",title:"Yuan 2.0-M32: Mixture of Experts with Attention Router",abstract:`  Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a
mixture-of-experts architecture with 32 experts of which 2 experts are active.
A new router network, Attention Router, is proposed and adopted for a more
efficient selection of experts, which improves the accuracy compared to the
model with classical router network. Yuan 2.0-M32 is trained with 2000B tokens
from scratch, and the training computation consumption is only 9.25% of a dense
model at the same parameter scale. Yuan 2.0-M32 demonstrates competitive
capability on coding, math, and various domains of expertise, with only 3.7B
active parameters of 40B in total, and 7.4 GFlops forward computation per
token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass
Llama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8
respectively. The models and source codes of Yuan 2.0-M32 are released at
Github1.
`,authors:"Shaohua Wu; Jiangang Luo; Xi Chen; Lingjun Li; Xudong Zhao; Tong Yu; Chao Wang; Yue Wang; Fei Wang; Weixu Qiao; Houbo He; Zeru Zhang; Zeyu Sun; Junxiong Mao; Chong Shen",status:0,relevancy:.35573598707865905,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.19376",date:"2024-05-28",title:`PureEBM: Universal Poison Purification via Mid-Run Dynamics of
  Energy-Based Models`,abstract:`  Data poisoning attacks pose a significant threat to the integrity of machine
learning models by leading to misclassification of target distribution test
data by injecting adversarial examples during training. Existing
state-of-the-art (SoTA) defense methods suffer from a variety of limitations,
such as significantly reduced generalization performance, specificity to
particular attack types and classifiers, and significant overhead during
training, making them impractical or limited for real-world applications. In
response to this challenge, we introduce a universal data purification method
that defends naturally trained classifiers from malicious white-, gray-, and
black-box image poisons by applying a universal stochastic preprocessing step
$\\Psi_{T}(x)$, realized by iterative Langevin sampling of a convergent Energy
Based Model (EBM) initialized with an image $x.$ Mid-run dynamics of
$\\Psi_{T}(x)$ purify poison information with minimal impact on features
important to the generalization of a classifier network. We show that the
contrastive learning process of EBMs allows them to remain universal purifiers,
even in the presence of poisoned EBM training data, and to achieve SoTA defense
on leading triggered poison Narcissus and triggerless poisons Gradient Matching
and Bullseye Polytope. This work is a subset of a larger framework introduced
in PureGen with a more detailed focus on EBM purification and poison defense.
`,authors:"Omead Pooladzandi; Jeffrey Jiang; Sunay Bhat; Gregory Pottie",status:0,relevancy:.35327272309902313,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18655",date:"2024-05-28",title:`CAVACHON: a hierarchical variational autoencoder to integrate
  multi-modal single-cell data`,abstract:`  Paired single-cell sequencing technologies enable the simultaneous
measurement of complementary modalities of molecular data at single-cell
resolution. Along with the advances in these technologies, many methods based
on variational autoencoders have been developed to integrate these data.
However, these methods do not explicitly incorporate prior biological
relationships between the data modalities, which could significantly enhance
modeling and interpretation. We propose a novel probabilistic learning
framework that explicitly incorporates conditional independence relationships
between multi-modal data as a directed acyclic graph using a generalized
hierarchical variational autoencoder. We demonstrate the versatility of our
framework across various applications pertinent to single-cell multi-omics data
integration. These include the isolation of common and distinct information
from different modalities, modality-specific differential analysis, and
integrated cell clustering. We anticipate that the proposed framework can
facilitate the construction of highly flexible graphical models that can
capture the complexities of biological hypotheses and unravel the connections
between different biological data types, such as different modalities of paired
single-cell multi-omics data. The implementation of the proposed framework can
be found in the repository https://github.com/kuijjerlab/CAVACHON.
`,authors:"Ping-Han Hsieh; Ru-Xiu Hsiao; Katalin Ferenc; Anthony Mathelier; Rebekka Burkholz; Chien-Yu Chen; Geir Kjetil Sandve; Tatiana Belova; Marieke Lydia Kuijjer",status:0,relevancy:.35166433201430425,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18580",date:"2024-05-28",title:`Artificial Intelligence in Industry 4.0: A Review of Integration
  Challenges for Industrial Systems`,abstract:`  In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that
can be leveraged by Artificial Intelligence (AI) for applications including
predictive maintenance and production planning. However, despite the
demonstrated potential of AI, its widespread adoption in sectors like
manufacturing remains limited. Our comprehensive review of recent literature,
including standards and reports, pinpoints key challenges: system integration,
data-related issues, managing workforce-related concerns and ensuring
trustworthy AI. A quantitative analysis highlights particular challenges and
topics that are important for practitioners but still need to be sufficiently
investigated by academics. The paper briefly discusses existing solutions to
these challenges and proposes avenues for future research. We hope that this
survey serves as a resource for practitioners evaluating the cost-benefit
implications of AI in CPS and for researchers aiming to address these urgent
challenges.
`,authors:"Alexander Windmann; Philipp Wittenberg; Marvin Schieseck; Oliver Niggemann",status:0,relevancy:.35158690085126376,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18320",date:"2024-05-28",title:"Self-Supervised Learning Based Handwriting Verification",abstract:`  We present SSL-HV: Self-Supervised Learning approaches applied to the task of
Handwriting Verification. This task involves determining whether a given pair
of handwritten images originate from the same or different writer distribution.
We have compared the performance of multiple generative, contrastive SSL
approaches against handcrafted feature extractors and supervised learning on
CEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)
outperforms other generative approaches achieving 76.3% accuracy, while
ResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization
(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using
a pre-trained VAE and VICReg for the downstream task of writer verification we
observed a relative improvement in accuracy of 6.7% and 9% over ResNet-18
supervised baseline with 10% writer labels.
`,authors:"Mihir Chauhan; Mohammad Abuzar Shaikh; Bina Ramamurthy; Mingchen Gao; Siwei Lyu; Sargur Srihari",status:0,relevancy:.34925654159443875,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18246",date:"2024-05-28",title:"Utilitarian Algorithm Configuration for Infinite Parameter Spaces",abstract:`  Utilitarian algorithm configuration is a general-purpose technique for
automatically searching the parameter space of a given algorithm to optimize
its performance, as measured by a given utility function, on a given set of
inputs. Recently introduced utilitarian configuration procedures offer
optimality guarantees about the returned parameterization while provably
adapting to the hardness of the underlying problem. However, the applicability
of these approaches is severely limited by the fact that they only search a
finite, relatively small set of parameters. They cannot effectively search the
configuration space of algorithms with continuous or uncountable parameters. In
this paper we introduce a new procedure, which we dub COUP (Continuous,
Optimistic Utilitarian Procrastination). COUP is designed to search infinite
parameter spaces efficiently to find good configurations quickly. Furthermore,
COUP maintains the theoretical benefits of previous utilitarian configuration
procedures when applied to finite parameter spaces but is significantly faster,
both provably and experimentally.
`,authors:"Devon Graham; Kevin Leyton-Brown",status:0,relevancy:.34869925130912893,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18492",date:"2024-05-28",title:`LLMs and Memorization: On Quality and Specificity of Copyright
  Compliance`,abstract:`  Memorization in large language models (LLMs) is a growing concern. LLMs have
been shown to easily reproduce parts of their training data, including
copyrighted work. This is an important problem to solve, as it may violate
existing copyright laws as well as the European AI Act. In this work, we
propose a systematic analysis to quantify the extent of potential copyright
infringements in LLMs using European law as an example. Unlike previous work,
we evaluate instruction-finetuned models in a realistic end-user scenario. Our
analysis builds on a proposed threshold of 160 characters, which we borrow from
the German Copyright Service Provider Act and a fuzzy text matching algorithm
to identify potentially copyright-infringing textual reproductions. The
specificity of countermeasures against copyright infringement is analyzed by
comparing model behavior on copyrighted and public domain data. We investigate
what behaviors models show instead of producing protected text (such as refusal
or hallucination) and provide a first legal assessment of these behaviors. We
find that there are huge differences in copyright compliance, specificity, and
appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous
perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing
a particularly low absolute number of potential copyright violations. Code will
be published soon.
`,authors:"Felix B Mueller; Rebekka Görge; Anna K Bernzen; Janna C Pirk; Maximilian Poretschkin",status:0,relevancy:.3367144131530433,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18548",date:"2024-05-28",title:`The Computational Complexity of Formal Reasoning for Encoder-Only
  Transformers`,abstract:`  We investigate challenges and possibilities of formal reasoning for
encoder-only transformers (EOT), meaning sound and complete methods for
verifying or interpreting behaviour. In detail, we condense related formal
reasoning tasks in the form of a naturally occurring satisfiability problem
(SAT). We find that SAT is undecidable if we consider EOT, commonly considered
in the expressiveness community. Furthermore, we identify practical scenarios
where SAT is decidable and establish corresponding complexity bounds. Besides
trivial cases, we find that quantized EOT, namely those restricted by some
fixed-width arithmetic, lead to the decidability of SAT due to their limited
attention capabilities. However, the problem remains difficult, as we establish
those scenarios where SAT is NEXPTIME-hard and those where we can show that it
is solvable in NEXPTIME for quantized EOT. To complement our theoretical
results, we put our findings and their implications in the overall perspective
of formal reasoning.
`,authors:"Marco Sälzer; Eric Alsmann; Martin Lange",status:0,relevancy:.33240166909271296,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17802",date:"2024-05-28",title:`Multi-level Interaction Modeling for Protein Mutational Effect
  Prediction`,abstract:`  Protein-protein interactions are central mediators in many biological
processes. Accurately predicting the effects of mutations on interactions is
crucial for guiding the modulation of these interactions, thereby playing a
significant role in therapeutic development and drug discovery. Mutations
generally affect interactions hierarchically across three levels: mutated
residues exhibit different sidechain conformations, which lead to changes in
the backbone conformation, eventually affecting the binding affinity between
proteins. However, existing methods typically focus only on sidechain-level
interaction modeling, resulting in suboptimal predictions. In this work, we
propose a self-supervised multi-level pre-training framework, ProMIM, to fully
capture all three levels of interactions with well-designed pretraining
objectives. Experiments show ProMIM outperforms all the baselines on the
standard benchmark, especially on mutations where significant changes in
backbone conformations may occur. In addition, leading results from zero-shot
evaluations for SARS-CoV-2 mutational effect prediction and antibody
optimization underscore the potential of ProMIM as a powerful next-generation
tool for developing novel therapeutic approaches and new drugs.
`,authors:"Yuanle Mo; Xin Hong; Bowen Gao; Yinjun Jia; Yanyan Lan",status:0,relevancy:.3319137877067708,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18172",date:"2024-05-28",title:`AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across
  Any Scenario`,abstract:`  While image-based virtual try-on has made significant strides, emerging
approaches still fall short of delivering high-fidelity and robust fitting
images across various scenarios, as their models suffer from issues of
ill-fitted garment styles and quality degrading during the training process,
not to mention the lack of support for various combinations of attire.
Therefore, we first propose a lightweight, scalable, operator known as Hydra
Block for attire combinations. This is achieved through a parallel attention
mechanism that facilitates the feature injection of multiple garments from
conditionally encoded branches into the main network. Secondly, to
significantly enhance the model's robustness and expressiveness in real-world
scenarios, we evolve its potential across diverse settings by synthesizing the
residuals of multiple models, as well as implementing a mask region boost
strategy to overcome the instability caused by information leakage in existing
models. Equipped with the above design, AnyFit surpasses all baselines on
high-resolution benchmarks and real-world data by a large gap, excelling in
producing well-fitting garments replete with photorealistic and rich details.
Furthermore, AnyFit's impressive performance on high-fidelity virtual try-ons
in any scenario from any image, paves a new path for future research within the
fashion community.
`,authors:"Yuhan Li; Hao Zhou; Wenxiang Shang; Ran Lin; Xuanhong Chen; Bingbing Ni",status:0,relevancy:.3279623426657089,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17921",date:"2024-05-28",title:"Towards Clinical AI Fairness: Filling Gaps in the Puzzle",abstract:`  The ethical integration of Artificial Intelligence (AI) in healthcare
necessitates addressing fairness-a concept that is highly context-specific
across medical fields. Extensive studies have been conducted to expand the
technical components of AI fairness, while tremendous calls for AI fairness
have been raised from healthcare. Despite this, a significant disconnect
persists between technical advancements and their practical clinical
applications, resulting in a lack of contextualized discussion of AI fairness
in clinical settings. Through a detailed evidence gap analysis, our review
systematically pinpoints several deficiencies concerning both healthcare data
and the provided AI fairness solutions. We highlight the scarcity of research
on AI fairness in many medical domains where AI technology is increasingly
utilized. Additionally, our analysis highlights a substantial reliance on group
fairness, aiming to ensure equality among demographic groups from a macro
healthcare system perspective; in contrast, individual fairness, focusing on
equity at a more granular level, is frequently overlooked. To bridge these
gaps, our review advances actionable strategies for both the healthcare and AI
research communities. Beyond applying existing AI fairness methods in
healthcare, we further emphasize the importance of involving healthcare
professionals to refine AI fairness concepts and methods to ensure contextually
relevant and ethically sound AI applications in healthcare.
`,authors:"Mingxuan Liu; Yilin Ning; Salinelat Teixayavong; Xiaoxuan Liu; Mayli Mertens; Yuqing Shang; Xin Li; Di Miao; Jie Xu; Daniel Shu Wei Ting; Lionel Tim-Ee Cheng; Jasmine Chiat Ling Ong; Zhen Ling Teo; Ting Fang Tan; Narrendar RaviChandran; Fei Wang; Leo Anthony Celi; Marcus Eng Hock Ong; Nan Liu",status:0,relevancy:.32604417023489884,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18658",date:"2024-05-28",title:`D-CoRP: Differentiable Connectivity Refinement for Functional Brain
  Networks`,abstract:`  Brain network is an important tool for understanding the brain, offering
insights for scientific research and clinical diagnosis. Existing models for
brain networks typically primarily focus on brain regions or overlook the
complexity of brain connectivities. MRI-derived brain network data is commonly
susceptible to connectivity noise, underscoring the necessity of incorporating
connectivities into the modeling of brain networks. To address this gap, we
introduce a differentiable module for refining brain connectivity. We develop
the multivariate optimization based on information bottleneck theory to address
the complexity of the brain network and filter noisy or redundant connections.
Also, our method functions as a flexible plugin that is adaptable to most graph
neural networks. Our extensive experimental results show that the proposed
method can significantly improve the performance of various baseline models and
outperform other state-of-the-art methods, indicating the effectiveness and
generalizability of the proposed method in refining brain network connectivity.
The code will be released for public availability.
`,authors:"Haoyu Hu; Hongrun Zhang; Chao Li",status:0,relevancy:.32580228008524625,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18387",date:"2024-05-28",title:`A Review and Implementation of Object Detection Models and Optimizations
  for Real-time Medical Mask Detection during the COVID-19 Pandemic`,abstract:`  Convolutional Neural Networks (CNN) are commonly used for the problem of
object detection thanks to their increased accuracy. Nevertheless, the
performance of CNN-based detection models is ambiguous when detection speed is
considered. To the best of our knowledge, there has not been sufficient
evaluation of the available methods in terms of the speed/accuracy trade-off in
related literature. This work assesses the most fundamental object detection
models on the Common Objects in Context (COCO) dataset with respect to this
trade-off, their memory consumption, and computational and storage cost. Next,
we select a highly efficient model called YOLOv5 to train on the topical and
unexplored dataset of human faces with medical masks, the Properly-Wearing
Masked Faces Dataset (PWMFD), and analyze the benefits of specific optimization
techniques for real-time medical mask detection: transfer learning, data
augmentations, and a Squeeze-and-Excitation attention mechanism. Using our
findings in the context of the COVID-19 pandemic, we propose an optimized model
based on YOLOv5s using transfer learning for the detection of correctly and
incorrectly worn medical masks that surpassed more than two times in speed (69
frames per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset
while maintaining the same level of mean Average Precision (67%).
`,authors:"Ioanna Gogou; Dimitrios Koutsomitropoulos",status:0,relevancy:.32400493530131347,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18426",date:"2024-05-28",title:"GFlow: Recovering 4D World from Monocular Video",abstract:`  Reconstructing 4D scenes from video inputs is a crucial yet challenging task.
Conventional methods usually rely on the assumptions of multi-view video
inputs, known camera parameters, or static scenes, all of which are typically
absent under in-the-wild scenarios. In this paper, we relax all these
constraints and tackle a highly ambitious but practical task, which we termed
as AnyV4D: we assume only one monocular video is available without any camera
parameters as input, and we aim to recover the dynamic 4D world alongside the
camera poses. To this end, we introduce GFlow, a new framework that utilizes
only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit
representation, entailing a flow of Gaussian splatting through space and time.
GFlow first clusters the scene into still and moving parts, then applies a
sequential optimization process that optimizes camera poses and the dynamics of
3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity
among neighboring points and smooth movement across frames. Since dynamic
scenes always introduce new content, we also propose a new pixel-wise
densification strategy for Gaussian points to integrate new visual content.
Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also
enables tracking of any points across frames without the need for prior
training and segments moving objects from the scene in an unsupervised way.
Additionally, the camera poses of each frame can be derived from GFlow,
allowing for rendering novel views of a video scene through changing camera
pose. By employing the explicit representation, we may readily conduct
scene-level or object-level editing as desired, underscoring its versatility
and power. Visit our project website at: https://littlepure2333.github.io/GFlow
`,authors:"Shizun Wang; Xingyi Yang; Qiuhong Shen; Zhenxiang Jiang; Xinchao Wang",status:0,relevancy:.3188414046306072,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18077",date:"2024-05-28",title:`Design Principles for Falsifiable, Replicable and Reproducible Empirical
  ML Research`,abstract:`  Empirical research plays a fundamental role in the machine learning domain.
At the heart of impactful empirical research lies the development of clear
research hypotheses, which then shape the design of experiments. The execution
of experiments must be carried out with precision to ensure reliable results,
followed by statistical analysis to interpret these outcomes. This process is
key to either supporting or refuting initial hypotheses. Despite its
importance, there is a high variability in research practices across the
machine learning community and no uniform understanding of quality criteria for
empirical research. To address this gap, we propose a model for the empirical
research process, accompanied by guidelines to uphold the validity of empirical
research. By embracing these recommendations, greater consistency, enhanced
reliability and increased impact can be achieved.
`,authors:"Daniel Vranješ; Oliver Niggemann",status:0,relevancy:.3180171849494975,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18627",date:"2024-05-28",title:`PureGen: Universal Data Purification for Train-Time Poison Defense via
  Generative Model Dynamics`,abstract:`  Train-time data poisoning attacks threaten machine learning models by
introducing adversarial examples during training, leading to misclassification.
Current defense methods often reduce generalization performance, are
attack-specific, and impose significant training overhead. To address this, we
introduce a set of universal data purification methods using a stochastic
transform, $\\Psi(x)$, realized via iterative Langevin dynamics of Energy-Based
Models (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both. These
approaches purify poisoned data with minimal impact on classifier
generalization. Our specially trained EBMs and DDPMs provide state-of-the-art
defense against various attacks (including Narcissus, Bullseye Polytope,
Gradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing
attack or classifier-specific information. We discuss performance trade-offs
and show that our methods remain highly effective even with poisoned or
distributionally shifted generative model training data.
`,authors:"Sunay Bhat; Jeffrey Jiang; Omead Pooladzandi; Alexander Branch; Gregory Pottie",status:0,relevancy:.317629100171516,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18016",date:"2024-05-28",title:"On Creativity and Open-Endedness",abstract:`  Artificial Life (ALife) as an interdisciplinary field draws inspiration and
influence from a variety of perspectives. Scientific progress crucially
depends, then, on concerted efforts to invite cross-disciplinary dialogue. The
goal of this paper is to revitalize discussions of potential connections
between the fields of Computational Creativity (CC) and ALife, focusing
specifically on the concept of Open-Endedness (OE); the primary goal of CC is
to endow artificial systems with creativity, and ALife has dedicated much
research effort into studying and synthesizing OE and artificial innovation.
However, despite the close proximity of these concepts, their use so far
remains confined to their respective communities, and their relationship is
largely unclear. We provide historical context for research in both domains,
and review the limited work connecting research on creativity and OE
explicitly. We then highlight specific questions to be considered, with the
eventual goals of (i) decreasing conceptual ambiguity by highlighting
similarities and differences between the concepts of OE, (ii) identifying
synergy effects of a research agenda that encompasses both OE and creativity,
and (iii) establishing a dialogue between ALife and CC research.
`,authors:"Lisa Soros; Alyssa Adams; Stefano Kalonaris; Olaf Witkowski; Christian Guckelsberger",status:0,relevancy:.31665143342258717,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18427",date:"2024-05-28",title:`Classifying Overlapping Gaussian Mixtures in High Dimensions: From
  Optimal Classifiers to Neural Nets`,abstract:`  We derive closed-form expressions for the Bayes optimal decision boundaries
in binary classification of high dimensional overlapping Gaussian mixture model
(GMM) data, and show how they depend on the eigenstructure of the class
covariances, for particularly interesting structured data. We empirically
demonstrate, through experiments on synthetic GMMs inspired by real-world data,
that deep neural networks trained for classification, learn predictors which
approximate the derived optimal classifiers. We further extend our study to
networks trained on authentic data, observing that decision thresholds
correlate with the covariance eigenvectors rather than the eigenvalues,
mirroring our GMM analysis. This provides theoretical insights regarding neural
networks' ability to perform probabilistic inference and distill statistical
patterns from intricate distributions.
`,authors:"Khen Cohen; Noam Levi; Yaron Oz",status:0,relevancy:.30853721631054176,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17839",date:"2024-05-28",title:"PeerFL: A Simulator for Peer-to-Peer Federated Learning at Scale",abstract:`  This work integrates peer-to-peer federated learning tools with NS3, a widely
used network simulator, to create a novel simulator designed to allow
heterogeneous device experiments in federated learning. This cross-platform
adaptability addresses a critical gap in existing simulation tools, enhancing
the overall utility and user experience. NS3 is leveraged to simulate WiFi
dynamics to facilitate federated learning experiments with participants that
move around physically during training, leading to dynamic network
characteristics. Our experiments showcase the simulator's efficiency in
computational resource utilization at scale, with a maximum of 450
heterogeneous devices modelled as participants in federated learning. This
positions it as a valuable tool for simulation-based investigations in
peer-to-peer federated learning. The framework is open source and available for
use and extension to the community.
`,authors:"Alka Luqman; Shivanshu Shekhar; Anupam Chattopadhyay",status:0,relevancy:.30535233545839,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18047",date:"2024-05-28",title:"2BP: 2-Stage Backpropagation",abstract:`  As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed
the memory capacity of a single accelerator, necessitating the sharding of
model parameters across multiple accelerators. Pipeline parallelism is a
commonly used sharding strategy for training large DNNs. However, current
implementations of pipeline parallelism are being unintentionally bottlenecked
by the automatic differentiation tools provided by ML frameworks. This paper
introduces 2-stage backpropagation (2BP). By splitting the backward propagation
step into two separate stages, we can reduce idle compute time. We tested 2BP
on various model architectures and pipelining schedules, achieving increases in
throughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in
throughput compared to traditional methods when training a LLaMa-like
transformer with 7 billion parameters across 4 GPUs.
`,authors:"Christopher Rae; Joseph K. L. Lee; James Richings",status:0,relevancy:.3050124597543611,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18624",date:"2024-05-28",title:`Enhancing IoT Security with CNN and LSTM-Based Intrusion Detection
  Systems`,abstract:`  Protecting Internet of things (IoT) devices against cyber attacks is
imperative owing to inherent security vulnerabilities. These vulnerabilities
can include a spectrum of sophisticated attacks that pose significant damage to
both individuals and organizations. Employing robust security measures like
intrusion detection systems (IDSs) is essential to solve these problems and
protect IoT systems from such attacks. In this context, our proposed IDS model
consists on a combination of convolutional neural network (CNN) and long
short-term memory (LSTM) deep learning (DL) models. This fusion facilitates the
detection and classification of IoT traffic into binary categories, benign and
malicious activities by leveraging the spatial feature extraction capabilities
of CNN for pattern recognition and the sequential memory retention of LSTM for
discerning complex temporal dependencies in achieving enhanced accuracy and
efficiency. In assessing the performance of our proposed model, the authors
employed the new CICIoT2023 dataset for both training and final testing, while
further validating the model's performance through a conclusive testing phase
utilizing the CICIDS2017 dataset. Our proposed model achieves an accuracy rate
of 98.42%, accompanied by a minimal loss of 0.0275. False positive rate(FPR) is
equally important, reaching 9.17% with an F1-score of 98.57%. These results
demonstrate the effectiveness of our proposed CNN-LSTM IDS model in fortifying
IoT environments against potential cyber threats.
`,authors:"Afrah Gueriani; Hamza Kheddar; Ahmed Cherif Mazari",status:0,relevancy:.2970899930717883,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18395",date:"2024-05-28",title:`MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit
  Tests with Autocorrelations`,abstract:`  A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis
tasks, such as grouping vehicle sensor trajectories, can be formulated as
clustering with given metric constraints. Existing metric-constrained
clustering algorithms overlook the rich correlation between feature similarity
and metric distance, i.e., metric autocorrelation. The model-based variations
of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance,
yet suffer from computational instability and complexity by using a
metric-constrained Expectation-Maximization procedure. In order to address
these two problems, we propose a novel clustering algorithm, MC-GTA
(Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its
objective is only composed of pairwise weighted sums of feature similarity
terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel
multivariate generalization of classic semivariogram). We show that MC-GTA is
effectively minimizing the total hinge loss for intra-cluster observation pairs
not passing goodness-of-fit tests, i.e., statistically not originating from the
same distribution. Experiments on 1D/2D synthetic and real-world datasets
demonstrate that MC-GTA successfully incorporates metric autocorrelation. It
outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in
NMI) with faster and stabler optimization (>10x speedup).
`,authors:"Zhangyu Wang; Gengchen Mai; Krzysztof Janowicz; Ni Lao",status:0,relevancy:.29563656452996523,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17873",date:"2024-05-28",title:`MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with
  Metric-Decoupled Mixed Precision Quantization`,abstract:`  Diffusion models have achieved significant visual generation quality.
However, their significant computational and memory costs pose challenge for
their application on resource-constrained mobile devices or even desktop GPUs.
Recent few-step diffusion models reduces the inference time by reducing the
denoising steps. However, their memory consumptions are still excessive. The
Post Training Quantization (PTQ) replaces high bit-width FP representation with
low-bit integer values (INT4/8) , which is an effective and efficient technique
to reduce the memory cost. However, when applying to few-step diffusion models,
existing quantization methods face challenges in preserving both the image
quality and text alignment. To address this issue, we propose an
mixed-precision quantization framework - MixDQ. Firstly, We design specialized
BOS-aware quantization method for highly sensitive text embedding quantization.
Then, we conduct metric-decoupled sensitivity analysis to measure the
sensitivity of each layer. Finally, we develop an integer-programming-based
method to conduct bit-width allocation. While existing quantization methods
fall short at W8A8, MixDQ could achieve W8A8 without performance loss, and W4A8
with negligible visual degradation. Compared with FP16, we achieve 3-4x
reduction in model size and memory cost, and 1.45x latency speedup.
`,authors:"Tianchen Zhao; Xuefei Ning; Tongcheng Fang; Enshu Liu; Guyue Huang; Zinan Lin; Shengen Yan; Guohao Dai; Yu Wang",status:0,relevancy:.295624977623422,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17750",date:"2024-05-28",title:"Magnitude-based Neuron Pruning for Backdoor Defens",abstract:`  Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks,
posing concerning threats to their reliable deployment. Recent research reveals
that backdoors can be erased from infected DNNs by pruning a specific group of
neurons, while how to effectively identify and remove these backdoor-associated
neurons remains an open challenge. In this paper, we investigate the
correlation between backdoor behavior and neuron magnitude, and find that
backdoor neurons deviate from the magnitude-saliency correlation of the model.
The deviation inspires us to propose a Magnitude-based Neuron Pruning (MNP)
method to detect and prune backdoor neurons. Specifically, MNP uses three
magnitude-guided objective functions to manipulate the magnitude-saliency
correlation of backdoor neurons, thus achieving the purpose of exposing
backdoor behavior, eliminating backdoor neurons and preserving clean neurons,
respectively. Experiments show our pruning strategy achieves state-of-the-art
backdoor defense performance against a variety of backdoor attacks with a
limited amount of clean data, demonstrating the crucial role of magnitude for
guiding backdoor defenses.
`,authors:"Nan Li; Haoyu Jiang; Ping Yi",status:0,relevancy:.2891580497427574,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18459",date:"2024-05-28",title:"Probing the Information Theoretical Roots of Spatial Dependence Measures",abstract:`  Intuitively, there is a relation between measures of spatial dependence and
information theoretical measures of entropy. For instance, we can provide an
intuition of why spatial data is special by stating that, on average, spatial
data samples contain less than expected information. Similarly, spatial data,
e.g., remotely sensed imagery, that is easy to compress is also likely to show
significant spatial autocorrelation. Formulating our (highly specific) core
concepts of spatial information theory in the widely used language of
information theory opens new perspectives on their differences and similarities
and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML
communities. Interestingly, however, this intuitive relation is challenging to
formalize and generalize, leading prior work to rely mostly on experimental
results, e.g., for describing landscape patterns. In this work, we will explore
the information theoretical roots of spatial autocorrelation, more specifically
Moran's I, through the lens of self-information (also known as surprisal) and
provide both formal proofs and experiments.
`,authors:"Zhangyu Wang; Krzysztof Janowicz; Gengchen Mai; Ivan Majic",status:0,relevancy:.2801216544419354,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18144",date:"2024-05-28",title:"4-bit Shampoo for Memory-Efficient Network Training",abstract:`  Second-order optimizers, maintaining a matrix termed a preconditioner, are
superior to first-order optimizers in both theory and practice. The states
forming the preconditioner and its inverse root restrict the maximum size of
models trained by second-order optimizers. To address this, compressing 32-bit
optimizer states to lower bitwidths has shown promise in reducing memory usage.
However, current approaches only pertain to first-order optimizers. In this
paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit
Shampoo, maintaining performance similar to that of 32-bit ones. We show that
quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is
remarkably better than quantizing the preconditioner itself both theoretically
and experimentally. By rectifying the orthogonality of the quantized
eigenvector matrix, we enhance the approximation of the preconditioner's
eigenvector matrix, which also benefits the computation of its inverse 4-th
root. Besides, we find that linear square quantization slightly outperforms
dynamic tree quantization when quantizing second-order optimizer states.
Evaluation on various networks for image classification demonstrates that our
4-bit Shampoo achieves comparable test accuracy to its 32-bit counterpart while
being more memory-efficient. The source code will be made available.
`,authors:"Sike Wang; Jia Li; Pan Zhou; Hua Huang",status:0,relevancy:.2794414385924491,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17901",date:"2024-05-28",title:`Near-Infrared and Low-Rank Adaptation of Vision Transformers in Remote
  Sensing`,abstract:`  Plant health can be monitored dynamically using multispectral sensors that
measure Near-Infrared reflectance (NIR). Despite this potential, obtaining and
annotating high-resolution NIR images poses a significant challenge for
training deep neural networks. Typically, large networks pre-trained on the RGB
domain are utilized to fine-tune infrared images. This practice introduces a
domain shift issue because of the differing visual traits between RGB and NIR
images.As an alternative to fine-tuning, a method called low-rank adaptation
(LoRA) enables more efficient training by optimizing rank-decomposition
matrices while keeping the original network weights frozen. However, existing
parameter-efficient adaptation strategies for remote sensing images focus on
RGB images and overlook domain shift issues in the NIR domain. Therefore, this
study investigates the potential benefits of using vision transformer (ViT)
backbones pre-trained in the RGB domain, with low-rank adaptation for
downstream tasks in the NIR domain. Extensive experiments demonstrate that
employing LoRA with pre-trained ViT backbones yields the best performance for
downstream tasks applied to NIR images.
`,authors:"Irem Ulku; O. Ozgur Tanriover; Erdem Akagündüz",status:0,relevancy:.27293085219579494,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18471",date:"2024-05-28",title:"Symbolic Regression for Beyond the Standard Model Physics",abstract:`  We propose symbolic regression as a powerful tool for studying Beyond the
Standard Model physics. As a benchmark model, we consider the so-called
Constrained Minimal Supersymmetric Standard Model, which has a four-dimensional
parameter space defined at the GUT scale. We provide a set of analytical
expressions that reproduce three low-energy observables of interest in terms of
the parameters of the theory: the Higgs mass, the contribution to the anomalous
magnetic moment of the muon, and the cold dark matter relic density. To
demonstrate the power of the approach, we employ the symbolic expressions in a
global fits analysis to derive the posterior probability densities of the
parameters, which are obtained extremely rapidly in comparison with
conventional methods.
`,authors:"Shehu AbdusSalam; Steve Abel; Miguel Crispim Romao",status:0,relevancy:.23516579085256617,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.17905",date:"2024-05-28",title:`Cycle-YOLO: A Efficient and Robust Framework for Pavement Damage
  Detection`,abstract:`  With the development of modern society, traffic volume continues to increase
in most countries worldwide, leading to an increase in the rate of pavement
damage Therefore, the real-time and highly accurate pavement damage detection
and maintenance have become the current need. In this paper, an enhanced
pavement damage detection method with CycleGAN and improved YOLOv5 algorithm is
presented. We selected 7644 self-collected images of pavement damage samples as
the initial dataset and augmented it by CycleGAN. Due to a substantial
difference between the images generated by CycleGAN and real road images, we
proposed a data enhancement method based on an improved Scharr filter,
CycleGAN, and Laplacian pyramid. To improve the target recognition effect on a
complex background and solve the problem that the spatial pyramid pooling-fast
module in the YOLOv5 network cannot handle multiscale targets, we introduced
the convolutional block attention module attention mechanism and proposed the
atrous spatial pyramid pooling with squeeze-and-excitation structure. In
addition, we optimized the loss function of YOLOv5 by replacing the CIoU with
EIoU. The experimental results showed that our algorithm achieved a precision
of 0.872, recall of 0.854, and mean average precision@0.5 of 0.882 in detecting
three main types of pavement damage: cracks, potholes, and patching. On the
GPU, its frames per second reached 68, meeting the requirements for real-time
detection. Its overall performance even exceeded the current more advanced
YOLOv7 and achieved good results in practical applications, providing a basis
for decision-making in pavement damage detection and prevention.
`,authors:"Zhengji Li; Xi Xiao; Jiacheng Xie; Yuxiao Fan; Wentao Wang; Gang Chen; Liqiang Zhang; Tianyang Wang",status:0,relevancy:.23485695375499982,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18299",date:"2024-05-28",title:`Deep Learning Innovations for Underwater Waste Detection: An In-Depth
  Analysis`,abstract:`  Addressing the issue of submerged underwater trash is crucial for
safeguarding aquatic ecosystems and preserving marine life. While identifying
debris present on the surface of water bodies is straightforward, assessing the
underwater submerged waste is a challenge due to the image distortions caused
by factors such as light refraction, absorption, suspended particles, color
shifts, and occlusion. This paper conducts a comprehensive review of
state-of-the-art architectures and on the existing datasets to establish a
baseline for submerged waste and trash detection. The primary goal remains to
establish the benchmark of the object localization techniques to be leveraged
by advanced underwater sensors and autonomous underwater vehicles. The ultimate
objective is to explore the underwater environment, to identify, and remove
underwater debris. The absence of benchmarks (dataset or algorithm) in many
researches emphasizes the need for a more robust algorithmic solution. Through
this research, we aim to give performance comparative analysis of various
underwater trash detection algorithms.
`,authors:"Jaskaran Singh Walia; Pavithra L K",status:0,relevancy:.20318523813863398,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}},{id:"2405.18383",date:"2024-05-28",title:`Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy
  Planning Automated Segmentation`,abstract:`  The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT)
challenge aims to advance automated segmentation algorithms using the largest
known multi-institutional dataset of radiotherapy planning brain MRIs with
expert-annotated target labels for patients with intact or post-operative
meningioma that underwent either conventional external beam radiotherapy or
stereotactic radiosurgery. Each case includes a defaced 3D post-contrast
T1-weighted radiotherapy planning MRI in its native acquisition space,
accompanied by a single-label "target volume" representing the gross tumor
volume (GTV) and any at-risk post-operative site. Target volume annotations
adhere to established radiotherapy planning protocols, ensuring consistency
across cases and institutions. For pre-operative meningiomas, the target volume
encompasses the entire GTV and associated nodular dural tail, while for
post-operative cases, it includes at-risk resection cavity margins as
determined by the treating institution. Case annotations were reviewed and
approved by expert neuroradiologists and radiation oncologists. Participating
teams will develop, containerize, and evaluate automated segmentation models
using this comprehensive dataset. Model performance will be assessed using the
lesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance. The
top-performing teams will be recognized at the Medical Image Computing and
Computer Assisted Intervention Conference in October 2024. BraTS-MEN-RT is
expected to significantly advance automated radiotherapy planning by enabling
precise tumor segmentation and facilitating tailored treatment, ultimately
improving patient outcomes.
`,authors:"Dominic LaBella; Katherine Schumacher; Michael Mix; Kevin Leu; Shan McBurney-Lin; Pierre Nedelec; Javier Villanueva-Meyer; Jonathan Shapey; Tom Vercauteren; Kazumi Chia; Omar Al-Salihi; Justin Leu; Lia Halasz; Yury Velichko; Chunhao Wang; John Kirkpatrick; Scott Floyd; Zachary J. Reitman; Trey Mullikin; Ulas Bagci; Sean Sachdev; Jona A. Hattangadi-Gluth; Tyler Seibert; Nikdokht Farid; Connor Puett; Matthew W. Pease; Kevin Shiue; Syed Muhammad Anwar; Shahriar Faghani; Muhammad Ammar Haider; Pranav Warman; Jake Albrecht; András Jakab; Mana Moassefi; Verena Chung; Alejandro Aristizabal; Alexandros Karargyris; Hasan Kassem; Sarthak Pati; Micah Sheller; Christina Huang; Aaron Coley; Siddharth Ghanta; Alex Schneider; Conrad Sharp; Rachit Saluja; Florian Kofler; Philipp Lohmann; Phillipp Vollmuth; Louis Gagnon; Maruf Adewole; Hongwei Bran Li; Anahita Fathi Kazerooni; Nourel Hoda Tahon; Udunna Anazodo; Ahmed W. Moawad; Bjoern Menze; Marius George Linguraru; Mariam Aboian; Benedikt Wiestler; Ujjwal Baid; Gian-Marco Conte; Andreas M. T. Rauschecker; Ayman Nada; Aly H. Abayazeed; Raymond Huang; Maria Correia de Verdier; Jeffrey D. Rudie; Spyridon Bakas; Evan Calabrese",status:0,relevancy:.20103659886064773,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:45.387Z",updatedAt:"2024-05-31T04:51:45.387Z",DatesTable:{value:"2024-05-28",status:"complete",count:161,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:45.389Z"}}]},{date:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31 00:09:48.114 +00:00",updatedAt:"2024-05-31 04:51:34.031 +00:00"},papers:[{id:"2405.17249",date:"2024-05-27",title:"Assessing LLMs Suitability for Knowledge Graph Completion",abstract:`  Recent work shown the capability of Large Language Models (LLMs) to solve
tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in
Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or
output results in a non-deterministic manner, thus leading to wrongly reasoned
responses, even if they satisfy the user's demands. To highlight opportunities
and challenges in knowledge graphs-related tasks, we experiment with two
distinguished LLMs, namely Mixtral-8x7B-Instruct-v0.1, and gpt-3.5-turbo-0125,
on Knowledge Graph Completion for static knowledge graphs, using prompts
constructed following the TELeR taxonomy, in Zero- and One-Shot contexts, on a
Task-Oriented Dialogue system use case. When evaluated using both strict and
flexible metrics measurement manners, our results show that LLMs could be fit
for such a task if prompts encapsulate sufficient information and relevant
examples.
`,authors:"Vasile Ionut Remus Iga; Gheorghe Cosmin Silaghi",status:0,relevancy:.6344067106470423,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16869",date:"2024-05-27",title:`Mixture of Modality Knowledge Experts for Robust Multi-modal Knowledge
  Graph Completion`,abstract:`  Multi-modal knowledge graph completion (MMKGC) aims to automatically discover
new knowledge triples in the given multi-modal knowledge graphs (MMKGs), which
is achieved by collaborative modeling the structural information concealed in
massive triples and the multi-modal features of the entities. Existing methods
tend to focus on crafting elegant entity-wise multi-modal fusion strategies,
yet they overlook the utilization of multi-perspective features concealed
within the modalities under diverse relational contexts. To address this issue,
we introduce a novel MMKGC framework with Mixture of Modality Knowledge experts
(MoMoK for short) to learn adaptive multi-modal embedding under intricate
relational contexts. We design relation-guided modality knowledge experts to
acquire relation-aware modality embeddings and integrate the predictions from
multi-modalities to achieve comprehensive decisions. Additionally, we
disentangle the experts by minimizing their mutual information. Experiments on
four public MMKG benchmarks demonstrate the outstanding performance of MoMoK
under complex scenarios.
`,authors:"Yichi Zhang; Zhuo Chen; Lingbing Guo; Yajing Xu; Binbin Hu; Ziqi Liu; Wen Zhang; Huajun Chen",status:0,relevancy:.6146425764128003,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:52:11.665Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16929",date:"2024-05-27",title:"Uncertainty Management in the Construction of Knowledge Graphs: a Survey",abstract:`  Knowledge Graphs (KGs) are a major asset for companies thanks to their great
flexibility in data representation and their numerous applications, e.g.,
vocabulary sharing, Q/A or recommendation systems. To build a KG it is a common
practice to rely on automatic methods for extracting knowledge from various
heterogeneous sources. But in a noisy and uncertain world, knowledge may not be
reliable and conflicts between data sources may occur. Integrating unreliable
data would directly impact the use of the KG, therefore such conflicts must be
resolved. This could be done manually by selecting the best data to integrate.
This first approach is highly accurate, but costly and time-consuming. That is
why recent efforts focus on automatic approaches, which represents a
challenging task since it requires handling the uncertainty of extracted
knowledge throughout its integration into the KG. We survey state-of-the-art
approaches in this direction and present constructions of both open and
enterprise KGs and how their quality is maintained. We then describe different
knowledge extraction methods, introducing additional uncertainty. We also
discuss downstream tasks after knowledge acquisition, including KG completion
using embedding models, knowledge alignment, and knowledge fusion in order to
address the problem of knowledge uncertainty in KG construction. We conclude
with a discussion on the remaining challenges and perspectives when
constructing a KG taking into account uncertainty.
`,authors:"Lucas Jarnac; Yoan Chabot; Miguel Couceiro",status:0,relevancy:.6016545632942065,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17243",date:"2024-05-27",title:`Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement
  Learning`,abstract:`  Both entropy-minimizing and entropy-maximizing (curiosity) objectives for
unsupervised reinforcement learning (RL) have been shown to be effective in
different environments, depending on the environment's level of natural
entropy. However, neither method alone results in an agent that will
consistently learn intelligent behavior across environments. In an effort to
find a single entropy-based method that will encourage emergent behaviors in
any environment, we propose an agent that can adapt its objective online,
depending on the entropy conditions by framing the choice as a multi-armed
bandit problem. We devise a novel intrinsic feedback signal for the bandit,
which captures the agent's ability to control the entropy in its environment.
We demonstrate that such agents can learn to control entropy and exhibit
emergent behaviors in both high- and low-entropy regimes and can learn skillful
behaviors in benchmark tasks. Videos of the trained agents and summarized
findings can be found on our project page
https://sites.google.com/view/surprise-adaptive-agents
`,authors:"Adriana Hugessen; Roger Creus Castanyer; Faisal Mohamed; Glen Berseth",status:0,relevancy:.5958386501330759,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17337",date:"2024-05-27",title:`Cost-efficient Knowledge-based Question Answering with Large Language
  Models`,abstract:`  Knowledge-based question answering (KBQA) is widely used in many scenarios
that necessitate domain knowledge. Large language models (LLMs) bring
opportunities to KBQA, while their costs are significantly higher and absence
of domain-specific knowledge during pre-training. We are motivated to combine
LLMs and prior small models on knowledge graphs (KGMs) for both inferential
accuracy and cost saving. However, it remains challenging since accuracy and
cost are not readily combined in the optimization as two distinct metrics. It
is also laborious for model selection since different models excel in diverse
knowledge. To this end, we propose Coke, a novel cost-efficient strategy for
KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize
calls to LLMs within limited budgets. We first formulate the accuracy
expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A
context-aware policy is optimized to further distinguish the expert model
subject to the question semantics. The overall decision is bounded by the cost
regret according to historical expenditure on failures. Extensive experiments
showcase the superior performance of Coke, which moves the Pareto frontier with
up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on
the benchmark datasets.
`,authors:"Junnan Dong; Qinggang Zhang; Chuang Zhou; Hao Chen; Daochen Zha; Xiao Huang",status:0,relevancy:.5892430830592176,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17346",date:"2024-05-27",title:"Prompt Optimization with Human Feedback",abstract:`  Large language models (LLMs) have demonstrated remarkable performances in
various tasks. However, the performance of LLMs heavily depends on the input
prompt, which has given rise to a number of recent works on prompt
optimization. However, previous works often require the availability of a
numeric score to assess the quality of every prompt. Unfortunately, when a
human user interacts with a black-box LLM, attaining such a score is often
infeasible and unreliable. Instead, it is usually significantly easier and more
reliable to obtain preference feedback from a human user, i.e., showing the
user the responses generated from a pair of prompts and asking the user which
one is preferred. Therefore, in this paper, we study the problem of prompt
optimization with human feedback (POHF), in which we aim to optimize the prompt
for a black-box LLM using only human preference feedback. Drawing inspiration
from dueling bandits, we design a theoretically principled strategy to select a
pair of prompts to query for preference feedback in every iteration, and hence
introduce our algorithm named automated POHF (APOHF). We apply our APOHF
algorithm to various tasks, including optimizing user instructions, prompt
optimization for text-to-image generative models, and response optimization
with human feedback (i.e., further refining the response using a variant of our
APOHF). The results demonstrate that our APOHF can efficiently find a good
prompt using a small number of preference feedback instances. Our code can be
found at \\url{https://github.com/xqlin98/APOHF}.
`,authors:"Xiaoqiang Lin; Zhongxiang Dai; Arun Verma; See-Kiong Ng; Patrick Jaillet; Bryan Kian Hsiang Low",status:0,relevancy:.580458580367271,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17386",date:"2024-05-27",title:"MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",abstract:`  Reasoning capabilities are crucial for Large Language Models (LLMs), yet a
notable gap exists between English and non-English languages. To bridge this
disparity, some works fine-tune LLMs to relearn reasoning capabilities in
non-English languages, while others replace non-English inputs with an external
model's outputs such as English translation text to circumvent the challenge of
LLM understanding non-English. Unfortunately, these methods often underutilize
the built-in skilled reasoning and useful language understanding capabilities
of LLMs. In order to better utilize the minds of reasoning and language
understanding in LLMs, we propose a new method, namely MindMerger, which merges
LLMs with the external language understanding capabilities from multilingual
models to boost the multilingual reasoning performance. Furthermore, a two-step
training scheme is introduced to first train to embeded the external
capabilities into LLMs and then train the collaborative utilization of the
external capabilities and the built-in capabilities in LLMs. Experiments on
three multilingual reasoning datasets and a language understanding dataset
demonstrate that MindMerger consistently outperforms all baselines, especially
in low-resource languages. Without updating the parameters of LLMs, the average
accuracy improved by 6.7% and 8.0% across all languages and low-resource
languages on the MGSM dataset, respectively.
`,authors:"Zixian Huang; Wenhao Zhu; Gong Cheng; Lei Li; Fei Yuan",status:0,relevancy:.5775147839163675,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16751",date:"2024-05-27",title:`LLM-Based Cooperative Agents using Information Relevance and Plan
  Validation`,abstract:`  We address the challenge of multi-agent cooperation, where agents achieve a
common goal by interacting with a 3D scene and cooperating with decentralized
agents under complex partial observations. This involves managing communication
costs and optimizing interaction trajectories in dynamic environments. Our
research focuses on three primary limitations of existing cooperative agent
systems. Firstly, current systems demonstrate inefficiency in managing acquired
information through observation, resulting in declining planning performance as
the environment becomes more complex with additional objects or goals.
Secondly, the neglect of false plans in partially observable settings leads to
suboptimal cooperative performance, as agents struggle to adapt to
environmental changes influenced by the unseen actions of other agents. Lastly,
the failure to incorporate spatial data into decision-making processes
restricts the agent's ability to construct optimized trajectories. To overcome
these limitations, we propose the RElevance and Validation-Enhanced Cooperative
Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.
REVECA leverages relevance assessment, plan validation, and spatial information
to enhance the efficiency and robustness of agent cooperation in dynamic and
partially observable environments while minimizing continuous communication
costs and effectively managing irrelevant dummy objects. Our extensive
experiments demonstrate the superiority of REVECA over previous approaches,
including those driven by GPT-4.0. Additionally, a user study highlights
REVECA's potential for achieving trustworthy human-AI cooperation. We expect
that REVECA will have significant applications in gaming, XR applications,
educational tools, and humanoid robots, contributing to substantial economic,
commercial, and academic advancements.
`,authors:"SeungWon Seo; Junhyeok Lee; SeongRae Noh; HyeongYeop Kang",status:0,relevancy:.5720936333177186,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16919",date:"2024-05-27",title:`VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large
  Multi-Modal Models`,abstract:`  While large multi-modal models (LMMs) have exhibited impressive capabilities
across diverse tasks, their effectiveness in handling complex tasks has been
limited by the prevailing single-step reasoning paradigm. To this end, this
paper proposes VoCoT, a multi-step Visually grounded object-centric
Chain-of-Thought reasoning framework tailored for inference with LMMs. VoCoT is
characterized by two key features: (1) object-centric reasoning paths that
revolve around cross-modal shared object-level information, and (2) visually
grounded representation of object concepts in a multi-modal interleaved and
aligned manner, which effectively bridges the modality gap within LMMs during
long-term generation. Additionally, we construct an instruction dataset to
facilitate LMMs in adapting to reasoning with VoCoT. By introducing VoCoT into
the prevalent open-source LMM architecture, we introduce VolCano. With only 7B
parameters and limited input resolution, VolCano demonstrates excellent
performance across various scenarios, surpassing SOTA models, including GPT-4V,
in tasks requiring complex reasoning. Our code, data and model will be
available at https://github.com/RupertLuo/VoCoT.
`,authors:"Zejun Li; Ruipu Luo; Jiwen Zhang; Minghui Qiu; Zhongyu Wei",status:0,relevancy:.5701995442935639,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17009",date:"2024-05-27",title:"Position: Foundation Agents as the Paradigm Shift for Decision Making",abstract:`  Decision making demands intricate interplay between perception, memory, and
reasoning to discern optimal policies. Conventional approaches to decision
making face challenges related to low sample efficiency and poor
generalization. In contrast, foundation models in language and vision have
showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the
construction of foundation agents as a transformative shift in the learning
paradigm of agents. This proposal is underpinned by the formulation of
foundation agents with their fundamental characteristics and challenges
motivated by the success of large language models (LLMs). Moreover, we specify
the roadmap of foundation agents from large interactive data collection or
generation, to self-supervised pretraining and adaptation, and knowledge and
value alignment with LLMs. Lastly, we pinpoint critical research questions
derived from the formulation and delineate trends for foundation agents
supported by real-world use cases, addressing both technical and theoretical
aspects to propel the field towards a more comprehensive and impactful future.
`,authors:"Xiaoqian Liu; Xingzhou Lou; Jianbin Jiao; Junge Zhang",status:0,relevancy:.5700282258642639,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16806",date:"2024-05-27",title:"Entity Alignment with Noisy Annotations from Large Language Models",abstract:`  Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying
equivalent entity pairs. While existing methods heavily rely on human-generated
labels, it is prohibitively expensive to incorporate cross-domain experts for
annotation in real-world scenarios. The advent of Large Language Models (LLMs)
presents new avenues for automating EA with annotations, inspired by their
comprehensive capability to process semantic information. However, it is
nontrivial to directly apply LLMs for EA since the annotation space in
real-world KGs is large. LLMs could also generate noisy labels that may mislead
the alignment. To this end, we propose a unified framework, LLM4EA, to
effectively leverage LLMs for EA. Specifically, we design a novel active
learning policy to significantly reduce the annotation space by prioritizing
the most valuable entities based on the entire inter-KG and intra-KG structure.
Moreover, we introduce an unsupervised label refiner to continuously enhance
label accuracy through in-depth probabilistic reasoning. We iteratively
optimize the policy based on the feedback from a base EA model. Extensive
experiments demonstrate the advantages of LLM4EA on four benchmark datasets in
terms of effectiveness, robustness, and efficiency. Codes are available via
https://github.com/chensyCN/llm4ea_official.
`,authors:"Shengyuan Chen; Qinggang Zhang; Junnan Dong; Wen Hua; Qing Li; Xiao Huang",status:0,relevancy:.5614687806599702,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17428",date:"2024-05-27",title:`NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding
  Models`,abstract:`  Decoder-only large language model (LLM)-based embedding models are beginning
to outperform BERT or T5-based embedding models in general-purpose text
embedding tasks, including dense vector-based retrieval. In this work, we
introduce the NV-Embed model with a variety of architectural designs and
training procedures to significantly enhance the performance of LLM as a
versatile embedding model, while maintaining its simplicity and
reproducibility. For model architecture, we propose a latent attention layer to
obtain pooled embeddings, which consistently improves retrieval and downstream
task accuracy compared to mean pooling or using the last <EOS> token embedding
from LLMs. To enhance representation learning, we remove the causal attention
mask of LLMs during contrastive training. For model training, we introduce a
two-stage contrastive instruction-tuning method. It first applies contrastive
training with instructions on retrieval datasets, utilizing in-batch negatives
and curated hard negative examples. At stage-2, it blends various non-retrieval
datasets into instruction tuning, which not only enhances non-retrieval task
accuracy but also improves retrieval performance. Combining these techniques,
our NV-Embed model, using only publicly available data, has achieved a
record-high score of 69.32, ranking No. 1 on the Massive Text Embedding
Benchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,
reranking, classification, clustering, and semantic textual similarity tasks.
Notably, our model also attains the highest score of 59.36 on 15 retrieval
tasks in the MTEB benchmark (also known as BEIR). We will open-source the model
at: https://huggingface.co/nvidia/NV-Embed-v1.
`,authors:"Chankyu Lee; Rajarshi Roy; Mengyao Xu; Jonathan Raiman; Mohammad Shoeybi; Bryan Catanzaro; Wei Ping",status:0,relevancy:.5595641712884117,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17287",date:"2024-05-27",title:"Opinion-Guided Reinforcement Learning",abstract:`  Human guidance is often desired in reinforcement learning to improve the
performance of the learning agent. However, human insights are often mere
opinions and educated guesses rather than well-formulated arguments. While
opinions are subject to uncertainty, e.g., due to partial informedness or
ignorance about a problem, they also emerge earlier than hard evidence could be
produced. Thus, guiding reinforcement learning agents through opinions offers
the potential for more performant learning processes, but comes with the
challenge of modeling and managing opinions in a formal way. In this article,
we present a method to guide reinforcement learning agents through opinions. To
this end, we provide an end-to-end method to model and manage advisors'
opinions. To assess the utility of the approach, we evaluate it with synthetic
and human advisors, at different levels of uncertainty, and under multiple
advise strategies. Our results indicate that opinions, even if uncertain,
improve the performance of reinforcement learning agents, resulting in higher
rewards, more efficient exploration, and a better reinforced policy. Although
we demonstrate our approach in a simplified topological running example, our
approach is applicable to complex problems with higher dimensions as well.
`,authors:"Kyanna Dagenais; Istvan David",status:0,relevancy:.5491430684706172,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17044",date:"2024-05-27",title:`Generation and human-expert evaluation of interesting research ideas
  using knowledge graphs and large language models`,abstract:`  Advanced artificial intelligence (AI) systems with access to millions of
research papers could inspire new research ideas that may not be conceived by
humans alone. However, how interesting are these AI-generated ideas, and how
can we improve their quality? Here, we introduce SciMuse, a system that uses an
evolving knowledge graph built from more than 58 million scientific papers to
generate personalized research ideas via an interface to GPT-4. We conducted a
large-scale human evaluation with over 100 research group leaders from the Max
Planck Society, who ranked more than 4,000 personalized research ideas based on
their level of interest. This evaluation allows us to understand the
relationships between scientific interest and the core properties of the
knowledge graph. We find that data-efficient machine learning can predict
research interest with high precision, allowing us to optimize the
interest-level of generated research ideas. This work represents a step towards
an artificial scientific muse that could catalyze unforeseen collaborations and
suggest interesting avenues for scientists.
`,authors:"Xuemei Gu; Mario Krenn",status:0,relevancy:.5401620945253656,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17076",date:"2024-05-27",title:`Leveraging small language models for Text2SPARQL tasks to improve the
  resilience of AI assistance`,abstract:`  In this work we will show that language models with less than one billion
parameters can be used to translate natural language to SPARQL queries after
fine-tuning. Using three different datasets ranging from academic to real
world, we identify prerequisites that the training data must fulfill in order
for the training to be successful. The goal is to empower users of semantic web
technology to use AI assistance with affordable commodity hardware, making them
more resilient against external factors.
`,authors:"Felix Brei; Johannes Frey; Lars-Peter Meyer",status:0,relevancy:.5393790350799168,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17398",date:"2024-05-27",title:`Vista: A Generalizable Driving World Model with High Fidelity and
  Versatile Controllability`,abstract:`  World models can foresee the outcomes of different actions, which is of
paramount importance for autonomous driving. Nevertheless, existing driving
world models still have limitations in generalization to unseen environments,
prediction fidelity of critical details, and action controllability for
flexible application. In this paper, we present Vista, a generalizable driving
world model with high fidelity and versatile controllability. Based on a
systematic diagnosis of existing methods, we introduce several key ingredients
to address these limitations. To accurately predict real-world dynamics at high
resolution, we propose two novel losses to promote the learning of moving
instances and structural information. We also devise an effective latent
replacement approach to inject historical frames as priors for coherent
long-horizon rollouts. For action controllability, we incorporate a versatile
set of controls from high-level intentions (command, goal point) to low-level
maneuvers (trajectory, angle, and speed) through an efficient learning
strategy. After large-scale training, the capabilities of Vista can seamlessly
generalize to different scenarios. Extensive experiments on multiple datasets
show that Vista outperforms the most advanced general-purpose video generator
in over 70% of comparisons and surpasses the best-performing driving world
model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize
the capacity of Vista itself to establish a generalizable reward for real-world
action evaluation without accessing the ground truth actions.
`,authors:"Shenyuan Gao; Jiazhi Yang; Li Chen; Kashyap Chitta; Yihang Qiu; Andreas Geiger; Jun Zhang; Hongyang Li",status:0,relevancy:.5358223377325635,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16907",date:"2024-05-27",title:`GTA: Generative Trajectory Augmentation with Guidance for Offline
  Reinforcement Learning`,abstract:`  Offline Reinforcement Learning (Offline RL) presents challenges of learning
effective decision-making policies from static datasets without any online
interactions. Data augmentation techniques, such as noise injection and data
synthesizing, aim to improve Q-function approximation by smoothing the learned
state-action region. However, these methods often fall short of directly
improving the quality of offline datasets, leading to suboptimal results. In
response, we introduce \\textbf{GTA}, Generative Trajectory Augmentation, a
novel generative data augmentation approach designed to enrich offline data by
augmenting trajectories to be both high-rewarding and dynamically plausible.
GTA applies a diffusion model within the data augmentation framework. GTA
partially noises original trajectories and then denoises them with
classifier-free guidance via conditioning on amplified return value. Our
results show that GTA, as a general data augmentation strategy, enhances the
performance of widely used offline RL algorithms in both dense and sparse
reward settings. Furthermore, we conduct a quality analysis of data augmented
by GTA and demonstrate that GTA improves the quality of the data. Our code is
available at https://github.com/Jaewoopudding/GTA
`,authors:"Jaewoo Lee; Sujin Yun; Taeyoung Yun; Jinkyoo Park",status:0,relevancy:.5272498199280046,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17430",date:"2024-05-27",title:"Matryoshka Multimodal Models",abstract:`  Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in
visual-linguistic reasoning. These models first embed images into a fixed large
number of visual tokens and then feed them into a Large Language Model (LLM).
However, this design causes an excessive number of tokens for dense visual
scenarios such as high-resolution images and videos, leading to great
inefficiency. While token pruning/merging methods do exist, they produce a
single length output for each image and do not afford flexibility in trading
off information density v.s. efficiency. Inspired by the concept of Matryoshka
Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent
visual content as nested sets of visual tokens that capture information across
multiple coarse-to-fine granularities. Our approach offers several unique
benefits for LMMs: (1) One can explicitly control the visual granularity per
test instance during inference, e.g. , adjusting the number of tokens used to
represent an image based on the anticipated complexity or simplicity of the
content; (2) M3 provides a framework for analyzing the granularity needed for
existing datasets, where we find that COCO-style benchmarks only need around ~9
visual tokens to obtain accuracy similar to that of using all 576 tokens; (3)
Our approach provides a foundation to explore the best trade-off between
performance and visual token length at sample level, where our investigation
reveals that a large gap exists between the oracle upper bound and current
fixed-scale representations.
`,authors:"Mu Cai; Jianwei Yang; Jianfeng Gao; Yong Jae Lee",status:0,relevancy:.5244891003036051,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17067",date:"2024-05-27",title:`Tokenization Matters! Degrading Large Language Models through
  Challenging Their Tokenization`,abstract:`  Large Language Models (LLMs) have shown remarkable capabilities in language
understanding and generation. Nonetheless, it was also witnessed that LLMs tend
to produce inaccurate responses to specific queries. This deficiency can be
traced to the tokenization step LLMs must undergo, which is an inevitable
limitation inherent to all LLMs. In fact, incorrect tokenization is the
critical point that hinders LLMs in understanding the input precisely, thus
leading to unsatisfactory output. To demonstrate this flaw of LLMs, we
construct an adversarial dataset, named as $\\textbf{ADT (Adversarial Dataset
for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs
to challenge LLMs' tokenization. ADT consists of two subsets: the manually
constructed ADT-Human and the automatically generated ADT-Auto. Our empirical
results reveal that our ADT is highly effective on challenging the tokenization
of leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus
degrading these LLMs' capabilities. Moreover, our method of automatic data
generation has been proven efficient and robust, which can be applied to any
open-source LLMs. To the best of our knowledge, our study is the first to
investigating LLMs' vulnerability in terms of challenging their token
segmentation, which will shed light on the subsequent research of improving
LLMs' capabilities through optimizing their tokenization process and
algorithms.
`,authors:"Dixuan Wang; Yanda Li; Junyuan Jiang; Zepeng Ding; Guochao Jiang; Jiaqing Liang; Deqing Yang",status:0,relevancy:.5234981328174072,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16755",date:"2024-05-27",title:"CHESS: Contextual Harnessing for Efficient SQL Synthesis",abstract:`  Utilizing large language models (LLMs) for transforming natural language
questions into SQL queries (text-to-SQL) is a promising yet challenging
approach, particularly when applied to real-world databases with complex and
extensive schemas. In particular, effectively incorporating data catalogs and
database values for SQL generation remains an obstacle, leading to suboptimal
solutions. We address this problem by proposing a new pipeline that effectively
retrieves relevant data and context, selects an efficient schema, and
synthesizes correct and efficient SQL queries. To increase retrieval precision,
our pipeline introduces a hierarchical retrieval method leveraging
model-generated keywords, locality-sensitive hashing indexing, and vector
databases. Additionally, we have developed an adaptive schema pruning technique
that adjusts based on the complexity of the problem and the model's context
size. Our approach generalizes to both frontier proprietary models like GPT-4
and open-source models such as Llama-3-70B. Through a series of ablation
studies, we demonstrate the effectiveness of each component of our pipeline and
its impact on the end-to-end performance. Our method achieves new
state-of-the-art performance on the cross-domain challenging BIRD dataset.
`,authors:"Shayan Talaei; Mohammadreza Pourreza; Yu-Chen Chang; Azalia Mirhoseini; Amin Saberi",status:0,relevancy:.5210629928378254,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17129",date:"2024-05-27",title:`TEII: Think, Explain, Interact and Iterate with Large Language Models to
  Solve Cross-lingual Emotion Detection`,abstract:`  Cross-lingual emotion detection allows us to analyze global trends, public
opinion, and social phenomena at scale. We participated in the Explainability
of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score
of 0.6046 on the evaluation set for the emotion detection sub-task. Our system
outperformed the baseline by more than 0.16 F1-score absolute, and ranked
second amongst competing systems. We conducted experiments using fine-tuning,
zero-shot learning, and few-shot learning for Large Language Model (LLM)-based
models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques.
Additionally, we introduced two novel methods: the Multi-Iteration Agentic
Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that
LLM-based approaches provided good performance on multilingual emotion
detection. Furthermore, ensembles combining all our experimented models yielded
higher F1-scores than any single approach alone.
`,authors:"Long Cheng; Qihao Shao; Christine Zhao; Sheng Bi; Gina-Anne Levow",status:0,relevancy:.5209382068261406,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17234",date:"2024-05-27",title:"Benchmarking General Purpose In-Context Learning",abstract:`  In-context learning (ICL) capabilities are becoming increasingly appealing
for building general intelligence due to their sample efficiency and
independence from artificial optimization skills. To enhance generalization,
biological neural systems primarily inherit learning capabilities and
subsequently refine their memory, acquiring diverse skills and knowledge
through extensive lifelong experiences. This process gives rise to the concept
of general-purpose in-context learning (GPICL). Compared to standard ICL, GPICL
addresses a broader range of tasks, extends learning horizons, and starts at a
lower zero-shot baseline. We introduce two lightweight but insightful
benchmarks specifically crafted to train and evaluate GPICL functionalities.
Each benchmark includes a vast number of tasks characterized by significant
task variance and minimal transferable knowledge among tasks, facilitating
lifelong in-context learning through continuous generation and interaction.
These features pose significant challenges for models that rely on context or
interactions to improve their proficiency, including language models, decision
models, and world models. Our experiments reveal that parameter scale alone may
not be crucial for ICL or GPICL, suggesting alternative approaches such as
increasing the scale of contexts and memory states.
`,authors:"Fan Wang; Chuan Lin; Yang Cao; Yu Kang",status:0,relevancy:.52039616867082,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16766",date:"2024-05-27",title:"Reframing the Relationship in Out-of-Distribution Detection",abstract:`  The remarkable achievements of Large Language Models (LLMs) have captivated
the attention of both academia and industry, transcending their initial role in
dialogue generation. The utilization of LLMs as intermediary agents in various
tasks has yielded promising results, sparking a wave of innovation in
artificial intelligence. Building on these breakthroughs, we introduce a novel
approach that integrates the agent paradigm into the Out-of-distribution (OOD)
detection task, aiming to enhance its robustness and adaptability. Our proposed
method, Concept Matching with Agent (CMA), employs neutral prompts as agents to
augment the CLIP-based OOD detection process. These agents function as dynamic
observers and communication hubs, interacting with both In-distribution (ID)
labels and data inputs to form vector triangle relationships. This triangular
framework offers a more nuanced approach than the traditional binary
relationship, allowing for better separation and identification of ID and OOD
inputs. Our extensive experimental results showcase the superior performance of
CMA over both zero-shot and training-required methods in a diverse array of
real-world scenarios.
`,authors:"YuXiao Lee; Xiaofeng Cao",status:0,relevancy:.5193883135881757,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17618",date:"2024-05-27",title:`Symmetric Reinforcement Learning Loss for Robust Learning on Diverse
  Tasks and Model Scales`,abstract:`  Reinforcement learning (RL) training is inherently unstable due to factors
such as moving targets and high gradient variance. Reinforcement Learning from
Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can
introduce additional difficulty. Differing preferences can complicate the
alignment process, and prediction errors in a trained reward model can become
more severe as the LLM generates unseen outputs. To enhance training
robustness, RL has adopted techniques from supervised learning, such as
ensembles and layer normalization. In this work, we improve the stability of RL
training by adapting the reverse cross entropy (RCE) from supervised learning
for noisy data to define a symmetric RL loss. We demonstrate performance
improvements across various tasks and scales. We conduct experiments in
discrete action tasks (Atari games) and continuous action space tasks (MuJoCo
benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with
and without added noise with especially notable performance in SPPO across
different hyperparameters. Furthermore, we validate the benefits of the
symmetric RL loss when using SPPO for large language models through improved
performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR
summarization tasks.
`,authors:"Ju-Seung Byun; Andrew Perrault",status:0,relevancy:.5161130411605034,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16994",date:"2024-05-27",title:"Vision-and-Language Navigation Generative Pretrained Transformer",abstract:`  In the Vision-and-Language Navigation (VLN) field, agents are tasked with
navigating real-world scenes guided by linguistic instructions. Enabling the
agent to adhere to instructions throughout the process of navigation represents
a significant challenge within the domain of VLN. To address this challenge,
common approaches often rely on encoders to explicitly record past locations
and actions, increasing model complexity and resource consumption. Our
proposal, the Vision-and-Language Navigation Generative Pretrained Transformer
(VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory
sequence dependencies, bypassing the need for historical encoding modules. This
method allows for direct historical information access through trajectory
sequence, enhancing efficiency. Furthermore, our model separates the training
process into offline pre-training with imitation learning and online
fine-tuning with reinforcement learning. This distinction allows for more
focused training objectives and improved performance. Performance assessments
on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art
encoder-based models.
`,authors:"Wen Hanlin",status:0,relevancy:.5154482457814616,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17706",date:"2024-05-27",title:`Video Enriched Retrieval Augmented Generation Using Aligned Video
  Captions`,abstract:`  In this work, we propose the use of "aligned visual captions" as a mechanism
for integrating information contained within videos into retrieval augmented
generation (RAG) based chat assistant systems. These captions are able to
describe the visual and audio content of videos in a large corpus while having
the advantage of being in a textual format that is both easy to reason about &
incorporate into large language model (LLM) prompts, but also typically require
less multimedia content to be inserted into the multimodal LLM context window,
where typical configurations can aggressively fill up the context window by
sampling video frames from the source video. Furthermore, visual captions can
be adapted to specific use cases by prompting the original foundational model /
captioner for particular visual details or fine tuning. In hopes of helping
advancing progress in this area, we curate a dataset and describe automatic
evaluation procedures on common RAG tasks.
`,authors:"Kevin Dela Rosa",status:0,relevancy:.515298130752256,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17631",date:"2024-05-27",title:`BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation
  Experiments`,abstract:`  Agents based on large language models have shown great potential in
accelerating scientific discovery by leveraging their rich background knowledge
and reasoning capabilities. Here, we develop BioDiscoveryAgent, an agent that
designs new experiments, reasons about their outcomes, and efficiently
navigates the hypothesis space to reach desired solutions. We demonstrate our
agent on the problem of designing genetic perturbation experiments, where the
aim is to find a small subset out of many possible genes that, when perturbed,
result in a specific phenotype (e.g., cell growth). Utilizing its biological
knowledge, BioDiscoveryAgent can uniquely design new experiments without the
need to train a machine learning model or explicitly design an acquisition
function. Moreover, BioDiscoveryAgent achieves an average of 18% improvement in
detecting desired phenotypes across five datasets, compared to existing
Bayesian optimization baselines specifically trained for this task. Our
evaluation includes one dataset that is unpublished, ensuring it is not part of
the language model's training data. Additionally, BioDiscoveryAgent predicts
gene combinations to perturb twice as accurately as a random baseline, a task
so far not explored in the context of closed-loop experiment design. The agent
also has access to tools for searching the biomedical literature, executing
code to analyze biological datasets, and prompting another agent to critically
evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every
stage, representing an accessible new paradigm in the computational design of
biological experiments with the potential to augment scientists' capabilities.
`,authors:"Yusuf Roohani; Jian Vora; Qian Huang; Zachary Steinhart; Alexander Marson; Percy Liang; Jure Leskovec",status:0,relevancy:.510634061615632,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17691",date:"2024-05-27",title:`Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and
  Partially Observable Environments`,abstract:`  Agents, whether software or hardware, perceive their environment through
sensors and act using actuators, often operating in dynamic, partially
observable settings. They face challenges like incomplete and noisy data,
unforeseen situations, and the need to adapt goals in real-time. Traditional
reasoning and ML methods, including Reinforcement Learning (RL), help but are
limited by data needs, predefined goals, and extensive exploration periods.
Ontologies offer a solution by integrating diverse information sources,
enhancing decision-making in complex environments. This thesis introduces an
ontology-enhanced decision-making model (OntoDeM) for autonomous agents.
OntoDeM enriches agents' domain knowledge, allowing them to interpret
unforeseen events, generate or adapt goals, and make better decisions. Key
contributions include: 1. An ontology-based method to improve agents' real-time
observations using prior knowledge. 2. The OntoDeM model for handling dynamic,
unforeseen situations by evolving or generating new goals. 3. Implementation
and evaluation in four real-world applications, demonstrating its
effectiveness. Compared to traditional and advanced learning algorithms,
OntoDeM shows superior performance in improving agents' observations and
decision-making in dynamic, partially observable environments.
`,authors:"Saeedeh Ghanadbashi; Fatemeh Golpayegani",status:0,relevancy:.5103616698161793,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16739",date:"2024-05-27",title:"Oracle-Efficient Reinforcement Learning for Max Value Ensembles",abstract:`  Reinforcement learning (RL) in large or infinite state spaces is notoriously
challenging, both theoretically (where worst-case sample and computational
complexities must scale with state space cardinality) and experimentally (where
function approximation and policy gradient techniques often scale poorly and
suffer from instability and high variance). One line of research attempting to
address these difficulties makes the natural assumption that we are given a
collection of heuristic base or $\\textit{constituent}$ policies upon which we
would like to improve in a scalable manner. In this work we aim to compete with
the $\\textit{max-following policy}$, which at each state follows the action of
whichever constituent policy has the highest value. The max-following policy is
always at least as good as the best constituent policy, and may be considerably
better. Our main result is an efficient algorithm that learns to compete with
the max-following policy, given only access to the constituent policies (but
not their value functions). In contrast to prior work in similar settings, our
theoretical results require only the minimal assumption of an ERM oracle for
value function approximation for the constituent policies (and not the global
optimal policy or the max-following policy itself) on samplable distributions.
We illustrate our algorithm's experimental effectiveness and behavior on
several robotic simulation testbeds.
`,authors:"Marcel Hussing; Michael Kearns; Aaron Roth; Sikata Bela Sengupta; Jessica Sorrell",status:0,relevancy:.508665402744301,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17324",date:"2024-05-27",title:"Leveraging Offline Data in Linear Latent Bandits",abstract:`  Sequential decision-making domains such as recommender systems, healthcare
and education often have unobserved heterogeneity in the population that can be
modeled using latent bandits $-$ a framework where an unobserved latent state
determines the model for a trajectory. While the latent bandit framework is
compelling, the extent of its generality is unclear. We first address this by
establishing a de Finetti theorem for decision processes, and show that
$\\textit{every}$ exchangeable and coherent stateless decision process is a
latent bandit. The latent bandit framework lends itself particularly well to
online learning with offline datasets, a problem of growing interest in
sequential decision-making. One can leverage offline latent bandit data to
learn a complex model for each latent state, so that an agent can simply learn
the latent state online to act optimally. We focus on a linear model for a
latent bandit with $d_A$-dimensional actions, where the latent states lie in an
unknown $d_K$-dimensional subspace for $d_K \\ll d_A$. We present SOLD, a novel
principled method to learn this subspace from short offline trajectories with
guarantees. We then provide two methods to leverage this subspace online:
LOCAL-UCB and ProBALL-UCB. We demonstrate that LOCAL-UCB enjoys $\\tilde
O(\\min(d_A\\sqrt{T}, d_K\\sqrt{T}(1+\\sqrt{d_AT/d_KN})))$ regret guarantees, where
the effective dimension is lower when the size $N$ of the offline dataset is
larger. ProBALL-UCB enjoys a slightly weaker guarantee, but is more practical
and computationally efficient. Finally, we establish the efficacy of our
methods using experiments on both synthetic data and real-life movie
recommendation data from MovieLens.
`,authors:"Chinmaya Kausik; Kevin Tan; Ambuj Tewari",status:0,relevancy:.5068921981905822,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16800",date:"2024-05-27",title:`TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing
  Graph and Text Mutual Transformations`,abstract:`  Text-Attributed Graphs (TAGs) enhance graph structures with natural language
descriptions, enabling detailed representation of data and their relationships
across a broad spectrum of real-world scenarios. Despite the potential for
deeper insights, existing TAG representation learning primarily relies on
supervised methods, necessitating extensive labeled data and limiting
applicability across diverse contexts. This paper introduces a new
self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA),
which overcomes these constraints by integrating TAGs' structural and semantic
dimensions. TAGA constructs two complementary views: Text-of-Graph view, which
organizes node texts into structured documents based on graph topology, and the
Graph-of-Text view, which converts textual nodes and connections into graph
data. By aligning representations from both views, TAGA captures joint textual
and structural information. In addition, a novel structure-preserving random
walk algorithm is proposed for efficient training on large-sized TAGs. Our
framework demonstrates strong performance in zero-shot and few-shot scenarios
across eight real-world datasets.
`,authors:"Zheng Zhang; Yuntong Hu; Bo Pan; Chen Ling; Liang Zhao",status:0,relevancy:.5066622232249672,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17626",date:"2024-05-27",title:"Matrix Low-Rank Approximation For Policy Gradient Methods",abstract:`  Estimating a policy that maps states to actions is a central problem in
reinforcement learning. Traditionally, policies are inferred from the so called
value functions (VFs), but exact VF computation suffers from the curse of
dimensionality. Policy gradient (PG) methods bypass this by learning directly a
parametric stochastic policy. Typically, the parameters of the policy are
estimated using neural networks (NNs) tuned via stochastic gradient descent.
However, finding adequate NN architectures can be challenging, and convergence
issues are common as well. In this paper, we put forth low-rank matrix-based
models to estimate efficiently the parameters of PG algorithms. We collect the
parameters of the stochastic policy into a matrix, and then, we leverage
matrix-completion techniques to promote (enforce) low rank. We demonstrate via
numerical studies how low-rank matrix-based policy models reduce the
computational and sample complexities relative to NN models, while achieving a
similar aggregated reward.
`,authors:"Sergio Rozada; Antonio G. Marques",status:0,relevancy:.5038693613310521,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16879",date:"2024-05-27",title:`Unsupervised Generative Feature Transformation via Graph Contrastive
  Pre-training and Multi-objective Fine-tuning`,abstract:`  Feature transformation is to derive a new feature set from original features
to augment the AI power of data. In many science domains such as material
performance screening, while feature transformation can model material formula
interactions and compositions and discover performance drivers, supervised
labels are collected from expensive and lengthy experiments. This issue
motivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior
literature, such as manual transformation, supervised feedback guided search,
and PCA, either relies on domain knowledge or expensive supervised feedback, or
suffers from large search space, or overlooks non-linear feature-feature
interactions. UFTL imposes a major challenge on existing methods: how to design
a new unsupervised paradigm that captures complex feature interactions and
avoids large search space? To fill this gap, we connect graph, contrastive, and
generative learning to develop a measurement-pretrain-finetune paradigm for
UFTL. For unsupervised feature set utility measurement, we propose a feature
value consistency preservation perspective and develop a mean discounted
cumulative gain like unsupervised metric to evaluate feature set utility. For
unsupervised feature set representation pretraining, we regard a feature set as
a feature-feature interaction graph, and develop an unsupervised graph
contrastive learning encoder to embed feature sets into vectors. For generative
transformation finetuning, we regard a feature set as a feature cross sequence
and feature transformation as sequential generation. We develop a deep
generative feature transformation model that coordinates the pretrained feature
set encoder and the gradient information extracted from a feature set utility
evaluator to optimize a transformed feature generator.
`,authors:"Wangyang Ying; Dongjie Wang; Xuanming Hu; Yuanchun Zhou; Charu C. Aggarwal; Yanjie Fu",status:0,relevancy:.5036297937191909,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17625",date:"2024-05-27",title:"Matrix Low-Rank Trust Region Policy Optimization",abstract:`  Most methods in reinforcement learning use a Policy Gradient (PG) approach to
learn a parametric stochastic policy that maps states to actions. The standard
approach is to implement such a mapping via a neural network (NN) whose
parameters are optimized using stochastic gradient descent. However, PG methods
are prone to large policy updates that can render learning inefficient. Trust
region algorithms, like Trust Region Policy Optimization (TRPO), constrain the
policy update step, ensuring monotonic improvements. This paper introduces
low-rank matrix-based models as an efficient alternative for estimating the
parameters of TRPO algorithms. By gathering the stochastic policy's parameters
into a matrix and applying matrix-completion techniques, we promote and enforce
low rank. Our numerical studies demonstrate that low-rank matrix-based policy
models effectively reduce both computational and sample complexities compared
to NN models, while maintaining comparable aggregated rewards.
`,authors:"Sergio Rozada; Antonio G. Marques",status:0,relevancy:.5003660473501306,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17057",date:"2024-05-27",title:`ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off
  Code Generation`,abstract:`  Code generation plays a crucial role in various tasks, such as code
auto-completion and mathematical reasoning. Previous work has proposed numerous
methods to enhance code generation performance, including integrating feedback
from the compiler. Inspired by this, we present ReflectionCoder, a novel
approach that effectively leverages reflection sequences constructed by
integrating compiler feedback to improve one-off code generation performance.
Furthermore, we propose reflection self-distillation and dynamically masked
distillation to effectively utilize these reflection sequences. Extensive
experiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E,
demonstrate that models fine-tuned with our method achieve state-of-the-art
performance. Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9
(76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo
and Claude-3-opus, and surpasses early GPT-4. Beyond the code domain, we
believe this approach can benefit other domains that focus on final results and
require long reasoning paths. Code and data are available at
https://github.com/SenseLLM/ReflectionCoder.
`,authors:"Houxing Ren; Mingjie Zhan; Zhongyuan Wu; Aojun Zhou; Junting Pan; Hongsheng Li",status:0,relevancy:.4977746859446158,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17640",date:"2024-05-27",title:`Probabilistically Plausible Counterfactual Explanations with Normalizing
  Flows`,abstract:`  We present PPCEF, a novel method for generating probabilistically plausible
counterfactual explanations (CFs). PPCEF advances beyond existing methods by
combining a probabilistic formulation that leverages the data distribution with
the optimization of plausibility within a unified framework. Compared to
reference approaches, our method enforces plausibility by directly optimizing
the explicit density function without assuming a particular family of
parametrized distributions. This ensures CFs are not only valid (i.e., achieve
class change) but also align with the underlying data's probability density.
For that purpose, our approach leverages normalizing flows as powerful density
estimators to capture the complex high-dimensional data distribution.
Furthermore, we introduce a novel loss that balances the trade-off between
achieving class change and maintaining closeness to the original instance while
also incorporating a probabilistic plausibility term. PPCEF's unconstrained
formulation allows for efficient gradient-based optimization with batch
processing, leading to orders of magnitude faster computation compared to prior
methods. Moreover, the unconstrained formulation of PPCEF allows for the
seamless integration of future constraints tailored to specific counterfactual
properties. Finally, extensive evaluations demonstrate PPCEF's superiority in
generating high-quality, probabilistically plausible counterfactual
explanations in high-dimensional tabular settings. This makes PPCEF a powerful
tool for not only interpreting complex machine learning models but also for
improving fairness, accountability, and trust in AI systems.
`,authors:"Patryk Wielopolski; Oleksii Furman; Jerzy Stefanowski; Maciej Zięba",status:0,relevancy:.49564031505195183,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17103",date:"2024-05-27",title:"Empowering Character-level Text Infilling by Eliminating Sub-Tokens",abstract:`  In infilling tasks, sub-tokens, representing instances where a complete token
is segmented into two parts, often emerge at the boundaries of prefixes,
middles, and suffixes. Traditional methods focused on training models at the
token level, leading to sub-optimal performance in character-level infilling
tasks during the inference stage. Alternately, some approaches considered
character-level infilling, but they relied on predicting sub-tokens in
inference, yet this strategy diminished ability in character-level infilling
tasks due to the large perplexity of the model on sub-tokens. In this paper, we
introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and
Ending character constraints. The proposed method addresses character-level
infilling tasks by utilizing a line-level format to avoid predicting any
sub-token in inference. In addition, we incorporate two special tokens to
signify the rest of the incomplete lines, thereby enhancing generation
guidance. Extensive experiments demonstrate that our proposed approach
surpasses previous methods, offering a significant advantage. Code is available
at https://github.com/SenseLLM/FIM-SE.
`,authors:"Houxing Ren; Mingjie Zhan; Zhongyuan Wu; Hongsheng Li",status:0,relevancy:.4952013521387346,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16899",date:"2024-05-27",title:`Partial Models for Building Adaptive Model-Based Reinforcement Learning
  Agents`,abstract:`  In neuroscience, one of the key behavioral tests for determining whether a
subject of study exhibits model-based behavior is to study its adaptiveness to
local changes in the environment. In reinforcement learning, however, recent
studies have shown that modern model-based agents display poor adaptivity to
such changes. The main reason for this is that modern agents are typically
designed to improve sample efficiency in single task settings and thus do not
take into account the challenges that can arise in other settings. In local
adaptation settings, one particularly important challenge is in quickly
building and maintaining a sufficiently accurate model after a local change.
This is challenging for deep model-based agents as their models and replay
buffers are monolithic structures lacking distribution shift handling
capabilities. In this study, we show that the conceptually simple idea of
partial models can allow deep model-based agents to overcome this challenge and
thus allow for building locally adaptive model-based agents. By modeling the
different parts of the state space through different models, the agent can not
only maintain a model that is accurate across the state space, but it can also
quickly adapt it in the presence of a local change in the environment. We
demonstrate this by showing that the use of partial models in agents such as
deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the
local changes in their environments.
`,authors:"Safa Alver; Ali Rahimi-Kalahroudi; Doina Precup",status:0,relevancy:.49376953607242435,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17022",date:"2024-05-27",title:"Compositional Few-Shot Class-Incremental Learning",abstract:`  Few-shot class-incremental learning (FSCIL) is proposed to continually learn
from novel classes with only a few samples after the (pre-)training on base
classes with sufficient data. However, this remains a challenge. In contrast,
humans can easily recognize novel classes with a few samples. Cognitive science
demonstrates that an important component of such human capability is
compositional learning. This involves identifying visual primitives from
learned knowledge and then composing new concepts using these transferred
primitives, making incremental learning both effective and interpretable. To
imitate human compositional learning, we propose a cognitive-inspired method
for the FSCIL task. We define and build a compositional model based on set
similarities, and then equip it with a primitive composition module and a
primitive reuse module. In the primitive composition module, we propose to
utilize the Centered Kernel Alignment (CKA) similarity to approximate the
similarity between primitive sets, allowing the training and evaluation based
on primitive compositions. In the primitive reuse module, we enhance primitive
reusability by classifying inputs based on primitives replaced with the closest
primitives from other classes. Experiments on three datasets validate our
method, showing it outperforms current state-of-the-art methods with improved
interpretability. Our code is available at
https://github.com/Zoilsen/Comp-FSCIL.
`,authors:"Yixiong Zou; Shanghang Zhang; Haichen Zhou; Yuhua Li; Ruixuan Li",status:0,relevancy:.49071128253474994,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17404",date:"2024-05-27",title:"Spectral Greedy Coresets for Graph Neural Networks",abstract:`  The ubiquity of large-scale graphs in node-classification tasks significantly
hinders the real-world applications of Graph Neural Networks (GNNs). Node
sampling, graph coarsening, and dataset condensation are effective strategies
for enhancing data efficiency. However, owing to the interdependence of graph
nodes, coreset selection, which selects subsets of the data examples, has not
been successfully applied to speed up GNN training on large graphs, warranting
special treatment. This paper studies graph coresets for GNNs and avoids the
interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs
around a node) based on their spectral embeddings. We decompose the coreset
selection problem for GNNs into two phases: a coarse selection of widely spread
ego graphs and a refined selection to diversify their topologies. We design a
greedy algorithm that approximately optimizes both objectives. Our spectral
greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates
the need for model pre-training, and applies to low-homophily graphs. Extensive
experiments on ten datasets demonstrate that SGGC outperforms other coreset
methods by a wide margin, generalizes well across GNN architectures, and is
much faster than graph condensation.
`,authors:"Mucong Ding; Yinhan He; Jundong Li; Furong Huang",status:0,relevancy:.4900781490971424,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16860",date:"2024-05-27",title:`Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias
  Towards Vision-Language Tasks`,abstract:`  Gender bias in vision-language models (VLMs) can reinforce harmful
stereotypes and discrimination. In this paper, we focus on mitigating gender
bias towards vision-language tasks. We identify object hallucination as the
essence of gender bias in VLMs. Existing VLMs tend to focus on salient or
familiar attributes in images but ignore contextualized nuances. Moreover, most
VLMs rely on the co-occurrence between specific objects and gender attributes
to infer the ignored features, ultimately resulting in gender bias. We propose
GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA
consists of two stages: narrative generation and answer inference. During
narrative generation, GAMA yields all-sided but gender-obfuscated narratives,
which prevents premature concentration on localized image features, especially
gender attributes. During answer inference, GAMA integrates the image,
generated narrative, and a task-specific question prompt to infer answers for
different vision-language tasks. This approach allows the model to rethink
gender attributes and answers. We conduct extensive experiments on GAMA,
demonstrating its debiasing and generalization ability.
`,authors:"Yunqi Zhang; Songda Li; Chunyuan Deng; Luyi Wang; Hui Zhao",status:0,relevancy:.4872769760274511,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17104",date:"2024-05-27",title:`LLM-Optic: Unveiling the Capabilities of Large Language Models for
  Universal Visual Grounding`,abstract:`  Visual grounding is an essential tool that links user-provided text queries
with query-specific regions within an image. Despite advancements in visual
grounding models, their ability to comprehend complex queries remains limited.
To overcome this limitation, we introduce LLM-Optic, an innovative method that
utilizes Large Language Models (LLMs) as an optical lens to enhance existing
visual grounding models in comprehending complex text queries involving
intricate text structures, multiple objects, or object spatial relationships,
situations that current models struggle with. LLM-Optic first employs an LLM as
a Text Grounder to interpret complex text queries and accurately identify
objects the user intends to locate. Then a pre-trained visual grounding model
is used to generate candidate bounding boxes given the refined query by the
Text Grounder. After that, LLM-Optic annotates the candidate bounding boxes
with numerical marks to establish a connection between text and specific image
regions, thereby linking two distinct modalities. Finally, it employs a Large
Multimodal Model (LMM) as a Visual Grounder to select the marked candidate
objects that best correspond to the original text query. Through LLM-Optic, we
have achieved universal visual grounding, which allows for the detection of
arbitrary objects specified by arbitrary human language input. Importantly, our
method achieves this enhancement without requiring additional training or
fine-tuning. Extensive experiments across various challenging benchmarks
demonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding
capabilities. Project Page: https://haoyu-zhao.github.io/LLM-Optic.github.io/.
`,authors:"Haoyu Zhao; Wenhang Ge; Ying-cong Chen",status:0,relevancy:.4836114182426331,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17512",date:"2024-05-27",title:"On Fairness of Low-Rank Adaptation of Large Models",abstract:`  Low-rank adaptation of large models, particularly LoRA, has gained traction
due to its computational efficiency. This efficiency, contrasted with the
prohibitive costs of full-model fine-tuning, means that practitioners often
turn to LoRA and sometimes without a complete understanding of its
ramifications. In this study, we focus on fairness and ask whether LoRA has an
unexamined impact on utility, calibration, and resistance to membership
inference across different subgroups (e.g., genders, races, religions) compared
to a full-model fine-tuning baseline. We present extensive experiments across
vision and language domains and across classification and generation tasks
using ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly,
experiments suggest that while one can isolate cases where LoRA exacerbates
model bias across subgroups, the pattern is inconsistent -- in many cases, LoRA
has equivalent or even improved fairness compared to the base model or its full
fine-tuning baseline. We also examine the complications of evaluating
fine-tuning fairness relating to task design and model token bias, calling for
more careful fairness evaluations in future work.
`,authors:"Zhoujie Ding; Ken Ziyu Liu; Pura Peetathawatchai; Berivan Isik; Sanmi Koyejo",status:0,relevancy:.4808035338972152,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17642",date:"2024-05-27",title:`Unifying Perspectives: Plausible Counterfactual Explanations on Global,
  Group-wise, and Local Levels`,abstract:`  Growing regulatory and societal pressures demand increased transparency in
AI, particularly in understanding the decisions made by complex machine
learning models. Counterfactual Explanations (CFs) have emerged as a promising
technique within Explainable AI (xAI), offering insights into individual model
predictions. However, to understand the systemic biases and disparate impacts
of AI models, it is crucial to move beyond local CFs and embrace global
explanations, which offer a~holistic view across diverse scenarios and
populations. Unfortunately, generating Global Counterfactual Explanations
(GCEs) faces challenges in computational complexity, defining the scope of
"global," and ensuring the explanations are both globally representative and
locally plausible. We introduce a novel unified approach for generating Local,
Group-wise, and Global Counterfactual Explanations for differentiable
classification models via gradient-based optimization to address these
challenges. This framework aims to bridge the gap between individual and
systemic insights, enabling a deeper understanding of model decisions and their
potential impact on diverse populations. Our approach further innovates by
incorporating a probabilistic plausibility criterion, enhancing actionability
and trustworthiness. By offering a cohesive solution to the optimization and
plausibility challenges in GCEs, our work significantly advances the
interpretability and accountability of AI models, marking a step forward in the
pursuit of transparent AI.
`,authors:"Patryk Wielopolski; Oleksii Furman; Jerzy Stefanowski; Maciej Zięba",status:0,relevancy:.47927225112713956,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16946",date:"2024-05-27",title:`Biological Neurons Compete with Deep Reinforcement Learning in Sample
  Efficiency in a Simulated Gameworld`,abstract:`  How do biological systems and machine learning algorithms compare in the
number of samples required to show significant improvements in completing a
task? We compared the learning efficiency of in vitro biological neural
networks to the state-of-the-art deep reinforcement learning (RL) algorithms in
a simplified simulation of the game \`Pong'. Using DishBrain, a system that
embodies in vitro neural networks with in silico computation using a
high-density multi-electrode array, we contrasted the learning rate and the
performance of these biological systems against time-matched learning from
three state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same
game environment. This allowed a meaningful comparison between biological
neural systems and deep RL. We find that when samples are limited to a
real-world time course, even these very simple biological cultures outperformed
deep RL algorithms across various game performance characteristics, implying a
higher sample efficiency. Ultimately, even when tested across multiple types of
information input to assess the impact of higher dimensional data input,
biological neurons showcased faster learning than all deep reinforcement
learning agents.
`,authors:"Moein Khajehnejad; Forough Habibollahi; Aswin Paul; Adeel Razi; Brett J. Kagan",status:0,relevancy:.4771512731192644,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17678",date:"2024-05-27",title:`TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial
  Robustness and Generalization Ability`,abstract:`  This work addresses the challenge of achieving zero-shot adversarial
robustness while preserving zero-shot generalization in large-scale foundation
models, with a focus on the popular Contrastive Language-Image Pre-training
(CLIP). Although foundation models were reported to have exceptional zero-shot
generalization, they are highly vulnerable to adversarial perturbations.
Existing methods achieve a comparable good tradeoff between zero-shot
adversarial robustness and generalization under small adversarial
perturbations. However, they fail to achieve a good tradeoff under large
adversarial perturbations. To this end, we propose a novel Text-Image Mutual
Awareness (TIMA) method that strikes a balance between zero-shot adversarial
robustness and generalization. More precisely, we propose an Image-Aware Text
(IAT) tuning mechanism that increases the inter-class distance of text
embeddings by incorporating the Minimum Hyperspherical Energy (MHE).
Simultaneously, fixed pre-trained image embeddings are used as cross-modal
auxiliary supervision to maintain the similarity between the MHE-tuned and
original text embeddings by the knowledge distillation, preserving semantic
information between different classes. Besides, we introduce a Text-Aware Image
(TAI) tuning mechanism, which increases inter-class distance between image
embeddings during the training stage by Text-distance based Adaptive Margin
(TAM). Similarly, a knowledge distillation is utilized to retain the similarity
between fine-tuned and pre-trained image embeddings. Extensive experimental
results demonstrate the effectiveness of our approach, showing impressive
zero-shot performance against a wide range of adversarial perturbations while
preserving the zero-shot generalization capabilities of the original CLIP
model.
`,authors:"Fengji Ma; Li Liu; Hei Victor Cheng",status:0,relevancy:.4738095882720026,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17258",date:"2024-05-27",title:`$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter
  Efficient Finetuning`,abstract:`  Low-rank adapters (LoRA) and their variants are popular parameter-efficient
fine-tuning (PEFT) techniques that closely match full model fine-tune
performance while requiring only a small number of additional parameters. These
additional LoRA parameters are specific to the base model being adapted. When
the base model needs to be deprecated and replaced with a new one, all the
associated LoRA modules need to be re-trained. Such re-training requires access
to the data used to train the LoRA for the original base model. This is
especially problematic for commercial cloud applications where the LoRA modules
and the base models are hosted by service providers who may not be allowed to
host proprietary client task data. To address this challenge, we propose
$\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer
of LoRAs across base models. Our approach relies on synthetic data to transfer
LoRA modules. Using large language models, we design a synthetic data generator
to approximate the data-generating process of the $\\textit{observed}$ task data
subset. Training on the resulting synthetic dataset transfers LoRA modules to
new models. We show the effectiveness of our approach using both LLama and
Gemma model families. Our approach achieves lossless (mostly improved) LoRA
transfer between models within and across different base model families, and
even between different PEFT methods, on a wide variety of tasks.
`,authors:"Runqian Wang; Soumya Ghosh; David Cox; Diego Antognini; Aude Oliva; Rogerio Feris; Leonid Karlinsky",status:0,relevancy:.47309652288824755,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16964",date:"2024-05-27",title:`Exploring the LLM Journey from Cognition to Expression with Linear
  Representations`,abstract:`  This paper presents an in-depth examination of the evolution and interplay of
cognitive and expressive capabilities in large language models (LLMs), with a
specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese
and English) LLM series. We define and explore the model's cognitive and
expressive capabilities through linear representations across three critical
phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning
from Human Feedback (RLHF). Cognitive capability is defined as the quantity and
quality of information conveyed by the neuron output vectors within the
network, similar to the neural signal processing in human cognition. Expressive
capability is defined as the model's capability to produce word-level output.
Our findings unveil a sequential development pattern, where cognitive abilities
are largely established during Pretraining, whereas expressive abilities
predominantly advance during SFT and RLHF. Statistical analyses confirm a
significant correlation between the two capabilities, suggesting that cognitive
capacity may limit expressive potential. The paper also explores the
theoretical underpinnings of these divergent developmental trajectories and
their connection to the LLMs' architectural design. Moreover, we evaluate
various optimization-independent strategies, such as few-shot learning and
repeated sampling, which bridge the gap between cognitive and expressive
capabilities. This research reveals the potential connection between the hidden
space and the output space, contributing valuable insights into the
interpretability and controllability of their training processes.
`,authors:"Yuzi Yan; Jialian Li; Yipin Zhang; Dong Yan",status:0,relevancy:.4689778665883011,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16851",date:"2024-05-27",title:"Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",abstract:`  Spiking neural networks (SNNs) are investigated as biologically inspired
models of neural computation, distinguished by their computational capability
and energy efficiency due to precise spiking times and sparse spikes with
event-driven computation. A significant question is how SNNs can emulate
human-like graph-based reasoning of concepts and relations, especially
leveraging the temporal domain optimally. This paper reveals that SNNs, when
amalgamated with synaptic delay and temporal coding, are proficient in
executing (knowledge) graph reasoning. It is elucidated that spiking time can
function as an additional dimension to encode relation properties via a
neural-generalized path formulation. Empirical results highlight the efficacy
of temporal delay in relation processing and showcase exemplary performance in
diverse graph reasoning tasks. The spiking model is theoretically estimated to
achieve $20\\times$ energy savings compared to non-spiking counterparts,
deepening insights into the capabilities and potential of biologically inspired
SNNs for efficient reasoning. The code is available at
https://github.com/pkuxmq/GRSNN.
`,authors:"Mingqing Xiao; Yixin Zhu; Di He; Zhouchen Lin",status:0,relevancy:.4680441503061067,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17372",date:"2024-05-27",title:`BehaviorGPT: Smart Agent Simulation for Autonomous Driving with
  Next-Patch Prediction`,abstract:`  Simulating realistic interactions among traffic agents is crucial for
efficiently validating the safety of autonomous driving systems. Existing
leading simulators primarily use an encoder-decoder structure to encode the
historical trajectories for future simulation. However, such a paradigm
complicates the model architecture, and the manual separation of history and
future trajectories leads to low data utilization. To address these challenges,
we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a
decoder-only, autoregressive architecture designed to simulate the sequential
motion of multiple agents. Crucially, our approach discards the traditional
separation between "history" and "future," treating each time step as the
"current" one, resulting in a simpler, more parameter- and data-efficient
design that scales seamlessly with data and computation. Additionally, we
introduce the Next-Patch Prediction Paradigm (NP3), which enables models to
reason at the patch level of trajectories and capture long-range
spatial-temporal interactions. BehaviorGPT ranks first across several metrics
on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in
multi-agent and agent-map interactions. We outperformed state-of-the-art models
with a realism score of 0.741 and improved the minADE metric to 1.540, with an
approximately 91.6% reduction in model parameters.
`,authors:"Zikang Zhou; Haibo Hu; Xinhong Chen; Jianping Wang; Nan Guan; Kui Wu; Yung-Hui Li; Yu-Kai Huang; Chun Jason Xue",status:0,relevancy:.46652290848841327,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17535",date:"2024-05-27",title:"Calibrated Dataset Condensation for Faster Hyperparameter Search",abstract:`  Dataset condensation can be used to reduce the computational cost of training
multiple models on a large dataset by condensing the training dataset into a
small synthetic set. State-of-the-art approaches rely on matching the model
gradients between the real and synthetic data. However, there is no theoretical
guarantee of the generalizability of the condensed data: data condensation
often generalizes poorly across hyperparameters/architectures in practice. This
paper considers a different condensation objective specifically geared toward
hyperparameter search. We aim to generate a synthetic validation dataset so
that the validation-performance rankings of the models, with different
hyperparameters, on the condensed and original datasets are comparable. We
propose a novel hyperparameter-calibrated dataset condensation (HCDC)
algorithm, which obtains the synthetic validation dataset by matching the
hyperparameter gradients computed via implicit differentiation and efficient
inverse Hessian approximation. Experiments demonstrate that the proposed
framework effectively maintains the validation-performance rankings of models
and speeds up hyperparameter/architecture search for tasks on both images and
graphs.
`,authors:"Mucong Ding; Yuancheng Xu; Tahseen Rabbani; Xiaoyu Liu; Brian Gravelle; Teresa Ranadive; Tai-Ching Tuan; Furong Huang",status:0,relevancy:.46648243627871344,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17628",date:"2024-05-27",title:"Tensor Low-rank Approximation of Finite-horizon Value Functions",abstract:`  The goal of reinforcement learning is estimating a policy that maps states to
actions and maximizes the cumulative reward of a Markov Decision Process (MDP).
This is oftentimes achieved by estimating first the optimal (reward) value
function (VF) associated with each state-action pair. When the MDP has an
infinite horizon, the optimal VFs and policies are stationary under mild
conditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary
with time. This poses a challenge since the number of VFs to estimate grows not
only with the size of the state-action space but also with the time horizon.
This paper proposes a non-parametric low-rank stochastic algorithm to
approximate the VFs of finite-horizon MDPs. First, we represent the (unknown)
VFs as a multi-dimensional array, or tensor, where time is one of the
dimensions. Then, we use rewards sampled from the MDP to estimate the optimal
VFs. More precisely, we use the (truncated) PARAFAC decomposition to design an
online low-rank algorithm that recovers the entries of the tensor of VFs. The
size of the low-rank PARAFAC model grows additively with respect to each of its
dimensions, rendering our approach efficient, as demonstrated via numerical
experiments.
`,authors:"Sergio Rozada; Antonio G. Marques",status:0,relevancy:.46178719348546093,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16830",date:"2024-05-27",title:`Structured Graph Network for Constrained Robot Crowd Navigation with Low
  Fidelity Simulation`,abstract:`  We investigate the feasibility of deploying reinforcement learning (RL)
policies for constrained crowd navigation using a low-fidelity simulator. We
introduce a representation of the dynamic environment, separating human and
obstacle representations. Humans are represented through detected states, while
obstacles are represented as computed point clouds based on maps and robot
localization. This representation enables RL policies trained in a low-fidelity
simulator to deploy in real world with a reduced sim2real gap. Additionally, we
propose a spatio-temporal graph to model the interactions between agents and
obstacles. Based on the graph, we use attention mechanisms to capture the
robot-human, human-human, and human-obstacle interactions. Our method
significantly improves navigation performance in both simulated and real-world
environments. Video demonstrations can be found at
https://sites.google.com/view/constrained-crowdnav/home.
`,authors:"Shuijing Liu; Kaiwen Hong; Neeloy Chakraborty; Katherine Driggs-Campbell",status:0,relevancy:.45884806849533766,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16752",date:"2024-05-27",title:"Model Ensembling for Constrained Optimization",abstract:`  There is a long history in machine learning of model ensembling, beginning
with boosting and bagging and continuing to the present day. Much of this
history has focused on combining models for classification and regression, but
recently there is interest in more complex settings such as ensembling policies
in reinforcement learning. Strong connections have also emerged between
ensembling and multicalibration techniques. In this work, we further
investigate these themes by considering a setting in which we wish to ensemble
models for multidimensional output predictions that are in turn used for
downstream optimization. More precisely, we imagine we are given a number of
models mapping a state space to multidimensional real-valued predictions. These
predictions form the coefficients of a linear objective that we would like to
optimize under specified constraints. The fundamental question we address is
how to improve and combine such models in a way that outperforms the best of
them in the downstream optimization problem. We apply multicalibration
techniques that lead to two provably efficient and convergent algorithms. The
first of these (the white box approach) requires being given models that map
states to output predictions, while the second (the \\emph{black box} approach)
requires only policies (mappings from states to solutions to the optimization
problem). For both, we provide convergence and utility guarantees. We conclude
by investigating the performance and behavior of the two algorithms in a
controlled experimental setting.
`,authors:"Ira Globus-Harris; Varun Gupta; Michael Kearns; Aaron Roth",status:0,relevancy:.4564235497196707,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17253",date:"2024-05-27",title:"Gaussian Embedding of Temporal Networks",abstract:`  Representing the nodes of continuous-time temporal graphs in a
low-dimensional latent space has wide-ranging applications, from prediction to
visualization. Yet, analyzing continuous-time relational data with timestamped
interactions introduces unique challenges due to its sparsity. Merely embedding
nodes as trajectories in the latent space overlooks this sparsity, emphasizing
the need to quantify uncertainty around the latent positions. In this paper, we
propose TGNE (\\textbf{T}emporal \\textbf{G}aussian \\textbf{N}etwork
\\textbf{E}mbedding), an innovative method that bridges two distinct strands of
literature: the statistical analysis of networks via Latent Space Models
(LSM)\\cite{Hoff2002} and temporal graph machine learning. TGNE embeds nodes as
piece-wise linear trajectories of Gaussian distributions in the latent space,
capturing both structural information and uncertainty around the trajectories.
We evaluate TGNE's effectiveness in reconstructing the original graph and
modelling uncertainty. The results demonstrate that TGNE generates competitive
time-varying embedding locations compared to common baselines for
reconstructing unobserved edge interactions based on observed edges.
Furthermore, the uncertainty estimates align with the time-varying degree
distribution in the network, providing valuable insights into the temporal
dynamics of the graph. To facilitate reproducibility, we provide an open-source
implementation of TGNE at \\url{https://github.com/aida-ugent/tgne}.
`,authors:"Raphaël Romero; Jefrey Lijffijt; Riccardo Rastelli; Marco Corneli; Tijl De Bie",status:0,relevancy:.45577913674969794,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17053",date:"2024-05-27",title:`WirelessLLM: Empowering Large Language Models Towards Wireless
  Intelligence`,abstract:`  The rapid evolution of wireless technologies and the growing complexity of
network infrastructures necessitate a paradigm shift in how communication
networks are designed, configured, and managed. Recent advancements in Large
Language Models (LLMs) have sparked interest in their potential to
revolutionize wireless communication systems. However, existing studies on LLMs
for wireless systems are limited to a direct application for telecom language
understanding. To empower LLMs with knowledge and expertise in the wireless
domain, this paper proposes WirelessLLM, a comprehensive framework for adapting
and enhancing LLMs to address the unique challenges and requirements of
wireless communication networks. We first identify three foundational
principles that underpin WirelessLLM: knowledge alignment, knowledge fusion,
and knowledge evolution. Then, we investigate the enabling technologies to
build WirelessLLM, including prompt engineering, retrieval augmented
generation, tool usage, multi-modal pre-training, and domain-specific
fine-tuning. Moreover, we present three case studies to demonstrate the
practical applicability and benefits of WirelessLLM for solving typical
problems in wireless networks. Finally, we conclude this paper by highlighting
key challenges and outlining potential avenues for future research.
`,authors:"Jiawei Shao; Jingwen Tong; Qiong Wu; Wei Guo; Zijian Li; Zehong Lin; Jun Zhang",status:0,relevancy:.455027440264399,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17272",date:"2024-05-27",title:`DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max
  Vehicle Routing Problems`,abstract:`  The min-max vehicle routing problem (min-max VRP) traverses all given
customers by assigning several routes and aims to minimize the length of the
longest route. Recently, reinforcement learning (RL)-based sequential planning
methods have exhibited advantages in solving efficiency and optimality.
However, these methods fail to exploit the problem-specific properties in
learning representations, resulting in less effective features for decoding
optimal routes. This paper considers the sequential planning process of min-max
VRPs as two coupled optimization tasks: customer partition for different routes
and customer navigation in each route (i.e., partition and navigation). To
effectively process min-max VRP instances, we present a novel attention-based
Partition-and-Navigation encoder (P&N Encoder) that learns distinct embeddings
for partition and navigation. Furthermore, we utilize an inherent symmetry in
decoding routes and develop an effective agent-permutation-symmetric (APS) loss
function. Experimental results demonstrate that the proposed
Decoupling-Partition-Navigation (DPN) method significantly surpasses existing
learning-based methods in both single-depot and multi-depot min-max VRPs. Our
code is available at
`,authors:"Zhi Zheng; Shunyu Yao; Zhenkun Wang; Xialiang Tong; Mingxuan Yuan; Ke Tang",status:0,relevancy:.453391192867578,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17523",date:"2024-05-27",title:"Locally Testing Model Detections for Semantic Global Concepts",abstract:`  Ensuring the quality of black-box Deep Neural Networks (DNNs) has become ever
more significant, especially in safety-critical domains such as automated
driving. While global concept encodings generally enable a user to test a model
for a specific concept, linking global concept encodings to the local
processing of single network inputs reveals their strengths and limitations.
Our proposed framework global-to-local Concept Attribution (glCA) uses
approaches from local (why a specific prediction originates) and global (how a
model works generally) eXplainable Artificial Intelligence (xAI) to test DNNs
for a predefined semantical concept locally. The approach allows for
conditioning local, post-hoc explanations on predefined semantic concepts
encoded as linear directions in the model's latent space. Pixel-exact scoring
concerning the global concept usage assists the tester in further understanding
the model processing of single data points for the selected concept. Our
approach has the advantage of fully covering the model-internal encoding of the
semantic concept and allowing the localization of relevant concept-related
information. The results show major differences in the local perception and
usage of individual global concept encodings and demand for further
investigations regarding obtaining thorough semantic concept encodings.
`,authors:"Franz Motzkus; Georgii Mikriukov; Christian Hellert; Ute Schmid",status:0,relevancy:.4531931431896975,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17216",date:"2024-05-27",title:"Autoformalizing Euclidean Geometry",abstract:`  Autoformalization involves automatically translating informal math into
formal theorems and proofs that are machine-verifiable. Euclidean geometry
provides an interesting and controllable domain for studying autoformalization.
In this paper, we introduce a neuro-symbolic framework for autoformalizing
Euclidean geometry, which combines domain knowledge, SMT solvers, and large
language models (LLMs). One challenge in Euclidean geometry is that informal
proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To
address this issue, we use theorem provers to fill in such diagrammatic
information automatically, so that the LLM only needs to autoformalize the
explicit textual steps, making it easier for the model. We also provide
automatic semantic evaluation for autoformalized theorem statements. We
construct LeanEuclid, an autoformalization benchmark consisting of problems
from Euclid's Elements and the UniGeo dataset formalized in the Lean proof
assistant. Experiments with GPT-4 and GPT-4V show the capability and
limitations of state-of-the-art LLMs on autoformalizing geometry problems. The
data and code are available at https://github.com/loganrjmurphy/LeanEuclid.
`,authors:"Logan Murphy; Kaiyu Yang; Jialiang Sun; Zhaoyu Li; Anima Anandkumar; Xujie Si",status:0,relevancy:.44660771314981684,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16792",date:"2024-05-27",title:"Laurel: Generating Dafny Assertions Using Large Language Models",abstract:`  Dafny is a popular verification language, which automates proofs by
outsourcing them to an SMT solver. This automation is not perfect, however, and
the solver often requires guidance in the form of helper assertions creating a
burden for the proof engineer. In this paper, we propose Laurel, a tool that
uses large language models (LLMs) to automatically generate helper assertions
for Dafny programs. To improve the success rate of LLMs in this task, we design
two domain-specific prompting techniques. First, we help the LLM determine the
location of the missing assertion by analyzing the verifier's error message and
inserting an assertion placeholder at that location. Second, we provide the LLM
with example assertions from the same codebase, which we select based on a new
lemma similarity metric. We evaluate our techniques on a dataset of helper
assertions we extracted from three real-world Dafny codebases. Our evaluation
shows that Laurel is able to generate over 50% of the required helper
assertions given only a few attempts, making LLMs a usable and affordable tool
to further automate practical program verification.
`,authors:"Eric Mugnier; Emmanuel Anaya Gonzalez; Ranjit Jhala; Nadia Polikarpova; Yuanyuan Zhou",status:0,relevancy:.44575798447962833,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16820",date:"2024-05-27",title:`Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT
  Even in Low-Resource Settings`,abstract:`  The rapid proliferation of generative AI has raised questions about the
competitiveness of lower-parameter, locally tunable, open-weight models
relative to high-parameter, API-guarded, closed-weight models in terms of
performance, domain adaptation, cost, and generalization. Centering
under-resourced yet risk-intolerant settings in government, research, and
healthcare, we see for-profit closed-weight models as incompatible with
requirements for transparency, privacy, adaptability, and standards of
evidence. Yet the performance penalty in using open-weight models, especially
in low-data and low-resource settings, is unclear.
  We assess the feasibility of using smaller, open-weight models to replace
GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to
only a single, low-cost GPU. We assess value-sensitive issues around bias,
privacy, and abstention on three additional tasks relevant to those topics. We
find that with relatively low effort, very low absolute monetary cost, and
relatively little data for fine-tuning, small open-weight models can achieve
competitive performance in domain-adapted tasks without sacrificing generality.
We then run experiments considering practical issues in bias, privacy, and
hallucination risk, finding that open models offer several benefits over closed
models. We intend this work as a case study in understanding the opportunity
cost of reproducibility and transparency over for-profit state-of-the-art zero
shot performance, finding this cost to be marginal under realistic settings.
`,authors:"Robert Wolfe; Isaac Slaughter; Bin Han; Bingbing Wen; Yiwei Yang; Lucas Rosenblatt; Bernease Herman; Eva Brown; Zening Qu; Nic Weber; Bill Howe",status:0,relevancy:.44423645899245234,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17607",date:"2024-05-27",title:`Advancing Cultural Inclusivity: Optimizing Embedding Spaces for Balanced
  Music Recommendations`,abstract:`  Popularity bias in music recommendation systems -- where artists and tracks
with the highest listen counts are recommended more often -- can also propagate
biases along demographic and cultural axes. In this work, we identify these
biases in recommendations for artists from underrepresented cultural groups in
prototype-based matrix factorization methods. Unlike traditional matrix
factorization methods, prototype-based approaches are interpretable. This
allows us to directly link the observed bias in recommendations for minority
artists (the effect) to specific properties of the embedding space (the cause).
We mitigate popularity bias in music recommendation through capturing both
users' and songs' cultural nuances in the embedding space. To address these
challenges while maintaining recommendation quality, we propose two novel
enhancements to the embedding space: i) we propose an approach to filter-out
the irrelevant prototypes used to represent each user and item to improve
generalizability, and ii) we introduce regularization techniques to reinforce a
more uniform distribution of prototypes within the embedding space. Our results
demonstrate significant improvements in reducing popularity bias and enhancing
demographic and cultural fairness in music recommendations while achieving
competitive -- if not better -- overall performance.
`,authors:"Armin Moradi; Nicola Neophytou; Golnoosh Farnadi",status:0,relevancy:.4365798993231671,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16847",date:"2024-05-27",title:`TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture
  Token Prediction`,abstract:`  Autoregressive next-token prediction is a standard pretraining method for
large-scale language models, but its application to vision tasks is hindered by
the non-sequential nature of image data, leading to cumulative errors. Most
vision models employ masked autoencoder (MAE) based pretraining, which faces
scalability issues. To address these challenges, we introduce
\\textbf{TokenUnify}, a novel pretraining method that integrates random token
prediction, next-token prediction, and next-all token prediction. We provide
theoretical evidence demonstrating that TokenUnify mitigates cumulative errors
in visual autoregression. Cooperated with TokenUnify, we have assembled a
large-scale electron microscopy (EM) image dataset with ultra-high resolution,
ideal for creating spatially correlated long sequences. This dataset includes
over 120 million annotated voxels, making it the largest neuron segmentation
dataset to date and providing a unified benchmark for experimental validation.
Leveraging the Mamba network inherently suited for long-sequence modeling on
this dataset, TokenUnify not only reduces the computational complexity but also
leads to a significant 45\\% improvement in segmentation performance on
downstream EM neuron segmentation tasks compared to existing methods.
Furthermore, TokenUnify demonstrates superior scalability over MAE and
traditional autoregressive methods, effectively bridging the gap between
pretraining strategies for language and vision models. Code is available at
\\url{https://github.com/ydchen0806/TokenUnify}.
`,authors:"Yinda Chen; Haoyuan Shi; Xiaoyu Liu; Te Shi; Ruobing Zhang; Dong Liu; Zhiwei Xiong; Feng Wu",status:0,relevancy:.4312004611110959,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17610",date:"2024-05-27",title:`Explainable machine learning multi-label classification of Spanish legal
  judgements`,abstract:`  Artificial Intelligence techniques such as Machine Learning (ML) have not
been exploited to their maximum potential in the legal domain. This has been
partially due to the insufficient explanations they provided about their
decisions. Automatic expert systems with explanatory capabilities can be
specially useful when legal practitioners search jurisprudence to gather
contextual knowledge for their cases. Therefore, we propose a hybrid system
that applies ML for multi-label classification of judgements (sentences) and
visual and natural language descriptions for explanation purposes, boosted by
Natural Language Processing techniques and deep legal reasoning to identify the
entities, such as the parties, involved. We are not aware of any prior work on
automatic multi-label classification of legal judgements also providing natural
language explanations to the end-users with comparable overall quality. Our
solution achieves over 85 % micro precision on a labelled data set annotated by
legal experts. This endorses its interest to relieve human experts from
monotonous labour-intensive legal classification tasks.
`,authors:"Francisco de Arriba-Pérez; Silvia García-Méndez; Francisco J. González-Castaño; Jaime González-González",status:0,relevancy:.43032799429604274,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17514",date:"2024-05-27",title:`AbstractBeam: Enhancing Bottom-Up Program Synthesis using Library
  Learning`,abstract:`  LambdaBeam is a state-of-the-art execution-guided algorithm for program
synthesis that incorporates higher-order functions, lambda functions, and
iterative loops into the Domain-Specific Language (DSL). LambdaBeam generates
every program from the start. Yet, many program blocks or subprograms occur
frequently in a given domain, e.g., loops to traverse a list. Thus, repeating
programs can be used to enhance the synthesis algorithm. However, LambdaBeam
fails to leverage this potential. For this purpose, we introduce AbstractBeam:
A novel program synthesis framework that employs Library Learning to identify
such program repetitions, integrates them into the DSL, and thus utilizes their
potential to boost LambdaBeam's synthesis algorithm. Our experimental
evaluations demonstrate that AbstractBeam significantly improves LambdaBeam's
performance in the LambdaBeam integer list manipulation domain. Additionally,
AbstractBeam's program generation is more efficient compared to LambdaBeam's
synthesis. Finally, our findings indicate that Library Learning is effective in
domains not specifically crafted to highlight its benefits.
`,authors:"Janis Zenkner; Lukas Dierkes; Tobias Sesterhenn; Chrisitan Bartelt",status:0,relevancy:.42920826620517527,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17358",date:"2024-05-27",title:"Rethinking Transformers in Solving POMDPs",abstract:`  Sequential decision-making algorithms such as reinforcement learning (RL) in
real-world scenarios inevitably face environments with partial observability.
This paper scrutinizes the effectiveness of a popular architecture, namely
Transformers, in Partially Observable Markov Decision Processes (POMDPs) and
reveals its theoretical limitations. We establish that regular languages, which
Transformers struggle to model, are reducible to POMDPs. This poses a
significant challenge for Transformers in learning POMDP-specific inductive
biases, due to their lack of inherent recurrence found in other models like
RNNs. This paper casts doubt on the prevalent belief in Transformers as
sequence models for RL and proposes to introduce a point-wise recurrent
structure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited
alternative for Partially Observable RL, with empirical results highlighting
the sub-optimal performance of the Transformer and considerable strength of
LRU.
`,authors:"Chenhao Lu; Ruizhe Shi; Yuyao Liu; Kaizhe Hu; Simon S. Du; Huazhe Xu",status:0,relevancy:.42899627156400677,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16883",date:"2024-05-27",title:"Scorch: A Library for Sparse Deep Learning",abstract:`  The rapid growth in the size of deep learning models strains the capabilities
of traditional dense computation paradigms. Leveraging sparse computation has
become increasingly popular for training and deploying large-scale models, but
existing deep learning frameworks lack extensive support for sparse operations.
To bridge this gap, we introduce Scorch, a library that seamlessly integrates
efficient sparse tensor computation into the PyTorch ecosystem, with an initial
focus on inference workloads on CPUs. Scorch provides a flexible and intuitive
interface for sparse tensors, supporting diverse sparse data structures. Scorch
introduces a compiler stack that automates key optimizations, including
automatic loop ordering, tiling, and format inference. Combined with a runtime
that adapts its execution to both dense and sparse data, Scorch delivers
substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations
without sacrificing usability. More importantly, Scorch enables efficient
computation of complex sparse operations that lack hand-optimized PyTorch
implementations. This flexibility is crucial for exploring novel sparse
architectures. We demonstrate Scorch's ease of use and performance gains on
diverse deep learning models across multiple domains. With only minimal code
changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end
tasks. Scorch's seamless integration and performance gains make it a valuable
addition to the PyTorch ecosystem. We believe Scorch will enable wider
exploration of sparsity as a tool for scaling deep learning and inform the
development of other sparse libraries.
`,authors:"Bobby Yan; Alexander J. Root; Trevor Gale; David Broman; Fredrik Kjolstad",status:0,relevancy:.4249211758629958,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17638",date:"2024-05-27",title:`The surprising efficiency of temporal difference learning for rare event
  prediction`,abstract:`  We quantify the efficiency of temporal difference (TD) learning over the
direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement
learning, with an emphasis on estimation of quantities related to rare events.
Policy evaluation is complicated in the rare event setting by the long
timescale of the event and by the need for \\emph{relative accuracy} in
estimates of very small values. Specifically, we focus on least-squares TD
(LSTD) prediction for finite state Markov chains, and show that LSTD can
achieve relative accuracy far more efficiently than MC. We prove a central
limit theorem for the LSTD estimator and upper bound the \\emph{relative
asymptotic variance} by simple quantities characterizing the connectivity of
states relative to the transition probabilities between them. Using this bound,
we show that, even when both the timescale of the rare event and the relative
accuracy of the MC estimator are exponentially large in the number of states,
LSTD maintains a fixed level of relative accuracy with a total number of
observed transitions of the Markov chain that is only \\emph{polynomially} large
in the number of states.
`,authors:"Xiaoou Cheng; Jonathan Weare",status:0,relevancy:.42290361176635993,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16730",date:"2024-05-27",title:`Latent Energy-Based Odyssey: Black-Box Optimization via Expanded
  Exploration in the Energy-Based Latent Space`,abstract:`  Offline Black-Box Optimization (BBO) aims at optimizing a black-box function
using the knowledge from a pre-collected offline dataset of function values and
corresponding input designs. However, the high-dimensional and
highly-multimodal input design space of black-box function pose inherent
challenges for most existing methods that model and operate directly upon input
designs. These issues include but are not limited to high sample complexity,
which relates to inaccurate approximation of black-box function; and
insufficient coverage and exploration of input design modes, which leads to
suboptimal proposal of new input designs. In this work, we consider finding a
latent space that serves as a compressed yet accurate representation of the
design-value joint space, enabling effective latent exploration of high-value
input design modes. To this end, we formulate an learnable energy-based latent
space, and propose Noise-intensified Telescoping density-Ratio Estimation
(NTRE) scheme for variational learning of an accurate latent space model
without costly Markov Chain Monte Carlo. The optimization process is then
exploration of high-value designs guided by the learned energy-based model in
the latent space, formulated as gradient-based sampling from a
latent-variable-parameterized inverse model. We show that our particular
parameterization encourages expanded exploration around high-value design
modes, motivated by inversion thinking of a fundamental result of conditional
covariance matrix typically used for variance reduction. We observe that our
method, backed by an accurately learned informative latent space and an
expanding-exploration model design, yields significant improvements over strong
previous methods on both synthetic and real world datasets such as the
design-bench suite.
`,authors:"Peiyu Yu; Dinghuai Zhang; Hengzhi He; Xiaojian Ma; Ruiyao Miao; Yifan Lu; Yasi Zhang; Deqian Kong; Ruiqi Gao; Jianwen Xie; Guang Cheng; Ying Nian Wu",status:0,relevancy:.4210100742843319,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17279",date:"2024-05-27",title:`Socially-Aware Shared Control Navigation for Assistive Mobile Robots in
  the Built Environment`,abstract:`  As the number of Persons with Disabilities (PWD), particularly those with one
or more physical impairments, increases, there is an increasing demand for
assistive robotic technologies that can support independent mobility in the
built environment and reduce the burden on caregivers. Current assistive
mobility platforms (e.g., robotic wheelchairs) often fail to incorporate user
preferences and control, leading to reduced trust and efficiency. Existing
shared control algorithms do not allow the incorporation of the user control
preferences inside the navigation framework or the path planning algorithm. In
addition, existing dynamic local planner algorithms for robotic wheelchairs do
not take into account the social spaces of people, potentially leading such
platforms to infringe upon these areas and cause discomfort. To address these
concerns, this work introduces a novel socially-aware shared autonomy-based
navigation system for assistive mobile robotic platforms.
  Our navigation framework comprises a Global Planner and a Local Planner. To
implement the Global Planner, the proposed approach introduces a novel User
Preference Field (UPF) theory within its global planning framework, explicitly
acknowledging user preferences to adeptly navigate away from congested areas.
For the Local Planner, we propose a Socially-aware Shared Control-based Model
Predictive Control with Dynamic Control Barrier Function (SS-MPC-DCBF) to
adjust movements in real-time, integrating user preferences for safer, more
autonomous navigation. Evaluation results show that our Global Planner aligns
closely with user preferences compared to baselines, and our Local Planner
demonstrates enhanced safety and efficiency in dynamic and static scenarios.
This integrated approach fosters trust and autonomy, crucial for the acceptance
of assistive mobility technologies in the built environment.
`,authors:"Yifan Xu; Qianwei Wang; Vineet Kamat; Carol Menassa",status:0,relevancy:.42095704544333123,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17708",date:"2024-05-27",title:`OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates
  of Multiple Estimators`,abstract:`  Offline policy evaluation (OPE) allows us to evaluate and estimate a new
sequential decision-making policy's performance by leveraging historical
interaction data collected from other policies. Evaluating a new policy online
without a confident estimate of its performance can lead to costly, unsafe, or
hazardous outcomes, especially in education and healthcare. Several OPE
estimators have been proposed in the last decade, many of which have
hyperparameters and require training. Unfortunately, choosing the best OPE
algorithm for each task and domain is still unclear. In this paper, we propose
a new algorithm that adaptively blends a set of OPE estimators given a dataset
without relying on an explicit selection using a statistical procedure. We
prove that our estimator is consistent and satisfies several desirable
properties for policy evaluation. Additionally, we demonstrate that when
compared to alternative approaches, our estimator can be used to select
higher-performing policies in healthcare and robotics. Our work contributes to
improving ease of use for a general-purpose, estimator-agnostic, off-policy
evaluation framework for offline RL.
`,authors:"Allen Nie; Yash Chandak; Christina J. Yuan; Anirudhan Badrinath; Yannis Flet-Berliac; Emma Brunskil",status:0,relevancy:.4208149681132872,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17182",date:"2024-05-27",title:`Exploring the Performance of Continuous-Time Dynamic Link Prediction
  Algorithms`,abstract:`  Dynamic Link Prediction (DLP) addresses the prediction of future links in
evolving networks. However, accurately portraying the performance of DLP
algorithms poses challenges that might impede progress in the field.
Importantly, common evaluation pipelines usually calculate ranking or binary
classification metrics, where the scores of observed interactions (positives)
are compared with those of randomly generated ones (negatives). However, a
single metric is not sufficient to fully capture the differences between DLP
algorithms, and is prone to overly optimistic performance evaluation. Instead,
an in-depth evaluation should reflect performance variations across different
nodes, edges, and time segments. In this work, we contribute tools to perform
such a comprehensive evaluation. (1) We propose Birth-Death diagrams, a simple
but powerful visualization technique that illustrates the effect of time-based
train-test splitting on the difficulty of DLP on a given dataset. (2) We
describe an exhaustive taxonomy of negative sampling methods that can be used
at evaluation time. (3) We carry out an empirical study of the effect of the
different negative sampling strategies. Our comparison between heuristics and
state-of-the-art memory-based methods on various real-world datasets confirms a
strong effect of using different negative sampling strategies on the test Area
Under the Curve (AUC). Moreover, we conduct a visual exploration of the
prediction, with additional insights on which different types of errors are
prominent over time.
`,authors:"Raphaël Romero; Maarten Buyl; Tijl De Bie; Jefrey Lijffijt",status:0,relevancy:.4138786378625492,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17403",date:"2024-05-27",title:`A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion
  Model Training`,abstract:`  Training diffusion models is always a computation-intensive task. In this
paper, we introduce a novel speed-up method for diffusion model training,
called, which is based on a closer look at time steps. Our key findings are: i)
Time steps can be empirically divided into acceleration, deceleration, and
convergence areas based on the process increment. ii) These time steps are
imbalanced, with many concentrated in the convergence area. iii) The
concentrated steps provide limited benefits for diffusion training. To address
this, we design an asymmetric sampling strategy that reduces the frequency of
steps from the convergence area while increasing the sampling probability for
steps from other areas. Additionally, we propose a weighting strategy to
emphasize the importance of time steps with rapid-change process increments. As
a plug-and-play and architecture-agnostic approach, SpeeD consistently achieves
3-times acceleration across various diffusion architectures, datasets, and
tasks. Notably, due to its simple design, our approach significantly reduces
the cost of diffusion model training with minimal overhead. Our research
enables more researchers to train diffusion models at a lower cost.
`,authors:"Kai Wang; Yukun Zhou; Mingjia Shi; Zhihang Yuan; Yuzhang Shang; Xiaojiang Peng; Hanwang Zhang; Yang You",status:0,relevancy:.41102471889139214,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16761",date:"2024-05-27",title:`Masked Face Recognition with Generative-to-Discriminative
  Representations`,abstract:`  Masked face recognition is important for social good but challenged by
diverse occlusions that cause insufficient or inaccurate representations. In
this work, we propose a unified deep network to learn
generative-to-discriminative representations for facilitating masked face
recognition. To this end, we split the network into three modules and learn
them on synthetic masked faces in a greedy module-wise pretraining manner.
First, we leverage a generative encoder pretrained for face inpainting and
finetune it to represent masked faces into category-aware descriptors.
Attribute to the generative encoder's ability in recovering context
information, the resulting descriptors can provide occlusion-robust
representations for masked faces, mitigating the effect of diverse masks. Then,
we incorporate a multi-layer convolutional network as a discriminative reformer
and learn it to convert the category-aware descriptors into identity-aware
vectors, where the learning is effectively supervised by distilling relation
knowledge from off-the-shelf face recognition model. In this way, the
discriminative reformer together with the generative encoder serves as the
pretrained backbone, providing general and discriminative representations
towards masked faces. Finally, we cascade one fully-connected layer following
by one softmax layer into a feature classifier and finetune it to identify the
reformed identity-aware vectors. Extensive experiments on synthetic and
realistic datasets demonstrate the effectiveness of our approach in recognizing
masked faces.
`,authors:"Shiming Ge; Weijia Guo; Chenyu Li; Junzheng Zhang; Yong Li; Dan Zeng",status:0,relevancy:.40972512199191435,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17139",date:"2024-05-27",title:`Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive
  Backbone Ensembling`,abstract:`  Contrastive Language-Image Pretraining (CLIP) stands out as a prominent
method for image representation learning. Various architectures, from vision
transformers (ViTs) to convolutional networks (ResNets) have been trained with
CLIP to serve as general solutions to diverse vision tasks. This paper explores
the differences across various CLIP-trained vision backbones. Despite using the
same data and training objective, we find that these architectures have notably
different representations, different classification performance across
datasets, and different robustness properties to certain types of image
perturbations. Our findings indicate a remarkable possible synergy across
backbones by leveraging their respective strengths. In principle,
classification accuracy could be improved by over 40 percentage with an
informed selection of the optimal backbone per test example.Using this insight,
we develop a straightforward yet powerful approach to adaptively ensemble
multiple backbones. The approach uses as few as one labeled example per class
to tune the adaptive combination of backbones. On a large collection of
datasets, the method achieves a remarkable increase in accuracy of up to 39.1%
over the best single backbone, well beyond traditional ensembles
`,authors:"Cristian Rodriguez-Opazo; Ehsan Abbasnejad; Damien Teney; Edison Marrese-Taylor; Hamed Damirchi; Anton van den Hengel",status:0,relevancy:.4096510192896936,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17604",date:"2024-05-27",title:"LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters",abstract:`  The recent trend in scaling language models has led to a growing demand for
parameter-efficient tuning (PEFT) methods such as LoRA (Low-Rank Adaptation).
LoRA consistently matches or surpasses the full fine-tuning baseline with fewer
parameters. However, handling numerous task-specific or user-specific LoRA
modules on top of a base model still presents significant storage challenges.
To address this, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small
number of parameters), a novel approach leveraging Singular Value Decomposition
(SVD) for parameter-efficient fine-tuning. LoRA-XS introduces a small r x r
weight matrix between frozen LoRA matrices, which are constructed by SVD of the
original weight matrix. Training only r x r weight matrices ensures
independence from model dimensions, enabling more parameter-efficient
fine-tuning, especially for larger models. LoRA-XS achieves a remarkable
reduction of trainable parameters by over 100x in 7B models compared to LoRA.
Our benchmarking across various scales, including GLUE, GSM8k, and MATH
benchmarks, shows that our approach outperforms LoRA and recent
state-of-the-art approaches like VeRA in terms of parameter efficiency while
maintaining competitive performance.
`,authors:"Klaudia Bałazy; Mohammadreza Banaei; Karl Aberer; Jacek Tabor",status:0,relevancy:.40911810903546764,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17187",date:"2024-05-27",title:"Memorize What Matters: Emergent Scene Decomposition from Multitraverse",abstract:`  Humans naturally retain memories of permanent elements, while ephemeral
moments often slip through the cracks of memory. This selective retention is
crucial for robotic perception, localization, and mapping. To endow robots with
this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised,
camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM
converts multitraverse RGB videos from the same region into a Gaussian-based
environmental map while concurrently performing 2D ephemeral object
segmentation. Our key observation is that the environment remains consistent
across traversals, while objects frequently change. This allows us to exploit
self-supervision from repeated traversals to achieve environment-object
decomposition. More specifically, 3DGM formulates multitraverse environmental
mapping as a robust differentiable rendering problem, treating pixels of the
environment and objects as inliers and outliers, respectively. Using robust
feature distillation, feature residuals mining, and robust optimization, 3DGM
jointly performs 2D segmentation and 3D mapping without human intervention. We
build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets,
to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and
neural rendering. Extensive results verify the effectiveness and potential of
our method for self-driving and robotics.
`,authors:"Yiming Li; Zehong Wang; Yue Wang; Zhiding Yu; Zan Gojcic; Marco Pavone; Chen Feng; Jose M. Alvarez",status:0,relevancy:.40309342238285495,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17202",date:"2024-05-27",title:"Efficient multi-prompt evaluation of LLMs",abstract:`  Most popular benchmarks for comparing LLMs rely on a limited set of prompt
templates, which may not fully capture the LLMs' abilities and can affect the
reproducibility of results on leaderboards. Many recent works empirically
verify prompt sensitivity and advocate for changes in LLM evaluation. In this
paper, we consider the problem of estimating the performance distribution
across many prompt variants instead of finding a single prompt to evaluate
with. We introduce PromptEval, a method for estimating performance across a
large set of prompts borrowing strength across prompts and examples to produce
accurate estimates under practical evaluation budgets. The resulting
distribution can be used to obtain performance quantiles to construct various
robust performance metrics (e.g., top 95% quantile or median). We prove that
PromptEval consistently estimates the performance distribution and demonstrate
its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench
Hard, and LMentry. For example, PromptEval can accurately estimate performance
quantiles across 100 prompt templates on MMLU with a budget equivalent to two
single-prompt evaluations. Our code and data can be found at
https://github.com/felipemaiapolo/prompt-eval.
`,authors:"Felipe Maia Polo; Ronald Xu; Lucas Weber; Mírian Silva; Onkar Bhardwaj; Leshem Choshen; Allysson Flavio Melo de Oliveira; Yuekai Sun; Mikhail Yurochkin",status:0,relevancy:.40162343599569783,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16783",date:"2024-05-27",title:`TrojFM: Resource-efficient Backdoor Attacks against Very Large
  Foundation Models`,abstract:`  One key challenge in backdoor attacks against large foundation models is the
resource limits. Backdoor attacks usually require retraining the target model,
which is impractical for very large foundation models. Existing backdoor
attacks are mainly designed for supervised classifiers or small foundation
models (e.g., BERT). None of these attacks has successfully compromised a very
large foundation model, such as Llama-3-70B, especially with limited
computational resources. In this paper, we propose TrojFM, a novel backdoor
attack tailored for very large foundation models. Our primary technical
contribution is the development of a novel backdoor injection method. This
method forces a backdoored model to generate similar hidden representations for
poisoned inputs regardless of their actual semantics. Our approach injects such
backdoors by fine-tuning only a very small proportion of model parameters. This
enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks
against very large foundation models under limited computational resources.
Moreover, we optimize the fine-tuning process with our customized QLoRA
technique, enabling launching our attack via only~\\textit{one A100 GPU}.
Furthermore, we design a new trigger injection method to ensure our attack
stealthiness. Through extensive experiments, we first demonstrate that TrojFM
can launch effective backdoor attacks against widely used large GPT-style
models without jeopardizing their normal functionalities (and outperforming
existing attacks on BERT-style models). Furthermore, we show that TrojFM is
resilient to SOTA defenses and is insensitive to changes in key
hyper-parameters. Finally, we conduct a resource analysis to quantify that our
method can significantly save computational and memory costs compared to
existing backdoor attacks.
`,authors:"Yuzhou. Nie; Yanting. Wang; Jinyuan. Jia; Michael J. De Lucia; Nathaniel D. Bastian; Wenbo. Guo; Dawn. Song",status:0,relevancy:.40051467139857766,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17293",date:"2024-05-27",title:"Efficient Ensembles Improve Training Data Attribution",abstract:`  Training data attribution (TDA) methods aim to quantify the influence of
individual training data points on the model predictions, with broad
applications in data-centric AI, such as mislabel detection, data selection,
and copyright compensation. However, existing methods in this field, which can
be categorized as retraining-based and gradient-based, have struggled with the
trade-off between computational efficiency and attribution efficacy.
Retraining-based methods can accurately attribute complex non-convex models but
are computationally prohibitive, while gradient-based methods are efficient but
often fail for non-convex models. Recent research has shown that augmenting
gradient-based methods with ensembles of multiple independently trained models
can achieve significantly better attribution efficacy. However, this approach
remains impractical for very large-scale applications.
  In this work, we discover that expensive, fully independent training is
unnecessary for ensembling the gradient-based methods, and we propose two
efficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative
to naive independent ensemble. These strategies significantly reduce training
time (up to 80%), serving time (up to 60%), and space cost (up to 80%) while
maintaining similar attribution efficacy to the naive independent ensemble. Our
extensive experimental results demonstrate that the proposed strategies are
effective across multiple TDA methods on diverse datasets and models, including
generative settings, significantly advancing the Pareto frontier of TDA methods
with better computational efficiency and attribution efficacy.
`,authors:"Junwei Deng; Ting-Wei Li; Shichang Zhang; Jiaqi Ma",status:0,relevancy:.39765589259092715,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16887",date:"2024-05-27",title:`A Large Language Model-based multi-agent manufacturing system for
  intelligent shopfloor`,abstract:`  As productivity advances, the demand of customers for multi-variety and
small-batch production is increasing, thereby putting forward higher
requirements for manufacturing systems. When production tasks frequent changes
due to this demand, traditional manufacturing systems often cannot response
promptly. The multi-agent manufacturing system is proposed to address this
problem. However, because of technical limitations, the negotiation among
agents in this kind of system is realized through predefined heuristic rules,
which is not intelligent enough to deal with the multi-variety and small batch
production. To this end, a Large Language Model-based (LLM-based) multi-agent
manufacturing system for intelligent shopfloor is proposed in the present
study. This system delineates the diverse agents and defines their
collaborative methods. The roles of the agents encompass Machine Server Agent
(MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and
Decision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability
of analyzing the shopfloor condition and choosing the most suitable machine, as
opposed to executing a predefined program artificially. The negotiation between
BAs and BIA is the most crucial step in connecting manufacturing resources.
With the support of TA and DA, BIA will finalize the distribution of orders,
relying on the information of each machine returned by BA. MSAs bears the
responsibility for connecting the agents with the physical shopfloor. This
system aims to distribute and transmit workpieces through the collaboration of
the agents with these distinct roles, distinguishing it from other scheduling
approaches. Comparative experiments were also conducted to validate the
performance of this system.
`,authors:"Zhen Zhao; Dunbing Tang; Haihua Zhu; Zequn Zhang; Kai Chen; Changchun Liu; Yuchen Ji",status:0,relevancy:.39624565173825943,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17051",date:"2024-05-27",title:`BeamVQ: Aligning Space-Time Forecasting Model via Self-training on
  Physics-aware Metrics`,abstract:`  Data-driven deep learning has emerged as the new paradigm to model complex
physical space-time systems. These data-driven methods learn patterns by
optimizing statistical metrics and tend to overlook the adherence to physical
laws, unlike traditional model-driven numerical methods. Thus, they often
generate predictions that are not physically realistic. On the other hand, by
sampling a large amount of high quality predictions from a data-driven model,
some predictions will be more physically plausible than the others and closer
to what will happen in the future. Based on this observation, we propose
\\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical
alignment of data-driven space-time forecasting models. The key of BeamVQ is to
train model on self-generated samples filtered with physics-aware metrics. To
be flexibly support different backbone architectures, BeamVQ leverages a code
bank to transform any encoder-decoder model to the continuous state space into
discrete codes. Afterwards, it iteratively employs beam search to sample
high-quality sequences, retains those with the highest physics-aware scores,
and trains model on the new dataset. Comprehensive experiments show that BeamVQ
not only gave an average statistical skill score boost for more than 32% for
ten backbones on five datasets, but also significantly enhances physics-aware
metrics.
`,authors:"Hao Wu; Xingjian Shi; Ziyue Huang; Penghao Zhao; Wei Xiong; Jinbao Xue; Yangyu Tao; Xiaomeng Huang; Weiyan Wang",status:0,relevancy:.39275063042312974,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17516",date:"2024-05-27",title:"Time Elastic Neural Networks",abstract:`  We introduce and detail an atypical neural network architecture, called time
elastic neural network (teNN), for multivariate time series classification. The
novelty compared to classical neural network architecture is that it explicitly
incorporates time warping ability, as well as a new way of considering
attention. In addition, this architecture is capable of learning a dropout
strategy, thus optimizing its own architecture.Behind the design of this
architecture, our overall objective is threefold: firstly, we are aiming at
improving the accuracy of instance based classification approaches that shows
quite good performances as far as enough training data is available. Secondly
we seek to reduce the computational complexity inherent to these methods to
improve their scalability. Ideally, we seek to find an acceptable balance
between these first two criteria. And finally, we seek to enhance the
explainability of the decision provided by this kind of neural architecture.The
experiment demonstrates that the stochastic gradient descent implemented to
train a teNN is quite effective. To the extent that the selection of some
critical meta-parameters is correct, convergence is generally smooth and
fast.While maintaining good accuracy, we get a drastic gain in scalability by
first reducing the required number of reference time series, i.e. the number of
teNN cells required. Secondly, we demonstrate that, during the training
process, the teNN succeeds in reducing the number of neurons required within
each cell. Finally, we show that the analysis of the activation and attention
matrices as well as the reference time series after training provides relevant
information to interpret and explain the classification results.The comparative
study that we have carried out and which concerns around thirty diverse and
multivariate datasets shows that the teNN obtains results comparable to those
of the state of the art, in particular similar to those of a network mixing
LSTM and CNN architectures for example.
`,authors:"Pierre-François Marteau",status:0,relevancy:.39145383705283754,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17097",date:"2024-05-27",title:`Evaluation of Multi-task Uncertainties in Joint Semantic Segmentation
  and Monocular Depth Estimation`,abstract:`  While a number of promising uncertainty quantification methods have been
proposed to address the prevailing shortcomings of deep neural networks like
overconfidence and lack of explainability, quantifying predictive uncertainties
in the context of joint semantic segmentation and monocular depth estimation
has not been explored yet. Since many real-world applications are multi-modal
in nature and, hence, have the potential to benefit from multi-task learning,
this is a substantial gap in current literature. To this end, we conduct a
comprehensive series of experiments to study how multi-task learning influences
the quality of uncertainty estimates in comparison to solving both tasks
separately.
`,authors:"Steven Landgraf; Markus Hillemann; Theodor Kapler; Markus Ulrich",status:0,relevancy:.3913547533545545,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17399",date:"2024-05-27",title:"Transformers Can Do Arithmetic with the Right Embeddings",abstract:`  The poor performance of transformers on arithmetic tasks seems to stem in
large part from their inability to keep track of the exact position of each
digit inside of a large span of digits. We mend this problem by adding an
embedding to each digit that encodes its position relative to the start of the
number. In addition to the boost these embeddings provide on their own, we show
that this fix enables architectural modifications such as input injection and
recurrent layers to improve performance even further.
  With positions resolved, we can study the logical extrapolation ability of
transformers. Can they solve arithmetic problems that are larger and more
complex than those in their training data? We find that training on only 20
digit numbers with a single GPU for one day, we can reach state-of-the-art
performance, achieving up to 99% accuracy on 100 digit addition problems.
Finally, we show that these gains in numeracy also unlock improvements on other
multi-step reasoning tasks including sorting and multiplication.
`,authors:"Sean McLeish; Arpit Bansal; Alex Stein; Neel Jain; John Kirchenbauer; Brian R. Bartoldson; Bhavya Kailkhura; Abhinav Bhatele; Jonas Geiping; Avi Schwarzschild; Tom Goldstein",status:0,relevancy:.3857384254624048,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17245",date:"2024-05-27",title:`Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ
  Transformer Inference`,abstract:`  Transformer-based models have unlocked a plethora of powerful intelligent
applications at the edge, such as voice assistant in smart home. Traditional
deployment approaches offload the inference workloads to the remote cloud
server, which would induce substantial pressure on the backbone network as well
as raise users' privacy concerns. To address that, in-situ inference has been
recently recognized for edge intelligence, but it still confronts significant
challenges stemming from the conflict between intensive workloads and limited
on-device computing resources. In this paper, we leverage our observation that
many edge environments usually comprise a rich set of accompanying trusted edge
devices with idle resources and propose Galaxy, a collaborative edge AI system
that breaks the resource walls across heterogeneous edge devices for efficient
Transformer inference acceleration. Galaxy introduces a novel hybrid model
parallelism to orchestrate collaborative inference, along with a
heterogeneity-aware parallelism planning for fully exploiting the resource
potential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of
communication and computation to mitigate the impact of tensor synchronizations
on inference latency under bandwidth-constrained edge environments. Extensive
evaluation based on prototype implementation demonstrates that Galaxy
remarkably outperforms state-of-the-art approaches under various edge
environment setups, achieving up to 2.5x end-to-end latency reduction.
`,authors:"Shengyuan Ye; Jiangsu Du; Liekang Zeng; Wenzhong Ou; Xiaowen Chu; Yutong Lu; Xu Chen",status:0,relevancy:.3832621662678949,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17345",date:"2024-05-27",title:"Exploring and steering the moral compass of Large Language Models",abstract:`  Large Language Models (LLMs) have become central to advancing automation and
decision-making across various sectors, raising significant ethical questions.
This study proposes a comprehensive comparative analysis of the most advanced
LLMs to assess their moral profiles. We subjected several state-of-the-art
models to a selection of ethical dilemmas and found that all the proprietary
ones are mostly utilitarian and all of the open-weights ones align mostly with
values-based ethics. Furthermore, when using the Moral Foundations
Questionnaire, all models we probed - except for Llama 2- displayed a strong
liberal bias. Lastly, in order to causally intervene in one of the studied
models, we propose a novel similarity-specific activation steering technique.
Using this method, we were able to reliably steer the model's moral compass to
different ethical schools. All of these results showcase that there is an
ethical dimension in already deployed LLMs, an aspect that is generally
overlooked.
`,authors:"Alejandro Tlaie",status:0,relevancy:.38238329991701725,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17060",date:"2024-05-27",title:"Graph Neural Networks on Quantum Computers",abstract:`  Graph Neural Networks (GNNs) are powerful machine learning models that excel
at analyzing structured data represented as graphs, demonstrating remarkable
performance in applications like social network analysis and recommendation
systems. However, classical GNNs face scalability challenges when dealing with
large-scale graphs. This paper proposes frameworks for implementing GNNs on
quantum computers to potentially address the challenges. We devise quantum
algorithms corresponding to the three fundamental types of classical GNNs:
Graph Convolutional Networks, Graph Attention Networks, and Message-Passing
GNNs. A complexity analysis of our quantum implementation of the Simplified
Graph Convolutional (SGC) Network shows potential quantum advantages over its
classical counterpart, with significant improvements in time and space
complexities. Our complexities can have trade-offs between the two: when
optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time
complexity in the input sizes (albeit at the cost of linear space complexity).
When optimizing for minimal qubit usage, the quantum SGC exhibits space
complexity logarithmic in the input sizes, offering an exponential reduction
compared to classical SGCs, while still maintaining better time complexity.
These results suggest our Quantum GNN frameworks could efficiently process
large-scale graphs. This work paves the way for implementing more advanced
Graph Neural Network models on quantum computers, opening new possibilities in
quantum machine learning for analyzing graph-structured data.
`,authors:"Yidong Liao; Xiao-Ming Zhang; Chris Ferrie",status:0,relevancy:.38163586675496286,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17573",date:"2024-05-27",title:`Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky
  ResNets`,abstract:`  We study Leaky ResNets, which interpolate between ResNets ($\\tilde{L}=0$) and
Fully-Connected nets ($\\tilde{L}\\to\\infty$) depending on an 'effective depth'
hyper-parameter $\\tilde{L}$. In the infinite depth limit, we study
'representation geodesics' $A_{p}$: continuous paths in representation space
(similar to NeuralODEs) from input $p=0$ to output $p=1$ that minimize the
parameter norm of the network. We give a Lagrangian and Hamiltonian
reformulation, which highlight the importance of two terms: a kinetic energy
which favors small layer derivatives $\\partial_{p}A_{p}$ and a potential energy
that favors low-dimensional representations, as measured by the 'Cost of
Identity'. The balance between these two forces offers an intuitive
understanding of feature learning in ResNets. We leverage this intuition to
explain the emergence of a bottleneck structure, as observed in previous work:
for large $\\tilde{L}$ the potential energy dominates and leads to a separation
of timescales, where the representation jumps rapidly from the high dimensional
inputs to a low-dimensional representation, move slowly inside the space of
low-dimensional representations, before jumping back to the potentially
high-dimensional outputs. Inspired by this phenomenon, we train with an
adaptive layer step-size to adapt to the separation of timescales.
`,authors:"Arthur Jacot; Alexandre Kaiser",status:0,relevancy:.3792870817675347,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16877",date:"2024-05-27",title:"Are Self-Attentions Effective for Time Series Forecasting?",abstract:`  Time series forecasting is crucial for applications across multiple domains
and various scenarios. Although Transformer models have dramatically shifted
the landscape of forecasting, their effectiveness remains debated. Recent
findings have indicated that simpler linear models might outperform complex
Transformer-based approaches, highlighting the potential for more streamlined
architectures. In this paper, we shift focus from the overall architecture of
the Transformer to the effectiveness of self-attentions for time series
forecasting. To this end, we introduce a new architecture, Cross-Attention-only
Time Series transformer (CATS), that rethinks the traditional Transformer
framework by eliminating self-attention and leveraging cross-attention
mechanisms instead. By establishing future horizon-dependent parameters as
queries and enhanced parameter sharing, our model not only improves long-term
forecasting accuracy but also reduces the number of parameters and memory
usage. Extensive experiment across various datasets demonstrates that our model
achieves superior performance with the lowest mean squared error and uses fewer
parameters compared to existing models.
`,authors:"Dongbin Kim; Jinseong Park; Jaewook Lee; Hoki Kim",status:0,relevancy:.37927154051262324,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17419",date:"2024-05-27",title:"MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities",abstract:`  Detecting out-of-distribution (OOD) samples is important for deploying
machine learning models in safety-critical applications such as autonomous
driving and robot-assisted surgery. Existing research has mainly focused on
unimodal scenarios on image data. However, real-world applications are
inherently multimodal, which makes it essential to leverage information from
multiple modalities to enhance the efficacy of OOD detection. To establish a
foundation for more realistic Multimodal OOD Detection, we introduce the
first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes
and varying modality combinations. We first evaluate existing unimodal OOD
detection algorithms on MultiOOD, observing that the mere inclusion of
additional modalities yields substantial improvements. This underscores the
importance of utilizing multiple modalities for OOD detection. Based on the
observation of Modality Prediction Discrepancy between in-distribution (ID) and
OOD data, and its strong correlation with OOD performance, we propose the
Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during
training. Moreover, we introduce a novel outlier synthesis method, NP-Mix,
which explores broader feature spaces by leveraging the information from
nearest neighbor classes and complements A2D to strengthen OOD detection
performance. Extensive experiments on MultiOOD demonstrate that training with
A2D and NP-Mix improves existing OOD detection algorithms by a large margin.
Our source code and MultiOOD benchmark are available at
https://github.com/donghao51/MultiOOD.
`,authors:"Hao Dong; Yue Zhao; Eleni Chatzi; Olga Fink",status:0,relevancy:.37847403289031745,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17429",date:"2024-05-27",title:`GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic
  Occupancy Prediction`,abstract:`  3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and
semantics of the surrounding scene and is an important task for the robustness
of vision-centric autonomous driving. Most existing methods employ dense grids
such as voxels as scene representations, which ignore the sparsity of occupancy
and the diversity of object scales and thus lead to unbalanced allocation of
resources. To address this, we propose an object-centric representation to
describe 3D scenes with sparse 3D semantic Gaussians where each Gaussian
represents a flexible region of interest and its semantic features. We
aggregate information from images through the attention mechanism and
iteratively refine the properties of 3D Gaussians including position,
covariance, and semantics. We then propose an efficient Gaussian-to-voxel
splatting method to generate 3D occupancy predictions, which only aggregates
the neighboring Gaussians for a certain position. We conduct extensive
experiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental
results demonstrate that GaussianFormer achieves comparable performance with
state-of-the-art methods with only 17.8% - 24.8% of their memory consumption.
Code is available at: https://github.com/huang-yh/GaussianFormer.
`,authors:"Yuanhui Huang; Wenzhao Zheng; Yunpeng Zhang; Jie Zhou; Jiwen Lu",status:0,relevancy:.3776902010456241,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17152",date:"2024-05-27",title:`CoSLight: Co-optimizing Collaborator Selection and Decision-making to
  Enhance Traffic Signal Control`,abstract:`  Effective multi-intersection collaboration is pivotal for
reinforcement-learning-based traffic signal control to alleviate congestion.
Existing work mainly chooses neighboring intersections as collaborators.
However, quite an amount of congestion, even some wide-range congestion, is
caused by non-neighbors failing to collaborate. To address these issues, we
propose to separate the collaborator selection as a second policy to be
learned, concurrently being updated with the original signal-controlling
policy. Specifically, the selection policy in real-time adaptively selects the
best teammates according to phase- and intersection-level features. Empirical
results on both synthetic and real-world datasets provide robust validation for
the superiority of our approach, offering significant improvements over
existing state-of-the-art methods. The code is available at
https://github.com/AnonymousAccountss/CoSLight.
`,authors:"Jingqing Ruan; Ziyue Li; Hua Wei; Haoyuan Jiang; Jiaming Lu; Xuantang Xiong; Hangyu Mao; Rui Zhao",status:0,relevancy:.3756238547396772,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16823",date:"2024-05-27",title:`Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled
  Self-Attention Injection`,abstract:`  While text-to-image models have achieved impressive capabilities in image
generation and editing, their application across various modalities often
necessitates training separate models. Inspired by existing method of single
image editing with self attention injection and video editing with shared
attention, we propose a novel unified editing framework that combines the
strengths of both approaches by utilizing only a basic 2D image text-to-image
(T2I) diffusion model. Specifically, we design a sampling method that
facilitates editing consecutive images while maintaining semantic consistency
utilizing shared self-attention features during both reference and consecutive
image sampling processes. Experimental results confirm that our method enables
editing across diverse modalities including 3D scenes, videos, and panorama
images.
`,authors:"Gihyun Kwon; Jangho Park; Jong Chul Ye",status:0,relevancy:.3709072341365782,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17072",date:"2024-05-27",title:`A novel framework for systematic propositional formula simplification
  based on existential graphs`,abstract:`  This paper presents a novel simplification calculus for propositional logic
derived from Peirce's existential graphs' rules of inference and implication
graphs. Our rules can be applied to propositional logic formulae in nested
form, are equivalence-preserving, guarantee a monotonically decreasing number
of variables, clauses and literals, and maximise the preservation of structural
problem information. Our techniques can also be seen as higher-level SAT
preprocessing, and we show how one of our rules (TWSR) generalises and
streamlines most of the known equivalence-preserving SAT preprocessing methods.
In addition, we propose a simplification procedure based on the systematic
application of two of our rules (EPR and TWSR) which is solver-agnostic and can
be used to simplify large Boolean satisfiability problems and propositional
formulae in arbitrary form, and we provide a formal analysis of its algorithmic
complexity in terms of space and time. Finally, we show how our rules can be
further extended with a novel n-ary implication graph to capture all known
equivalence-preserving preprocessing procedures.
`,authors:"Jordina Francès de Mas; Juliana Bowles",status:0,relevancy:.37072588822425234,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17580",date:"2024-05-27",title:"Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes",abstract:`  The training dynamics of linear networks are well studied in two distinct
setups: the lazy regime and balanced/active regime, depending on the
initialization and width of the network. We provide a surprisingly simple
unyfing formula for the evolution of the learned matrix that contains as
special cases both lazy and balanced regimes but also a mixed regime in between
the two. In the mixed regime, a part of the network is lazy while the other is
balanced. More precisely the network is lazy along singular values that are
below a certain threshold and balanced along those that are above the same
threshold. At initialization, all singular values are lazy, allowing for the
network to align itself with the task, so that later in time, when some of the
singular value cross the threshold and become active they will converge rapidly
(convergence in the balanced regime is notoriously difficult in the absence of
alignment). The mixed regime is the \`best of both worlds': it converges from
any random initialization (in contrast to balanced dynamics which require
special initialization), and has a low rank bias (absent in the lazy dynamics).
This allows us to prove an almost complete phase diagram of training behavior
as a function of the variance at initialization and the width, for a MSE
training task.
`,authors:"Zhenfeng Tu; Santiago Aranguri; Arthur Jacot",status:0,relevancy:.37054077698222954,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16852",date:"2024-05-27",title:"EM Distillation for One-step Diffusion Models",abstract:`  While diffusion models can learn complex distributions, sampling requires a
computationally expensive iterative process. Existing distillation methods
enable efficient sampling, but have notable limitations, such as performance
degradation with very few sampling steps, reliance on training data access, or
mode-seeking optimization that may fail to capture the full distribution. We
propose EM Distillation (EMD), a maximum likelihood-based approach that
distills a diffusion model to a one-step generator model with minimal loss of
perceptual quality. Our approach is derived through the lens of
Expectation-Maximization (EM), where the generator parameters are updated using
samples from the joint distribution of the diffusion teacher prior and inferred
generator latents. We develop a reparametrized sampling scheme and a noise
cancellation technique that together stabilizes the distillation process. We
further reveal an interesting connection of our method with existing methods
that minimize mode-seeking KL. EMD outperforms existing one-step generative
methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares
favorably with prior work on distilling text-to-image diffusion models.
`,authors:"Sirui Xie; Zhisheng Xiao; Diederik P Kingma; Tingbo Hou; Ying Nian Wu; Kevin Patrick Murphy; Tim Salimans; Ben Poole; Ruiqi Gao",status:0,relevancy:.36452710936895305,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16876",date:"2024-05-27",title:"Transfer Learning for Diffusion Models",abstract:`  Diffusion models, a specific type of generative model, have achieved
unprecedented performance in recent years and consistently produce high-quality
synthetic samples. A critical prerequisite for their notable success lies in
the presence of a substantial number of training samples, which can be
impractical in real-world applications due to high collection costs or
associated risks. Consequently, various finetuning and regularization
approaches have been proposed to transfer knowledge from existing pre-trained
models to specific target domains with limited data. This paper introduces the
Transfer Guided Diffusion Process (TGDP), a novel approach distinct from
conventional finetuning and regularization methods. We prove that the optimal
diffusion model for the target domain integrates pre-trained diffusion models
on the source domain with additional guidance from a domain classifier. We
further extend TGDP to a conditional version for modeling the joint
distribution of data and its corresponding labels, together with two additional
regularization terms to enhance the model performance. We validate the
effectiveness of TGDP on Gaussian mixture simulations and on real
electrocardiogram (ECG) datasets.
`,authors:"Yidong Ouyang; Liyan Xie; Hongyuan Zha; Guang Cheng",status:0,relevancy:.35964317158352443,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17537",date:"2024-05-27",title:`BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring
  at Scale`,abstract:`  Measuring biodiversity is crucial for understanding ecosystem health. While
prior works have developed machine learning models for the taxonomic
classification of photographic images and DNA separately, in this work, we
introduce a multimodal approach combining both, using CLIP-style contrastive
learning to align images, DNA barcodes, and textual data in a unified embedding
space. This allows for accurate classification of both known and unknown insect
species without task-specific fine-tuning, leveraging contrastive learning for
the first time to fuse DNA and image data. Our method surpasses previous
single-modality approaches in accuracy by over 11% on zero-shot learning tasks,
showcasing its effectiveness in biodiversity studies.
`,authors:"ZeMing Gong; Austin T. Wang; Joakim Bruslund Haurum; Scott C. Lowe; Graham W. Taylor; Angel X. Chang",status:0,relevancy:.35020639554003075,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17533",date:"2024-05-27",title:`PAE: LLM-based Product Attribute Extraction for E-Commerce Fashion
  Trends`,abstract:`  Product attribute extraction is an growing field in e-commerce business, with
several applications including product ranking, product recommendation, future
assortment planning and improving online shopping customer experiences.
Understanding the customer needs is critical part of online business,
specifically fashion products. Retailers uses assortment planning to determine
the mix of products to offer in each store and channel, stay responsive to
market dynamics and to manage inventory and catalogs. The goal is to offer the
right styles, in the right sizes and colors, through the right channels. When
shoppers find products that meet their needs and desires, they are more likely
to return for future purchases, fostering customer loyalty. Product attributes
are a key factor in assortment planning. In this paper we present PAE, a
product attribute extraction algorithm for future trend reports consisting text
and images in PDF format. Most existing methods focus on attribute extraction
from titles or product descriptions or utilize visual information from existing
product images. Compared to the prior works, our work focuses on attribute
extraction from PDF files where upcoming fashion trends are explained. This
work proposes a more comprehensive framework that fully utilizes the different
modalities for attribute extraction and help retailers to plan the assortment
in advance. Our contributions are three-fold: (a) We develop PAE, an efficient
framework to extract attributes from unstructured data (text and images); (b)
We provide catalog matching methodology based on BERT representations to
discover the existing attributes using upcoming attribute values; (c) We
conduct extensive experiments with several baselines and show that PAE is an
effective, flexible and on par or superior (avg 92.5% F1-Score) framework to
existing state-of-the-art for attribute value extraction task.
`,authors:"Apurva Sinha; Ekta Gujral",status:0,relevancy:.3497941600497627,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17284",date:"2024-05-27",title:`An NLP Crosswalk Between the Common Core State Standards and NAEP Item
  Specifications`,abstract:`  Natural language processing (NLP) is rapidly developing for applications in
educational assessment. In this paper, I describe an NLP-based procedure that
can be used to support subject matter experts in establishing a crosswalk
between item specifications and content standards. This paper extends recent
work by proposing and demonstrating the use of multivariate similarity based on
embedding vectors for sentences or texts. In particular, a hybrid regression
procedure is demonstrated for establishing the match of each content standard
to multiple item specifications. The procedure is used to evaluate the match of
the Common Core State Standards (CCSS) for mathematics at grade 4 to the
corresponding item specifications for the 2026 National Assessment of
Educational Progress (NAEP).
`,authors:"Gregory Camilli",status:0,relevancy:.34863222355677714,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17034",date:"2024-05-27",title:"FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",abstract:`  Fairness-aware Graph Neural Networks (GNNs) often face a challenging
trade-off, where prioritizing fairness may require compromising utility. In
this work, we re-examine fairness through the lens of spectral graph theory,
aiming to reconcile fairness and utility within the framework of spectral graph
learning. We explore the correlation between sensitive features and spectrum in
GNNs, using theoretical analysis to delineate the similarity between original
sensitive features and those after convolution under different spectrum. Our
analysis reveals a reduction in the impact of similarity when the eigenvectors
associated with the largest magnitude eigenvalue exhibit directional
similarity. Based on these theoretical insights, we propose FUGNN, a novel
spectral graph learning approach that harmonizes the conflict between fairness
and utility. FUGNN ensures algorithmic fairness and utility by truncating the
spectrum and optimizing eigenvector distribution during the encoding process.
The fairness-aware eigenvector selection reduces the impact of convolution on
sensitive features while concurrently minimizing the sacrifice of utility.
FUGNN further optimizes the distribution of eigenvectors through a transformer
architecture. By incorporating the optimized spectrum into the graph
convolution network, FUGNN effectively learns node representations. Experiments
on six real-world datasets demonstrate the superiority of FUGNN over baseline
methods. The codes are available at https://github.com/yushuowiki/FUGNN.
`,authors:"Renqiang Luo; Huafei Huang; Shuo Yu; Zhuoyang Han; Estrid He; Xiuzhen Zhang; Feng Xia",status:0,relevancy:.34688222616025866,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16922",date:"2024-05-27",title:`Theories of synaptic memory consolidation and intelligent plasticity for
  continual learning`,abstract:`  Humans and animals learn throughout life. Such continual learning is crucial
for intelligence. In this chapter, we examine the pivotal role plasticity
mechanisms with complex internal synaptic dynamics could play in enabling this
ability in neural networks. By surveying theoretical research, we highlight two
fundamental enablers for continual learning. First, synaptic plasticity
mechanisms must maintain and evolve an internal state over several behaviorally
relevant timescales. Second, plasticity algorithms must leverage the internal
state to intelligently regulate plasticity at individual synapses to facilitate
the seamless integration of new memories while avoiding detrimental
interference with existing ones. Our chapter covers successful applications of
these principles to deep neural networks and underscores the significance of
synaptic metaplasticity in sustaining continual learning capabilities. Finally,
we outline avenues for further research to understand the brain's superb
continual learning abilities and harness similar mechanisms for artificial
intelligence systems.
`,authors:"Friedemann Zenke; Axel Laborieux",status:0,relevancy:.34419065660735604,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17088",date:"2024-05-27",title:"Phase Transitions in the Output Distribution of Large Language Models",abstract:`  In a physical system, changing parameters such as temperature can induce a
phase transition: an abrupt change from one state of matter to another.
Analogous phenomena have recently been observed in large language models.
Typically, the task of identifying phase transitions requires human analysis
and some prior understanding of the system to narrow down which low-dimensional
properties to monitor and analyze. Statistical methods for the automated
detection of phase transitions from data have recently been proposed within the
physics community. These methods are largely system agnostic and, as shown
here, can be adapted to study the behavior of large language models. In
particular, we quantify distributional changes in the generated output via
statistical distances, which can be efficiently estimated with access to the
probability distribution over next-tokens. This versatile approach is capable
of discovering new phases of behavior and unexplored transitions -- an ability
that is particularly exciting in light of the rapid development of language
models and their emergent capabilities.
`,authors:"Julian Arnold; Flemming Holtorf; Frank Schäfer; Niels Lörch",status:0,relevancy:.33974698551099825,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17110",date:"2024-05-27",title:`Superpixelwise Low-rank Approximation based Partial Label Learning for
  Hyperspectral Image Classification`,abstract:`  Insufficient prior knowledge of a captured hyperspectral image (HSI) scene
may lead the experts or the automatic labeling systems to offer incorrect
labels or ambiguous labels (i.e., assigning each training sample to a group of
candidate labels, among which only one of them is valid; this is also known as
partial label learning) during the labeling process. Accordingly, how to learn
from such data with ambiguous labels is a problem of great practical
importance. In this paper, we propose a novel superpixelwise low-rank
approximation (LRA)-based partial label learning method, namely SLAP, which is
the first to take into account partial label learning in HSI classification.
SLAP is mainly composed of two phases: disambiguating the training labels and
acquiring the predictive model. Specifically, in the first phase, we propose a
superpixelwise LRA-based model, preparing the affinity graph for the subsequent
label propagation process while extracting the discriminative representation to
enhance the following classification task of the second phase. Then to
disambiguate the training labels, label propagation propagates the labeling
information via the affinity graph of training pixels. In the second phase, we
take advantage of the resulting disambiguated training labels and the
discriminative representations to enhance the classification performance. The
extensive experiments validate the advantage of the proposed SLAP method over
state-of-the-art methods.
`,authors:"Shujun Yang; Yu Zhang; Yao Ding; Danfeng Hong",status:0,relevancy:.33528598412051314,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17527",date:"2024-05-27",title:"Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers",abstract:`  Deep models have recently emerged as a promising tool to solve partial
differential equations (PDEs), known as neural PDE solvers. While neural
solvers trained from either simulation data or physics-informed loss can solve
the PDEs reasonably well, they are mainly restricted to a specific set of PDEs,
e.g. a certain equation or a finite set of coefficients. This bottleneck limits
the generalizability of neural solvers, which is widely recognized as its major
advantage over numerical solvers. In this paper, we present the Universal PDE
solver (Unisolver) capable of solving a wide scope of PDEs by leveraging a
Transformer pre-trained on diverse data and conditioned on diverse PDEs.
Instead of simply scaling up data and parameters, Unisolver stems from the
theoretical analysis of the PDE-solving process. Our key finding is that a PDE
solution is fundamentally under the control of a series of PDE components, e.g.
equation symbols, coefficients, and initial and boundary conditions. Inspired
by the mathematical structure of PDEs, we define a complete set of PDE
components and correspondingly embed them as domain-wise (e.g. equation
symbols) and point-wise (e.g. boundaries) conditions for Transformer PDE
solvers. Integrating physical insights with recent Transformer advances,
Unisolver achieves consistent state-of-the-art results on three challenging
large-scale benchmarks, showing impressive gains and endowing favorable
generalizability and scalability.
`,authors:"Zhou Hang; Yuezhou Ma; Haixu Wu; Haowen Wang; Mingsheng Long",status:0,relevancy:.33298675729878147,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17556",date:"2024-05-27",title:"Probabilistic Verification of Neural Networks using Branch and Bound",abstract:`  Probabilistic verification of neural networks is concerned with formally
analysing the output distribution of a neural network under a probability
distribution of the inputs. Examples of probabilistic verification include
verifying the demographic parity fairness notion or quantifying the safety of a
neural network. We present a new algorithm for the probabilistic verification
of neural networks based on an algorithm for computing and iteratively refining
lower and upper bounds on probabilities over the outputs of a neural network.
By applying state-of-the-art bound propagation and branch and bound techniques
from non-probabilistic neural network verification, our algorithm significantly
outpaces existing probabilistic verification algorithms, reducing solving times
for various benchmarks from the literature from tens of minutes to tens of
seconds. Furthermore, our algorithm compares favourably even to dedicated
algorithms for restricted subsets of probabilistic verification. We complement
our empirical evaluation with a theoretical analysis, proving that our
algorithm is sound and, under mildly restrictive conditions, also complete when
using a suitable set of heuristics.
`,authors:"David Boetius; Stefan Leue; Tobias Sutter",status:0,relevancy:.3321800163674924,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17422",date:"2024-05-27",title:"Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection",abstract:`  3D object detection aims to recover the 3D information of concerning objects
and serves as the fundamental task of autonomous driving perception. Its
performance greatly depends on the scale of labeled training data, yet it is
costly to obtain high-quality annotations for point cloud data. While
conventional methods focus on generating pseudo-labels for unlabeled samples as
supplements for training, the structural nature of 3D point cloud data
facilitates the composition of objects and backgrounds to synthesize realistic
scenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS)
method to generate adaptive synthetic scenes to improve the generalization of
the detection models. We obtain pseudo-labels for unlabeled objects and
generate diverse scenes with different compositions of objects and backgrounds.
As the scene synthesis is sensitive to the quality of pseudo-labels, we further
propose a hardness-aware strategy to reduce the effect of low-quality
pseudo-labels and maintain a dynamic pseudo-database to ensure the diversity
and quality of synthetic scenes. Extensive experimental results on the widely
used KITTI and Waymo datasets demonstrate the superiority of the proposed HASS
method, which outperforms existing semi-supervised learning methods on 3D
object detection. Code: https://github.com/wzzheng/HASS.
`,authors:"Shuai Zeng; Wenzhao Zheng; Jiwen Lu; Haibin Yan",status:0,relevancy:.32968740132693775,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16797",date:"2024-05-27",title:"A Real-Time Voice Activity Detection Based On Lightweight Neural",abstract:`  Voice activity detection (VAD) is the task of detecting speech in an audio
stream, which is challenging due to numerous unseen noises and low
signal-to-noise ratios in real environments. Recently, neural network-based
VADs have alleviated the degradation of performance to some extent. However,
the majority of existing studies have employed excessively large models and
incorporated future context, while neglecting to evaluate the operational
efficiency and latency of the models. In this paper, we propose a lightweight
and real-time neural network called MagicNet, which utilizes casual and depth
separable 1-D convolutions and GRU. Without relying on future features as
input, our proposed model is compared with two state-of-the-art algorithms on
synthesized in-domain and out-domain test datasets. The evaluation results
demonstrate that MagicNet can achieve improved performance and robustness with
fewer parameter costs.
`,authors:"Jidong Jia; Pei Zhao; Di Wang",status:0,relevancy:.3168383313386973,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17569",date:"2024-05-27",title:`Discriminant audio properties in deep learning based respiratory
  insufficiency detection in Brazilian Portuguese`,abstract:`  This work investigates Artificial Intelligence (AI) systems that detect
respiratory insufficiency (RI) by analyzing speech audios, thus treating speech
as a RI biomarker. Previous works collected RI data (P1) from COVID-19 patients
during the first phase of the pandemic and trained modern AI models, such as
CNNs and Transformers, which achieved $96.5\\%$ accuracy, showing the
feasibility of RI detection via AI. Here, we collect RI patient data (P2) with
several causes besides COVID-19, aiming at extending AI-based RI detection. We
also collected control data from hospital patients without RI. We show that the
considered models, when trained on P1, do not generalize to P2, indicating that
COVID-19 RI has features that may not be found in all RI types.
`,authors:"Marcelo Matheus Gauy; Larissa Cristina Berti; Arnaldo Cândido Jr; Augusto Camargo Neto; Alfredo Goldman; Anna Sara Shafferman Levin; Marcus Martins; Beatriz Raposo de Medeiros; Marcelo Queiroz; Ester Cerdeira Sabino; Flaviane Romani Fernandes Svartman; Marcelo Finger",status:0,relevancy:.3138868821358264,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16836",date:"2024-05-27",title:`Enhancing Fast Feed Forward Networks with Load Balancing and a Master
  Leaf Node`,abstract:`  Fast feedforward networks (FFFs) are a class of neural networks that exploit
the observation that different regions of the input space activate distinct
subsets of neurons in wide networks. FFFs partition the input space into
separate sections using a differentiable binary tree of neurons and during
inference descend the binary tree in order to improve computational efficiency.
Inspired by Mixture of Experts (MoE) research, we propose the incorporation of
load balancing and Master Leaf techniques into the FFF architecture to improve
performance and simplify the training process. We reproduce experiments found
in literature and present results on FFF models enhanced using these
techniques. The proposed architecture and training recipe achieves up to 16.3%
and 3% absolute classification accuracy increase in training and test accuracy,
respectively, compared to the original FFF architecture. Additionally, we
observe a smaller variance in the results compared to those reported in prior
research. These findings demonstrate the potential of integrating MoE-inspired
techniques into FFFs for developing more accurate and efficient models.
`,authors:"Andreas Charalampopoulos; Nikolas Chatzis; Foivos Ntoulas-Panagiotopoulos; Charilaos Papaioannou; Alexandros Potamianos",status:0,relevancy:.3118953246492042,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17653",date:"2024-05-27",title:`InversionView: A General-Purpose Method for Reading Information from
  Neural Activations`,abstract:`  The inner workings of neural networks can be better understood if we can
fully decipher the information encoded in neural activations. In this paper, we
argue that this information is embodied by the subset of inputs that give rise
to similar activations. Computing such subsets is nontrivial as the input space
is exponentially large. We propose InversionView, which allows us to
practically inspect this subset by sampling from a trained decoder model
conditioned on activations. This helps uncover the information content of
activation vectors, and facilitates understanding of the algorithms implemented
by transformer models. We present three case studies where we investigate
models ranging from small transformers to GPT-2. In these studies, we
demonstrate the characteristics of our method, show the distinctive advantages
it offers, and provide causally verified circuits.
`,authors:"Xinting Huang; Madhur Panwar; Navin Goyal; Michael Hahn",status:0,relevancy:.3087470043038405,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17025",date:"2024-05-27",title:`SWAT: Scalable and Efficient Window Attention-based Transformers
  Acceleration on FPGAs`,abstract:`  Efficiently supporting long context length is crucial for Transformer models.
The quadratic complexity of the self-attention computation plagues traditional
Transformers. Sliding window-based static sparse attention mitigates the
problem by limiting the attention scope of the input tokens, reducing the
theoretical complexity from quadratic to linear. Although the sparsity induced
by window attention is highly structured, it does not align perfectly with the
microarchitecture of the conventional accelerators, leading to suboptimal
implementation. In response, we propose a dataflow-aware FPGA-based accelerator
design, SWAT, that efficiently leverages the sparsity to achieve scalable
performance for long input. The proposed microarchitecture is based on a design
that maximizes data reuse by using a combination of row-wise dataflow, kernel
fusion optimization, and an input-stationary design considering the distributed
memory and computation resources of FPGA. Consequently, it achieves up to
22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency
compared to the baseline FPGA-based accelerator and 15$\\times$ energy
efficiency compared to GPU-based solution.
`,authors:"Zhenyu Bai; Pranav Dangi; Huize Li; Tulika Mitra",status:0,relevancy:.30787864208193405,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16961",date:"2024-05-27",title:`Blind Data Adaptation to tackle Covariate Shift in Operational
  Steganalysis`,abstract:`  The proliferation of image manipulation for unethical purposes poses
significant challenges in social networks. One particularly concerning method
is Image Steganography, allowing individuals to hide illegal information in
digital images without arousing suspicions. Such a technique pose severe
security risks, making it crucial to develop effective steganalysis methods
enabling to detect manipulated images for clandestine communications. Although
significant advancements have been achieved with machine learning models, a
critical issue remains: the disparity between the controlled datasets used to
train steganalysis models against real-world datasets of forensic
practitioners, undermining severely the practical effectiveness of standardized
steganalysis models. In this paper, we address this issue focusing on a
realistic scenario where practitioners lack crucial information about the
limited target set of images under analysis, including details about their
development process and even whereas it contains manipulated images or not. By
leveraging geometric alignment and distribution matching of source and target
residuals, we develop TADA (Target Alignment through Data Adaptation), a novel
methodology enabling to emulate sources aligned with specific targets in
steganalysis, which is also relevant for highly unbalanced targets. The
emulator is represented by a light convolutional network trained to align
distributions of image residuals. Experimental validation demonstrates the
potential of our strategy over traditional methods fighting covariate shift in
steganalysis.
`,authors:"Rony Abecidan; Vincent Itier; Jérémie Boulanger; Patrick Bas; Tomáš Pevný",status:0,relevancy:.3063693757652065,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17412",date:"2024-05-27",title:`Towards One Model for Classical Dimensionality Reduction: A
  Probabilistic Perspective on UMAP and t-SNE`,abstract:`  This paper shows that the dimensionality reduction methods, UMAP and t-SNE,
can be approximately recast as MAP inference methods corresponding to a
generalized Wishart-based model introduced in ProbDR. This interpretation
offers deeper theoretical insights into these algorithms, while introducing
tools with which similar dimensionality reduction methods can be studied.
`,authors:"Aditya Ravuri; Neil D. Lawrence",status:0,relevancy:.3054431804955444,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16867",date:"2024-05-27",title:"Clustering-based Learning for UAV Tracking and Pose Estimation",abstract:`  UAV tracking and pose estimation plays an imperative role in various
UAV-related missions, such as formation control and anti-UAV measures.
Accurately detecting and tracking UAVs in a 3D space remains a particularly
challenging problem, as it requires extracting sparse features of micro UAVs
from different flight environments and continuously matching correspondences,
especially during agile flight. Generally, cameras and LiDARs are the two main
types of sensors used to capture UAV trajectories in flight. However, both
sensors have limitations in UAV classification and pose estimation. This
technical report briefly introduces the method proposed by our team "NTU-ICG"
for the CVPR 2024 UG2+ Challenge Track 5. This work develops a clustering-based
learning detection approach, CL-Det, for UAV tracking and pose estimation using
two types of LiDARs, namely Livox Avia and LiDAR 360. We combine the
information from the two data sources to locate drones in 3D. We first align
the timestamps of Livox Avia data and LiDAR 360 data and then separate the
point cloud of objects of interest (OOIs) from the environment. The point cloud
of OOIs is clustered using the DBSCAN method, with the midpoint of the largest
cluster assumed to be the UAV position. Furthermore, we utilize historical
estimations to fill in missing data. The proposed method shows competitive pose
estimation performance and ranks 5th on the final leaderboard of the CVPR 2024
UG2+ Challenge.
`,authors:"Jiaping Xiao; Phumrapee Pisutsin; Cheng Wen Tsao; Mir Feroskhan",status:0,relevancy:.30175949012852477,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17637",date:"2024-05-27",title:`The Economic Implications of Large Language Model Selection on Earnings
  and Return on Investment: A Decision Theoretic Model`,abstract:`  Selecting language models in business contexts requires a careful analysis of
the final financial benefits of the investment. However, the emphasis of
academia and industry analysis of LLM is solely on performance. This work
introduces a framework to evaluate LLMs, focusing on the earnings and return on
investment aspects that should be taken into account in business decision
making. We use a decision-theoretic approach to compare the financial impact of
different LLMs, considering variables such as the cost per token, the
probability of success in the specific task, and the gain and losses associated
with LLMs use. The study reveals how the superior accuracy of more expensive
models can, under certain conditions, justify a greater investment through more
significant earnings but not necessarily a larger RoI. This article provides a
framework for companies looking to optimize their technology choices, ensuring
that investment in cutting-edge technology aligns with strategic financial
objectives. In addition, we discuss how changes in operational variables
influence the economics of using LLMs, offering practical insights for
enterprise settings, finding that the predicted gain and loss and the different
probabilities of success and failure are the variables that most impact the
sensitivity of the models.
`,authors:"Geraldo Xexéo; Filipe Braida; Marcus Parreiras; Paulo Xavier",status:0,relevancy:.3003379031232045,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17672",date:"2024-05-27",title:`Exploring Loss Design Techniques For Decision Tree Robustness To Label
  Noise`,abstract:`  In the real world, data is often noisy, affecting not only the quality of
features but also the accuracy of labels. Current research on mitigating label
errors stems primarily from advances in deep learning, and a gap exists in
exploring interpretable models, particularly those rooted in decision trees. In
this study, we investigate whether ideas from deep learning loss design can be
applied to improve the robustness of decision trees. In particular, we show
that loss correction and symmetric losses, both standard approaches, are not
effective. We argue that other directions need to be explored to improve the
robustness of decision trees to label noise.
`,authors:"Lukasz Sztukiewicz; Jack Henry Good; Artur Dubrawski",status:0,relevancy:.28276645161404157,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16807",date:"2024-05-27",title:"Extreme Compression of Adaptive Neural Images",abstract:`  Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm
for signal representation, from images and audio to 3D scenes and videos. The
fundamental idea is to represent a signal as a continuous and differentiable
neural network. This idea offers unprecedented benefits such as continuous
resolution and memory efficiency, enabling new compression techniques. However,
representing data as neural networks poses new challenges. For instance, given
a 2D image as a neural network, how can we further compress such a neural
image?. In this work, we present a novel analysis on compressing neural fields,
with the focus on images. We also introduce Adaptive Neural Images (ANI), an
efficient neural representation that enables adaptation to different inference
or transmission requirements. Our proposed method allows to reduce the
bits-per-pixel (bpp) of the neural image by 4x, without losing sensitive
details or harming fidelity. We achieve this thanks to our successful
implementation of 4-bit neural representations. Our work offers a new framework
for developing compressed neural fields.
`,authors:"Leo Hoshikawa; Marcos V. Conde; Takeshi Ohashi; Atsushi Irie",status:0,relevancy:.27942434519936177,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.16956",date:"2024-05-27",title:`Functional Programming Paradigm of Python for Scientific Computation
  Pipeline Integration`,abstract:`  The advent of modern data processing has led to an increasing tendency
towards interdisciplinarity, which frequently involves the importation of
different technical approaches. Consequently, there is an urgent need for a
unified data control system to facilitate the integration of varying libraries.
This integration is of profound significance in accelerating prototype
verification, optimising algorithm performance and minimising maintenance
costs. This paper presents a novel functional programming (FP) paradigm based
on the Python architecture and associated suites in programming practice,
designed for the integration of pipelines of different data mapping operations.
In particular, the solution is intended for the integration of scientific
computation flows, which affords a robust yet flexible solution for the
aforementioned challenges.
`,authors:"Chen Zhang; Lecheng Jia; Wei Zhang; Ning Wen",status:0,relevancy:.26921365042337875,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17176",date:"2024-05-27",title:`DreamMat: High-quality PBR Material Generation with Geometry- and
  Light-aware Diffusion Models`,abstract:`  2D diffusion model, which often contains unwanted baked-in shading effects
and results in unrealistic rendering effects in the downstream applications.
Generating Physically Based Rendering (PBR) materials instead of just RGB
textures would be a promising solution. However, directly distilling the PBR
material parameters from 2D diffusion models still suffers from incorrect
material decomposition, such as baked-in shading effects in albedo. We
introduce DreamMat, an innovative approach to resolve the aforementioned
problem, to generate high-quality PBR materials from text descriptions. We find
out that the main reason for the incorrect material distillation is that
large-scale 2D diffusion models are only trained to generate final shading
colors, resulting in insufficient constraints on material decomposition during
distillation. To tackle this problem, we first finetune a new light-aware 2D
diffusion model to condition on a given lighting environment and generate the
shading results on this specific lighting condition. Then, by applying the same
environment lights in the material distillation, DreamMat can generate
high-quality PBR materials that are not only consistent with the given geometry
but also free from any baked-in shading effects in albedo. Extensive
experiments demonstrate that the materials produced through our methods exhibit
greater visual appeal to users and achieve significantly superior rendering
quality compared to baseline methods, which are preferable for downstream tasks
such as game and film production.
`,authors:"Yuqing Zhang; Yuan Liu; Zhiyu Xie; Lei Yang; Zhongyuan Liu; Mengzhou Yang; Runze Zhang; Qilong Kou; Cheng Lin; Wenping Wang; Xiaogang Jin",status:0,relevancy:.25740473378688034,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17413",date:"2024-05-27",title:`Enhancing Music Genre Classification through Multi-Algorithm Analysis
  and User-Friendly Visualization`,abstract:`  The aim of this study is to teach an algorithm how to recognize different
types of music. Users will submit songs for analysis. Since the algorithm
hasn't heard these songs before, it needs to figure out what makes each song
unique. It does this by breaking down the songs into different parts and
studying things like rhythm, melody, and tone via supervised learning because
the program learns from examples that are already labelled. One important thing
to consider when classifying music is its genre, which can be quite complex. To
ensure accuracy, we use five different algorithms, each working independently,
to analyze the songs. This helps us get a more complete understanding of each
song's characteristics. Therefore, our goal is to correctly identify the genre
of each submitted song. Once the analysis is done, the results are presented
using a graphing tool, making it easy for users to understand and provide
feedback.
`,authors:"Navin Kamuni; Dheerendra Panwar",status:0,relevancy:.2500404254243823,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17116",date:"2024-05-27",title:"Mixtures of Unsupervised Lexicon Classification",abstract:`  This paper presents a mixture version of the method-of-moment unsupervised
lexicon classification by an incorporation of a Dirichlet process.
`,authors:"Peratham Wiriyathammabhum",status:0,relevancy:.23446744584451407,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17576",date:"2024-05-27",title:`Container pre-marshalling problem minimizing CV@R under uncertainty of
  ship arrival times`,abstract:`  This paper is concerned with the container pre-marshalling problem, which
involves relocating containers in the storage area so that they can be
efficiently loaded onto ships without reshuffles. In reality, however, ship
arrival times are affected by various external factors, which can cause the
order of container retrieval to be different from the initial plan. To
represent such uncertainty, we generate multiple scenarios from a multivariate
probability distribution of ship arrival times. We derive a mixed-integer
linear optimization model to find an optimal container layout such that the
conditional value-at-risk is minimized for the number of misplaced containers
responsible for reshuffles. Moreover, we devise an exact algorithm based on the
cutting-plane method to handle large-scale problems. Numerical experiments
using synthetic datasets demonstrate that our method can produce high-quality
container layouts compared with the conventional robust optimization model.
Additionally, our algorithm can speed up the computation of solving large-scale
problems.
`,authors:"Daiki Ikuma; Shunnosuke Ikeda; Noriyoshi Sukegawa; Yuichi Takano",status:0,relevancy:.2171130374575132,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17038",date:"2024-05-27",title:`Advancements in Tactile Hand Gesture Recognition for Enhanced
  Human-Machine Interaction`,abstract:`  Motivated by the growing interest in enhancing intuitive physical
Human-Machine Interaction (HRI/HVI), this study aims to propose a robust
tactile hand gesture recognition system. We performed a comprehensive
evaluation of different hand gesture recognition approaches for a large area
tactile sensing interface (touch interface) constructed from conductive
textiles. Our evaluation encompassed traditional feature engineering methods,
as well as contemporary deep learning techniques capable of real-time
interpretation of a range of hand gestures, accommodating variations in hand
sizes, movement velocities, applied pressure levels, and interaction points.
Our extensive analysis of the various methods makes a significant contribution
to tactile-based gesture recognition in the field of human-machine interaction.
`,authors:"Chiara Fumelli; Anirvan Dutta; Mohsen Kaboli",status:0,relevancy:.19371136395567634,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}},{id:"2405.17676",date:"2024-05-27",title:`Utilising a Quantum Hybrid Solver for Bi-objective Quadratic Assignment
  Problems`,abstract:`  The intersection between quantum computing and optimisation has been an area
of interest in recent years. There have been numerous studies exploring the
application of quantum and quantum-hybrid solvers to various optimisation
problems. This work explores scalarisation methods within the context of
solving the bi-objective quadratic assignment problem using a quantum-hybrid
solver. We show results that are consistent with previous research on a
different Ising machine.
`,authors:"Mayowa Ayodele",status:0,relevancy:.18695488808112448,isStarred:!1,keywords:null,createdAt:"2024-05-31T04:51:34.029Z",updatedAt:"2024-05-31T04:51:34.029Z",DatesTable:{value:"2024-05-27",status:"complete",count:126,createdAt:"2024-05-31T00:09:48.114Z",updatedAt:"2024-05-31T04:51:34.031Z"}}]}],calendarStateAtom=atom("loading"),lastRecordReachedAtom=atom(!1),calendarModelAtomBase=atom([]),calendarModelAtom=splitAtom(calendarModelAtomBase),fetchCalendarModelAtom=atom(null,async(a,i)=>{try{i(calendarStateAtom,"loading"),i(lastRecordReachedAtom,!1);const o=calendar;console.log("Calendar Model: ",o),i(calendarModelAtomBase,o);const s=o.length>0;i(calendarStateAtom,s?"ready":"backfill")}catch(o){console.error("Failed to fetch calendar",o),i(calendarStateAtom,"error")}}),calendarLoadMoreAtom=atom(null,async(a,i,o)=>{try{i(lastRecordReachedAtom,!1),i(lastRecordReachedAtom,!0)}catch(s){console.error("Failed to load more calendar dates",s),i(calendarStateAtom,"error")}}),calendarLoadMonthAtom=atom(null,async(a,i,o)=>{i(lastRecordReachedAtom,!1);try{i(calendarModelAtomBase,calendar),i(calendarStateAtom,"ready")}catch(s){console.error("Failed to load more calendar dates",s),i(calendarStateAtom,"error")}}),resetDateStatusCalenderAtom=atom(null,async(a,i,o)=>{try{const{data:s}=await resetDateStatus(o);if(!s)return;const $=a(calendarModelAtomBase),j=$.findIndex(et=>et.date.value===o);if(j===-1)return;const _e={...$[j]};_e.date.status="pending",i(calendarModelAtomBase,et=>[...et.slice(0,j),_e,...et.slice(j+1)])}catch(s){console.error("Failed to reset date status",s),i(calendarStateAtom,"error")}}),scrapePapersAtom=atom(null,async(a,i,o)=>{let $=a(calendarModelAtom).find(j=>{const{date:_e}=a(j);return _e.value===o});if(!$){console.error("Date not found",o);return}try{i($,j=>({...j,date:{...j.date,status:"scraping"}})),await scrapeDate(o)}catch(j){console.error("Scraping failed:",j),i($,_e=>({..._e,date:{..._e.date,status:"error"}}))}}),scrollableContainerRefAtom$1=atom(null),updatePaperInCalenderAtom=atom(null,async(a,i,{date:o,id:s,changes:$})=>{let _e=a(calendarModelAtom).find(nt=>{const{date:at}=a(nt);return at.value===o});if(!_e)return;const{field:et,value:tt}=$;i(_e,nt=>({...nt,papers:nt.papers.map(at=>at.id===s?{...at,[et]:tt}:at)}))});function scrollToElement({element:a,container:i={},options:o,method:s="scrollIntoView"}){const $=i.current||a,j=1500;return a?new Promise(_e=>{let et;const tt=()=>{$.removeEventListener("scrollend",nt),clearTimeout(et)},nt=()=>{tt(),_e(!0)};switch(et=setTimeout(()=>{tt(),_e(!1)},j),$.addEventListener("scrollend",nt),s){case"scrollIntoView":a.scrollIntoView(o);break;case"scrollTo":$.scrollTo(o);break;default:tt(),_e(!1)}}):Promise.reject("Element not provided or does not exist.")}function getDaysInMonth(a){const[i,o]=a.split(" "),s=parseInt(o,10),j={January:0,February:1,March:2,April:3,May:4,June:5,July:6,August:7,September:8,October:9,November:10,December:11}[i];if(j===void 0)throw new Error("Invalid month name");return new Date(s,j+1,0).getDate()}var Done={},_interopRequireDefault$r=interopRequireDefaultExports;Object.defineProperty(Done,"__esModule",{value:!0});var default_1$r=Done.default=void 0,_createSvgIcon$r=_interopRequireDefault$r(requireCreateSvgIcon()),_jsxRuntime$r=jsxRuntimeExports;default_1$r=Done.default=(0,_createSvgIcon$r.default)((0,_jsxRuntime$r.jsx)("path",{d:"M9 16.2 4.8 12l-1.4 1.4L9 19 21 7l-1.4-1.4z"}),"Done");function DateList(){const[a]=useAtom(datesRowsAtom),[,i]=useAtom(fetchDatesSidebarDataAtom),o=reactExports.useRef(null);return reactExports.useEffect(()=>{i()},[i]),jsxRuntimeExports.jsx(List$2,{ref:o,sx:{overflow:"auto",overflowX:"hidden",flexGrow:1},children:a.map(({month:s,dates:$})=>jsxRuntimeExports.jsx(Month,{month:s,dates:$,container:o},s))})}function Month({month:a,dates:i,container:o}){const[s,$]=reactExports.useState(!1),j=reactExports.useMemo(()=>getDaysInMonth(a),[a]);reactExports.useEffect(()=>{const ut=i.every(ot=>ot.status==="complete");i.length==j&&ut&&$(!0)},[i]);const[_e]=useAtom(selectedDateAtom),[et,tt]=useAtom(openMonthAtom),[nt,at]=useAtom(lastOpenMonthAtom);useAtom(calendarLoadMonthAtom),useAtom(calendarStateAtom);const it=reactExports.useRef({}),st=useLocation(),lt=ut=>{tt(et===ut?"":ut)},ct=async ut=>{const ot=it.current[ut];st.pathname.startsWith("/calendar");const dt=nt!==ut;ot&&await scrollToElement({element:ot,container:o,options:{behavior:"smooth",block:"start"},method:"scrollIntoView"}),dt&&at(ut)};function rt(ut){return formatDateParts(ut,{weekday:"long",day:"2-digit"})}return jsxRuntimeExports.jsxs("div",{ref:ut=>it.current[a]=ut,children:[jsxRuntimeExports.jsx(MonthItem,{onClick:()=>lt(a),sx:{fontWeight:"bolder"},children:jsxRuntimeExports.jsx(ListItemText$1,{primary:jsxRuntimeExports.jsxs("span",{style:{fontWeight:"600",color:"rgba(232, 230, 227, 0.85)"},className:"flex justify-between",children:[a,s?jsxRuntimeExports.jsx(default_1$r,{sx:{color:colors.palette.success.main}}):null]}),sx:{paddingBottom:"4px",marginLeft:"5%",color:"rgba(232, 230, 227, 0.1)"}})}),jsxRuntimeExports.jsx(Collapse$1,{in:et===a,timeout:"auto",onEntered:()=>ct(a),children:jsxRuntimeExports.jsx(List$2,{component:"div",children:i.map(ut=>{const[ot,dt]=rt(ut.value);return jsxRuntimeExports.jsx(Link$2,{to:`/date/${ut.value}`,children:jsxRuntimeExports.jsx(ListItemButton$1,{selected:_e===ut.value,children:jsxRuntimeExports.jsx(ListItemText$1,{primary:jsxRuntimeExports.jsx(DateDisplay,{formattedDate:ot,formattedWeekday:dt,count:ut.count}),sx:{paddingLeft:"14px"}})})},"date-"+ut.value)})})})]},a)}const MonthItem=styled$1(ListItemButton$1)(({theme:a})=>({whiteSpace:"nowrap",borderBottom:"1px solid rgba(140, 130, 115, 0.22)"})),dateStyle={padding:"4px 16px 4px 0px",borderRight:"1px solid rgba(255, 255, 255, 0.1)",whiteSpace:"nowrap",color:"rgba(232, 230, 227, 0.6)"},weekdayStyle={paddingLeft:"16px",whiteSpace:"nowrap"};function DateDisplay({formattedDate:a,formattedWeekday:i,count:o}){return jsxRuntimeExports.jsxs("div",{style:{position:"relative"},children:[" ",jsxRuntimeExports.jsx("span",{style:dateStyle,children:a}),jsxRuntimeExports.jsx("span",{style:weekdayStyle,children:i}),o&&jsxRuntimeExports.jsx("span",{style:{padding:".3rem",whiteSpace:"nowrap",opacity:.4,borderRadius:"30%",fontSize:".8em",marginTop:"-.2rem",marginLeft:".2rem"},children:o})]})}const sidebarOpenAtom=atom(!0),NavItem=styled$1(ListItemButton$1)(({theme:a})=>({marginLeft:".5rem"})),Sidebar=()=>{const a=useNavigate(),[i]=useAtom(sidebarOpenAtom);return jsxRuntimeExports.jsxs(Box$1,{sx:{width:i?240:0,minWidth:0,transition:"width .3s ease",bgcolor:"background.paper",overflow:"hidden",display:"flex",flexDirection:"column",flexShrink:0,paddingTop:"4rem"},children:[jsxRuntimeExports.jsxs(List$2,{component:"nav",sx:{borderTop:"1px solid rgba(0, 0, 0, 0.12)",backgroundColor:colors.palette.background.default,borderBottom:"1px solid rgba(140, 130, 115, 0.22)"},children:[jsxRuntimeExports.jsxs(NavItem,{onClick:()=>a("search"),sx:{},children:[jsxRuntimeExports.jsx(ListItemIcon$1,{children:jsxRuntimeExports.jsx(default_1$t,{})}),jsxRuntimeExports.jsx(ListItemText$1,{primary:"Search"})]}),jsxRuntimeExports.jsxs(NavItem,{onClick:()=>a("backfill"),children:[jsxRuntimeExports.jsx(ListItemIcon$1,{children:jsxRuntimeExports.jsx(default_1$s,{})}),jsxRuntimeExports.jsx(ListItemText$1,{primary:"Backfill"})]})]}),jsxRuntimeExports.jsx(DateList,{})]})},baseX=-50,verticalY=.325,verticalRotate=0,arrowY=.2,arrowRotate=25,invert=a=>a*-1,transformString=(a,i)=>`translateX(${baseX}%) translateY(${a}rem) rotate(${i}deg)`,transformValues={top:{open:transformString(invert(verticalY),verticalRotate),hover:transformString(invert(arrowY),arrowRotate),closed:transformString(invert(arrowY),invert(arrowRotate))},bottom:{open:transformString(verticalY,verticalRotate),hover:transformString(arrowY,invert(arrowRotate)),closed:transformString(arrowY,arrowRotate)}};function SidebarToggleButton(){const[a,i]=useAtom(sidebarOpenAtom),[o,s]=reactExports.useState(!1);reactExports.useEffect(()=>{const _e=et=>{if(et.target.tagName==="INPUT"||et.target.tagName==="TEXTAREA"||et.target.tagName==="SELECT"||et.target.isContentEditable)return;const tt=et.keyCode===37,nt=et.keyCode===39;tt?i(!1):nt&&i(!0)};return window.addEventListener("keydown",_e),()=>{window.removeEventListener("keydown",_e)}},[i]);const $={position:"absolute",left:"50%",width:"0.25rem",height:"0.75rem",backgroundColor:"grey",transition:"transform 0.3s ease",borderRadius:"9999px"},j=_e=>{const et=o&&a?"hover":a?"open":"closed";return{...$,transform:transformValues[_e][et]}};return jsxRuntimeExports.jsx("div",{style:{position:"absolute",top:"50%",transform:"translateY(-50%)",padding:"30px 0",cursor:"pointer",zIndex:1201},onClick:()=>i(!a),onMouseEnter:()=>s(!0),onMouseLeave:()=>s(!1),children:jsxRuntimeExports.jsxs("div",{style:{padding:"0rem 1rem"},children:[jsxRuntimeExports.jsx("div",{style:j("top")}),jsxRuntimeExports.jsx("div",{style:j("bottom")})]})})}var Menu={},_interopRequireDefault$q=interopRequireDefaultExports;Object.defineProperty(Menu,"__esModule",{value:!0});var default_1$q=Menu.default=void 0,_createSvgIcon$q=_interopRequireDefault$q(requireCreateSvgIcon()),_jsxRuntime$q=jsxRuntimeExports;default_1$q=Menu.default=(0,_createSvgIcon$q.default)((0,_jsxRuntime$q.jsx)("path",{d:"M3 18h18v-2H3zm0-5h18v-2H3zm0-7v2h18V6z"}),"Menu");const createId=()=>Math.random().toString(36).substring(2,9),alertsAtom=atom([]),snackbarsAtom=atom([]),addAlertAtom=atom(null,(a,i,{id:o,message:s,type:$,autoClose:j})=>{const _e=a(alertsAtom);_e.some(et=>et.id===o)||i(alertsAtom,[..._e,{id:o||createId(),message:s,type:$,autoClose:j}])}),addSnackAtom=atom(null,(a,i,{id:o,message:s,autoClose:$})=>{const j=a(snackbarsAtom);j.some(_e=>_e.id===o)||i(snackbarsAtom,[...j,{id:o||createId(),message:s,autoClose:$}])}),featureDisabledAlertAtom=atom(null,(a,i)=>{i(addSnackAtom,{message:"This feature is not available in the public demo",autoClose:!0})});function SlideTransition(a){return jsxRuntimeExports.jsx(Slide$1,{...a,direction:"left"})}const NotificationManager=()=>{const[a,i]=useAtom(alertsAtom),[o,s]=useAtom(snackbarsAtom);return jsxRuntimeExports.jsxs("div",{children:[a.map((_e,et)=>jsxRuntimeExports.jsx(Snackbar$1,{open:!0,autoHideDuration:_e.autoClose?6e3:null,onClose:()=>$(_e.id),anchorOrigin:{vertical:"bottom",horizontal:"right"},style:{bottom:et*60+20,transition:"bottom 0.3s ease-in-out"},ClickAwayListenerProps:{onClickAway:()=>null},TransitionComponent:SlideTransition,children:jsxRuntimeExports.jsx(Alert$1,{onClose:()=>$(_e.id),severity:_e.type||"warning",sx:{width:"100%"},children:_e.message})},_e.id)),o.map((_e,et)=>jsxRuntimeExports.jsx(Snackbar$1,{open:!0,autoHideDuration:_e.autoClose?3e3:null,onClose:()=>j(_e.id),anchorOrigin:{vertical:"bottom",horizontal:"right"},style:{bottom:(a.length+et)*60+20,transition:"bottom 0.3s ease-in-out"},ClickAwayListenerProps:{onClickAway:()=>null},TransitionComponent:SlideTransition,children:jsxRuntimeExports.jsx(Alert$1,{onClose:()=>j(_e.id),severity:"info",sx:{width:"100%"},children:_e.message})},_e.id))]});function $(_e){i(et=>et.filter(tt=>tt.id!==_e))}function j(_e){s(et=>et.filter(tt=>tt.id!==_e))}},isNewUserAtom=atom(!1);function Layout(){useNavigate();const[a,i]=useAtom(isNewUserAtom);return jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsxs(Box$1,{sx:{display:"flex",maxHeight:"100vh",transition:"all 0.5s ease-in-out",backgroundColor:colors.palette.background.default},children:[jsxRuntimeExports.jsx(TitleArea,{isNewUser:a}),!a&&jsxRuntimeExports.jsx(Sidebar,{}),jsxRuntimeExports.jsxs(Box$1,{component:"main",sx:{flexGrow:1},children:[jsxRuntimeExports.jsx(AppBar$1,{position:"sticky",sx:{backgroundColor:colors.palette.background.default,boxShadow:"none"},children:jsxRuntimeExports.jsx(Toolbar$1,{sx:{display:"flex",justifyContent:"flex-end"},children:!a&&jsxRuntimeExports.jsx(SearchInput,{})})}),!a&&jsxRuntimeExports.jsx(SidebarToggleButton,{}),jsxRuntimeExports.jsx(Outlet,{})]})]}),jsxRuntimeExports.jsx(NotificationManager,{})]})}function TitleArea({isNewUser:a}){const[,i]=useAtom(sidebarOpenAtom),o=()=>{i(s=>!s)};return jsxRuntimeExports.jsxs("div",{style:{display:"flex",marginLeft:".4rem",width:"230px",position:"absolute",top:".7rem",left:"1.4rem",zIndex:9999},children:[!a&&jsxRuntimeExports.jsx(IconButton$1,{edge:"start",color:"inherit","aria-label":"menu",sx:{mr:2},onClick:o,children:jsxRuntimeExports.jsx(default_1$q,{})}),jsxRuntimeExports.jsx(Link$2,{color:"inherit",to:a?"/onboard":"/calendar",children:jsxRuntimeExports.jsxs("div",{style:{display:"flex",flexDirection:"row"},children:[jsxRuntimeExports.jsx(Typography$1,{variant:"h6",component:"div",sx:{borderRadius:"10%",padding:"2px 7px",backgroundColor:"rgba(0, 0, 0, 0.3)"},children:"Curate"}),jsxRuntimeExports.jsx(Typography$1,{variant:"h6",component:"div",sx:{padding:"4px 4px"},children:"GPT"})]})})]})}function SearchInput(){const[a,i]=reactExports.useState(""),o=useSetAtom(featureDisabledAlertAtom),s=j=>{i(j.target.value)},$=j=>{j.preventDefault(),o()};return jsxRuntimeExports.jsxs("form",{onSubmit:$,style:{display:"flex"},children:[" ",jsxRuntimeExports.jsx(TextField$1,{id:"query-input",label:"Keyword",value:a,size:"small",onChange:s,InputLabelProps:{style:{color:"#9e9e9e"}},sx:{borderTopRightRadius:"0",borderBottomRightRadius:"0",backgroundColor:"rgba(0, 0, 0, 0.3)"},InputProps:{sx:{borderTopRightRadius:"0",borderBottomRightRadius:"0"}},fullWidth:!0}),jsxRuntimeExports.jsx(Button$1,{type:"submit",color:"primary",variant:"contained",sx:{borderTopLeftRadius:"0",borderBottomLeftRadius:"0",boxShadow:"none"},children:"Search"})]})}const getColorShadeRedToGreen=a=>{const i=[0,145,0],o=[145,145,0],s=[145,0,0],$=(_e,et,tt)=>_e.map((nt,at)=>Math.round(nt+tt*(et[at]-nt)));return`rgb(${(a<=.5?$(s,o,a*2):$(o,i,(a-.5)*2)).join(", ")})`},popoverTargetAtom=atom(null),isSummaryOpenAtom=atom(!1),anchorElAtom=atom(null),popoverRefAtom=atom(null),hoverTimeoutAtom=atom(null);function roundScore(a){return(a*100).toFixed(0)}const padding=-8,PopoverText=styled$1(Paper$1)(({theme:a})=>({maxWidth:"400px",padding:a.spacing(2),color:a.palette.common.white,borderRadius:a.shape.borderRadius,boxShadow:"0px 2px 8px rgba(0, 0, 0, 0.15)"})),ScoreDiv=styled$1(Box$1)(({theme:a,score:i})=>({display:"inline-block",float:"left",backgroundColor:getColorShadeRedToGreen(i),color:a.palette.common.white,borderRadius:a.shape.borderRadius,padding:"4px 8px",fontWeight:"bold",letterSpacing:"0.1em",border:"1px solid rgba(255, 255, 255, 0.4)",textShadow:"1px 1px 2px rgba(0, 0, 0, 0.1)",margin:"0 12px 0px 0",filter:"brightness(0.9)"})),SummaryPopover=()=>{const[a,i]=useAtom(isSummaryOpenAtom),[o]=useAtom(anchorElAtom),[s,$]=useAtom(popoverRefAtom),[j]=useAtom(popoverTargetAtom);let{relevancy:_e}=j||{relevancy:0};const[et,tt]=reactExports.useState((j==null?void 0:j.abstract)||""),[nt,at]=useAtom(hoverTimeoutAtom),it=lt=>{$(lt)},st=lt=>{nt&&clearTimeout(nt);const ct=lt.relatedTarget;!(s!=null&&s.contains(ct))&&ct&&i(!1)};return reactExports.useEffect(()=>{tt((j==null?void 0:j.abstract)||"")},[j]),reactExports.useEffect(()=>{if(a&&o&&s){const lt=o.getBoundingClientRect(),ct=s.getBoundingClientRect(),rt=window.innerWidth,ut=window.innerHeight;let ot=lt.left+(lt.width-ct.width)/2;ot<0?ot=0:ot+ct.width>rt&&(ot=rt-ct.width);let dt=ct.height,pt=lt.top-dt-padding,mt=lt.bottom+padding,ft;const ht=()=>ft=pt+11,yt=()=>ft=mt+12,bt=pt<0,gt=mt+dt>ut;if(bt)if(gt){const xt=lt.top+lt.height/2>ut/2;xt?ht():yt();const vt=xt?lt.y:ut-mt;tt(slice(j==null?void 0:j.abstract,vt*2.3)+"...")}else yt();else ht();s.style.left=`${ot}px`,s.style.top=`${ft}px`}},[o,s,et]),jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:a&&jsxRuntimeExports.jsx("div",{onMouseLeave:st,ref:it,style:{position:"absolute",zIndex:9999,cursor:"pointer"},children:jsxRuntimeExports.jsxs(PopoverText,{children:[jsxRuntimeExports.jsx(ScoreDiv,{score:_e,children:`${roundScore(_e)}%`}),jsxRuntimeExports.jsx(Typography$1,{variant:"body2",children:et})]})})})};function slice(a,i){return a?a.length>i?a.slice(0,i):a:""}const ResetState=({date:a,resetStatusAtom:i})=>{const o=useSetAtom(featureDisabledAlertAtom),s=()=>{o()};return jsxRuntimeExports.jsxs("div",{className:"flex flex-col items-center p-6 bg-white rounded-lg shadow-md max-w-md mx-auto my-8",children:[jsxRuntimeExports.jsx("div",{className:"font-medium text-gray-800 text-lg mb-2",children:"Issue finding papers"}),jsxRuntimeExports.jsxs("ul",{className:"text-sm text-gray-700 mb-4 list-disc list-inside",children:[jsxRuntimeExports.jsx("li",{children:"Make sure Chroma DB is running"}),jsxRuntimeExports.jsx("li",{children:"ArXiv's servers may be down"}),jsxRuntimeExports.jsx("li",{children:"Maybe we broke something.. again"}),jsxRuntimeExports.jsx("li",{children:"Actually no papers submitted (unlikely)"})]}),jsxRuntimeExports.jsx("button",{className:"bg-red-500 text-white active:bg-red-600 font-bold uppercase text-xs px-6 py-3 rounded shadow hover:shadow-lg outline-none focus:outline-none ease-linear transition-all duration-150",type:"button",onClick:s,children:"Reset Status"})]})};var PaperState;(function(a){a[a.initial=0]="initial",a[a.approved=1]="approved",a[a.generated=2]="generated",a[a.published=3]="published"})(PaperState||(PaperState={}));var StarBorderOutlined={},_interopRequireDefault$p=interopRequireDefaultExports;Object.defineProperty(StarBorderOutlined,"__esModule",{value:!0});var default_1$p=StarBorderOutlined.default=void 0,_createSvgIcon$p=_interopRequireDefault$p(requireCreateSvgIcon()),_jsxRuntime$p=jsxRuntimeExports;default_1$p=StarBorderOutlined.default=(0,_createSvgIcon$p.default)((0,_jsxRuntime$p.jsx)("path",{d:"m22 9.24-7.19-.62L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27 18.18 21l-1.63-7.03zM12 15.4l-3.76 2.27 1-4.28-3.32-2.88 4.38-.38L12 6.1l1.71 4.04 4.38.38-3.32 2.88 1 4.28z"}),"StarBorderOutlined");var StarOutlined={},_interopRequireDefault$o=interopRequireDefaultExports;Object.defineProperty(StarOutlined,"__esModule",{value:!0});var default_1$o=StarOutlined.default=void 0,_createSvgIcon$o=_interopRequireDefault$o(requireCreateSvgIcon()),_jsxRuntime$o=jsxRuntimeExports;default_1$o=StarOutlined.default=(0,_createSvgIcon$o.default)((0,_jsxRuntime$o.jsx)("path",{d:"M12 17.27 18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"}),"StarOutlined");function Favorite({paper:a={}}){const i=useSetAtom(featureDisabledAlertAtom),o=async()=>{i()};return jsxRuntimeExports.jsx(IconButton$1,{onClick:o,children:a.isStarred?jsxRuntimeExports.jsx(default_1$o,{color:"warning"}):jsxRuntimeExports.jsx(default_1$p,{color:"warning"})})}const rootAssetsPath="http://localhost:5173/assets/",getThumbnailUrl=a=>{const i=a.video;return i!=null&&i.thumbnailUrl?`${rootAssetsPath}thumbnails/${i==null?void 0:i.thumbnailUrl}`:`${rootAssetsPath}thumbnails/default/arxiv-bg.jpg`};function Thumbnail({paper:a,shadow:i=!1}){const o=useNavigate(),s=$=>{$.stopPropagation();const j=et=>$.target.tagName===et;j("BUTTON")||j("path")||j("svg")||j("LI")||o(`/paper/${a.id}`)};return jsxRuntimeExports.jsxs(Box$1,{onClick:s,sx:{cursor:"pointer",position:"relative",width:"320px",height:"180px",border:`2px solid ${getColorShadeRedToGreen(a.relevancy)}`,borderBottom:`10px solid ${getColorShadeRedToGreen(a.relevancy)}`,"&:hover":{border:"2px solid white",borderBottom:"10px solid white"}},className:"thumb-img",children:[jsxRuntimeExports.jsx("img",{src:getThumbnailUrl(a),alt:a.title,style:{width:"100%",height:"100%",borderRadius:"4px",borderBottomRightRadius:"0px",borderBottomLeftRadius:"0px"}}),jsxRuntimeExports.jsx(Actions$1,{paper:a}),jsxRuntimeExports.jsx(PaperTitle$1,{paper:a})]},a.id)}function Actions$1({paper:a}){return jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(ButtonGroup$1,{variant:"outlined","aria-label":"paper actions"}),jsxRuntimeExports.jsx("div",{style:{position:"absolute",bottom:0,right:0,backgroundColor:"rgba(38,4,4,.95)",height:"2.3rem",borderBottomLeftRadius:"4px",borderBottomRightRadius:"4px",zIndex:999},children:jsxRuntimeExports.jsx(Favorite,{paper:a})})]})}function PaperTitle$1({paper:a}){return jsxRuntimeExports.jsx("div",{style:{position:"absolute",bottom:0,left:0,right:0,padding:"8px",backgroundColor:"rgba(0, 0, 0, 0.8)",color:"white",textAlign:"left"},children:jsxRuntimeExports.jsx("span",{children:a.title})})}function PaperTile({paper:a,currentPage:i,previousPage:o,imagesPerPage:s,index:$,shadow:j=!1,inCarousel:_e=!1}){const[,et]=useAtom(anchorElAtom),[,tt]=useAtom(isSummaryOpenAtom),[nt]=useAtom(popoverRefAtom),[,at]=useAtom(popoverTargetAtom),[it,st]=useAtom(hoverTimeoutAtom);let lt=!1;if(_e){const ut=$>=(i-1)*s&&$<i*s,ot=$>=(o-1)*s&&$<o*s;lt=!ut&&!ot}const ct=ut=>ot=>{const dt=ht=>ot.target.tagName===ht,pt=dt("BUTTON")||dt("path")||dt("svg")||dt("DIV");if(it&&clearTimeout(it),pt)return;const mt=ot.currentTarget,ft=setTimeout(()=>{at(ut),et(mt),tt(!0)},10);st(ft)},rt=ut=>{it&&clearTimeout(it);const ot=ut.relatedTarget;(!ot||!(nt!=null&&nt.contains(ot)))&&tt(!1)};return jsxRuntimeExports.jsx("div",{className:lt?"offscreen-image":"",onMouseOver:ct(a),onMouseLeave:rt,children:jsxRuntimeExports.jsx(Thumbnail,{paper:a,shadow:j})})}function List({papers:a,date:i}){const[o,s]=reactExports.useState(1),[$,j]=reactExports.useState(2),_e=(nt,at)=>{j(o),s(at)},et=a.length,tt=4;return jsxRuntimeExports.jsx("div",{className:"wrapper",style:{margin:"1em"},children:jsxRuntimeExports.jsx("div",{className:"carousel-container",children:a.length===0?jsxRuntimeExports.jsx(ResetState,{date:i,resetStatusAtom:resetDateStatusCalenderAtom}):jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(Carousel,{papers:a,imagesPerPage:tt,previousPage:$,currentPage:o}),jsxRuntimeExports.jsx("div",{className:"pagination-wrapper",children:jsxRuntimeExports.jsx(Pagination$2,{count:Math.ceil(et/tt),shape:"rounded",color:"secondary",page:o,onChange:_e})})]})})})}function Carousel({papers:a,imagesPerPage:i,previousPage:o,currentPage:s}){const $=parseInt(getComputedStyle(document.documentElement).fontSize),_e=i*(320+$*1*2);return jsxRuntimeExports.jsx("div",{className:"carousel-wrapper",style:{transform:`translateX(-${(s-1)*_e}px)`},children:a.map((et,tt)=>jsxRuntimeExports.jsx(PaperTile,{paper:et,currentPage:s,previousPage:o,imagesPerPage:i,index:tt,inCarousel:!0},et.id))})}var Description={},_interopRequireDefault$n=interopRequireDefaultExports;Object.defineProperty(Description,"__esModule",{value:!0});var default_1$n=Description.default=void 0,_createSvgIcon$n=_interopRequireDefault$n(requireCreateSvgIcon()),_jsxRuntime$n=jsxRuntimeExports;default_1$n=Description.default=(0,_createSvgIcon$n.default)((0,_jsxRuntime$n.jsx)("path",{d:"M14 2H6c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8zm2 16H8v-2h8zm0-4H8v-2h8zm-3-5V3.5L18.5 9z"}),"Description");const Scraping=()=>jsxRuntimeExports.jsxs(Box$1,{display:"flex",flexDirection:"column",alignItems:"center",gap:3,marginTop:3,marginBottom:4,children:[jsxRuntimeExports.jsxs(Typography$1,{variant:"h3",children:["Scraping",jsxRuntimeExports.jsx(default_1$n,{fontSize:"inherit",sx:{margin:1,marginBottom:2}})]}),jsxRuntimeExports.jsx("div",{style:{width:500},children:jsxRuntimeExports.jsx(LinearProgress$1,{})})]});var StarHalf={},_interopRequireDefault$m=interopRequireDefaultExports;Object.defineProperty(StarHalf,"__esModule",{value:!0});var default_1$m=StarHalf.default=void 0,_createSvgIcon$m=_interopRequireDefault$m(requireCreateSvgIcon()),_jsxRuntime$m=jsxRuntimeExports;default_1$m=StarHalf.default=(0,_createSvgIcon$m.default)((0,_jsxRuntime$m.jsx)("path",{d:"m22 9.24-7.19-.62L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27 18.18 21l-1.63-7.03zM12 15.4V6.1l1.71 4.04 4.38.38-3.32 2.88 1 4.28z"}),"StarHalf");const Ranking=()=>jsxRuntimeExports.jsxs(Box$1,{display:"flex",flexDirection:"column",alignItems:"center",gap:3,marginTop:3,marginBottom:4,children:[jsxRuntimeExports.jsxs(Typography$1,{variant:"h3",children:["Ranking",jsxRuntimeExports.jsx(default_1$m,{fontSize:"inherit",sx:{margin:1,marginBottom:2}})]}),jsxRuntimeExports.jsx("div",{style:{width:500},children:jsxRuntimeExports.jsx(LinearProgress$1,{})})]});function ActionButton({date:a,scrapeAtom:i}){const o=useSetAtom(featureDisabledAlertAtom),s=()=>{o()};return jsxRuntimeExports.jsxs(Box$1,{display:"flex",flexDirection:"column",alignItems:"center",gap:3,margin:3,children:[jsxRuntimeExports.jsx(Typography$1,{variant:"h3",sx:{opacity:.5},children:"Not Scraped"}),jsxRuntimeExports.jsx(Box$1,{display:"flex",gap:2,children:jsxRuntimeExports.jsxs(Button$1,{variant:"contained",onClick:s,sx:{width:"25rem",height:"5rem",position:"relative",fontWeight:700},children:[jsxRuntimeExports.jsxs("div",{className:"flex opacity-20 absolute justify-between w-full px-7 pointer-events-none",children:[jsxRuntimeExports.jsx(default_1$m,{sx:{height:"4rem",width:"4rem"}}),jsxRuntimeExports.jsx(default_1$n,{sx:{height:"4rem",width:"4rem"}})]}),"Scrape & Rank Papers"]})})]})}const ErrorState=()=>jsxRuntimeExports.jsx("div",{className:"flex flex-col items-center p-6 max-w-md mx-auto my-8",children:jsxRuntimeExports.jsx(Typography$1,{variant:"h3",sx:{opacity:.3,color:"red"},children:"Error Occurred"})});function ScrapeStatus({date:a,scrapeAtom:i,status:o}){const s={pending:jsxRuntimeExports.jsx(ActionButton,{date:a,scrapeAtom:i}),scraping:jsxRuntimeExports.jsx(Scraping,{}),ranking:jsxRuntimeExports.jsx(Ranking,{}),error:jsxRuntimeExports.jsx(ErrorState,{})};return jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:s[o]})}function RowItem({dateAtom:a,isFocalElement:i}){useAtom(selectedDateAtom);const{date:o,papers:s}=useAtomValue(a),{value:$,status:j}=o,_e=useNavigate(),et=reactExports.useCallback(at=>formatDate$1(at,{weekday:"short",month:"short",day:"2-digit"}),[]),tt=at=>it=>{const st=ct=>it.target.tagName===ct;st("BUTTON")||st("path")||st("svg")||st("LI")||_e(`/date/${at.value}`)},nt=j==="complete";return jsxRuntimeExports.jsxs(Box$1,{sx:{display:"flex",flexDirection:"column",alignItems:"center",paddingTop:2,paddingBottom:2,alignSelf:"center",width:"65%",backgroundColor:nt?"transparent":colors.palette.background.paper,margin:"0rem 2rem",marginBottom:".2rem"},children:[jsxRuntimeExports.jsx(Typography$1,{onClick:tt(o),className:"date-row-title",variant:"h5",sx:{textDecoration:"none",marginBottom:"4px",marginTop:".5em",backgroundColor:nt?colors.palette.background.paper:colors.palette.background.default,webkitBackgroundClip:"text",webkitTextFillColor:"transparent",padding:".25em 1em .25em 1em",borderRadius:"5px",fontWeight:"bold",display:"inline-block",fontFamily:'"Roboto", "Helvetica", "Arial", sans-serif',letterSpacing:"0.01em",textShadow:"1px 1px 2px rgba(0, 0, 0, 0.1)",cursor:"pointer"},children:et($)}),j==="complete"?jsxRuntimeExports.jsx(List,{papers:s,date:$}):jsxRuntimeExports.jsx(ScrapeStatus,{status:j,date:$,scrapeAtom:scrapePapersAtom})]})}function DateRows(){var tt;const a=useAtomValue(calendarModelAtom),[i]=useAtom(scrollableContainerRefAtom$1),[,o]=useAtom(isSummaryOpenAtom),$=a.length>0?a[a.length-1]:emptyObjectAtom,j=useAtomValue($),_e=a.length,et=useSetAtom(updatePaperInCalenderAtom);return reactExports.useEffect(()=>{const nt=at=>{et(at.detail)};return window.addEventListener("paperUpdate",nt),()=>{o(!1),window.removeEventListener("paperUpdate",nt)}},[]),reactExports.useEffect(()=>{const nt=i==null?void 0:i.current;if(!nt)return;if(_e===5){nt.scrollTo({top:0,behavior:"smooth"});return}const at=nt.scrollHeight;nt.scrollTo({top:at,behavior:"smooth"})},[_e]),jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(SummaryPopover,{}),a.map((nt,at)=>{const it=at===a.length-1;return jsxRuntimeExports.jsx(RowItem,{dateAtom:nt,isFocalElement:it},`date-${at}`)}),jsxRuntimeExports.jsx(LoadMoreButton,{dbCursor:(tt=j==null?void 0:j.date)==null?void 0:tt.value})]})}const LoadMoreButton=({dbCursor:a})=>{const[,i]=useAtom(calendarLoadMoreAtom),[o]=useAtom(lastRecordReachedAtom),[s,$]=reactExports.useState(!1),j=async()=>{s||($(!0),await i(a),$(!1))};return jsxRuntimeExports.jsx("div",{className:"flex justify-center mt-8 pb-16",children:jsxRuntimeExports.jsx(Button$1,{variant:"contained",color:"secondary",disabled:s||o,onClick:j,className:"text-white bg-red-500 hover:bg-red-700 ... your tailwind classes here ...",children:s?jsxRuntimeExports.jsx(CircularProgress$1,{size:24,color:"inherit"}):o?"All records loaded":"Load More"})})};function DatesPlaceholder(){const a=Array(3).fill(null);return jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:a.map((i,o)=>jsxRuntimeExports.jsxs(Box$1,{sx:{display:"flex",flexDirection:"column",alignItems:"center",borderBottom:"1px solid rgba(0, 0, 0, 0.3)",paddingTop:2,paddingBottom:2,cursor:"pointer",margin:".5rem 2rem"},children:[jsxRuntimeExports.jsx("div",{style:{width:"188px",height:"45px",marginBottom:"4px",marginTop:".6em",backgroundColor:colors.palette.background.paper,padding:".25em 1em .25em 1em",borderRadius:"5px",transform:"skewX(-5deg)",display:"inline-block"}}),jsxRuntimeExports.jsx(PlaceholderList,{}),jsxRuntimeExports.jsx("div",{style:{width:"17rem",height:"1.2rem",backgroundColor:colors.palette.background.paper,marginBottom:"1em",marginTop:"1.5rem"}})]},"date-placeholder-"+o))})}function PlaceholderList(){const a=Array(4).fill(null);return jsxRuntimeExports.jsx("div",{className:"wrapper w-full",children:jsxRuntimeExports.jsx("div",{className:"carousel-container",children:jsxRuntimeExports.jsx("div",{className:"carousel-wrapper flex justify-between",children:a.map((i,o)=>jsxRuntimeExports.jsx("div",{className:"placeholder-thumbnail",children:jsxRuntimeExports.jsx(ThumbnailPlaceholder,{})},o))})})})}function ThumbnailPlaceholder({shadow:a=!1}){return jsxRuntimeExports.jsx("div",{style:{position:"relative",width:"320px",height:"180px",backgroundColor:colors.palette.background.paper,borderBottomRightRadius:"4px",borderBottomLeftRadius:"4px",boxShadow:a?"0px 2px 15px rgba(0, 0, 0, 0.6)":"none",padding:"1em"},className:"thumb-img"})}function CalendarMain(){const[,a]=useAtom(fetchCalendarModelAtom),[,i]=useAtom(calendarLoadMonthAtom),[o]=useAtom(datesRowsAtom),[s]=useAtom(openMonthAtom),[$]=useAtom(calendarStateAtom),j=useNavigate(),_e=$==="backfill";return reactExports.useEffect(()=>{_e&&j("/backfill")},[$]),reactExports.useEffect(()=>{var tt,nt;const et=(nt=(tt=o.find(at=>at.month===s))==null?void 0:tt.dates[0])==null?void 0:nt.value;s&&et?i(et):a()},[a]),jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:jsxRuntimeExports.jsx(MainContent,{})})}function MainContent(){const[a]=useAtom(calendarStateAtom),i=a==="loading",o=a==="error",s=a==="backfill";return jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:i?jsxRuntimeExports.jsx(DatesPlaceholder,{}):o||s?jsxRuntimeExports.jsx("div",{className:" place-self-center",children:"Failed to fetch calendar data"}):jsxRuntimeExports.jsx(DateRows,{})})}var KeyboardArrowUp={},_interopRequireDefault$l=interopRequireDefaultExports;Object.defineProperty(KeyboardArrowUp,"__esModule",{value:!0});var default_1$l=KeyboardArrowUp.default=void 0,_createSvgIcon$l=_interopRequireDefault$l(requireCreateSvgIcon()),_jsxRuntime$l=jsxRuntimeExports;default_1$l=KeyboardArrowUp.default=(0,_createSvgIcon$l.default)((0,_jsxRuntime$l.jsx)("path",{d:"M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"}),"KeyboardArrowUp");const height="calc(100vh - 65px)",PageLayout=reactExports.forwardRef(({children:a,compact:i=!0,...o},s)=>{const[$,j]=reactExports.useState(!1);s=s||reactExports.createRef();const _e=()=>{if(s&&"current"in s&&s.current){const{scrollTop:tt}=s.current;j(tt>200)}};reactExports.useEffect(()=>{const tt=s&&"current"in s?s.current:null;if(tt)return tt.addEventListener("scroll",_e),()=>tt.removeEventListener("scroll",_e)},[s]);const et=()=>{s&&"current"in s&&s.current&&s.current.scrollTo({top:0,behavior:"smooth"})};return jsxRuntimeExports.jsx(Box$1,{ref:s,sx:{overflowY:"auto",flexGrow:1,height,justifyContent:"center",display:"flex",overflowAnchor:"none"},...o,children:jsxRuntimeExports.jsxs("div",{style:{width:i?"90%":"100%",display:"flex",flexDirection:"column"},children:[a,$&&jsxRuntimeExports.jsx(Fab$1,{color:"secondary",size:"small",onClick:et,sx:{position:"fixed",bottom:"2rem",right:"2rem"},children:jsxRuntimeExports.jsx(default_1$l,{})})]})})}),pagesListening=atom({}),SocketListener=({eventName:a,handleEvent:i,id:o=""})=>(useAtom(pagesListening),null),Calendar=()=>{const[,a]=useAtom(scrollableContainerRefAtom$1),[i,o]=useAtom(isSummaryOpenAtom),s=useSetAtom(addAlertAtom),$=reactExports.useRef(null);reactExports.useEffect(()=>{a($)},[a]);const j=()=>{i&&o(!1)},_e=({key:et,status:tt,data:nt})=>{if(tt==="error"){const at=dayjs(et).format("MM/DD/YYYY");s({message:`There was a problem scraping papers for ${at}`,id:at})}};return jsxRuntimeExports.jsxs(PageLayout,{compact:!1,ref:$,onScroll:j,className:"calendar",children:[jsxRuntimeExports.jsx(CalendarMain,{}),jsxRuntimeExports.jsx(SocketListener,{eventName:"date_status",handleEvent:_e})]})},tabValueAtom$1=atom("table"),searchKeywordAtom=atom(""),scrapingStateAtom=atom("pending"),dateEntryStateAtom=atom("loading"),dateEntryPapersAtom=atom([]),filteredPapersAtom=atom(a=>{const i=a(dateEntryPapersAtom),o=a(searchKeywordAtom).toLowerCase();return o.trim()?i.filter(s=>s.title.toLowerCase().includes(o)||s.abstract.toLowerCase().includes(o)||s.authors.split(";").some($=>$.toLowerCase().includes(o))):i},async(a,i,o)=>{i(dateEntryPapersAtom,o)}),fetchPapersByDateAtom=atom(null,async(a,i,o)=>{if(!o){console.error("Date not found",o);return}i(dateEntryStateAtom,"loading");try{const s=calendar[1],{date:$,papers:j}=s;if(!$)throw new Error("Date not found");$.status==="complete"&&j.length===0?i(dateEntryStateAtom,"unexpected"):(i(dateEntryPapersAtom,j),i(dateEntryStateAtom,$.status))}catch(s){console.error("Failed to fetch papers for date",s),i(dateEntryStateAtom,"error")}}),scrapePapersDateEntryAtom=atom(null,async(a,i,o)=>{try{i(scrapingStateAtom,"scraping"),await scrapeDate(o)}catch(s){console.error("Scraping failed:",s),i(scrapingStateAtom,"pending"),i(dateEntryStateAtom,"error")}}),resetDateEntryStatusAtom=atom(null,async(a,i,o)=>{try{const{data:s}=await resetDateStatus(o);if(!s)return;i(dateEntryStateAtom,"pending"),i(scrapingStateAtom,"pending")}catch(s){console.error("Failed to reset date status",s),i(dateEntryStateAtom,"error")}}),ScoreBadge=styled(Badge$1)(({theme:a,count:i})=>({"& .MuiBadge-badge":{top:"50%",right:"100%",backgroundColor:colors.palette.secondary.main,color:a.palette.common.white,borderRadius:a.shape.borderRadius,width:"2rem",padding:"4px 8px",fontWeight:"bold",letterSpacing:"0.2em",border:"1px solid rgba(255, 255, 255, 0.4)"}})),PageTitle=({value:a,count:i})=>{const[o]=useAtom(dateEntryStateAtom),s=reactExports.useMemo(()=>{const[$,j,_e,et]=formatDateParts(a,{weekday:"long",month:"long",day:"2-digit",year:"numeric"});return`${$}, ${j} ${_e}, ${et}`},[a]);return jsxRuntimeExports.jsx(Box$1,{display:"flex",flexDirection:"column",alignItems:"center",children:jsxRuntimeExports.jsx(ScoreBadge,{badgeContent:`${i}`,count:i,max:999,children:jsxRuntimeExports.jsx(Typography$1,{variant:"h4",sx:{background:colors.palette.background.paper,padding:".5em 2em .5em 2em",borderRadius:"5px",letterSpacing:"0.0075em"},children:o==="error"?`Error loading date ${a}`:s})})})},ThumbPapersGrid=({papers:a,isLoading:i=!1,placeholderRows:o=4})=>{const[,s]=useAtom(isSummaryOpenAtom);return reactExports.useEffect(()=>()=>s(!1),[]),jsxRuntimeExports.jsxs("div",{style:{display:"flex",flexWrap:"wrap",gap:"2em",marginBottom:"2em",paddingTop:"1rem",justifyContent:"space-between"},children:[i?jsxRuntimeExports.jsx(GridPlaceholder,{placeholderRows:o}):jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:a.map(($,j)=>jsxRuntimeExports.jsx("div",{children:jsxRuntimeExports.jsx(PaperTile,{paper:$,shadow:!0})},`${$.id}-${j}`))}),jsxRuntimeExports.jsx(SummaryPopover,{})]})},GridPlaceholder=({placeholderRows:a})=>jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:Array(a).fill(null).map((i,o)=>jsxRuntimeExports.jsx(PlaceholderList,{},o))}),SearchAndActions=({showingTable:a})=>{const[,i]=useAtom(searchKeywordAtom),o=s=>{i(s.target.value)};return jsxRuntimeExports.jsx(Box$1,{display:"flex",alignItems:"flex-start",justifyContent:"space-between",flexDirection:"row",gap:2,style:{marginTop:"2em",marginBottom:"1em",marginRight:a?"2em":"6em",width:"80%"},children:jsxRuntimeExports.jsx(Box$1,{sx:{width:"100%"},children:jsxRuntimeExports.jsx(TextField$1,{color:"secondary",label:"Search",variant:"outlined",fullWidth:!0,onChange:o})})})};function Relevancy({paper:a,margin:i}){return jsxRuntimeExports.jsx(Tooltip$1,{title:`${roundScore(a.relevancy)}%`,children:jsxRuntimeExports.jsx("div",{style:{verticalAlign:"middle",width:"20px",height:"20px",borderRadius:"50%",border:"1px solid black",backgroundColor:getColorShadeRedToGreen(a.relevancy),display:"inline-block",margin:i||"0 .65em 0 0"}})})}const paperStates={[PaperState.initial]:{label:"initial",color:"primary"},[PaperState.approved]:{label:"approved",color:"success"},[PaperState.generated]:{label:"generated",color:"warning"},[PaperState.published]:{label:"published",color:"secondary"}},Row=({paper:a,index:i})=>(paperStates[a.status],a.status,PaperState.published,a.status,PaperState.approved,jsxRuntimeExports.jsxs(TableRow$1,{children:[jsxRuntimeExports.jsx(TableCell$1,{align:"left",sx:{fontSize:"1.075rem",padding:0,"&:hover":{backgroundColor:colors.palette.background.paper}},children:jsxRuntimeExports.jsxs(Link$2,{to:`/paper/${a.id}`,style:{display:"block",padding:"1em",textDecoration:"none",color:"inherit"},children:[jsxRuntimeExports.jsx(Relevancy,{paper:a}),a.title]})}),jsxRuntimeExports.jsx(TableCell$1,{align:"center",children:jsxRuntimeExports.jsx(Favorite,{paper:a})})]},i)),TablePlaceholder=({placeholderRows:a})=>jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:Array.from(new Array(a)).map((i,o)=>jsxRuntimeExports.jsxs(TableRow$1,{children:[jsxRuntimeExports.jsx(TableCell$1,{children:jsxRuntimeExports.jsx(Skeleton$1,{animation:"wave",height:50,width:"80em"})}),jsxRuntimeExports.jsx(TableCell$1,{children:jsxRuntimeExports.jsx(Skeleton$1,{animation:"wave",height:50,width:"2em"})})]},o))}),PapersTable=({papers:a=[],isLoading:i=!1,placeholderRows:o=5})=>jsxRuntimeExports.jsx(TableContainer$1,{sx:{marginTop:3,margin:"0 auto",minWidth:"100%"},children:jsxRuntimeExports.jsxs(Table$1,{children:[jsxRuntimeExports.jsx(TableHead$1,{children:jsxRuntimeExports.jsxs(TableRow$1,{children:[jsxRuntimeExports.jsx(TableCell$1,{align:"left",children:"Paper Title"}),jsxRuntimeExports.jsx(TableCell$1,{align:"left"})]})}),jsxRuntimeExports.jsx(TableBody$1,{children:i?jsxRuntimeExports.jsx(TablePlaceholder,{placeholderRows:o}):a.map((s,$)=>jsxRuntimeExports.jsx(Row,{paper:s,index:$},$))})]})});var Toc={},_interopRequireDefault$k=interopRequireDefaultExports;Object.defineProperty(Toc,"__esModule",{value:!0});var default_1$k=Toc.default=void 0,_createSvgIcon$k=_interopRequireDefault$k(requireCreateSvgIcon()),_jsxRuntime$k=jsxRuntimeExports;default_1$k=Toc.default=(0,_createSvgIcon$k.default)((0,_jsxRuntime$k.jsx)("path",{d:"M3 9h14V7H3zm0 4h14v-2H3zm0 4h14v-2H3zm16 0h2v-2h-2zm0-10v2h2V7zm0 6h2v-2h-2z"}),"Toc");var Apps={},_interopRequireDefault$j=interopRequireDefaultExports;Object.defineProperty(Apps,"__esModule",{value:!0});var default_1$j=Apps.default=void 0,_createSvgIcon$j=_interopRequireDefault$j(requireCreateSvgIcon()),_jsxRuntime$j=jsxRuntimeExports;default_1$j=Apps.default=(0,_createSvgIcon$j.default)((0,_jsxRuntime$j.jsx)("path",{d:"M4 8h4V4H4zm6 12h4v-4h-4zm-6 0h4v-4H4zm0-6h4v-4H4zm6 0h4v-4h-4zm6-10v4h4V4zm-6 4h4V4h-4zm6 6h4v-4h-4zm0 6h4v-4h-4z"}),"Apps");const MainTabs=({papersAtom:a,isLoading:i=!1,slideUp:o=!1})=>{const[s,$]=useAtom(tabValueAtom$1),j=useSetAtom(updatePaperInListAtom),_e=useAtomValue(a||emptyListAtom),et=(nt,at)=>{$(at)};reactExports.useEffect(()=>{const nt=at=>{const{id:it,changes:st}=at.detail,{field:lt,value:ct}=st;j({papersListAtom:dateEntryPapersAtom,id:it,field:lt,newValue:ct})};return window.addEventListener("paperUpdate",nt),()=>{window.removeEventListener("paperUpdate",nt)}},[]);const tt={};return jsxRuntimeExports.jsxs(Box$1,{sx:tt,children:[jsxRuntimeExports.jsxs("div",{className:"flex justify-between align-middle items-center",children:[jsxRuntimeExports.jsx(SearchAndActions,{showingTable:s==="table"}),jsxRuntimeExports.jsxs(ToggleButtonGroup$1,{color:"secondary",value:s,exclusive:!0,onChange:et,"aria-label":"Data view",children:[jsxRuntimeExports.jsx(ToggleButton$1,{value:"table",children:jsxRuntimeExports.jsx(default_1$k,{})}),jsxRuntimeExports.jsx(ToggleButton$1,{value:"grid",children:jsxRuntimeExports.jsx(default_1$j,{})})]})]}),s==="table"&&jsxRuntimeExports.jsx(PapersTable,{papers:_e,isLoading:i}),s==="grid"&&jsxRuntimeExports.jsx(ThumbPapersGrid,{papers:_e,isLoading:i})]})};function DateEntryPage(){let{dateId:a}=useParams();a=a||"";const[,i]=useAtom(fetchPapersByDateAtom),[o]=useAtom(dateEntryPapersAtom),s=useSetAtom(dateEntryStateAtom),[$]=useAtom(dateEntryStateAtom);return reactExports.useEffect(()=>(i(a),()=>{s("loading")}),[a]),jsxRuntimeExports.jsxs(PageLayout,{padding:3,style:{marginTop:3,margin:"0 auto"},children:[jsxRuntimeExports.jsx(PageTitle,{value:a,count:o.length}),jsxRuntimeExports.jsx(RenderByState$2,{dateId:a,state:$})]})}function RenderByState$2({dateId:a,state:i}){const[o,s]=useAtom(scrapingStateAtom),$=useSetAtom(dateEntryStateAtom),j=useSetAtom(dateEntryPapersAtom),_e=useSetAtom(addAlertAtom),et=useSetAtom(updateSidebarDataAtom),tt=({key:nt,status:at,data:it})=>{if(at==="complete"?(j(it),it.length===0?$("unexpected"):$("complete"),s("pending")):s(at),at==="error"){const st=dayjs(nt).format("MM/DD/YYYY");_e({message:`There was a problem scraping papers for ${st}`,id:st})}et({key:nt,status:at,count:it==null?void 0:it.length})};switch(i){case"loading":return jsxRuntimeExports.jsx(MainTabs,{isLoading:!0,slideUp:!0});case"error":return jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{});case"unexpected":return jsxRuntimeExports.jsx("div",{children:jsxRuntimeExports.jsx(ResetState,{date:a,resetStatusAtom:resetDateEntryStatusAtom})});case"pending":return jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(ScrapeStatus,{status:o,date:a,scrapeAtom:scrapePapersDateEntryAtom}),jsxRuntimeExports.jsx(SocketListener,{eventName:"date_status",handleEvent:tt})]});case"complete":default:return jsxRuntimeExports.jsx(MainTabs,{papersAtom:filteredPapersAtom,slideUp:!0})}}const scrollableContainerRefAtom=atom(null),paperAtom=atom(void 0),pdfModalOpen=atom(!1),pageStateAtom=atom("loading"),fetchPaperAtom=atom(null,async(a,i,o)=>{if(!o){console.error("Paper id not provided",o);return}i(pageStateAtom,"loading");try{const s=calendar[1].papers[1];if(console.log("Paper fetched: ",{paper:s}),!s){i(pageStateAtom,"error");return}i(paperAtom,s),i(pageStateAtom,"ready")}catch(s){console.error(`Failed to fetch paper with id: ${o}`,s),i(pageStateAtom,"error")}}),style={position:"absolute",top:"50%",left:"50%",transform:"translate(-50%, -50%)",width:400,bgcolor:"background.paper",border:"2px solid #000",boxShadow:24,p:4,maxHeight:"98vh",minHeight:"20vh",overflowY:"auto"};function ModalWrapper({children:a,open:i,handleClose:o,width:s=500}){return jsxRuntimeExports.jsx(Modal$1,{open:i,onClose:o,"aria-labelledby":"modal-title","aria-describedby":"modal-description",children:jsxRuntimeExports.jsx(Box$1,{sx:{...style,width:s},children:a})})}function commonjsRequire(a){throw new Error('Could not dynamically require "'+a+'". Please configure the dynamicRequireTargets or/and ignoreDynamicRequires option of @rollup/plugin-commonjs appropriately for this require call to work.')}var pdf$1={exports:{}};const __viteBrowserExternal={},__viteBrowserExternal$1=Object.freeze(Object.defineProperty({__proto__:null,default:__viteBrowserExternal},Symbol.toStringTag,{value:"Module"})),require$$5=getAugmentedNamespace(__viteBrowserExternal$1);(function(module,exports){(function(i,o){module.exports=i.pdfjsLib=o()})(globalThis,()=>(()=>{var __webpack_modules__=[,(a,i)=>{var Zn;Object.defineProperty(i,"__esModule",{value:!0}),i.VerbosityLevel=i.Util=i.UnknownErrorException=i.UnexpectedResponseException=i.TextRenderingMode=i.RenderingIntentFlag=i.PromiseCapability=i.PermissionFlag=i.PasswordResponses=i.PasswordException=i.PageActionEventType=i.OPS=i.MissingPDFException=i.MAX_IMAGE_SIZE_TO_CACHE=i.LINE_FACTOR=i.LINE_DESCENT_FACTOR=i.InvalidPDFException=i.ImageKind=i.IDENTITY_MATRIX=i.FormatError=i.FeatureTest=i.FONT_IDENTITY_MATRIX=i.DocumentActionEventType=i.CMapCompressionType=i.BaseException=i.BASELINE_FACTOR=i.AnnotationType=i.AnnotationReplyType=i.AnnotationPrefix=i.AnnotationMode=i.AnnotationFlag=i.AnnotationFieldFlag=i.AnnotationEditorType=i.AnnotationEditorPrefix=i.AnnotationEditorParamsType=i.AnnotationBorderStyleType=i.AnnotationActionEventType=i.AbortException=void 0,i.assert=jt,i.bytesToString=zt,i.createValidAbsoluteUrl=Xt,i.getModificationDate=Mn,i.getUuid=Wn,i.getVerbosityLevel=Et,i.info=Dt,i.isArrayBuffer=vn,i.isArrayEqual=$n,i.isNodeJS=void 0,i.normalizeUnicode=Hn,i.objectFromMap=tn,i.objectSize=qt,i.setVerbosityLevel=Tt,i.shadow=sn,i.string32=Wt,i.stringToBytes=Gt,i.stringToPDFString=hn,i.stringToUTF8String=en,i.unreachable=Ct,i.utf8StringToString=Jt,i.warn=It;const o=typeof process=="object"&&process+""=="[object process]"&&!process.versions.nw&&!(process.versions.electron&&process.type&&process.type!=="browser");i.isNodeJS=o;const s=[1,0,0,1,0,0];i.IDENTITY_MATRIX=s;const $=[.001,0,0,.001,0,0];i.FONT_IDENTITY_MATRIX=$;const j=1e7;i.MAX_IMAGE_SIZE_TO_CACHE=j;const _e=1.35;i.LINE_FACTOR=_e;const et=.35;i.LINE_DESCENT_FACTOR=et;const tt=et/_e;i.BASELINE_FACTOR=tt;const nt={ANY:1,DISPLAY:2,PRINT:4,SAVE:8,ANNOTATIONS_FORMS:16,ANNOTATIONS_STORAGE:32,ANNOTATIONS_DISABLE:64,OPLIST:256};i.RenderingIntentFlag=nt;const at={DISABLE:0,ENABLE:1,ENABLE_FORMS:2,ENABLE_STORAGE:3};i.AnnotationMode=at;const it="pdfjs_internal_editor_";i.AnnotationEditorPrefix=it;const st={DISABLE:-1,NONE:0,FREETEXT:3,STAMP:13,INK:15};i.AnnotationEditorType=st;const lt={RESIZE:1,CREATE:2,FREETEXT_SIZE:11,FREETEXT_COLOR:12,FREETEXT_OPACITY:13,INK_COLOR:21,INK_THICKNESS:22,INK_OPACITY:23};i.AnnotationEditorParamsType=lt;const ct={PRINT:4,MODIFY_CONTENTS:8,COPY:16,MODIFY_ANNOTATIONS:32,FILL_INTERACTIVE_FORMS:256,COPY_FOR_ACCESSIBILITY:512,ASSEMBLE:1024,PRINT_HIGH_QUALITY:2048};i.PermissionFlag=ct;const rt={FILL:0,STROKE:1,FILL_STROKE:2,INVISIBLE:3,FILL_ADD_TO_PATH:4,STROKE_ADD_TO_PATH:5,FILL_STROKE_ADD_TO_PATH:6,ADD_TO_PATH:7,FILL_STROKE_MASK:3,ADD_TO_PATH_FLAG:4};i.TextRenderingMode=rt;const ut={GRAYSCALE_1BPP:1,RGB_24BPP:2,RGBA_32BPP:3};i.ImageKind=ut;const ot={TEXT:1,LINK:2,FREETEXT:3,LINE:4,SQUARE:5,CIRCLE:6,POLYGON:7,POLYLINE:8,HIGHLIGHT:9,UNDERLINE:10,SQUIGGLY:11,STRIKEOUT:12,STAMP:13,CARET:14,INK:15,POPUP:16,FILEATTACHMENT:17,SOUND:18,MOVIE:19,WIDGET:20,SCREEN:21,PRINTERMARK:22,TRAPNET:23,WATERMARK:24,THREED:25,REDACT:26};i.AnnotationType=ot;const dt={GROUP:"Group",REPLY:"R"};i.AnnotationReplyType=dt;const pt={INVISIBLE:1,HIDDEN:2,PRINT:4,NOZOOM:8,NOROTATE:16,NOVIEW:32,READONLY:64,LOCKED:128,TOGGLENOVIEW:256,LOCKEDCONTENTS:512};i.AnnotationFlag=pt;const mt={READONLY:1,REQUIRED:2,NOEXPORT:4,MULTILINE:4096,PASSWORD:8192,NOTOGGLETOOFF:16384,RADIO:32768,PUSHBUTTON:65536,COMBO:131072,EDIT:262144,SORT:524288,FILESELECT:1048576,MULTISELECT:2097152,DONOTSPELLCHECK:4194304,DONOTSCROLL:8388608,COMB:16777216,RICHTEXT:33554432,RADIOSINUNISON:33554432,COMMITONSELCHANGE:67108864};i.AnnotationFieldFlag=mt;const ft={SOLID:1,DASHED:2,BEVELED:3,INSET:4,UNDERLINE:5};i.AnnotationBorderStyleType=ft;const ht={E:"Mouse Enter",X:"Mouse Exit",D:"Mouse Down",U:"Mouse Up",Fo:"Focus",Bl:"Blur",PO:"PageOpen",PC:"PageClose",PV:"PageVisible",PI:"PageInvisible",K:"Keystroke",F:"Format",V:"Validate",C:"Calculate"};i.AnnotationActionEventType=ht;const yt={WC:"WillClose",WS:"WillSave",DS:"DidSave",WP:"WillPrint",DP:"DidPrint"};i.DocumentActionEventType=yt;const bt={O:"PageOpen",C:"PageClose"};i.PageActionEventType=bt;const gt={ERRORS:0,WARNINGS:1,INFOS:5};i.VerbosityLevel=gt;const xt={NONE:0,BINARY:1};i.CMapCompressionType=xt;const vt={dependency:1,setLineWidth:2,setLineCap:3,setLineJoin:4,setMiterLimit:5,setDash:6,setRenderingIntent:7,setFlatness:8,setGState:9,save:10,restore:11,transform:12,moveTo:13,lineTo:14,curveTo:15,curveTo2:16,curveTo3:17,closePath:18,rectangle:19,stroke:20,closeStroke:21,fill:22,eoFill:23,fillStroke:24,eoFillStroke:25,closeFillStroke:26,closeEOFillStroke:27,endPath:28,clip:29,eoClip:30,beginText:31,endText:32,setCharSpacing:33,setWordSpacing:34,setHScale:35,setLeading:36,setFont:37,setTextRenderingMode:38,setTextRise:39,moveText:40,setLeadingMoveText:41,setTextMatrix:42,nextLine:43,showText:44,showSpacedText:45,nextLineShowText:46,nextLineSetSpacingShowText:47,setCharWidth:48,setCharWidthAndBounds:49,setStrokeColorSpace:50,setFillColorSpace:51,setStrokeColor:52,setStrokeColorN:53,setFillColor:54,setFillColorN:55,setStrokeGray:56,setFillGray:57,setStrokeRGBColor:58,setFillRGBColor:59,setStrokeCMYKColor:60,setFillCMYKColor:61,shadingFill:62,beginInlineImage:63,beginImageData:64,endInlineImage:65,paintXObject:66,markPoint:67,markPointProps:68,beginMarkedContent:69,beginMarkedContentProps:70,endMarkedContent:71,beginCompat:72,endCompat:73,paintFormXObjectBegin:74,paintFormXObjectEnd:75,beginGroup:76,endGroup:77,beginAnnotation:80,endAnnotation:81,paintImageMaskXObject:83,paintImageMaskXObjectGroup:84,paintImageXObject:85,paintInlineImageXObject:86,paintInlineImageXObjectGroup:87,paintImageXObjectRepeat:88,paintImageMaskXObjectRepeat:89,paintSolidColorImageMask:90,constructPath:91};i.OPS=vt;const Lt={NEED_PASSWORD:1,INCORRECT_PASSWORD:2};i.PasswordResponses=Lt;let $t=gt.WARNINGS;function Tt(bn){Number.isInteger(bn)&&($t=bn)}function Et(){return $t}function Dt(bn){$t>=gt.INFOS&&console.log(`Info: ${bn}`)}function It(bn){$t>=gt.WARNINGS&&console.log(`Warning: ${bn}`)}function Ct(bn){throw new Error(bn)}function jt(bn,dn){bn||Ct(dn)}function Zt(bn){switch(bn==null?void 0:bn.protocol){case"http:":case"https:":case"ftp:":case"mailto:":case"tel:":return!0;default:return!1}}function Xt(bn,dn=null,an=null){if(!bn)return null;try{if(an&&typeof bn=="string"){if(an.addDefaultProtocol&&bn.startsWith("www.")){const Dn=bn.match(/\./g);(Dn==null?void 0:Dn.length)>=2&&(bn=`http://${bn}`)}if(an.tryConvertEncoding)try{bn=en(bn)}catch{}}const In=dn?new URL(bn,dn):new URL(bn);if(Zt(In))return In}catch{}return null}function sn(bn,dn,an,In=!1){return Object.defineProperty(bn,dn,{value:an,enumerable:!In,configurable:!0,writable:!1}),an}const Ft=function(){function dn(an,In){this.constructor===dn&&Ct("Cannot initialize BaseException."),this.message=an,this.name=In}return dn.prototype=new Error,dn.constructor=dn,dn}();i.BaseException=Ft;class wt extends Ft{constructor(dn,an){super(dn,"PasswordException"),this.code=an}}i.PasswordException=wt;class kt extends Ft{constructor(dn,an){super(dn,"UnknownErrorException"),this.details=an}}i.UnknownErrorException=kt;class At extends Ft{constructor(dn){super(dn,"InvalidPDFException")}}i.InvalidPDFException=At;class Pt extends Ft{constructor(dn){super(dn,"MissingPDFException")}}i.MissingPDFException=Pt;class Mt extends Ft{constructor(dn,an){super(dn,"UnexpectedResponseException"),this.status=an}}i.UnexpectedResponseException=Mt;class Ot extends Ft{constructor(dn){super(dn,"FormatError")}}i.FormatError=Ot;class Bt extends Ft{constructor(dn){super(dn,"AbortException")}}i.AbortException=Bt;function zt(bn){(typeof bn!="object"||(bn==null?void 0:bn.length)===void 0)&&Ct("Invalid argument for bytesToString");const dn=bn.length,an=8192;if(dn<an)return String.fromCharCode.apply(null,bn);const In=[];for(let Dn=0;Dn<dn;Dn+=an){const Xn=Math.min(Dn+an,dn),Yn=bn.subarray(Dn,Xn);In.push(String.fromCharCode.apply(null,Yn))}return In.join("")}function Gt(bn){typeof bn!="string"&&Ct("Invalid argument for stringToBytes");const dn=bn.length,an=new Uint8Array(dn);for(let In=0;In<dn;++In)an[In]=bn.charCodeAt(In)&255;return an}function Wt(bn){return String.fromCharCode(bn>>24&255,bn>>16&255,bn>>8&255,bn&255)}function qt(bn){return Object.keys(bn).length}function tn(bn){const dn=Object.create(null);for(const[an,In]of bn)dn[an]=In;return dn}function ln(){const bn=new Uint8Array(4);return bn[0]=1,new Uint32Array(bn.buffer,0,1)[0]===1}function gn(){try{return new Function(""),!0}catch{return!1}}class yn{static get isLittleEndian(){return sn(this,"isLittleEndian",ln())}static get isEvalSupported(){return sn(this,"isEvalSupported",gn())}static get isOffscreenCanvasSupported(){return sn(this,"isOffscreenCanvasSupported",typeof OffscreenCanvas<"u")}static get platform(){return typeof navigator>"u"?sn(this,"platform",{isWin:!1,isMac:!1}):sn(this,"platform",{isWin:navigator.platform.includes("Win"),isMac:navigator.platform.includes("Mac")})}static get isCSSRoundSupported(){var dn,an;return sn(this,"isCSSRoundSupported",(an=(dn=globalThis.CSS)==null?void 0:dn.supports)==null?void 0:an.call(dn,"width: round(1.5px, 1px)"))}}i.FeatureTest=yn;const Pn=[...Array(256).keys()].map(bn=>bn.toString(16).padStart(2,"0"));class cn{static makeHexColor(dn,an,In){return`#${Pn[dn]}${Pn[an]}${Pn[In]}`}static scaleMinMax(dn,an){let In;dn[0]?(dn[0]<0&&(In=an[0],an[0]=an[1],an[1]=In),an[0]*=dn[0],an[1]*=dn[0],dn[3]<0&&(In=an[2],an[2]=an[3],an[3]=In),an[2]*=dn[3],an[3]*=dn[3]):(In=an[0],an[0]=an[2],an[2]=In,In=an[1],an[1]=an[3],an[3]=In,dn[1]<0&&(In=an[2],an[2]=an[3],an[3]=In),an[2]*=dn[1],an[3]*=dn[1],dn[2]<0&&(In=an[0],an[0]=an[1],an[1]=In),an[0]*=dn[2],an[1]*=dn[2]),an[0]+=dn[4],an[1]+=dn[4],an[2]+=dn[5],an[3]+=dn[5]}static transform(dn,an){return[dn[0]*an[0]+dn[2]*an[1],dn[1]*an[0]+dn[3]*an[1],dn[0]*an[2]+dn[2]*an[3],dn[1]*an[2]+dn[3]*an[3],dn[0]*an[4]+dn[2]*an[5]+dn[4],dn[1]*an[4]+dn[3]*an[5]+dn[5]]}static applyTransform(dn,an){const In=dn[0]*an[0]+dn[1]*an[2]+an[4],Dn=dn[0]*an[1]+dn[1]*an[3]+an[5];return[In,Dn]}static applyInverseTransform(dn,an){const In=an[0]*an[3]-an[1]*an[2],Dn=(dn[0]*an[3]-dn[1]*an[2]+an[2]*an[5]-an[4]*an[3])/In,Xn=(-dn[0]*an[1]+dn[1]*an[0]+an[4]*an[1]-an[5]*an[0])/In;return[Dn,Xn]}static getAxialAlignedBoundingBox(dn,an){const In=this.applyTransform(dn,an),Dn=this.applyTransform(dn.slice(2,4),an),Xn=this.applyTransform([dn[0],dn[3]],an),Yn=this.applyTransform([dn[2],dn[1]],an);return[Math.min(In[0],Dn[0],Xn[0],Yn[0]),Math.min(In[1],Dn[1],Xn[1],Yn[1]),Math.max(In[0],Dn[0],Xn[0],Yn[0]),Math.max(In[1],Dn[1],Xn[1],Yn[1])]}static inverseTransform(dn){const an=dn[0]*dn[3]-dn[1]*dn[2];return[dn[3]/an,-dn[1]/an,-dn[2]/an,dn[0]/an,(dn[2]*dn[5]-dn[4]*dn[3])/an,(dn[4]*dn[1]-dn[5]*dn[0])/an]}static singularValueDecompose2dScale(dn){const an=[dn[0],dn[2],dn[1],dn[3]],In=dn[0]*an[0]+dn[1]*an[2],Dn=dn[0]*an[1]+dn[1]*an[3],Xn=dn[2]*an[0]+dn[3]*an[2],Yn=dn[2]*an[1]+dn[3]*an[3],pn=(In+Yn)/2,Ht=Math.sqrt((In+Yn)**2-4*(In*Yn-Xn*Dn))/2,Kt=pn+Ht||1,nn=pn-Ht||1;return[Math.sqrt(Kt),Math.sqrt(nn)]}static normalizeRect(dn){const an=dn.slice(0);return dn[0]>dn[2]&&(an[0]=dn[2],an[2]=dn[0]),dn[1]>dn[3]&&(an[1]=dn[3],an[3]=dn[1]),an}static intersect(dn,an){const In=Math.max(Math.min(dn[0],dn[2]),Math.min(an[0],an[2])),Dn=Math.min(Math.max(dn[0],dn[2]),Math.max(an[0],an[2]));if(In>Dn)return null;const Xn=Math.max(Math.min(dn[1],dn[3]),Math.min(an[1],an[3])),Yn=Math.min(Math.max(dn[1],dn[3]),Math.max(an[1],an[3]));return Xn>Yn?null:[In,Xn,Dn,Yn]}static bezierBoundingBox(dn,an,In,Dn,Xn,Yn,pn,Ht){const Kt=[],nn=[[],[]];let kn,Rn,Un,Tn,Nn,Cn,Ut,Rt;for(let rn=0;rn<2;++rn){if(rn===0?(Rn=6*dn-12*In+6*Xn,kn=-3*dn+9*In-9*Xn+3*pn,Un=3*In-3*dn):(Rn=6*an-12*Dn+6*Yn,kn=-3*an+9*Dn-9*Yn+3*Ht,Un=3*Dn-3*an),Math.abs(kn)<1e-12){if(Math.abs(Rn)<1e-12)continue;Tn=-Un/Rn,0<Tn&&Tn<1&&Kt.push(Tn);continue}Ut=Rn*Rn-4*Un*kn,Rt=Math.sqrt(Ut),!(Ut<0)&&(Nn=(-Rn+Rt)/(2*kn),0<Nn&&Nn<1&&Kt.push(Nn),Cn=(-Rn-Rt)/(2*kn),0<Cn&&Cn<1&&Kt.push(Cn))}let Nt=Kt.length,Vt;const Qt=Nt;for(;Nt--;)Tn=Kt[Nt],Vt=1-Tn,nn[0][Nt]=Vt*Vt*Vt*dn+3*Vt*Vt*Tn*In+3*Vt*Tn*Tn*Xn+Tn*Tn*Tn*pn,nn[1][Nt]=Vt*Vt*Vt*an+3*Vt*Vt*Tn*Dn+3*Vt*Tn*Tn*Yn+Tn*Tn*Tn*Ht;return nn[0][Qt]=dn,nn[1][Qt]=an,nn[0][Qt+1]=pn,nn[1][Qt+1]=Ht,nn[0].length=nn[1].length=Qt+2,[Math.min(...nn[0]),Math.min(...nn[1]),Math.max(...nn[0]),Math.max(...nn[1])]}}i.Util=cn;const xn=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,728,711,710,729,733,731,730,732,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8226,8224,8225,8230,8212,8211,402,8260,8249,8250,8722,8240,8222,8220,8221,8216,8217,8218,8482,64257,64258,321,338,352,376,381,305,322,339,353,382,0,8364];function hn(bn){if(bn[0]>="ï"){let an;if(bn[0]==="þ"&&bn[1]==="ÿ"?an="utf-16be":bn[0]==="ÿ"&&bn[1]==="þ"?an="utf-16le":bn[0]==="ï"&&bn[1]==="»"&&bn[2]==="¿"&&(an="utf-8"),an)try{const In=new TextDecoder(an,{fatal:!0}),Dn=Gt(bn);return In.decode(Dn)}catch(In){It(`stringToPDFString: "${In}".`)}}const dn=[];for(let an=0,In=bn.length;an<In;an++){const Dn=xn[bn.charCodeAt(an)];dn.push(Dn?String.fromCharCode(Dn):bn.charAt(an))}return dn.join("")}function en(bn){return decodeURIComponent(escape(bn))}function Jt(bn){return unescape(encodeURIComponent(bn))}function vn(bn){return typeof bn=="object"&&(bn==null?void 0:bn.byteLength)!==void 0}function $n(bn,dn){if(bn.length!==dn.length)return!1;for(let an=0,In=bn.length;an<In;an++)if(bn[an]!==dn[an])return!1;return!0}function Mn(bn=new Date){return[bn.getUTCFullYear().toString(),(bn.getUTCMonth()+1).toString().padStart(2,"0"),bn.getUTCDate().toString().padStart(2,"0"),bn.getUTCHours().toString().padStart(2,"0"),bn.getUTCMinutes().toString().padStart(2,"0"),bn.getUTCSeconds().toString().padStart(2,"0")].join("")}class On{constructor(){Yt(this,Zn,!1);this.promise=new Promise((dn,an)=>{this.resolve=In=>{wn(this,Zn,!0),dn(In)},this.reject=In=>{wn(this,Zn,!0),an(In)}})}get settled(){return St(this,Zn)}}Zn=new WeakMap,i.PromiseCapability=On;let En=null,Bn=null;function Hn(bn){return En||(En=/([\u00a0\u00b5\u037e\u0eb3\u2000-\u200a\u202f\u2126\ufb00-\ufb04\ufb06\ufb20-\ufb36\ufb38-\ufb3c\ufb3e\ufb40-\ufb41\ufb43-\ufb44\ufb46-\ufba1\ufba4-\ufba9\ufbae-\ufbb1\ufbd3-\ufbdc\ufbde-\ufbe7\ufbea-\ufbf8\ufbfc-\ufbfd\ufc00-\ufc5d\ufc64-\ufcf1\ufcf5-\ufd3d\ufd88\ufdf4\ufdfa-\ufdfb\ufe71\ufe77\ufe79\ufe7b\ufe7d]+)|(\ufb05+)/gu,Bn=new Map([["ﬅ","ſt"]])),bn.replaceAll(En,(dn,an,In)=>an?an.normalize("NFKC"):Bn.get(In))}function Wn(){if(typeof crypto<"u"&&typeof(crypto==null?void 0:crypto.randomUUID)=="function")return crypto.randomUUID();const bn=new Uint8Array(32);if(typeof crypto<"u"&&typeof(crypto==null?void 0:crypto.getRandomValues)=="function")crypto.getRandomValues(bn);else for(let dn=0;dn<32;dn++)bn[dn]=Math.floor(Math.random()*255);return zt(bn)}const _n="pdfjs_internal_id_";i.AnnotationPrefix=_n},(__unused_webpack_module,exports,__w_pdfjs_require__)=>{var a,o,s,$,$a,_e,po,tt,nt,at,it,st,lt,ct,rt,ut,ho,dt,pt,Eo,ft,ht;Object.defineProperty(exports,"__esModule",{value:!0}),exports.RenderTask=exports.PDFWorkerUtil=exports.PDFWorker=exports.PDFPageProxy=exports.PDFDocumentProxy=exports.PDFDocumentLoadingTask=exports.PDFDataRangeTransport=exports.LoopbackPort=exports.DefaultStandardFontDataFactory=exports.DefaultFilterFactory=exports.DefaultCanvasFactory=exports.DefaultCMapReaderFactory=void 0,Object.defineProperty(exports,"SVGGraphics",{enumerable:!0,get:function(){return _displaySvg.SVGGraphics}}),exports.build=void 0,exports.getDocument=getDocument,exports.version=void 0;var _util=__w_pdfjs_require__(1),_annotation_storage=__w_pdfjs_require__(3),_display_utils=__w_pdfjs_require__(6),_font_loader=__w_pdfjs_require__(9),_displayNode_utils=__w_pdfjs_require__(10),_canvas=__w_pdfjs_require__(11),_worker_options=__w_pdfjs_require__(14),_message_handler=__w_pdfjs_require__(15),_metadata=__w_pdfjs_require__(16),_optional_content_config=__w_pdfjs_require__(17),_transport_stream=__w_pdfjs_require__(18),_displayFetch_stream=__w_pdfjs_require__(19),_displayNetwork=__w_pdfjs_require__(22),_displayNode_stream=__w_pdfjs_require__(23),_displaySvg=__w_pdfjs_require__(24),_xfa_text=__w_pdfjs_require__(25);const DEFAULT_RANGE_CHUNK_SIZE=65536,RENDERING_CANCELLED_TIMEOUT=100,DELAYED_CLEANUP_TIMEOUT=5e3,DefaultCanvasFactory=_util.isNodeJS?_displayNode_utils.NodeCanvasFactory:_display_utils.DOMCanvasFactory;exports.DefaultCanvasFactory=DefaultCanvasFactory;const DefaultCMapReaderFactory=_util.isNodeJS?_displayNode_utils.NodeCMapReaderFactory:_display_utils.DOMCMapReaderFactory;exports.DefaultCMapReaderFactory=DefaultCMapReaderFactory;const DefaultFilterFactory=_util.isNodeJS?_displayNode_utils.NodeFilterFactory:_display_utils.DOMFilterFactory;exports.DefaultFilterFactory=DefaultFilterFactory;const DefaultStandardFontDataFactory=_util.isNodeJS?_displayNode_utils.NodeStandardFontDataFactory:_display_utils.DOMStandardFontDataFactory;exports.DefaultStandardFontDataFactory=DefaultStandardFontDataFactory;function getDocument(bt){if(typeof bt=="string"||bt instanceof URL?bt={url:bt}:(0,_util.isArrayBuffer)(bt)&&(bt={data:bt}),typeof bt!="object")throw new Error("Invalid parameter in getDocument, need parameter object.");if(!bt.url&&!bt.data&&!bt.range)throw new Error("Invalid parameter object: need either .data, .range or .url");const gt=new PDFDocumentLoadingTask,{docId:xt}=gt,vt=bt.url?getUrlProp(bt.url):null,Lt=bt.data?getDataProp(bt.data):null,$t=bt.httpHeaders||null,Tt=bt.withCredentials===!0,Et=bt.password??null,Dt=bt.range instanceof PDFDataRangeTransport?bt.range:null,It=Number.isInteger(bt.rangeChunkSize)&&bt.rangeChunkSize>0?bt.rangeChunkSize:DEFAULT_RANGE_CHUNK_SIZE;let Ct=bt.worker instanceof PDFWorker?bt.worker:null;const jt=bt.verbosity,Zt=typeof bt.docBaseUrl=="string"&&!(0,_display_utils.isDataScheme)(bt.docBaseUrl)?bt.docBaseUrl:null,Xt=typeof bt.cMapUrl=="string"?bt.cMapUrl:null,sn=bt.cMapPacked!==!1,Ft=bt.CMapReaderFactory||DefaultCMapReaderFactory,wt=typeof bt.standardFontDataUrl=="string"?bt.standardFontDataUrl:null,kt=bt.StandardFontDataFactory||DefaultStandardFontDataFactory,At=bt.stopAtErrors!==!0,Pt=Number.isInteger(bt.maxImageSize)&&bt.maxImageSize>-1?bt.maxImageSize:-1,Mt=bt.isEvalSupported!==!1,Ot=typeof bt.isOffscreenCanvasSupported=="boolean"?bt.isOffscreenCanvasSupported:!_util.isNodeJS,Bt=Number.isInteger(bt.canvasMaxAreaInBytes)?bt.canvasMaxAreaInBytes:-1,zt=typeof bt.disableFontFace=="boolean"?bt.disableFontFace:_util.isNodeJS,Gt=bt.fontExtraProperties===!0,Wt=bt.enableXfa===!0,qt=bt.ownerDocument||globalThis.document,tn=bt.disableRange===!0,ln=bt.disableStream===!0,gn=bt.disableAutoFetch===!0,yn=bt.pdfBug===!0,Pn=Dt?Dt.length:bt.length??NaN,cn=typeof bt.useSystemFonts=="boolean"?bt.useSystemFonts:!_util.isNodeJS&&!zt,xn=typeof bt.useWorkerFetch=="boolean"?bt.useWorkerFetch:Ft===_display_utils.DOMCMapReaderFactory&&kt===_display_utils.DOMStandardFontDataFactory&&Xt&&wt&&(0,_display_utils.isValidFetchUrl)(Xt,document.baseURI)&&(0,_display_utils.isValidFetchUrl)(wt,document.baseURI),hn=bt.canvasFactory||new DefaultCanvasFactory({ownerDocument:qt}),en=bt.filterFactory||new DefaultFilterFactory({docId:xt,ownerDocument:qt}),Jt=null;(0,_util.setVerbosityLevel)(jt);const vn={canvasFactory:hn,filterFactory:en};if(xn||(vn.cMapReaderFactory=new Ft({baseUrl:Xt,isCompressed:sn}),vn.standardFontDataFactory=new kt({baseUrl:wt})),!Ct){const On={verbosity:jt,port:_worker_options.GlobalWorkerOptions.workerPort};Ct=On.port?PDFWorker.fromPort(On):new PDFWorker(On),gt._worker=Ct}const $n={docId:xt,apiVersion:"3.11.174",data:Lt,password:Et,disableAutoFetch:gn,rangeChunkSize:It,length:Pn,docBaseUrl:Zt,enableXfa:Wt,evaluatorOptions:{maxImageSize:Pt,disableFontFace:zt,ignoreErrors:At,isEvalSupported:Mt,isOffscreenCanvasSupported:Ot,canvasMaxAreaInBytes:Bt,fontExtraProperties:Gt,useSystemFonts:cn,cMapUrl:xn?Xt:null,standardFontDataUrl:xn?wt:null}},Mn={ignoreErrors:At,isEvalSupported:Mt,disableFontFace:zt,fontExtraProperties:Gt,enableXfa:Wt,ownerDocument:qt,disableAutoFetch:gn,pdfBug:yn,styleElement:Jt};return Ct.promise.then(function(){if(gt.destroyed)throw new Error("Loading aborted");const On=_fetchDocument(Ct,$n),En=new Promise(function(Bn){let Hn;Dt?Hn=new _transport_stream.PDFDataTransportStream({length:Pn,initialData:Dt.initialData,progressiveDone:Dt.progressiveDone,contentDispositionFilename:Dt.contentDispositionFilename,disableRange:tn,disableStream:ln},Dt):Lt||(Hn=(_n=>_util.isNodeJS?new _displayNode_stream.PDFNodeStream(_n):(0,_display_utils.isValidFetchUrl)(_n.url)?new _displayFetch_stream.PDFFetchStream(_n):new _displayNetwork.PDFNetworkStream(_n))({url:vt,length:Pn,httpHeaders:$t,withCredentials:Tt,rangeChunkSize:It,disableRange:tn,disableStream:ln})),Bn(Hn)});return Promise.all([On,En]).then(function([Bn,Hn]){if(gt.destroyed)throw new Error("Loading aborted");const Wn=new _message_handler.MessageHandler(xt,Bn,Ct.port),_n=new WorkerTransport(Wn,gt,Hn,Mn,vn);gt._transport=_n,Wn.send("Ready",null)})}).catch(gt._capability.reject),gt}async function _fetchDocument(bt,gt){if(bt.destroyed)throw new Error("Worker was destroyed");const xt=await bt.messageHandler.sendWithPromise("GetDocRequest",gt,gt.data?[gt.data.buffer]:null);if(bt.destroyed)throw new Error("Worker was destroyed");return xt}function getUrlProp(bt){if(bt instanceof URL)return bt.href;try{return new URL(bt,window.location).href}catch{if(_util.isNodeJS&&typeof bt=="string")return bt}throw new Error("Invalid PDF url data: either string or URL-object is expected in the url property.")}function getDataProp(bt){if(_util.isNodeJS&&typeof Buffer<"u"&&bt instanceof Buffer)throw new Error("Please provide binary data as `Uint8Array`, rather than `Buffer`.");if(bt instanceof Uint8Array&&bt.byteLength===bt.buffer.byteLength)return bt;if(typeof bt=="string")return(0,_util.stringToBytes)(bt);if(typeof bt=="object"&&!isNaN(bt==null?void 0:bt.length)||(0,_util.isArrayBuffer)(bt))return new Uint8Array(bt);throw new Error("Invalid PDF binary data: either TypedArray, string, or array-like object is expected in the data property.")}const i=class i{constructor(){this._capability=new _util.PromiseCapability,this._transport=null,this._worker=null,this.docId=`d${ao(i,a)._++}`,this.destroyed=!1,this.onPassword=null,this.onProgress=null}get promise(){return this._capability.promise}async destroy(){var gt,xt,vt;this.destroyed=!0;try{(gt=this._worker)!=null&&gt.port&&(this._worker._pendingDestroy=!0),await((xt=this._transport)==null?void 0:xt.destroy())}catch(Lt){throw(vt=this._worker)!=null&&vt.port&&delete this._worker._pendingDestroy,Lt}this._transport=null,this._worker&&(this._worker.destroy(),this._worker=null)}};a=new WeakMap,Yt(i,a,0);let PDFDocumentLoadingTask=i;exports.PDFDocumentLoadingTask=PDFDocumentLoadingTask;class PDFDataRangeTransport{constructor(gt,xt,vt=!1,Lt=null){this.length=gt,this.initialData=xt,this.progressiveDone=vt,this.contentDispositionFilename=Lt,this._rangeListeners=[],this._progressListeners=[],this._progressiveReadListeners=[],this._progressiveDoneListeners=[],this._readyCapability=new _util.PromiseCapability}addRangeListener(gt){this._rangeListeners.push(gt)}addProgressListener(gt){this._progressListeners.push(gt)}addProgressiveReadListener(gt){this._progressiveReadListeners.push(gt)}addProgressiveDoneListener(gt){this._progressiveDoneListeners.push(gt)}onDataRange(gt,xt){for(const vt of this._rangeListeners)vt(gt,xt)}onDataProgress(gt,xt){this._readyCapability.promise.then(()=>{for(const vt of this._progressListeners)vt(gt,xt)})}onDataProgressiveRead(gt){this._readyCapability.promise.then(()=>{for(const xt of this._progressiveReadListeners)xt(gt)})}onDataProgressiveDone(){this._readyCapability.promise.then(()=>{for(const gt of this._progressiveDoneListeners)gt()})}transportReady(){this._readyCapability.resolve()}requestDataRange(gt,xt){(0,_util.unreachable)("Abstract method PDFDataRangeTransport.requestDataRange")}abort(){}}exports.PDFDataRangeTransport=PDFDataRangeTransport;class PDFDocumentProxy{constructor(gt,xt){this._pdfInfo=gt,this._transport=xt,Object.defineProperty(this,"getJavaScript",{value:()=>((0,_display_utils.deprecated)("`PDFDocumentProxy.getJavaScript`, please use `PDFDocumentProxy.getJSActions` instead."),this.getJSActions().then(vt=>{if(!vt)return vt;const Lt=[];for(const $t in vt)Lt.push(...vt[$t]);return Lt}))})}get annotationStorage(){return this._transport.annotationStorage}get filterFactory(){return this._transport.filterFactory}get numPages(){return this._pdfInfo.numPages}get fingerprints(){return this._pdfInfo.fingerprints}get isPureXfa(){return(0,_util.shadow)(this,"isPureXfa",!!this._transport._htmlForXfa)}get allXfaHtml(){return this._transport._htmlForXfa}getPage(gt){return this._transport.getPage(gt)}getPageIndex(gt){return this._transport.getPageIndex(gt)}getDestinations(){return this._transport.getDestinations()}getDestination(gt){return this._transport.getDestination(gt)}getPageLabels(){return this._transport.getPageLabels()}getPageLayout(){return this._transport.getPageLayout()}getPageMode(){return this._transport.getPageMode()}getViewerPreferences(){return this._transport.getViewerPreferences()}getOpenAction(){return this._transport.getOpenAction()}getAttachments(){return this._transport.getAttachments()}getJSActions(){return this._transport.getDocJSActions()}getOutline(){return this._transport.getOutline()}getOptionalContentConfig(){return this._transport.getOptionalContentConfig()}getPermissions(){return this._transport.getPermissions()}getMetadata(){return this._transport.getMetadata()}getMarkInfo(){return this._transport.getMarkInfo()}getData(){return this._transport.getData()}saveDocument(){return this._transport.saveDocument()}getDownloadInfo(){return this._transport.downloadInfoCapability.promise}cleanup(gt=!1){return this._transport.startCleanup(gt||this.isPureXfa)}destroy(){return this.loadingTask.destroy()}get loadingParams(){return this._transport.loadingParams}get loadingTask(){return this._transport.loadingTask}getFieldObjects(){return this._transport.getFieldObjects()}hasJSActions(){return this._transport.hasJSActions()}getCalculationOrderIds(){return this._transport.getCalculationOrderIds()}}exports.PDFDocumentProxy=PDFDocumentProxy;class PDFPageProxy{constructor(gt,xt,vt,Lt=!1){Yt(this,$);Yt(this,_e);Yt(this,o,null);Yt(this,s,!1);this._pageIndex=gt,this._pageInfo=xt,this._transport=vt,this._stats=Lt?new _display_utils.StatTimer:null,this._pdfBug=Lt,this.commonObjs=vt.commonObjs,this.objs=new PDFObjects,this._maybeCleanupAfterRender=!1,this._intentStates=new Map,this.destroyed=!1}get pageNumber(){return this._pageIndex+1}get rotate(){return this._pageInfo.rotate}get ref(){return this._pageInfo.ref}get userUnit(){return this._pageInfo.userUnit}get view(){return this._pageInfo.view}getViewport({scale:gt,rotation:xt=this.rotate,offsetX:vt=0,offsetY:Lt=0,dontFlip:$t=!1}={}){return new _display_utils.PageViewport({viewBox:this.view,scale:gt,rotation:xt,offsetX:vt,offsetY:Lt,dontFlip:$t})}getAnnotations({intent:gt="display"}={}){const xt=this._transport.getRenderingIntent(gt);return this._transport.getAnnotations(this._pageIndex,xt.renderingIntent)}getJSActions(){return this._transport.getPageJSActions(this._pageIndex)}get filterFactory(){return this._transport.filterFactory}get isPureXfa(){return(0,_util.shadow)(this,"isPureXfa",!!this._transport._htmlForXfa)}async getXfa(){var gt;return((gt=this._transport._htmlForXfa)==null?void 0:gt.children[this._pageIndex])||null}render({canvasContext:gt,viewport:xt,intent:vt="display",annotationMode:Lt=_util.AnnotationMode.ENABLE,transform:$t=null,background:Tt=null,optionalContentConfigPromise:Et=null,annotationCanvasMap:Dt=null,pageColors:It=null,printAnnotationStorage:Ct=null}){var kt,At;(kt=this._stats)==null||kt.time("Overall");const jt=this._transport.getRenderingIntent(vt,Lt,Ct);wn(this,s,!1),un(this,_e,po).call(this),Et||(Et=this._transport.getOptionalContentConfig());let Zt=this._intentStates.get(jt.cacheKey);Zt||(Zt=Object.create(null),this._intentStates.set(jt.cacheKey,Zt)),Zt.streamReaderCancelTimeout&&(clearTimeout(Zt.streamReaderCancelTimeout),Zt.streamReaderCancelTimeout=null);const Xt=!!(jt.renderingIntent&_util.RenderingIntentFlag.PRINT);Zt.displayReadyCapability||(Zt.displayReadyCapability=new _util.PromiseCapability,Zt.operatorList={fnArray:[],argsArray:[],lastChunk:!1,separateAnnots:null},(At=this._stats)==null||At.time("Page Request"),this._pumpOperatorList(jt));const sn=Pt=>{var Mt,Ot;Zt.renderTasks.delete(Ft),(this._maybeCleanupAfterRender||Xt)&&wn(this,s,!0),un(this,$,$a).call(this,!Xt),Pt?(Ft.capability.reject(Pt),this._abortOperatorList({intentState:Zt,reason:Pt instanceof Error?Pt:new Error(Pt)})):Ft.capability.resolve(),(Mt=this._stats)==null||Mt.timeEnd("Rendering"),(Ot=this._stats)==null||Ot.timeEnd("Overall")},Ft=new InternalRenderTask({callback:sn,params:{canvasContext:gt,viewport:xt,transform:$t,background:Tt},objs:this.objs,commonObjs:this.commonObjs,annotationCanvasMap:Dt,operatorList:Zt.operatorList,pageIndex:this._pageIndex,canvasFactory:this._transport.canvasFactory,filterFactory:this._transport.filterFactory,useRequestAnimationFrame:!Xt,pdfBug:this._pdfBug,pageColors:It});(Zt.renderTasks||(Zt.renderTasks=new Set)).add(Ft);const wt=Ft.task;return Promise.all([Zt.displayReadyCapability.promise,Et]).then(([Pt,Mt])=>{var Ot;if(this.destroyed){sn();return}(Ot=this._stats)==null||Ot.time("Rendering"),Ft.initializeGraphics({transparency:Pt,optionalContentConfig:Mt}),Ft.operatorListChanged()}).catch(sn),wt}getOperatorList({intent:gt="display",annotationMode:xt=_util.AnnotationMode.ENABLE,printAnnotationStorage:vt=null}={}){var Dt;function Lt(){Tt.operatorList.lastChunk&&(Tt.opListReadCapability.resolve(Tt.operatorList),Tt.renderTasks.delete(Et))}const $t=this._transport.getRenderingIntent(gt,xt,vt,!0);let Tt=this._intentStates.get($t.cacheKey);Tt||(Tt=Object.create(null),this._intentStates.set($t.cacheKey,Tt));let Et;return Tt.opListReadCapability||(Et=Object.create(null),Et.operatorListChanged=Lt,Tt.opListReadCapability=new _util.PromiseCapability,(Tt.renderTasks||(Tt.renderTasks=new Set)).add(Et),Tt.operatorList={fnArray:[],argsArray:[],lastChunk:!1,separateAnnots:null},(Dt=this._stats)==null||Dt.time("Page Request"),this._pumpOperatorList($t)),Tt.opListReadCapability.promise}streamTextContent({includeMarkedContent:gt=!1,disableNormalization:xt=!1}={}){return this._transport.messageHandler.sendWithStream("GetTextContent",{pageIndex:this._pageIndex,includeMarkedContent:gt===!0,disableNormalization:xt===!0},{highWaterMark:100,size(Lt){return Lt.items.length}})}getTextContent(gt={}){if(this._transport._htmlForXfa)return this.getXfa().then(vt=>_xfa_text.XfaText.textContent(vt));const xt=this.streamTextContent(gt);return new Promise(function(vt,Lt){function $t(){Tt.read().then(function({value:Dt,done:It}){if(It){vt(Et);return}Object.assign(Et.styles,Dt.styles),Et.items.push(...Dt.items),$t()},Lt)}const Tt=xt.getReader(),Et={items:[],styles:Object.create(null)};$t()})}getStructTree(){return this._transport.getStructTree(this._pageIndex)}_destroy(){this.destroyed=!0;const gt=[];for(const xt of this._intentStates.values())if(this._abortOperatorList({intentState:xt,reason:new Error("Page was destroyed."),force:!0}),!xt.opListReadCapability)for(const vt of xt.renderTasks)gt.push(vt.completed),vt.cancel();return this.objs.clear(),wn(this,s,!1),un(this,_e,po).call(this),Promise.all(gt)}cleanup(gt=!1){wn(this,s,!0);const xt=un(this,$,$a).call(this,!1);return gt&&xt&&this._stats&&(this._stats=new _display_utils.StatTimer),xt}_startRenderPage(gt,xt){var Lt,$t;const vt=this._intentStates.get(xt);vt&&((Lt=this._stats)==null||Lt.timeEnd("Page Request"),($t=vt.displayReadyCapability)==null||$t.resolve(gt))}_renderPageChunk(gt,xt){for(let vt=0,Lt=gt.length;vt<Lt;vt++)xt.operatorList.fnArray.push(gt.fnArray[vt]),xt.operatorList.argsArray.push(gt.argsArray[vt]);xt.operatorList.lastChunk=gt.lastChunk,xt.operatorList.separateAnnots=gt.separateAnnots;for(const vt of xt.renderTasks)vt.operatorListChanged();gt.lastChunk&&un(this,$,$a).call(this,!0)}_pumpOperatorList({renderingIntent:gt,cacheKey:xt,annotationStorageSerializable:vt}){const{map:Lt,transfers:$t}=vt,Et=this._transport.messageHandler.sendWithStream("GetOperatorList",{pageIndex:this._pageIndex,intent:gt,cacheKey:xt,annotationStorage:Lt},$t).getReader(),Dt=this._intentStates.get(xt);Dt.streamReader=Et;const It=()=>{Et.read().then(({value:Ct,done:jt})=>{if(jt){Dt.streamReader=null;return}this._transport.destroyed||(this._renderPageChunk(Ct,Dt),It())},Ct=>{if(Dt.streamReader=null,!this._transport.destroyed){if(Dt.operatorList){Dt.operatorList.lastChunk=!0;for(const jt of Dt.renderTasks)jt.operatorListChanged();un(this,$,$a).call(this,!0)}if(Dt.displayReadyCapability)Dt.displayReadyCapability.reject(Ct);else if(Dt.opListReadCapability)Dt.opListReadCapability.reject(Ct);else throw Ct}})};It()}_abortOperatorList({intentState:gt,reason:xt,force:vt=!1}){if(gt.streamReader){if(gt.streamReaderCancelTimeout&&(clearTimeout(gt.streamReaderCancelTimeout),gt.streamReaderCancelTimeout=null),!vt){if(gt.renderTasks.size>0)return;if(xt instanceof _display_utils.RenderingCancelledException){let Lt=RENDERING_CANCELLED_TIMEOUT;xt.extraDelay>0&&xt.extraDelay<1e3&&(Lt+=xt.extraDelay),gt.streamReaderCancelTimeout=setTimeout(()=>{gt.streamReaderCancelTimeout=null,this._abortOperatorList({intentState:gt,reason:xt,force:!0})},Lt);return}}if(gt.streamReader.cancel(new _util.AbortException(xt.message)).catch(()=>{}),gt.streamReader=null,!this._transport.destroyed){for(const[Lt,$t]of this._intentStates)if($t===gt){this._intentStates.delete(Lt);break}this.cleanup()}}}get stats(){return this._stats}}o=new WeakMap,s=new WeakMap,$=new WeakSet,$a=function(gt=!1){if(un(this,_e,po).call(this),!St(this,s)||this.destroyed)return!1;if(gt)return wn(this,o,setTimeout(()=>{wn(this,o,null),un(this,$,$a).call(this,!1)},DELAYED_CLEANUP_TIMEOUT)),!1;for(const{renderTasks:xt,operatorList:vt}of this._intentStates.values())if(xt.size>0||!vt.lastChunk)return!1;return this._intentStates.clear(),this.objs.clear(),wn(this,s,!1),!0},_e=new WeakSet,po=function(){St(this,o)&&(clearTimeout(St(this,o)),wn(this,o,null))},exports.PDFPageProxy=PDFPageProxy;class LoopbackPort{constructor(){Yt(this,tt,new Set);Yt(this,nt,Promise.resolve())}postMessage(gt,xt){const vt={data:structuredClone(gt,xt?{transfer:xt}:null)};St(this,nt).then(()=>{for(const Lt of St(this,tt))Lt.call(this,vt)})}addEventListener(gt,xt){St(this,tt).add(xt)}removeEventListener(gt,xt){St(this,tt).delete(xt)}terminate(){St(this,tt).clear()}}tt=new WeakMap,nt=new WeakMap,exports.LoopbackPort=LoopbackPort;const PDFWorkerUtil={isWorkerDisabled:!1,fallbackWorkerSrc:null,fakeWorkerId:0};exports.PDFWorkerUtil=PDFWorkerUtil;{if(_util.isNodeJS&&typeof commonjsRequire=="function")PDFWorkerUtil.isWorkerDisabled=!0,PDFWorkerUtil.fallbackWorkerSrc="./pdf.worker.js";else if(typeof document=="object"){const bt=(at=document==null?void 0:document.currentScript)==null?void 0:at.src;bt&&(PDFWorkerUtil.fallbackWorkerSrc=bt.replace(/(\.(?:min\.)?js)(\?.*)?$/i,".worker$1$2"))}PDFWorkerUtil.isSameOrigin=function(bt,gt){let xt;try{if(xt=new URL(bt),!xt.origin||xt.origin==="null")return!1}catch{return!1}const vt=new URL(gt,xt);return xt.origin===vt.origin},PDFWorkerUtil.createCDNWrapper=function(bt){const gt=`importScripts("${bt}");`;return URL.createObjectURL(new Blob([gt]))}}const _PDFWorker=class _PDFWorker{constructor({name:bt=null,port:gt=null,verbosity:xt=(0,_util.getVerbosityLevel)()}={}){var vt;if(this.name=bt,this.destroyed=!1,this.verbosity=xt,this._readyCapability=new _util.PromiseCapability,this._port=null,this._webWorker=null,this._messageHandler=null,gt){if((vt=St(_PDFWorker,it))!=null&&vt.has(gt))throw new Error("Cannot use more than one PDFWorker per port.");(St(_PDFWorker,it)||wn(_PDFWorker,it,new WeakMap)).set(gt,this),this._initializeFromPort(gt);return}this._initialize()}get promise(){return this._readyCapability.promise}get port(){return this._port}get messageHandler(){return this._messageHandler}_initializeFromPort(bt){this._port=bt,this._messageHandler=new _message_handler.MessageHandler("main","worker",bt),this._messageHandler.on("ready",function(){}),this._readyCapability.resolve(),this._messageHandler.send("configure",{verbosity:this.verbosity})}_initialize(){if(!PDFWorkerUtil.isWorkerDisabled&&!_PDFWorker._mainThreadWorkerMessageHandler){let{workerSrc:bt}=_PDFWorker;try{PDFWorkerUtil.isSameOrigin(window.location.href,bt)||(bt=PDFWorkerUtil.createCDNWrapper(new URL(bt,window.location).href));const gt=new Worker(bt),xt=new _message_handler.MessageHandler("main","worker",gt),vt=()=>{gt.removeEventListener("error",Lt),xt.destroy(),gt.terminate(),this.destroyed?this._readyCapability.reject(new Error("Worker was destroyed")):this._setupFakeWorker()},Lt=()=>{this._webWorker||vt()};gt.addEventListener("error",Lt),xt.on("test",Tt=>{if(gt.removeEventListener("error",Lt),this.destroyed){vt();return}Tt?(this._messageHandler=xt,this._port=gt,this._webWorker=gt,this._readyCapability.resolve(),xt.send("configure",{verbosity:this.verbosity})):(this._setupFakeWorker(),xt.destroy(),gt.terminate())}),xt.on("ready",Tt=>{if(gt.removeEventListener("error",Lt),this.destroyed){vt();return}try{$t()}catch{this._setupFakeWorker()}});const $t=()=>{const Tt=new Uint8Array;xt.send("test",Tt,[Tt.buffer])};$t();return}catch{(0,_util.info)("The worker has been disabled.")}}this._setupFakeWorker()}_setupFakeWorker(){PDFWorkerUtil.isWorkerDisabled||((0,_util.warn)("Setting up fake worker."),PDFWorkerUtil.isWorkerDisabled=!0),_PDFWorker._setupFakeWorkerGlobal.then(bt=>{if(this.destroyed){this._readyCapability.reject(new Error("Worker was destroyed"));return}const gt=new LoopbackPort;this._port=gt;const xt=`fake${PDFWorkerUtil.fakeWorkerId++}`,vt=new _message_handler.MessageHandler(xt+"_worker",xt,gt);bt.setup(vt,gt);const Lt=new _message_handler.MessageHandler(xt,xt+"_worker",gt);this._messageHandler=Lt,this._readyCapability.resolve(),Lt.send("configure",{verbosity:this.verbosity})}).catch(bt=>{this._readyCapability.reject(new Error(`Setting up fake worker failed: "${bt.message}".`))})}destroy(){var bt;this.destroyed=!0,this._webWorker&&(this._webWorker.terminate(),this._webWorker=null),(bt=St(_PDFWorker,it))==null||bt.delete(this._port),this._port=null,this._messageHandler&&(this._messageHandler.destroy(),this._messageHandler=null)}static fromPort(bt){var xt;if(!(bt!=null&&bt.port))throw new Error("PDFWorker.fromPort - invalid method signature.");const gt=(xt=St(this,it))==null?void 0:xt.get(bt.port);if(gt){if(gt._pendingDestroy)throw new Error("PDFWorker.fromPort - the worker is being destroyed.\nPlease remember to await `PDFDocumentLoadingTask.destroy()`-calls.");return gt}return new _PDFWorker(bt)}static get workerSrc(){if(_worker_options.GlobalWorkerOptions.workerSrc)return _worker_options.GlobalWorkerOptions.workerSrc;if(PDFWorkerUtil.fallbackWorkerSrc!==null)return _util.isNodeJS||(0,_display_utils.deprecated)('No "GlobalWorkerOptions.workerSrc" specified.'),PDFWorkerUtil.fallbackWorkerSrc;throw new Error('No "GlobalWorkerOptions.workerSrc" specified.')}static get _mainThreadWorkerMessageHandler(){var bt;try{return((bt=globalThis.pdfjsWorker)==null?void 0:bt.WorkerMessageHandler)||null}catch{return null}}static get _setupFakeWorkerGlobal(){const loader=async()=>{const mainWorkerMessageHandler=this._mainThreadWorkerMessageHandler;if(mainWorkerMessageHandler)return mainWorkerMessageHandler;if(_util.isNodeJS&&typeof commonjsRequire=="function"){const worker=eval("require")(this.workerSrc);return worker.WorkerMessageHandler}return await(0,_display_utils.loadScript)(this.workerSrc),window.pdfjsWorker.WorkerMessageHandler};return(0,_util.shadow)(this,"_setupFakeWorkerGlobal",loader())}};it=new WeakMap,Yt(_PDFWorker,it,void 0);let PDFWorker=_PDFWorker;exports.PDFWorker=PDFWorker;class WorkerTransport{constructor(gt,xt,vt,Lt,$t){Yt(this,ut);Yt(this,st,new Map);Yt(this,lt,new Map);Yt(this,ct,new Map);Yt(this,rt,null);this.messageHandler=gt,this.loadingTask=xt,this.commonObjs=new PDFObjects,this.fontLoader=new _font_loader.FontLoader({ownerDocument:Lt.ownerDocument,styleElement:Lt.styleElement}),this._params=Lt,this.canvasFactory=$t.canvasFactory,this.filterFactory=$t.filterFactory,this.cMapReaderFactory=$t.cMapReaderFactory,this.standardFontDataFactory=$t.standardFontDataFactory,this.destroyed=!1,this.destroyCapability=null,this._networkStream=vt,this._fullReader=null,this._lastProgress=null,this.downloadInfoCapability=new _util.PromiseCapability,this.setupMessageHandler()}get annotationStorage(){return(0,_util.shadow)(this,"annotationStorage",new _annotation_storage.AnnotationStorage)}getRenderingIntent(gt,xt=_util.AnnotationMode.ENABLE,vt=null,Lt=!1){let $t=_util.RenderingIntentFlag.DISPLAY,Tt=_annotation_storage.SerializableEmpty;switch(gt){case"any":$t=_util.RenderingIntentFlag.ANY;break;case"display":break;case"print":$t=_util.RenderingIntentFlag.PRINT;break;default:(0,_util.warn)(`getRenderingIntent - invalid intent: ${gt}`)}switch(xt){case _util.AnnotationMode.DISABLE:$t+=_util.RenderingIntentFlag.ANNOTATIONS_DISABLE;break;case _util.AnnotationMode.ENABLE:break;case _util.AnnotationMode.ENABLE_FORMS:$t+=_util.RenderingIntentFlag.ANNOTATIONS_FORMS;break;case _util.AnnotationMode.ENABLE_STORAGE:$t+=_util.RenderingIntentFlag.ANNOTATIONS_STORAGE,Tt=($t&_util.RenderingIntentFlag.PRINT&&vt instanceof _annotation_storage.PrintAnnotationStorage?vt:this.annotationStorage).serializable;break;default:(0,_util.warn)(`getRenderingIntent - invalid annotationMode: ${xt}`)}return Lt&&($t+=_util.RenderingIntentFlag.OPLIST),{renderingIntent:$t,cacheKey:`${$t}_${Tt.hash}`,annotationStorageSerializable:Tt}}destroy(){var vt;if(this.destroyCapability)return this.destroyCapability.promise;this.destroyed=!0,this.destroyCapability=new _util.PromiseCapability,(vt=St(this,rt))==null||vt.reject(new Error("Worker was destroyed during onPassword callback"));const gt=[];for(const Lt of St(this,lt).values())gt.push(Lt._destroy());St(this,lt).clear(),St(this,ct).clear(),this.hasOwnProperty("annotationStorage")&&this.annotationStorage.resetModified();const xt=this.messageHandler.sendWithPromise("Terminate",null);return gt.push(xt),Promise.all(gt).then(()=>{var Lt;this.commonObjs.clear(),this.fontLoader.clear(),St(this,st).clear(),this.filterFactory.destroy(),(Lt=this._networkStream)==null||Lt.cancelAllRequests(new _util.AbortException("Worker was terminated.")),this.messageHandler&&(this.messageHandler.destroy(),this.messageHandler=null),this.destroyCapability.resolve()},this.destroyCapability.reject),this.destroyCapability.promise}setupMessageHandler(){const{messageHandler:gt,loadingTask:xt}=this;gt.on("GetReader",(vt,Lt)=>{(0,_util.assert)(this._networkStream,"GetReader - no `IPDFStream` instance available."),this._fullReader=this._networkStream.getFullReader(),this._fullReader.onProgress=$t=>{this._lastProgress={loaded:$t.loaded,total:$t.total}},Lt.onPull=()=>{this._fullReader.read().then(function({value:$t,done:Tt}){if(Tt){Lt.close();return}(0,_util.assert)($t instanceof ArrayBuffer,"GetReader - expected an ArrayBuffer."),Lt.enqueue(new Uint8Array($t),1,[$t])}).catch($t=>{Lt.error($t)})},Lt.onCancel=$t=>{this._fullReader.cancel($t),Lt.ready.catch(Tt=>{if(!this.destroyed)throw Tt})}}),gt.on("ReaderHeadersReady",vt=>{const Lt=new _util.PromiseCapability,$t=this._fullReader;return $t.headersReady.then(()=>{var Tt;(!$t.isStreamingSupported||!$t.isRangeSupported)&&(this._lastProgress&&((Tt=xt.onProgress)==null||Tt.call(xt,this._lastProgress)),$t.onProgress=Et=>{var Dt;(Dt=xt.onProgress)==null||Dt.call(xt,{loaded:Et.loaded,total:Et.total})}),Lt.resolve({isStreamingSupported:$t.isStreamingSupported,isRangeSupported:$t.isRangeSupported,contentLength:$t.contentLength})},Lt.reject),Lt.promise}),gt.on("GetRangeReader",(vt,Lt)=>{(0,_util.assert)(this._networkStream,"GetRangeReader - no `IPDFStream` instance available.");const $t=this._networkStream.getRangeReader(vt.begin,vt.end);if(!$t){Lt.close();return}Lt.onPull=()=>{$t.read().then(function({value:Tt,done:Et}){if(Et){Lt.close();return}(0,_util.assert)(Tt instanceof ArrayBuffer,"GetRangeReader - expected an ArrayBuffer."),Lt.enqueue(new Uint8Array(Tt),1,[Tt])}).catch(Tt=>{Lt.error(Tt)})},Lt.onCancel=Tt=>{$t.cancel(Tt),Lt.ready.catch(Et=>{if(!this.destroyed)throw Et})}}),gt.on("GetDoc",({pdfInfo:vt})=>{this._numPages=vt.numPages,this._htmlForXfa=vt.htmlForXfa,delete vt.htmlForXfa,xt._capability.resolve(new PDFDocumentProxy(vt,this))}),gt.on("DocException",function(vt){let Lt;switch(vt.name){case"PasswordException":Lt=new _util.PasswordException(vt.message,vt.code);break;case"InvalidPDFException":Lt=new _util.InvalidPDFException(vt.message);break;case"MissingPDFException":Lt=new _util.MissingPDFException(vt.message);break;case"UnexpectedResponseException":Lt=new _util.UnexpectedResponseException(vt.message,vt.status);break;case"UnknownErrorException":Lt=new _util.UnknownErrorException(vt.message,vt.details);break;default:(0,_util.unreachable)("DocException - expected a valid Error.")}xt._capability.reject(Lt)}),gt.on("PasswordRequest",vt=>{if(wn(this,rt,new _util.PromiseCapability),xt.onPassword){const Lt=$t=>{$t instanceof Error?St(this,rt).reject($t):St(this,rt).resolve({password:$t})};try{xt.onPassword(Lt,vt.code)}catch($t){St(this,rt).reject($t)}}else St(this,rt).reject(new _util.PasswordException(vt.message,vt.code));return St(this,rt).promise}),gt.on("DataLoaded",vt=>{var Lt;(Lt=xt.onProgress)==null||Lt.call(xt,{loaded:vt.length,total:vt.length}),this.downloadInfoCapability.resolve(vt)}),gt.on("StartRenderPage",vt=>{if(this.destroyed)return;St(this,lt).get(vt.pageIndex)._startRenderPage(vt.transparency,vt.cacheKey)}),gt.on("commonobj",([vt,Lt,$t])=>{var Tt;if(!this.destroyed&&!this.commonObjs.has(vt))switch(Lt){case"Font":const Et=this._params;if("error"in $t){const Ct=$t.error;(0,_util.warn)(`Error during font loading: ${Ct}`),this.commonObjs.resolve(vt,Ct);break}const Dt=Et.pdfBug&&((Tt=globalThis.FontInspector)!=null&&Tt.enabled)?(Ct,jt)=>globalThis.FontInspector.fontAdded(Ct,jt):null,It=new _font_loader.FontFaceObject($t,{isEvalSupported:Et.isEvalSupported,disableFontFace:Et.disableFontFace,ignoreErrors:Et.ignoreErrors,inspectFont:Dt});this.fontLoader.bind(It).catch(Ct=>gt.sendWithPromise("FontFallback",{id:vt})).finally(()=>{!Et.fontExtraProperties&&It.data&&(It.data=null),this.commonObjs.resolve(vt,It)});break;case"FontPath":case"Image":case"Pattern":this.commonObjs.resolve(vt,$t);break;default:throw new Error(`Got unknown common object type ${Lt}`)}}),gt.on("obj",([vt,Lt,$t,Tt])=>{var Dt;if(this.destroyed)return;const Et=St(this,lt).get(Lt);if(!Et.objs.has(vt))switch($t){case"Image":if(Et.objs.resolve(vt,Tt),Tt){let It;if(Tt.bitmap){const{width:Ct,height:jt}=Tt;It=Ct*jt*4}else It=((Dt=Tt.data)==null?void 0:Dt.length)||0;It>_util.MAX_IMAGE_SIZE_TO_CACHE&&(Et._maybeCleanupAfterRender=!0)}break;case"Pattern":Et.objs.resolve(vt,Tt);break;default:throw new Error(`Got unknown object type ${$t}`)}}),gt.on("DocProgress",vt=>{var Lt;this.destroyed||(Lt=xt.onProgress)==null||Lt.call(xt,{loaded:vt.loaded,total:vt.total})}),gt.on("FetchBuiltInCMap",vt=>this.destroyed?Promise.reject(new Error("Worker was destroyed.")):this.cMapReaderFactory?this.cMapReaderFactory.fetch(vt):Promise.reject(new Error("CMapReaderFactory not initialized, see the `useWorkerFetch` parameter."))),gt.on("FetchStandardFontData",vt=>this.destroyed?Promise.reject(new Error("Worker was destroyed.")):this.standardFontDataFactory?this.standardFontDataFactory.fetch(vt):Promise.reject(new Error("StandardFontDataFactory not initialized, see the `useWorkerFetch` parameter.")))}getData(){return this.messageHandler.sendWithPromise("GetData",null)}saveDocument(){var vt;this.annotationStorage.size<=0&&(0,_util.warn)("saveDocument called while `annotationStorage` is empty, please use the getData-method instead.");const{map:gt,transfers:xt}=this.annotationStorage.serializable;return this.messageHandler.sendWithPromise("SaveDocument",{isPureXfa:!!this._htmlForXfa,numPages:this._numPages,annotationStorage:gt,filename:((vt=this._fullReader)==null?void 0:vt.filename)??null},xt).finally(()=>{this.annotationStorage.resetModified()})}getPage(gt){if(!Number.isInteger(gt)||gt<=0||gt>this._numPages)return Promise.reject(new Error("Invalid page request."));const xt=gt-1,vt=St(this,ct).get(xt);if(vt)return vt;const Lt=this.messageHandler.sendWithPromise("GetPage",{pageIndex:xt}).then($t=>{if(this.destroyed)throw new Error("Transport destroyed");const Tt=new PDFPageProxy(xt,$t,this,this._params.pdfBug);return St(this,lt).set(xt,Tt),Tt});return St(this,ct).set(xt,Lt),Lt}getPageIndex(gt){return typeof gt!="object"||gt===null||!Number.isInteger(gt.num)||gt.num<0||!Number.isInteger(gt.gen)||gt.gen<0?Promise.reject(new Error("Invalid pageIndex request.")):this.messageHandler.sendWithPromise("GetPageIndex",{num:gt.num,gen:gt.gen})}getAnnotations(gt,xt){return this.messageHandler.sendWithPromise("GetAnnotations",{pageIndex:gt,intent:xt})}getFieldObjects(){return un(this,ut,ho).call(this,"GetFieldObjects")}hasJSActions(){return un(this,ut,ho).call(this,"HasJSActions")}getCalculationOrderIds(){return this.messageHandler.sendWithPromise("GetCalculationOrderIds",null)}getDestinations(){return this.messageHandler.sendWithPromise("GetDestinations",null)}getDestination(gt){return typeof gt!="string"?Promise.reject(new Error("Invalid destination request.")):this.messageHandler.sendWithPromise("GetDestination",{id:gt})}getPageLabels(){return this.messageHandler.sendWithPromise("GetPageLabels",null)}getPageLayout(){return this.messageHandler.sendWithPromise("GetPageLayout",null)}getPageMode(){return this.messageHandler.sendWithPromise("GetPageMode",null)}getViewerPreferences(){return this.messageHandler.sendWithPromise("GetViewerPreferences",null)}getOpenAction(){return this.messageHandler.sendWithPromise("GetOpenAction",null)}getAttachments(){return this.messageHandler.sendWithPromise("GetAttachments",null)}getDocJSActions(){return un(this,ut,ho).call(this,"GetDocJSActions")}getPageJSActions(gt){return this.messageHandler.sendWithPromise("GetPageJSActions",{pageIndex:gt})}getStructTree(gt){return this.messageHandler.sendWithPromise("GetStructTree",{pageIndex:gt})}getOutline(){return this.messageHandler.sendWithPromise("GetOutline",null)}getOptionalContentConfig(){return this.messageHandler.sendWithPromise("GetOptionalContentConfig",null).then(gt=>new _optional_content_config.OptionalContentConfig(gt))}getPermissions(){return this.messageHandler.sendWithPromise("GetPermissions",null)}getMetadata(){const gt="GetMetadata",xt=St(this,st).get(gt);if(xt)return xt;const vt=this.messageHandler.sendWithPromise(gt,null).then(Lt=>{var $t,Tt;return{info:Lt[0],metadata:Lt[1]?new _metadata.Metadata(Lt[1]):null,contentDispositionFilename:(($t=this._fullReader)==null?void 0:$t.filename)??null,contentLength:((Tt=this._fullReader)==null?void 0:Tt.contentLength)??null}});return St(this,st).set(gt,vt),vt}getMarkInfo(){return this.messageHandler.sendWithPromise("GetMarkInfo",null)}async startCleanup(gt=!1){if(!this.destroyed){await this.messageHandler.sendWithPromise("Cleanup",null);for(const xt of St(this,lt).values())if(!xt.cleanup())throw new Error(`startCleanup: Page ${xt.pageNumber} is currently rendering.`);this.commonObjs.clear(),gt||this.fontLoader.clear(),St(this,st).clear(),this.filterFactory.destroy(!0)}}get loadingParams(){const{disableAutoFetch:gt,enableXfa:xt}=this._params;return(0,_util.shadow)(this,"loadingParams",{disableAutoFetch:gt,enableXfa:xt})}}st=new WeakMap,lt=new WeakMap,ct=new WeakMap,rt=new WeakMap,ut=new WeakSet,ho=function(gt,xt=null){const vt=St(this,st).get(gt);if(vt)return vt;const Lt=this.messageHandler.sendWithPromise(gt,xt);return St(this,st).set(gt,Lt),Lt};class PDFObjects{constructor(){Yt(this,pt);Yt(this,dt,Object.create(null))}get(gt,xt=null){if(xt){const Lt=un(this,pt,Eo).call(this,gt);return Lt.capability.promise.then(()=>xt(Lt.data)),null}const vt=St(this,dt)[gt];if(!(vt!=null&&vt.capability.settled))throw new Error(`Requesting object that isn't resolved yet ${gt}.`);return vt.data}has(gt){const xt=St(this,dt)[gt];return(xt==null?void 0:xt.capability.settled)||!1}resolve(gt,xt=null){const vt=un(this,pt,Eo).call(this,gt);vt.data=xt,vt.capability.resolve()}clear(){var gt;for(const xt in St(this,dt)){const{data:vt}=St(this,dt)[xt];(gt=vt==null?void 0:vt.bitmap)==null||gt.close()}wn(this,dt,Object.create(null))}}dt=new WeakMap,pt=new WeakSet,Eo=function(gt){var xt;return(xt=St(this,dt))[gt]||(xt[gt]={capability:new _util.PromiseCapability,data:null})};class RenderTask{constructor(gt){Yt(this,ft,null);wn(this,ft,gt),this.onContinue=null}get promise(){return St(this,ft).capability.promise}cancel(gt=0){St(this,ft).cancel(null,gt)}get separateAnnots(){const{separateAnnots:gt}=St(this,ft).operatorList;if(!gt)return!1;const{annotationCanvasMap:xt}=St(this,ft);return gt.form||gt.canvas&&(xt==null?void 0:xt.size)>0}}ft=new WeakMap,exports.RenderTask=RenderTask;const yt=class yt{constructor({callback:gt,params:xt,objs:vt,commonObjs:Lt,annotationCanvasMap:$t,operatorList:Tt,pageIndex:Et,canvasFactory:Dt,filterFactory:It,useRequestAnimationFrame:Ct=!1,pdfBug:jt=!1,pageColors:Zt=null}){this.callback=gt,this.params=xt,this.objs=vt,this.commonObjs=Lt,this.annotationCanvasMap=$t,this.operatorListIdx=null,this.operatorList=Tt,this._pageIndex=Et,this.canvasFactory=Dt,this.filterFactory=It,this._pdfBug=jt,this.pageColors=Zt,this.running=!1,this.graphicsReadyCallback=null,this.graphicsReady=!1,this._useRequestAnimationFrame=Ct===!0&&typeof window<"u",this.cancelled=!1,this.capability=new _util.PromiseCapability,this.task=new RenderTask(this),this._cancelBound=this.cancel.bind(this),this._continueBound=this._continue.bind(this),this._scheduleNextBound=this._scheduleNext.bind(this),this._nextBound=this._next.bind(this),this._canvas=xt.canvasContext.canvas}get completed(){return this.capability.promise.catch(function(){})}initializeGraphics({transparency:gt=!1,optionalContentConfig:xt}){var Et,Dt;if(this.cancelled)return;if(this._canvas){if(St(yt,ht).has(this._canvas))throw new Error("Cannot use the same canvas during multiple render() operations. Use different canvas or ensure previous operations were cancelled or completed.");St(yt,ht).add(this._canvas)}this._pdfBug&&((Et=globalThis.StepperManager)!=null&&Et.enabled)&&(this.stepper=globalThis.StepperManager.create(this._pageIndex),this.stepper.init(this.operatorList),this.stepper.nextBreakPoint=this.stepper.getNextBreakPoint());const{canvasContext:vt,viewport:Lt,transform:$t,background:Tt}=this.params;this.gfx=new _canvas.CanvasGraphics(vt,this.commonObjs,this.objs,this.canvasFactory,this.filterFactory,{optionalContentConfig:xt},this.annotationCanvasMap,this.pageColors),this.gfx.beginDrawing({transform:$t,viewport:Lt,transparency:gt,background:Tt}),this.operatorListIdx=0,this.graphicsReady=!0,(Dt=this.graphicsReadyCallback)==null||Dt.call(this)}cancel(gt=null,xt=0){var vt;this.running=!1,this.cancelled=!0,(vt=this.gfx)==null||vt.endDrawing(),St(yt,ht).delete(this._canvas),this.callback(gt||new _display_utils.RenderingCancelledException(`Rendering cancelled, page ${this._pageIndex+1}`,xt))}operatorListChanged(){var gt;if(!this.graphicsReady){this.graphicsReadyCallback||(this.graphicsReadyCallback=this._continueBound);return}(gt=this.stepper)==null||gt.updateOperatorList(this.operatorList),!this.running&&this._continue()}_continue(){this.running=!0,!this.cancelled&&(this.task.onContinue?this.task.onContinue(this._scheduleNextBound):this._scheduleNext())}_scheduleNext(){this._useRequestAnimationFrame?window.requestAnimationFrame(()=>{this._nextBound().catch(this._cancelBound)}):Promise.resolve().then(this._nextBound).catch(this._cancelBound)}async _next(){this.cancelled||(this.operatorListIdx=this.gfx.executeOperatorList(this.operatorList,this.operatorListIdx,this._continueBound,this.stepper),this.operatorListIdx===this.operatorList.argsArray.length&&(this.running=!1,this.operatorList.lastChunk&&(this.gfx.endDrawing(),St(yt,ht).delete(this._canvas),this.callback())))}};ht=new WeakMap,Yt(yt,ht,new WeakSet);let InternalRenderTask=yt;const version="3.11.174";exports.version=version;const build="ce8716743";exports.build=build},(a,i,o)=>{var nt,at,it,lr,lt;Object.defineProperty(i,"__esModule",{value:!0}),i.SerializableEmpty=i.PrintAnnotationStorage=i.AnnotationStorage=void 0;var s=o(1),$=o(4),j=o(8);const _e=Object.freeze({map:null,hash:"",transfers:void 0});i.SerializableEmpty=_e;class et{constructor(){Yt(this,it);Yt(this,nt,!1);Yt(this,at,new Map);this.onSetModified=null,this.onResetModified=null,this.onAnnotationEditor=null}getValue(rt,ut){const ot=St(this,at).get(rt);return ot===void 0?ut:Object.assign(ut,ot)}getRawValue(rt){return St(this,at).get(rt)}remove(rt){if(St(this,at).delete(rt),St(this,at).size===0&&this.resetModified(),typeof this.onAnnotationEditor=="function"){for(const ut of St(this,at).values())if(ut instanceof $.AnnotationEditor)return;this.onAnnotationEditor(null)}}setValue(rt,ut){const ot=St(this,at).get(rt);let dt=!1;if(ot!==void 0)for(const[pt,mt]of Object.entries(ut))ot[pt]!==mt&&(dt=!0,ot[pt]=mt);else dt=!0,St(this,at).set(rt,ut);dt&&un(this,it,lr).call(this),ut instanceof $.AnnotationEditor&&typeof this.onAnnotationEditor=="function"&&this.onAnnotationEditor(ut.constructor._type)}has(rt){return St(this,at).has(rt)}getAll(){return St(this,at).size>0?(0,s.objectFromMap)(St(this,at)):null}setAll(rt){for(const[ut,ot]of Object.entries(rt))this.setValue(ut,ot)}get size(){return St(this,at).size}resetModified(){St(this,nt)&&(wn(this,nt,!1),typeof this.onResetModified=="function"&&this.onResetModified())}get print(){return new tt(this)}get serializable(){if(St(this,at).size===0)return _e;const rt=new Map,ut=new j.MurmurHash3_64,ot=[],dt=Object.create(null);let pt=!1;for(const[mt,ft]of St(this,at)){const ht=ft instanceof $.AnnotationEditor?ft.serialize(!1,dt):ft;ht&&(rt.set(mt,ht),ut.update(`${mt}:${JSON.stringify(ht)}`),pt||(pt=!!ht.bitmap))}if(pt)for(const mt of rt.values())mt.bitmap&&ot.push(mt.bitmap);return rt.size>0?{map:rt,hash:ut.hexdigest(),transfers:ot}:_e}}nt=new WeakMap,at=new WeakMap,it=new WeakSet,lr=function(){St(this,nt)||(wn(this,nt,!0),typeof this.onSetModified=="function"&&this.onSetModified())},i.AnnotationStorage=et;class tt extends et{constructor(ut){super();Yt(this,lt,void 0);const{map:ot,hash:dt,transfers:pt}=ut.serializable,mt=structuredClone(ot,pt?{transfer:pt}:null);wn(this,lt,{map:mt,hash:dt,transfers:pt})}get print(){(0,s.unreachable)("Should not call PrintAnnotationStorage.print")}get serializable(){return St(this,lt)}}lt=new WeakMap,i.PrintAnnotationStorage=tt},(a,i,o)=>{var tt,nt,at,it,st,lt,ct,rt,ut,ot,dt,pt,mt,ft,ht,Ro,bt,Po,xt,Lo,Lt,Mo,Tt,cr,Dt,dr,Ct,ur,Zt,_o,sn,pr;Object.defineProperty(i,"__esModule",{value:!0}),i.AnnotationEditor=void 0;var s=o(5),$=o(1),j=o(6);const wt=class wt{constructor(At){Yt(this,ht);Yt(this,bt);Yt(this,Lt);Yt(this,Tt);Yt(this,Dt);Yt(this,Ct);Yt(this,Zt);Yt(this,sn);Yt(this,tt,"");Yt(this,nt,!1);Yt(this,at,null);Yt(this,it,null);Yt(this,st,null);Yt(this,lt,!1);Yt(this,ct,null);Yt(this,rt,this.focusin.bind(this));Yt(this,ut,this.focusout.bind(this));Yt(this,ot,!1);Yt(this,dt,!1);Yt(this,pt,!1);Jn(this,"_initialOptions",Object.create(null));Jn(this,"_uiManager",null);Jn(this,"_focusEventsAllowed",!0);Jn(this,"_l10nPromise",null);Yt(this,mt,!1);Yt(this,ft,wt._zIndex++);this.constructor===wt&&(0,$.unreachable)("Cannot initialize AnnotationEditor."),this.parent=At.parent,this.id=At.id,this.width=this.height=null,this.pageIndex=At.parent.pageIndex,this.name=At.name,this.div=null,this._uiManager=At.uiManager,this.annotationElementId=null,this._willKeepAspectRatio=!1,this._initialOptions.isCentered=At.isCentered,this._structTreeParentId=null;const{rotation:Pt,rawDims:{pageWidth:Mt,pageHeight:Ot,pageX:Bt,pageY:zt}}=this.parent.viewport;this.rotation=Pt,this.pageRotation=(360+Pt-this._uiManager.viewParameters.rotation)%360,this.pageDimensions=[Mt,Ot],this.pageTranslation=[Bt,zt];const[Gt,Wt]=this.parentDimensions;this.x=At.x/Gt,this.y=At.y/Wt,this.isAttachedToDOM=!1,this.deleted=!1}get editorType(){return Object.getPrototypeOf(this).constructor._type}static get _defaultLineColor(){return(0,$.shadow)(this,"_defaultLineColor",this._colorManager.getHexCode("CanvasText"))}static deleteAnnotationElement(At){const Pt=new et({id:At.parent.getNextId(),parent:At.parent,uiManager:At._uiManager});Pt.annotationElementId=At.annotationElementId,Pt.deleted=!0,Pt._uiManager.addToAnnotationStorage(Pt)}static initialize(At,Pt=null){if(wt._l10nPromise||(wt._l10nPromise=new Map(["editor_alt_text_button_label","editor_alt_text_edit_button_label","editor_alt_text_decorative_tooltip"].map(Ot=>[Ot,At.get(Ot)]))),Pt!=null&&Pt.strings)for(const Ot of Pt.strings)wt._l10nPromise.set(Ot,At.get(Ot));if(wt._borderLineWidth!==-1)return;const Mt=getComputedStyle(document.documentElement);wt._borderLineWidth=parseFloat(Mt.getPropertyValue("--outline-width"))||0}static updateDefaultParams(At,Pt){}static get defaultPropertiesToUpdate(){return[]}static isHandlingMimeForPasting(At){return!1}static paste(At,Pt){(0,$.unreachable)("Not implemented")}get propertiesToUpdate(){return[]}get _isDraggable(){return St(this,mt)}set _isDraggable(At){var Pt;wn(this,mt,At),(Pt=this.div)==null||Pt.classList.toggle("draggable",At)}center(){const[At,Pt]=this.pageDimensions;switch(this.parentRotation){case 90:this.x-=this.height*Pt/(At*2),this.y+=this.width*At/(Pt*2);break;case 180:this.x+=this.width/2,this.y+=this.height/2;break;case 270:this.x+=this.height*Pt/(At*2),this.y-=this.width*At/(Pt*2);break;default:this.x-=this.width/2,this.y-=this.height/2;break}this.fixAndSetPosition()}addCommands(At){this._uiManager.addCommands(At)}get currentLayer(){return this._uiManager.currentLayer}setInBackground(){this.div.style.zIndex=0}setInForeground(){this.div.style.zIndex=St(this,ft)}setParent(At){At!==null&&(this.pageIndex=At.pageIndex,this.pageDimensions=At.pageDimensions),this.parent=At}focusin(At){this._focusEventsAllowed&&(St(this,ot)?wn(this,ot,!1):this.parent.setSelected(this))}focusout(At){var Mt;if(!this._focusEventsAllowed||!this.isAttachedToDOM)return;const Pt=At.relatedTarget;Pt!=null&&Pt.closest(`#${this.id}`)||(At.preventDefault(),(Mt=this.parent)!=null&&Mt.isMultipleSelection||this.commitOrRemove())}commitOrRemove(){this.isEmpty()?this.remove():this.commit()}commit(){this.addToAnnotationStorage()}addToAnnotationStorage(){this._uiManager.addToAnnotationStorage(this)}setAt(At,Pt,Mt,Ot){const[Bt,zt]=this.parentDimensions;[Mt,Ot]=this.screenToPageTranslation(Mt,Ot),this.x=(At+Mt)/Bt,this.y=(Pt+Ot)/zt,this.fixAndSetPosition()}translate(At,Pt){un(this,ht,Ro).call(this,this.parentDimensions,At,Pt)}translateInPage(At,Pt){un(this,ht,Ro).call(this,this.pageDimensions,At,Pt),this.div.scrollIntoView({block:"nearest"})}drag(At,Pt){const[Mt,Ot]=this.parentDimensions;if(this.x+=At/Mt,this.y+=Pt/Ot,this.parent&&(this.x<0||this.x>1||this.y<0||this.y>1)){const{x:qt,y:tn}=this.div.getBoundingClientRect();this.parent.findNewParent(this,qt,tn)&&(this.x-=Math.floor(this.x),this.y-=Math.floor(this.y))}let{x:Bt,y:zt}=this;const[Gt,Wt]=un(this,bt,Po).call(this);Bt+=Gt,zt+=Wt,this.div.style.left=`${(100*Bt).toFixed(2)}%`,this.div.style.top=`${(100*zt).toFixed(2)}%`,this.div.scrollIntoView({block:"nearest"})}fixAndSetPosition(){const[At,Pt]=this.pageDimensions;let{x:Mt,y:Ot,width:Bt,height:zt}=this;switch(Bt*=At,zt*=Pt,Mt*=At,Ot*=Pt,this.rotation){case 0:Mt=Math.max(0,Math.min(At-Bt,Mt)),Ot=Math.max(0,Math.min(Pt-zt,Ot));break;case 90:Mt=Math.max(0,Math.min(At-zt,Mt)),Ot=Math.min(Pt,Math.max(Bt,Ot));break;case 180:Mt=Math.min(At,Math.max(Bt,Mt)),Ot=Math.min(Pt,Math.max(zt,Ot));break;case 270:Mt=Math.min(At,Math.max(zt,Mt)),Ot=Math.max(0,Math.min(Pt-Bt,Ot));break}this.x=Mt/=At,this.y=Ot/=Pt;const[Gt,Wt]=un(this,bt,Po).call(this);Mt+=Gt,Ot+=Wt;const{style:qt}=this.div;qt.left=`${(100*Mt).toFixed(2)}%`,qt.top=`${(100*Ot).toFixed(2)}%`,this.moveInDOM()}screenToPageTranslation(At,Pt){var Mt;return un(Mt=wt,xt,Lo).call(Mt,At,Pt,this.parentRotation)}pageTranslationToScreen(At,Pt){var Mt;return un(Mt=wt,xt,Lo).call(Mt,At,Pt,360-this.parentRotation)}get parentScale(){return this._uiManager.viewParameters.realScale}get parentRotation(){return(this._uiManager.viewParameters.rotation+this.pageRotation)%360}get parentDimensions(){const{parentScale:At,pageDimensions:[Pt,Mt]}=this,Ot=Pt*At,Bt=Mt*At;return $.FeatureTest.isCSSRoundSupported?[Math.round(Ot),Math.round(Bt)]:[Ot,Bt]}setDims(At,Pt){var Bt;const[Mt,Ot]=this.parentDimensions;this.div.style.width=`${(100*At/Mt).toFixed(2)}%`,St(this,lt)||(this.div.style.height=`${(100*Pt/Ot).toFixed(2)}%`),(Bt=St(this,at))==null||Bt.classList.toggle("small",At<wt.SMALL_EDITOR_SIZE||Pt<wt.SMALL_EDITOR_SIZE)}fixDims(){const{style:At}=this.div,{height:Pt,width:Mt}=At,Ot=Mt.endsWith("%"),Bt=!St(this,lt)&&Pt.endsWith("%");if(Ot&&Bt)return;const[zt,Gt]=this.parentDimensions;Ot||(At.width=`${(100*parseFloat(Mt)/zt).toFixed(2)}%`),!St(this,lt)&&!Bt&&(At.height=`${(100*parseFloat(Pt)/Gt).toFixed(2)}%`)}getInitialTranslation(){return[0,0]}async addAltTextButton(){if(St(this,at))return;const At=wn(this,at,document.createElement("button"));At.className="altText";const Pt=await wt._l10nPromise.get("editor_alt_text_button_label");At.textContent=Pt,At.setAttribute("aria-label",Pt),At.tabIndex="0",At.addEventListener("contextmenu",j.noContextMenu),At.addEventListener("pointerdown",Mt=>Mt.stopPropagation()),At.addEventListener("click",Mt=>{Mt.preventDefault(),this._uiManager.editAltText(this)},{capture:!0}),At.addEventListener("keydown",Mt=>{Mt.target===At&&Mt.key==="Enter"&&(Mt.preventDefault(),this._uiManager.editAltText(this))}),un(this,Zt,_o).call(this),this.div.append(At),wt.SMALL_EDITOR_SIZE||(wt.SMALL_EDITOR_SIZE=Math.min(128,Math.round(At.getBoundingClientRect().width*1.4)))}getClientDimensions(){return this.div.getBoundingClientRect()}get altTextData(){return{altText:St(this,tt),decorative:St(this,nt)}}set altTextData({altText:At,decorative:Pt}){St(this,tt)===At&&St(this,nt)===Pt||(wn(this,tt,At),wn(this,nt,Pt),un(this,Zt,_o).call(this))}render(){this.div=document.createElement("div"),this.div.setAttribute("data-editor-rotation",(360-this.rotation)%360),this.div.className=this.name,this.div.setAttribute("id",this.id),this.div.setAttribute("tabIndex",0),this.setInForeground(),this.div.addEventListener("focusin",St(this,rt)),this.div.addEventListener("focusout",St(this,ut));const[At,Pt]=this.parentDimensions;this.parentRotation%180!==0&&(this.div.style.maxWidth=`${(100*Pt/At).toFixed(2)}%`,this.div.style.maxHeight=`${(100*At/Pt).toFixed(2)}%`);const[Mt,Ot]=this.getInitialTranslation();return this.translate(Mt,Ot),(0,s.bindEvents)(this,this.div,["pointerdown"]),this.div}pointerdown(At){const{isMac:Pt}=$.FeatureTest.platform;if(At.button!==0||At.ctrlKey&&Pt){At.preventDefault();return}wn(this,ot,!0),un(this,sn,pr).call(this,At)}moveInDOM(){var At;(At=this.parent)==null||At.moveEditorInDOM(this)}_setParentAndPosition(At,Pt,Mt){At.changeParent(this),this.x=Pt,this.y=Mt,this.fixAndSetPosition()}getRect(At,Pt){const Mt=this.parentScale,[Ot,Bt]=this.pageDimensions,[zt,Gt]=this.pageTranslation,Wt=At/Mt,qt=Pt/Mt,tn=this.x*Ot,ln=this.y*Bt,gn=this.width*Ot,yn=this.height*Bt;switch(this.rotation){case 0:return[tn+Wt+zt,Bt-ln-qt-yn+Gt,tn+Wt+gn+zt,Bt-ln-qt+Gt];case 90:return[tn+qt+zt,Bt-ln+Wt+Gt,tn+qt+yn+zt,Bt-ln+Wt+gn+Gt];case 180:return[tn-Wt-gn+zt,Bt-ln+qt+Gt,tn-Wt+zt,Bt-ln+qt+yn+Gt];case 270:return[tn-qt-yn+zt,Bt-ln-Wt-gn+Gt,tn-qt+zt,Bt-ln-Wt+Gt];default:throw new Error("Invalid rotation")}}getRectInCurrentCoords(At,Pt){const[Mt,Ot,Bt,zt]=At,Gt=Bt-Mt,Wt=zt-Ot;switch(this.rotation){case 0:return[Mt,Pt-zt,Gt,Wt];case 90:return[Mt,Pt-Ot,Wt,Gt];case 180:return[Bt,Pt-Ot,Gt,Wt];case 270:return[Bt,Pt-zt,Wt,Gt];default:throw new Error("Invalid rotation")}}onceAdded(){}isEmpty(){return!1}enableEditMode(){wn(this,pt,!0)}disableEditMode(){wn(this,pt,!1)}isInEditMode(){return St(this,pt)}shouldGetKeyboardEvents(){return!1}needsToBeRebuilt(){return this.div&&!this.isAttachedToDOM}rebuild(){var At,Pt;(At=this.div)==null||At.addEventListener("focusin",St(this,rt)),(Pt=this.div)==null||Pt.addEventListener("focusout",St(this,ut))}serialize(At=!1,Pt=null){(0,$.unreachable)("An editor must be serializable")}static deserialize(At,Pt,Mt){const Ot=new this.prototype.constructor({parent:Pt,id:Pt.getNextId(),uiManager:Mt});Ot.rotation=At.rotation;const[Bt,zt]=Ot.pageDimensions,[Gt,Wt,qt,tn]=Ot.getRectInCurrentCoords(At.rect,zt);return Ot.x=Gt/Bt,Ot.y=Wt/zt,Ot.width=qt/Bt,Ot.height=tn/zt,Ot}remove(){var At;this.div.removeEventListener("focusin",St(this,rt)),this.div.removeEventListener("focusout",St(this,ut)),this.isEmpty()||this.commit(),this.parent?this.parent.remove(this):this._uiManager.removeEditor(this),(At=St(this,at))==null||At.remove(),wn(this,at,null),wn(this,it,null)}get isResizable(){return!1}makeResizable(){this.isResizable&&(un(this,Tt,cr).call(this),St(this,ct).classList.remove("hidden"))}select(){var At;this.makeResizable(),(At=this.div)==null||At.classList.add("selectedEditor")}unselect(){var At,Pt,Mt;(At=St(this,ct))==null||At.classList.add("hidden"),(Pt=this.div)==null||Pt.classList.remove("selectedEditor"),(Mt=this.div)!=null&&Mt.contains(document.activeElement)&&this._uiManager.currentLayer.div.focus()}updateParams(At,Pt){}disableEditing(){St(this,at)&&(St(this,at).hidden=!0)}enableEditing(){St(this,at)&&(St(this,at).hidden=!1)}enterInEditMode(){}get contentDiv(){return this.div}get isEditing(){return St(this,dt)}set isEditing(At){wn(this,dt,At),this.parent&&(At?(this.parent.setSelected(this),this.parent.setActiveEditor(this)):this.parent.setActiveEditor(null))}setAspectRatio(At,Pt){wn(this,lt,!0);const Mt=At/Pt,{style:Ot}=this.div;Ot.aspectRatio=Mt,Ot.height="auto"}static get MIN_SIZE(){return 16}};tt=new WeakMap,nt=new WeakMap,at=new WeakMap,it=new WeakMap,st=new WeakMap,lt=new WeakMap,ct=new WeakMap,rt=new WeakMap,ut=new WeakMap,ot=new WeakMap,dt=new WeakMap,pt=new WeakMap,mt=new WeakMap,ft=new WeakMap,ht=new WeakSet,Ro=function([At,Pt],Mt,Ot){[Mt,Ot]=this.screenToPageTranslation(Mt,Ot),this.x+=Mt/At,this.y+=Ot/Pt,this.fixAndSetPosition()},bt=new WeakSet,Po=function(){const[At,Pt]=this.parentDimensions,{_borderLineWidth:Mt}=wt,Ot=Mt/At,Bt=Mt/Pt;switch(this.rotation){case 90:return[-Ot,Bt];case 180:return[Ot,Bt];case 270:return[Ot,-Bt];default:return[-Ot,-Bt]}},xt=new WeakSet,Lo=function(At,Pt,Mt){switch(Mt){case 90:return[Pt,-At];case 180:return[-At,-Pt];case 270:return[-Pt,At];default:return[At,Pt]}},Lt=new WeakSet,Mo=function(At){switch(At){case 90:{const[Pt,Mt]=this.pageDimensions;return[0,-Pt/Mt,Mt/Pt,0]}case 180:return[-1,0,0,-1];case 270:{const[Pt,Mt]=this.pageDimensions;return[0,Pt/Mt,-Mt/Pt,0]}default:return[1,0,0,1]}},Tt=new WeakSet,cr=function(){if(St(this,ct))return;wn(this,ct,document.createElement("div")),St(this,ct).classList.add("resizers");const At=["topLeft","topRight","bottomRight","bottomLeft"];this._willKeepAspectRatio||At.push("topMiddle","middleRight","bottomMiddle","middleLeft");for(const Pt of At){const Mt=document.createElement("div");St(this,ct).append(Mt),Mt.classList.add("resizer",Pt),Mt.addEventListener("pointerdown",un(this,Dt,dr).bind(this,Pt)),Mt.addEventListener("contextmenu",j.noContextMenu)}this.div.prepend(St(this,ct))},Dt=new WeakSet,dr=function(At,Pt){Pt.preventDefault();const{isMac:Mt}=$.FeatureTest.platform;if(Pt.button!==0||Pt.ctrlKey&&Mt)return;const Ot=un(this,Ct,ur).bind(this,At),Bt=this._isDraggable;this._isDraggable=!1;const zt={passive:!0,capture:!0};window.addEventListener("pointermove",Ot,zt);const Gt=this.x,Wt=this.y,qt=this.width,tn=this.height,ln=this.parent.div.style.cursor,gn=this.div.style.cursor;this.div.style.cursor=this.parent.div.style.cursor=window.getComputedStyle(Pt.target).cursor;const yn=()=>{this._isDraggable=Bt,window.removeEventListener("pointerup",yn),window.removeEventListener("blur",yn),window.removeEventListener("pointermove",Ot,zt),this.parent.div.style.cursor=ln,this.div.style.cursor=gn;const Pn=this.x,cn=this.y,xn=this.width,hn=this.height;Pn===Gt&&cn===Wt&&xn===qt&&hn===tn||this.addCommands({cmd:()=>{this.width=xn,this.height=hn,this.x=Pn,this.y=cn;const[en,Jt]=this.parentDimensions;this.setDims(en*xn,Jt*hn),this.fixAndSetPosition()},undo:()=>{this.width=qt,this.height=tn,this.x=Gt,this.y=Wt;const[en,Jt]=this.parentDimensions;this.setDims(en*qt,Jt*tn),this.fixAndSetPosition()},mustExec:!0})};window.addEventListener("pointerup",yn),window.addEventListener("blur",yn)},Ct=new WeakSet,ur=function(At,Pt){const[Mt,Ot]=this.parentDimensions,Bt=this.x,zt=this.y,Gt=this.width,Wt=this.height,qt=wt.MIN_SIZE/Mt,tn=wt.MIN_SIZE/Ot,ln=In=>Math.round(In*1e4)/1e4,gn=un(this,Lt,Mo).call(this,this.rotation),yn=(In,Dn)=>[gn[0]*In+gn[2]*Dn,gn[1]*In+gn[3]*Dn],Pn=un(this,Lt,Mo).call(this,360-this.rotation),cn=(In,Dn)=>[Pn[0]*In+Pn[2]*Dn,Pn[1]*In+Pn[3]*Dn];let xn,hn,en=!1,Jt=!1;switch(At){case"topLeft":en=!0,xn=(In,Dn)=>[0,0],hn=(In,Dn)=>[In,Dn];break;case"topMiddle":xn=(In,Dn)=>[In/2,0],hn=(In,Dn)=>[In/2,Dn];break;case"topRight":en=!0,xn=(In,Dn)=>[In,0],hn=(In,Dn)=>[0,Dn];break;case"middleRight":Jt=!0,xn=(In,Dn)=>[In,Dn/2],hn=(In,Dn)=>[0,Dn/2];break;case"bottomRight":en=!0,xn=(In,Dn)=>[In,Dn],hn=(In,Dn)=>[0,0];break;case"bottomMiddle":xn=(In,Dn)=>[In/2,Dn],hn=(In,Dn)=>[In/2,0];break;case"bottomLeft":en=!0,xn=(In,Dn)=>[0,Dn],hn=(In,Dn)=>[In,0];break;case"middleLeft":Jt=!0,xn=(In,Dn)=>[0,Dn/2],hn=(In,Dn)=>[In,Dn/2];break}const vn=xn(Gt,Wt),$n=hn(Gt,Wt);let Mn=yn(...$n);const On=ln(Bt+Mn[0]),En=ln(zt+Mn[1]);let Bn=1,Hn=1,[Wn,_n]=this.screenToPageTranslation(Pt.movementX,Pt.movementY);if([Wn,_n]=cn(Wn/Mt,_n/Ot),en){const In=Math.hypot(Gt,Wt);Bn=Hn=Math.max(Math.min(Math.hypot($n[0]-vn[0]-Wn,$n[1]-vn[1]-_n)/In,1/Gt,1/Wt),qt/Gt,tn/Wt)}else Jt?Bn=Math.max(qt,Math.min(1,Math.abs($n[0]-vn[0]-Wn)))/Gt:Hn=Math.max(tn,Math.min(1,Math.abs($n[1]-vn[1]-_n)))/Wt;const Zn=ln(Gt*Bn),bn=ln(Wt*Hn);Mn=yn(...hn(Zn,bn));const dn=On-Mn[0],an=En-Mn[1];this.width=Zn,this.height=bn,this.x=dn,this.y=an,this.setDims(Mt*Zn,Ot*bn),this.fixAndSetPosition()},Zt=new WeakSet,_o=async function(){var Mt;const At=St(this,at);if(!At)return;if(!St(this,tt)&&!St(this,nt)){At.classList.remove("done"),(Mt=St(this,it))==null||Mt.remove();return}wt._l10nPromise.get("editor_alt_text_edit_button_label").then(Ot=>{At.setAttribute("aria-label",Ot)});let Pt=St(this,it);if(!Pt){wn(this,it,Pt=document.createElement("span")),Pt.className="tooltip",Pt.setAttribute("role","tooltip");const Ot=Pt.id=`alt-text-tooltip-${this.id}`;At.setAttribute("aria-describedby",Ot);const Bt=100;At.addEventListener("mouseenter",()=>{wn(this,st,setTimeout(()=>{wn(this,st,null),St(this,it).classList.add("show"),this._uiManager._eventBus.dispatch("reporttelemetry",{source:this,details:{type:"editing",subtype:this.editorType,data:{action:"alt_text_tooltip"}}})},Bt))}),At.addEventListener("mouseleave",()=>{var zt;clearTimeout(St(this,st)),wn(this,st,null),(zt=St(this,it))==null||zt.classList.remove("show")})}At.classList.add("done"),Pt.innerText=St(this,nt)?await wt._l10nPromise.get("editor_alt_text_decorative_tooltip"):St(this,tt),Pt.parentNode||At.append(Pt)},sn=new WeakSet,pr=function(At){if(!this._isDraggable)return;const Pt=this._uiManager.isSelected(this);this._uiManager.setUpDragSession();let Mt,Ot;Pt&&(Mt={passive:!0,capture:!0},Ot=zt=>{const[Gt,Wt]=this.screenToPageTranslation(zt.movementX,zt.movementY);this._uiManager.dragSelectedEditors(Gt,Wt)},window.addEventListener("pointermove",Ot,Mt));const Bt=()=>{if(window.removeEventListener("pointerup",Bt),window.removeEventListener("blur",Bt),Pt&&window.removeEventListener("pointermove",Ot,Mt),wn(this,ot,!1),!this._uiManager.endDragSession()){const{isMac:zt}=$.FeatureTest.platform;At.ctrlKey&&!zt||At.shiftKey||At.metaKey&&zt?this.parent.toggleSelected(this):this.parent.setSelected(this)}};window.addEventListener("pointerup",Bt),window.addEventListener("blur",Bt)},Yt(wt,xt),Jn(wt,"_borderLineWidth",-1),Jn(wt,"_colorManager",new s.ColorManager),Jn(wt,"_zIndex",1),Jn(wt,"SMALL_EDITOR_SIZE",0);let _e=wt;i.AnnotationEditor=_e;class et extends _e{constructor(At){super(At),this.annotationElementId=At.annotationElementId,this.deleted=!0}serialize(){return{id:this.annotationElementId,deleted:!0,pageIndex:this.pageIndex}}}},(a,i,o)=>{var lt,ct,rt,ut,ot,Do,mt,ft,ht,yt,bt,hr,vt,Lt,$t,Tt,Et,Dt,It,Ct,jt,Zt,Xt,sn,Ft,wt,kt,At,Pt,Mt,Ot,Bt,zt,Gt,Wt,qt,tn,ln,gn,yn,Pn,cn,xn,hn,en,Jt,vn,fr,Mn,$o,En,Io,Hn,fo,_n,jo,bn,Oo,an,ga,Dn,io,Yn,mr,Ht,gr,nn,Fo,Rn,oo,Tn,No;Object.defineProperty(i,"__esModule",{value:!0}),i.KeyboardManager=i.CommandManager=i.ColorManager=i.AnnotationEditorUIManager=void 0,i.bindEvents=j,i.opacityToHex=_e;var s=o(1),$=o(6);function j(Ut,Rt,Nt){for(const Vt of Nt)Rt.addEventListener(Vt,Ut[Vt].bind(Ut))}function _e(Ut){return Math.round(Math.min(255,Math.max(1,255*Ut))).toString(16).padStart(2,"0")}class et{constructor(){Yt(this,lt,0)}getId(){return`${s.AnnotationEditorPrefix}${ao(this,lt)._++}`}}lt=new WeakMap;const pt=class pt{constructor(){Yt(this,ot);Yt(this,ct,(0,s.getUuid)());Yt(this,rt,0);Yt(this,ut,null)}static get _isSVGFittingCanvas(){const Rt='data:image/svg+xml;charset=UTF-8,<svg viewBox="0 0 1 1" width="1" height="1" xmlns="http://www.w3.org/2000/svg"><rect width="1" height="1" style="fill:red;"/></svg>',Vt=new OffscreenCanvas(1,3).getContext("2d"),Qt=new Image;Qt.src=Rt;const rn=Qt.decode().then(()=>(Vt.drawImage(Qt,0,0,1,1,0,0,1,3),new Uint32Array(Vt.getImageData(0,0,1,1).data.buffer)[0]===0));return(0,s.shadow)(this,"_isSVGFittingCanvas",rn)}async getFromFile(Rt){const{lastModified:Nt,name:Vt,size:Qt,type:rn}=Rt;return un(this,ot,Do).call(this,`${Nt}_${Vt}_${Qt}_${rn}`,Rt)}async getFromUrl(Rt){return un(this,ot,Do).call(this,Rt,Rt)}async getFromId(Rt){St(this,ut)||wn(this,ut,new Map);const Nt=St(this,ut).get(Rt);return Nt?Nt.bitmap?(Nt.refCounter+=1,Nt):Nt.file?this.getFromFile(Nt.file):this.getFromUrl(Nt.url):null}getSvgUrl(Rt){const Nt=St(this,ut).get(Rt);return Nt!=null&&Nt.isSvg?Nt.svgUrl:null}deleteId(Rt){St(this,ut)||wn(this,ut,new Map);const Nt=St(this,ut).get(Rt);Nt&&(Nt.refCounter-=1,Nt.refCounter===0&&(Nt.bitmap=null))}isValidId(Rt){return Rt.startsWith(`image_${St(this,ct)}_`)}};ct=new WeakMap,rt=new WeakMap,ut=new WeakMap,ot=new WeakSet,Do=async function(Rt,Nt){St(this,ut)||wn(this,ut,new Map);let Vt=St(this,ut).get(Rt);if(Vt===null)return null;if(Vt!=null&&Vt.bitmap)return Vt.refCounter+=1,Vt;try{Vt||(Vt={bitmap:null,id:`image_${St(this,ct)}_${ao(this,rt)._++}`,refCounter:0,isSvg:!1});let Qt;if(typeof Nt=="string"){Vt.url=Nt;const rn=await fetch(Nt);if(!rn.ok)throw new Error(rn.statusText);Qt=await rn.blob()}else Qt=Vt.file=Nt;if(Qt.type==="image/svg+xml"){const rn=pt._isSVGFittingCanvas,fn=new FileReader,Ln=new Image,zn=new Promise((on,mn)=>{Ln.onload=()=>{Vt.bitmap=Ln,Vt.isSvg=!0,on()},fn.onload=async()=>{const Sn=Vt.svgUrl=fn.result;Ln.src=await rn?`${Sn}#svgView(preserveAspectRatio(none))`:Sn},Ln.onerror=fn.onerror=mn});fn.readAsDataURL(Qt),await zn}else Vt.bitmap=await createImageBitmap(Qt);Vt.refCounter=1}catch(Qt){console.error(Qt),Vt=null}return St(this,ut).set(Rt,Vt),Vt&&St(this,ut).set(Vt.id,Vt),Vt};let tt=pt;class nt{constructor(Rt=128){Yt(this,mt,[]);Yt(this,ft,!1);Yt(this,ht,void 0);Yt(this,yt,-1);wn(this,ht,Rt)}add({cmd:Rt,undo:Nt,mustExec:Vt,type:Qt=NaN,overwriteIfSameType:rn=!1,keepUndo:fn=!1}){if(Vt&&Rt(),St(this,ft))return;const Ln={cmd:Rt,undo:Nt,type:Qt};if(St(this,yt)===-1){St(this,mt).length>0&&(St(this,mt).length=0),wn(this,yt,0),St(this,mt).push(Ln);return}if(rn&&St(this,mt)[St(this,yt)].type===Qt){fn&&(Ln.undo=St(this,mt)[St(this,yt)].undo),St(this,mt)[St(this,yt)]=Ln;return}const zn=St(this,yt)+1;zn===St(this,ht)?St(this,mt).splice(0,1):(wn(this,yt,zn),zn<St(this,mt).length&&St(this,mt).splice(zn)),St(this,mt).push(Ln)}undo(){St(this,yt)!==-1&&(wn(this,ft,!0),St(this,mt)[St(this,yt)].undo(),wn(this,ft,!1),wn(this,yt,St(this,yt)-1))}redo(){St(this,yt)<St(this,mt).length-1&&(wn(this,yt,St(this,yt)+1),wn(this,ft,!0),St(this,mt)[St(this,yt)].cmd(),wn(this,ft,!1))}hasSomethingToUndo(){return St(this,yt)!==-1}hasSomethingToRedo(){return St(this,yt)<St(this,mt).length-1}destroy(){wn(this,mt,null)}}mt=new WeakMap,ft=new WeakMap,ht=new WeakMap,yt=new WeakMap,i.CommandManager=nt;class at{constructor(Rt){Yt(this,bt);this.buffer=[],this.callbacks=new Map,this.allKeys=new Set;const{isMac:Nt}=s.FeatureTest.platform;for(const[Vt,Qt,rn={}]of Rt)for(const fn of Vt){const Ln=fn.startsWith("mac+");Nt&&Ln?(this.callbacks.set(fn.slice(4),{callback:Qt,options:rn}),this.allKeys.add(fn.split("+").at(-1))):!Nt&&!Ln&&(this.callbacks.set(fn,{callback:Qt,options:rn}),this.allKeys.add(fn.split("+").at(-1)))}}exec(Rt,Nt){if(!this.allKeys.has(Nt.key))return;const Vt=this.callbacks.get(un(this,bt,hr).call(this,Nt));if(!Vt)return;const{callback:Qt,options:{bubbles:rn=!1,args:fn=[],checker:Ln=null}}=Vt;Ln&&!Ln(Rt,Nt)||(Qt.bind(Rt,...fn)(),rn||(Nt.stopPropagation(),Nt.preventDefault()))}}bt=new WeakSet,hr=function(Rt){Rt.altKey&&this.buffer.push("alt"),Rt.ctrlKey&&this.buffer.push("ctrl"),Rt.metaKey&&this.buffer.push("meta"),Rt.shiftKey&&this.buffer.push("shift"),this.buffer.push(Rt.key);const Nt=this.buffer.join("+");return this.buffer.length=0,Nt},i.KeyboardManager=at;const xt=class xt{get _colors(){const Rt=new Map([["CanvasText",null],["Canvas",null]]);return(0,$.getColorValues)(Rt),(0,s.shadow)(this,"_colors",Rt)}convert(Rt){const Nt=(0,$.getRGB)(Rt);if(!window.matchMedia("(forced-colors: active)").matches)return Nt;for(const[Vt,Qt]of this._colors)if(Qt.every((rn,fn)=>rn===Nt[fn]))return xt._colorsMapping.get(Vt);return Nt}getHexCode(Rt){const Nt=this._colors.get(Rt);return Nt?s.Util.makeHexColor(...Nt):Rt}};Jn(xt,"_colorsMapping",new Map([["CanvasText",[0,0,0]],["Canvas",[255,255,255]]]));let it=xt;i.ColorManager=it;const Cn=class Cn{constructor(Rt,Nt,Vt,Qt,rn,fn){Yt(this,vn);Yt(this,Mn);Yt(this,En);Yt(this,Hn);Yt(this,_n);Yt(this,bn);Yt(this,an);Yt(this,Dn);Yt(this,Yn);Yt(this,Ht);Yt(this,nn);Yt(this,Rn);Yt(this,Tn);Yt(this,vt,null);Yt(this,Lt,new Map);Yt(this,$t,new Map);Yt(this,Tt,null);Yt(this,Et,null);Yt(this,Dt,new nt);Yt(this,It,0);Yt(this,Ct,new Set);Yt(this,jt,null);Yt(this,Zt,null);Yt(this,Xt,new Set);Yt(this,sn,null);Yt(this,Ft,new et);Yt(this,wt,!1);Yt(this,kt,!1);Yt(this,At,null);Yt(this,Pt,s.AnnotationEditorType.NONE);Yt(this,Mt,new Set);Yt(this,Ot,null);Yt(this,Bt,this.blur.bind(this));Yt(this,zt,this.focus.bind(this));Yt(this,Gt,this.copy.bind(this));Yt(this,Wt,this.cut.bind(this));Yt(this,qt,this.paste.bind(this));Yt(this,tn,this.keydown.bind(this));Yt(this,ln,this.onEditingAction.bind(this));Yt(this,gn,this.onPageChanging.bind(this));Yt(this,yn,this.onScaleChanging.bind(this));Yt(this,Pn,this.onRotationChanging.bind(this));Yt(this,cn,{isEditing:!1,isEmpty:!0,hasSomethingToUndo:!1,hasSomethingToRedo:!1,hasSelectedEditor:!1});Yt(this,xn,[0,0]);Yt(this,hn,null);Yt(this,en,null);Yt(this,Jt,null);wn(this,en,Rt),wn(this,Jt,Nt),wn(this,Tt,Vt),this._eventBus=Qt,this._eventBus._on("editingaction",St(this,ln)),this._eventBus._on("pagechanging",St(this,gn)),this._eventBus._on("scalechanging",St(this,yn)),this._eventBus._on("rotationchanging",St(this,Pn)),wn(this,Et,rn.annotationStorage),wn(this,sn,rn.filterFactory),wn(this,Ot,fn),this.viewParameters={realScale:$.PixelsPerInch.PDF_TO_CSS_UNITS,rotation:0}}static get _keyboardManager(){const Rt=Cn.prototype,Nt=rn=>{const{activeElement:fn}=document;return fn&&St(rn,en).contains(fn)&&rn.hasSomethingToControl()},Vt=this.TRANSLATE_SMALL,Qt=this.TRANSLATE_BIG;return(0,s.shadow)(this,"_keyboardManager",new at([[["ctrl+a","mac+meta+a"],Rt.selectAll],[["ctrl+z","mac+meta+z"],Rt.undo],[["ctrl+y","ctrl+shift+z","mac+meta+shift+z","ctrl+shift+Z","mac+meta+shift+Z"],Rt.redo],[["Backspace","alt+Backspace","ctrl+Backspace","shift+Backspace","mac+Backspace","mac+alt+Backspace","mac+ctrl+Backspace","Delete","ctrl+Delete","shift+Delete","mac+Delete"],Rt.delete],[["Escape","mac+Escape"],Rt.unselectAll],[["ArrowLeft","mac+ArrowLeft"],Rt.translateSelectedEditors,{args:[-Vt,0],checker:Nt}],[["ctrl+ArrowLeft","mac+shift+ArrowLeft"],Rt.translateSelectedEditors,{args:[-Qt,0],checker:Nt}],[["ArrowRight","mac+ArrowRight"],Rt.translateSelectedEditors,{args:[Vt,0],checker:Nt}],[["ctrl+ArrowRight","mac+shift+ArrowRight"],Rt.translateSelectedEditors,{args:[Qt,0],checker:Nt}],[["ArrowUp","mac+ArrowUp"],Rt.translateSelectedEditors,{args:[0,-Vt],checker:Nt}],[["ctrl+ArrowUp","mac+shift+ArrowUp"],Rt.translateSelectedEditors,{args:[0,-Qt],checker:Nt}],[["ArrowDown","mac+ArrowDown"],Rt.translateSelectedEditors,{args:[0,Vt],checker:Nt}],[["ctrl+ArrowDown","mac+shift+ArrowDown"],Rt.translateSelectedEditors,{args:[0,Qt],checker:Nt}]]))}destroy(){un(this,Hn,fo).call(this),un(this,Mn,$o).call(this),this._eventBus._off("editingaction",St(this,ln)),this._eventBus._off("pagechanging",St(this,gn)),this._eventBus._off("scalechanging",St(this,yn)),this._eventBus._off("rotationchanging",St(this,Pn));for(const Rt of St(this,$t).values())Rt.destroy();St(this,$t).clear(),St(this,Lt).clear(),St(this,Xt).clear(),wn(this,vt,null),St(this,Mt).clear(),St(this,Dt).destroy(),St(this,Tt).destroy()}get hcmFilter(){return(0,s.shadow)(this,"hcmFilter",St(this,Ot)?St(this,sn).addHCMFilter(St(this,Ot).foreground,St(this,Ot).background):"none")}get direction(){return(0,s.shadow)(this,"direction",getComputedStyle(St(this,en)).direction)}editAltText(Rt){var Nt;(Nt=St(this,Tt))==null||Nt.editAltText(this,Rt)}onPageChanging({pageNumber:Rt}){wn(this,It,Rt-1)}focusMainContainer(){St(this,en).focus()}findParent(Rt,Nt){for(const Vt of St(this,$t).values()){const{x:Qt,y:rn,width:fn,height:Ln}=Vt.div.getBoundingClientRect();if(Rt>=Qt&&Rt<=Qt+fn&&Nt>=rn&&Nt<=rn+Ln)return Vt}return null}disableUserSelect(Rt=!1){St(this,Jt).classList.toggle("noUserSelect",Rt)}addShouldRescale(Rt){St(this,Xt).add(Rt)}removeShouldRescale(Rt){St(this,Xt).delete(Rt)}onScaleChanging({scale:Rt}){this.commitOrRemove(),this.viewParameters.realScale=Rt*$.PixelsPerInch.PDF_TO_CSS_UNITS;for(const Nt of St(this,Xt))Nt.onScaleChanging()}onRotationChanging({pagesRotation:Rt}){this.commitOrRemove(),this.viewParameters.rotation=Rt}addToAnnotationStorage(Rt){!Rt.isEmpty()&&St(this,Et)&&!St(this,Et).has(Rt.id)&&St(this,Et).setValue(Rt.id,Rt)}blur(){if(!this.hasSelection)return;const{activeElement:Rt}=document;for(const Nt of St(this,Mt))if(Nt.div.contains(Rt)){wn(this,At,[Nt,Rt]),Nt._focusEventsAllowed=!1;break}}focus(){if(!St(this,At))return;const[Rt,Nt]=St(this,At);wn(this,At,null),Nt.addEventListener("focusin",()=>{Rt._focusEventsAllowed=!0},{once:!0}),Nt.focus()}addEditListeners(){un(this,En,Io).call(this),un(this,_n,jo).call(this)}removeEditListeners(){un(this,Hn,fo).call(this),un(this,bn,Oo).call(this)}copy(Rt){var Vt;if(Rt.preventDefault(),(Vt=St(this,vt))==null||Vt.commitOrRemove(),!this.hasSelection)return;const Nt=[];for(const Qt of St(this,Mt)){const rn=Qt.serialize(!0);rn&&Nt.push(rn)}Nt.length!==0&&Rt.clipboardData.setData("application/pdfjs",JSON.stringify(Nt))}cut(Rt){this.copy(Rt),this.delete()}paste(Rt){Rt.preventDefault();const{clipboardData:Nt}=Rt;for(const rn of Nt.items)for(const fn of St(this,Zt))if(fn.isHandlingMimeForPasting(rn.type)){fn.paste(rn,this.currentLayer);return}let Vt=Nt.getData("application/pdfjs");if(!Vt)return;try{Vt=JSON.parse(Vt)}catch(rn){(0,s.warn)(`paste: "${rn.message}".`);return}if(!Array.isArray(Vt))return;this.unselectAll();const Qt=this.currentLayer;try{const rn=[];for(const zn of Vt){const on=Qt.deserialize(zn);if(!on)return;rn.push(on)}const fn=()=>{for(const zn of rn)un(this,nn,Fo).call(this,zn);un(this,Tn,No).call(this,rn)},Ln=()=>{for(const zn of rn)zn.remove()};this.addCommands({cmd:fn,undo:Ln,mustExec:!0})}catch(rn){(0,s.warn)(`paste: "${rn.message}".`)}}keydown(Rt){var Nt;(Nt=this.getActive())!=null&&Nt.shouldGetKeyboardEvents()||Cn._keyboardManager.exec(this,Rt)}onEditingAction(Rt){["undo","redo","delete","selectAll"].includes(Rt.name)&&this[Rt.name]()}setEditingState(Rt){Rt?(un(this,vn,fr).call(this),un(this,En,Io).call(this),un(this,_n,jo).call(this),un(this,an,ga).call(this,{isEditing:St(this,Pt)!==s.AnnotationEditorType.NONE,isEmpty:un(this,Rn,oo).call(this),hasSomethingToUndo:St(this,Dt).hasSomethingToUndo(),hasSomethingToRedo:St(this,Dt).hasSomethingToRedo(),hasSelectedEditor:!1})):(un(this,Mn,$o).call(this),un(this,Hn,fo).call(this),un(this,bn,Oo).call(this),un(this,an,ga).call(this,{isEditing:!1}),this.disableUserSelect(!1))}registerEditorTypes(Rt){if(!St(this,Zt)){wn(this,Zt,Rt);for(const Nt of St(this,Zt))un(this,Dn,io).call(this,Nt.defaultPropertiesToUpdate)}}getId(){return St(this,Ft).getId()}get currentLayer(){return St(this,$t).get(St(this,It))}getLayer(Rt){return St(this,$t).get(Rt)}get currentPageIndex(){return St(this,It)}addLayer(Rt){St(this,$t).set(Rt.pageIndex,Rt),St(this,wt)?Rt.enable():Rt.disable()}removeLayer(Rt){St(this,$t).delete(Rt.pageIndex)}updateMode(Rt,Nt=null){if(St(this,Pt)!==Rt){if(wn(this,Pt,Rt),Rt===s.AnnotationEditorType.NONE){this.setEditingState(!1),un(this,Ht,gr).call(this);return}this.setEditingState(!0),un(this,Yn,mr).call(this),this.unselectAll();for(const Vt of St(this,$t).values())Vt.updateMode(Rt);if(Nt){for(const Vt of St(this,Lt).values())if(Vt.annotationElementId===Nt){this.setSelected(Vt),Vt.enterInEditMode();break}}}}updateToolbar(Rt){Rt!==St(this,Pt)&&this._eventBus.dispatch("switchannotationeditormode",{source:this,mode:Rt})}updateParams(Rt,Nt){if(St(this,Zt)){if(Rt===s.AnnotationEditorParamsType.CREATE){this.currentLayer.addNewEditor(Rt);return}for(const Vt of St(this,Mt))Vt.updateParams(Rt,Nt);for(const Vt of St(this,Zt))Vt.updateDefaultParams(Rt,Nt)}}enableWaiting(Rt=!1){if(St(this,kt)!==Rt){wn(this,kt,Rt);for(const Nt of St(this,$t).values())Rt?Nt.disableClick():Nt.enableClick(),Nt.div.classList.toggle("waiting",Rt)}}getEditors(Rt){const Nt=[];for(const Vt of St(this,Lt).values())Vt.pageIndex===Rt&&Nt.push(Vt);return Nt}getEditor(Rt){return St(this,Lt).get(Rt)}addEditor(Rt){St(this,Lt).set(Rt.id,Rt)}removeEditor(Rt){var Nt;St(this,Lt).delete(Rt.id),this.unselect(Rt),(!Rt.annotationElementId||!St(this,Ct).has(Rt.annotationElementId))&&((Nt=St(this,Et))==null||Nt.remove(Rt.id))}addDeletedAnnotationElement(Rt){St(this,Ct).add(Rt.annotationElementId),Rt.deleted=!0}isDeletedAnnotationElement(Rt){return St(this,Ct).has(Rt)}removeDeletedAnnotationElement(Rt){St(this,Ct).delete(Rt.annotationElementId),Rt.deleted=!1}setActiveEditor(Rt){St(this,vt)!==Rt&&(wn(this,vt,Rt),Rt&&un(this,Dn,io).call(this,Rt.propertiesToUpdate))}toggleSelected(Rt){if(St(this,Mt).has(Rt)){St(this,Mt).delete(Rt),Rt.unselect(),un(this,an,ga).call(this,{hasSelectedEditor:this.hasSelection});return}St(this,Mt).add(Rt),Rt.select(),un(this,Dn,io).call(this,Rt.propertiesToUpdate),un(this,an,ga).call(this,{hasSelectedEditor:!0})}setSelected(Rt){for(const Nt of St(this,Mt))Nt!==Rt&&Nt.unselect();St(this,Mt).clear(),St(this,Mt).add(Rt),Rt.select(),un(this,Dn,io).call(this,Rt.propertiesToUpdate),un(this,an,ga).call(this,{hasSelectedEditor:!0})}isSelected(Rt){return St(this,Mt).has(Rt)}unselect(Rt){Rt.unselect(),St(this,Mt).delete(Rt),un(this,an,ga).call(this,{hasSelectedEditor:this.hasSelection})}get hasSelection(){return St(this,Mt).size!==0}undo(){St(this,Dt).undo(),un(this,an,ga).call(this,{hasSomethingToUndo:St(this,Dt).hasSomethingToUndo(),hasSomethingToRedo:!0,isEmpty:un(this,Rn,oo).call(this)})}redo(){St(this,Dt).redo(),un(this,an,ga).call(this,{hasSomethingToUndo:!0,hasSomethingToRedo:St(this,Dt).hasSomethingToRedo(),isEmpty:un(this,Rn,oo).call(this)})}addCommands(Rt){St(this,Dt).add(Rt),un(this,an,ga).call(this,{hasSomethingToUndo:!0,hasSomethingToRedo:!1,isEmpty:un(this,Rn,oo).call(this)})}delete(){if(this.commitOrRemove(),!this.hasSelection)return;const Rt=[...St(this,Mt)],Nt=()=>{for(const Qt of Rt)Qt.remove()},Vt=()=>{for(const Qt of Rt)un(this,nn,Fo).call(this,Qt)};this.addCommands({cmd:Nt,undo:Vt,mustExec:!0})}commitOrRemove(){var Rt;(Rt=St(this,vt))==null||Rt.commitOrRemove()}hasSomethingToControl(){return St(this,vt)||this.hasSelection}selectAll(){for(const Rt of St(this,Mt))Rt.commit();un(this,Tn,No).call(this,St(this,Lt).values())}unselectAll(){if(St(this,vt)){St(this,vt).commitOrRemove();return}if(this.hasSelection){for(const Rt of St(this,Mt))Rt.unselect();St(this,Mt).clear(),un(this,an,ga).call(this,{hasSelectedEditor:!1})}}translateSelectedEditors(Rt,Nt,Vt=!1){if(Vt||this.commitOrRemove(),!this.hasSelection)return;St(this,xn)[0]+=Rt,St(this,xn)[1]+=Nt;const[Qt,rn]=St(this,xn),fn=[...St(this,Mt)],Ln=1e3;St(this,hn)&&clearTimeout(St(this,hn)),wn(this,hn,setTimeout(()=>{wn(this,hn,null),St(this,xn)[0]=St(this,xn)[1]=0,this.addCommands({cmd:()=>{for(const zn of fn)St(this,Lt).has(zn.id)&&zn.translateInPage(Qt,rn)},undo:()=>{for(const zn of fn)St(this,Lt).has(zn.id)&&zn.translateInPage(-Qt,-rn)},mustExec:!1})},Ln));for(const zn of fn)zn.translateInPage(Rt,Nt)}setUpDragSession(){if(this.hasSelection){this.disableUserSelect(!0),wn(this,jt,new Map);for(const Rt of St(this,Mt))St(this,jt).set(Rt,{savedX:Rt.x,savedY:Rt.y,savedPageIndex:Rt.pageIndex,newX:0,newY:0,newPageIndex:-1})}}endDragSession(){if(!St(this,jt))return!1;this.disableUserSelect(!1);const Rt=St(this,jt);wn(this,jt,null);let Nt=!1;for(const[{x:Qt,y:rn,pageIndex:fn},Ln]of Rt)Ln.newX=Qt,Ln.newY=rn,Ln.newPageIndex=fn,Nt||(Nt=Qt!==Ln.savedX||rn!==Ln.savedY||fn!==Ln.savedPageIndex);if(!Nt)return!1;const Vt=(Qt,rn,fn,Ln)=>{if(St(this,Lt).has(Qt.id)){const zn=St(this,$t).get(Ln);zn?Qt._setParentAndPosition(zn,rn,fn):(Qt.pageIndex=Ln,Qt.x=rn,Qt.y=fn)}};return this.addCommands({cmd:()=>{for(const[Qt,{newX:rn,newY:fn,newPageIndex:Ln}]of Rt)Vt(Qt,rn,fn,Ln)},undo:()=>{for(const[Qt,{savedX:rn,savedY:fn,savedPageIndex:Ln}]of Rt)Vt(Qt,rn,fn,Ln)},mustExec:!0}),!0}dragSelectedEditors(Rt,Nt){if(St(this,jt))for(const Vt of St(this,jt).keys())Vt.drag(Rt,Nt)}rebuild(Rt){if(Rt.parent===null){const Nt=this.getLayer(Rt.pageIndex);Nt?(Nt.changeParent(Rt),Nt.addOrRebuild(Rt)):(this.addEditor(Rt),this.addToAnnotationStorage(Rt),Rt.rebuild())}else Rt.parent.addOrRebuild(Rt)}isActive(Rt){return St(this,vt)===Rt}getActive(){return St(this,vt)}getMode(){return St(this,Pt)}get imageManager(){return(0,s.shadow)(this,"imageManager",new tt)}};vt=new WeakMap,Lt=new WeakMap,$t=new WeakMap,Tt=new WeakMap,Et=new WeakMap,Dt=new WeakMap,It=new WeakMap,Ct=new WeakMap,jt=new WeakMap,Zt=new WeakMap,Xt=new WeakMap,sn=new WeakMap,Ft=new WeakMap,wt=new WeakMap,kt=new WeakMap,At=new WeakMap,Pt=new WeakMap,Mt=new WeakMap,Ot=new WeakMap,Bt=new WeakMap,zt=new WeakMap,Gt=new WeakMap,Wt=new WeakMap,qt=new WeakMap,tn=new WeakMap,ln=new WeakMap,gn=new WeakMap,yn=new WeakMap,Pn=new WeakMap,cn=new WeakMap,xn=new WeakMap,hn=new WeakMap,en=new WeakMap,Jt=new WeakMap,vn=new WeakSet,fr=function(){window.addEventListener("focus",St(this,zt)),window.addEventListener("blur",St(this,Bt))},Mn=new WeakSet,$o=function(){window.removeEventListener("focus",St(this,zt)),window.removeEventListener("blur",St(this,Bt))},En=new WeakSet,Io=function(){window.addEventListener("keydown",St(this,tn),{capture:!0})},Hn=new WeakSet,fo=function(){window.removeEventListener("keydown",St(this,tn),{capture:!0})},_n=new WeakSet,jo=function(){document.addEventListener("copy",St(this,Gt)),document.addEventListener("cut",St(this,Wt)),document.addEventListener("paste",St(this,qt))},bn=new WeakSet,Oo=function(){document.removeEventListener("copy",St(this,Gt)),document.removeEventListener("cut",St(this,Wt)),document.removeEventListener("paste",St(this,qt))},an=new WeakSet,ga=function(Rt){Object.entries(Rt).some(([Vt,Qt])=>St(this,cn)[Vt]!==Qt)&&this._eventBus.dispatch("annotationeditorstateschanged",{source:this,details:Object.assign(St(this,cn),Rt)})},Dn=new WeakSet,io=function(Rt){this._eventBus.dispatch("annotationeditorparamschanged",{source:this,details:Rt})},Yn=new WeakSet,mr=function(){if(!St(this,wt)){wn(this,wt,!0);for(const Rt of St(this,$t).values())Rt.enable()}},Ht=new WeakSet,gr=function(){if(this.unselectAll(),St(this,wt)){wn(this,wt,!1);for(const Rt of St(this,$t).values())Rt.disable()}},nn=new WeakSet,Fo=function(Rt){const Nt=St(this,$t).get(Rt.pageIndex);Nt?Nt.addOrRebuild(Rt):this.addEditor(Rt)},Rn=new WeakSet,oo=function(){if(St(this,Lt).size===0)return!0;if(St(this,Lt).size===1)for(const Rt of St(this,Lt).values())return Rt.isEmpty();return!1},Tn=new WeakSet,No=function(Rt){St(this,Mt).clear();for(const Nt of Rt)Nt.isEmpty()||(St(this,Mt).add(Nt),Nt.select());un(this,an,ga).call(this,{hasSelectedEditor:!0})},Jn(Cn,"TRANSLATE_SMALL",1),Jn(Cn,"TRANSLATE_BIG",10);let st=Cn;i.AnnotationEditorUIManager=st},(a,i,o)=>{var It,Ct,jt,Zt,Xt,sn,Ft,wt,kt,At,Pt,Mt,_i,Bt,eo,Gt,zo,qt,mo,ln,go,yn,ro,cn,so;Object.defineProperty(i,"__esModule",{value:!0}),i.StatTimer=i.RenderingCancelledException=i.PixelsPerInch=i.PageViewport=i.PDFDateString=i.DOMStandardFontDataFactory=i.DOMSVGFactory=i.DOMFilterFactory=i.DOMCanvasFactory=i.DOMCMapReaderFactory=void 0,i.deprecated=yt,i.getColorValues=Lt,i.getCurrentTransform=$t,i.getCurrentTransformInverse=Tt,i.getFilenameFromUrl=ot,i.getPdfFilenameFromUrl=dt,i.getRGB=vt,i.getXfaPageViewport=xt,i.isDataScheme=rt,i.isPdfFile=ut,i.isValidFetchUrl=mt,i.loadScript=ht,i.noContextMenu=ft,i.setLayerDimensions=Et;var s=o(7),$=o(1);const j="http://www.w3.org/2000/svg",Dt=class Dt{};Jn(Dt,"CSS",96),Jn(Dt,"PDF",72),Jn(Dt,"PDF_TO_CSS_UNITS",Dt.CSS/Dt.PDF);let _e=Dt;i.PixelsPerInch=_e;class et extends s.BaseFilterFactory{constructor({docId:Jt,ownerDocument:vn=globalThis.document}={}){super();Yt(this,Mt);Yt(this,Bt);Yt(this,Gt);Yt(this,qt);Yt(this,ln);Yt(this,yn);Yt(this,cn);Yt(this,It,void 0);Yt(this,Ct,void 0);Yt(this,jt,void 0);Yt(this,Zt,void 0);Yt(this,Xt,void 0);Yt(this,sn,void 0);Yt(this,Ft,void 0);Yt(this,wt,void 0);Yt(this,kt,void 0);Yt(this,At,void 0);Yt(this,Pt,0);wn(this,jt,Jt),wn(this,Zt,vn)}addFilter(Jt){if(!Jt)return"none";let vn=St(this,Mt,_i).get(Jt);if(vn)return vn;let $n,Mn,On,En;if(Jt.length===1){const _n=Jt[0],Zn=new Array(256);for(let bn=0;bn<256;bn++)Zn[bn]=_n[bn]/255;En=$n=Mn=On=Zn.join(",")}else{const[_n,Zn,bn]=Jt,dn=new Array(256),an=new Array(256),In=new Array(256);for(let Dn=0;Dn<256;Dn++)dn[Dn]=_n[Dn]/255,an[Dn]=Zn[Dn]/255,In[Dn]=bn[Dn]/255;$n=dn.join(","),Mn=an.join(","),On=In.join(","),En=`${$n}${Mn}${On}`}if(vn=St(this,Mt,_i).get(En),vn)return St(this,Mt,_i).set(Jt,vn),vn;const Bn=`g_${St(this,jt)}_transfer_map_${ao(this,Pt)._++}`,Hn=`url(#${Bn})`;St(this,Mt,_i).set(Jt,Hn),St(this,Mt,_i).set(En,Hn);const Wn=un(this,qt,mo).call(this,Bn);return un(this,yn,ro).call(this,$n,Mn,On,Wn),Hn}addHCMFilter(Jt,vn){var Zn;const $n=`${Jt}-${vn}`;if(St(this,sn)===$n)return St(this,Ft);if(wn(this,sn,$n),wn(this,Ft,"none"),(Zn=St(this,Xt))==null||Zn.remove(),!Jt||!vn)return St(this,Ft);const Mn=un(this,cn,so).call(this,Jt);Jt=$.Util.makeHexColor(...Mn);const On=un(this,cn,so).call(this,vn);if(vn=$.Util.makeHexColor(...On),St(this,Bt,eo).style.color="",Jt==="#000000"&&vn==="#ffffff"||Jt===vn)return St(this,Ft);const En=new Array(256);for(let bn=0;bn<=255;bn++){const dn=bn/255;En[bn]=dn<=.03928?dn/12.92:((dn+.055)/1.055)**2.4}const Bn=En.join(","),Hn=`g_${St(this,jt)}_hcm_filter`,Wn=wn(this,wt,un(this,qt,mo).call(this,Hn));un(this,yn,ro).call(this,Bn,Bn,Bn,Wn),un(this,Gt,zo).call(this,Wn);const _n=(bn,dn)=>{const an=Mn[bn]/255,In=On[bn]/255,Dn=new Array(dn+1);for(let Xn=0;Xn<=dn;Xn++)Dn[Xn]=an+Xn/dn*(In-an);return Dn.join(",")};return un(this,yn,ro).call(this,_n(0,5),_n(1,5),_n(2,5),Wn),wn(this,Ft,`url(#${Hn})`),St(this,Ft)}addHighlightHCMFilter(Jt,vn,$n,Mn){var In;const On=`${Jt}-${vn}-${$n}-${Mn}`;if(St(this,kt)===On)return St(this,At);if(wn(this,kt,On),wn(this,At,"none"),(In=St(this,wt))==null||In.remove(),!Jt||!vn)return St(this,At);const[En,Bn]=[Jt,vn].map(un(this,cn,so).bind(this));let Hn=Math.round(.2126*En[0]+.7152*En[1]+.0722*En[2]),Wn=Math.round(.2126*Bn[0]+.7152*Bn[1]+.0722*Bn[2]),[_n,Zn]=[$n,Mn].map(un(this,cn,so).bind(this));Wn<Hn&&([Hn,Wn,_n,Zn]=[Wn,Hn,Zn,_n]),St(this,Bt,eo).style.color="";const bn=(Dn,Xn,Yn)=>{const pn=new Array(256),Ht=(Wn-Hn)/Yn,Kt=Dn/255,nn=(Xn-Dn)/(255*Yn);let kn=0;for(let Rn=0;Rn<=Yn;Rn++){const Un=Math.round(Hn+Rn*Ht),Tn=Kt+Rn*nn;for(let Nn=kn;Nn<=Un;Nn++)pn[Nn]=Tn;kn=Un+1}for(let Rn=kn;Rn<256;Rn++)pn[Rn]=pn[kn-1];return pn.join(",")},dn=`g_${St(this,jt)}_hcm_highlight_filter`,an=wn(this,wt,un(this,qt,mo).call(this,dn));return un(this,Gt,zo).call(this,an),un(this,yn,ro).call(this,bn(_n[0],Zn[0],5),bn(_n[1],Zn[1],5),bn(_n[2],Zn[2],5),an),wn(this,At,`url(#${dn})`),St(this,At)}destroy(Jt=!1){Jt&&(St(this,Ft)||St(this,At))||(St(this,Ct)&&(St(this,Ct).parentNode.parentNode.remove(),wn(this,Ct,null)),St(this,It)&&(St(this,It).clear(),wn(this,It,null)),wn(this,Pt,0))}}It=new WeakMap,Ct=new WeakMap,jt=new WeakMap,Zt=new WeakMap,Xt=new WeakMap,sn=new WeakMap,Ft=new WeakMap,wt=new WeakMap,kt=new WeakMap,At=new WeakMap,Pt=new WeakMap,Mt=new WeakSet,_i=function(){return St(this,It)||wn(this,It,new Map)},Bt=new WeakSet,eo=function(){if(!St(this,Ct)){const Jt=St(this,Zt).createElement("div"),{style:vn}=Jt;vn.visibility="hidden",vn.contain="strict",vn.width=vn.height=0,vn.position="absolute",vn.top=vn.left=0,vn.zIndex=-1;const $n=St(this,Zt).createElementNS(j,"svg");$n.setAttribute("width",0),$n.setAttribute("height",0),wn(this,Ct,St(this,Zt).createElementNS(j,"defs")),Jt.append($n),$n.append(St(this,Ct)),St(this,Zt).body.append(Jt)}return St(this,Ct)},Gt=new WeakSet,zo=function(Jt){const vn=St(this,Zt).createElementNS(j,"feColorMatrix");vn.setAttribute("type","matrix"),vn.setAttribute("values","0.2126 0.7152 0.0722 0 0 0.2126 0.7152 0.0722 0 0 0.2126 0.7152 0.0722 0 0 0 0 0 1 0"),Jt.append(vn)},qt=new WeakSet,mo=function(Jt){const vn=St(this,Zt).createElementNS(j,"filter");return vn.setAttribute("color-interpolation-filters","sRGB"),vn.setAttribute("id",Jt),St(this,Bt,eo).append(vn),vn},ln=new WeakSet,go=function(Jt,vn,$n){const Mn=St(this,Zt).createElementNS(j,vn);Mn.setAttribute("type","discrete"),Mn.setAttribute("tableValues",$n),Jt.append(Mn)},yn=new WeakSet,ro=function(Jt,vn,$n,Mn){const On=St(this,Zt).createElementNS(j,"feComponentTransfer");Mn.append(On),un(this,ln,go).call(this,On,"feFuncR",Jt),un(this,ln,go).call(this,On,"feFuncG",vn),un(this,ln,go).call(this,On,"feFuncB",$n)},cn=new WeakSet,so=function(Jt){return St(this,Bt,eo).style.color=Jt,vt(getComputedStyle(St(this,Bt,eo)).getPropertyValue("color"))},i.DOMFilterFactory=et;class tt extends s.BaseCanvasFactory{constructor({ownerDocument:en=globalThis.document}={}){super(),this._document=en}_createCanvas(en,Jt){const vn=this._document.createElement("canvas");return vn.width=en,vn.height=Jt,vn}}i.DOMCanvasFactory=tt;async function nt(hn,en=!1){if(mt(hn,document.baseURI)){const Jt=await fetch(hn);if(!Jt.ok)throw new Error(Jt.statusText);return en?new Uint8Array(await Jt.arrayBuffer()):(0,$.stringToBytes)(await Jt.text())}return new Promise((Jt,vn)=>{const $n=new XMLHttpRequest;$n.open("GET",hn,!0),en&&($n.responseType="arraybuffer"),$n.onreadystatechange=()=>{if($n.readyState===XMLHttpRequest.DONE){if($n.status===200||$n.status===0){let Mn;if(en&&$n.response?Mn=new Uint8Array($n.response):!en&&$n.responseText&&(Mn=(0,$.stringToBytes)($n.responseText)),Mn){Jt(Mn);return}}vn(new Error($n.statusText))}},$n.send(null)})}class at extends s.BaseCMapReaderFactory{_fetchData(en,Jt){return nt(en,this.isCompressed).then(vn=>({cMapData:vn,compressionType:Jt}))}}i.DOMCMapReaderFactory=at;class it extends s.BaseStandardFontDataFactory{_fetchData(en){return nt(en,!0)}}i.DOMStandardFontDataFactory=it;class st extends s.BaseSVGFactory{_createSVG(en){return document.createElementNS(j,en)}}i.DOMSVGFactory=st;class lt{constructor({viewBox:en,scale:Jt,rotation:vn,offsetX:$n=0,offsetY:Mn=0,dontFlip:On=!1}){this.viewBox=en,this.scale=Jt,this.rotation=vn,this.offsetX=$n,this.offsetY=Mn;const En=(en[2]+en[0])/2,Bn=(en[3]+en[1])/2;let Hn,Wn,_n,Zn;switch(vn%=360,vn<0&&(vn+=360),vn){case 180:Hn=-1,Wn=0,_n=0,Zn=1;break;case 90:Hn=0,Wn=1,_n=1,Zn=0;break;case 270:Hn=0,Wn=-1,_n=-1,Zn=0;break;case 0:Hn=1,Wn=0,_n=0,Zn=-1;break;default:throw new Error("PageViewport: Invalid rotation, must be a multiple of 90 degrees.")}On&&(_n=-_n,Zn=-Zn);let bn,dn,an,In;Hn===0?(bn=Math.abs(Bn-en[1])*Jt+$n,dn=Math.abs(En-en[0])*Jt+Mn,an=(en[3]-en[1])*Jt,In=(en[2]-en[0])*Jt):(bn=Math.abs(En-en[0])*Jt+$n,dn=Math.abs(Bn-en[1])*Jt+Mn,an=(en[2]-en[0])*Jt,In=(en[3]-en[1])*Jt),this.transform=[Hn*Jt,Wn*Jt,_n*Jt,Zn*Jt,bn-Hn*Jt*En-_n*Jt*Bn,dn-Wn*Jt*En-Zn*Jt*Bn],this.width=an,this.height=In}get rawDims(){const{viewBox:en}=this;return(0,$.shadow)(this,"rawDims",{pageWidth:en[2]-en[0],pageHeight:en[3]-en[1],pageX:en[0],pageY:en[1]})}clone({scale:en=this.scale,rotation:Jt=this.rotation,offsetX:vn=this.offsetX,offsetY:$n=this.offsetY,dontFlip:Mn=!1}={}){return new lt({viewBox:this.viewBox.slice(),scale:en,rotation:Jt,offsetX:vn,offsetY:$n,dontFlip:Mn})}convertToViewportPoint(en,Jt){return $.Util.applyTransform([en,Jt],this.transform)}convertToViewportRectangle(en){const Jt=$.Util.applyTransform([en[0],en[1]],this.transform),vn=$.Util.applyTransform([en[2],en[3]],this.transform);return[Jt[0],Jt[1],vn[0],vn[1]]}convertToPdfPoint(en,Jt){return $.Util.applyInverseTransform([en,Jt],this.transform)}}i.PageViewport=lt;class ct extends $.BaseException{constructor(en,Jt=0){super(en,"RenderingCancelledException"),this.extraDelay=Jt}}i.RenderingCancelledException=ct;function rt(hn){const en=hn.length;let Jt=0;for(;Jt<en&&hn[Jt].trim()==="";)Jt++;return hn.substring(Jt,Jt+5).toLowerCase()==="data:"}function ut(hn){return typeof hn=="string"&&/\.pdf$/i.test(hn)}function ot(hn,en=!1){return en||([hn]=hn.split(/[#?]/,1)),hn.substring(hn.lastIndexOf("/")+1)}function dt(hn,en="document.pdf"){if(typeof hn!="string")return en;if(rt(hn))return(0,$.warn)('getPdfFilenameFromUrl: ignore "data:"-URL for performance reasons.'),en;const Jt=/^(?:(?:[^:]+:)?\/\/[^/]+)?([^?#]*)(\?[^#]*)?(#.*)?$/,vn=/[^/?#=]+\.pdf\b(?!.*\.pdf\b)/i,$n=Jt.exec(hn);let Mn=vn.exec($n[1])||vn.exec($n[2])||vn.exec($n[3]);if(Mn&&(Mn=Mn[0],Mn.includes("%")))try{Mn=vn.exec(decodeURIComponent(Mn))[0]}catch{}return Mn||en}class pt{constructor(){Jn(this,"started",Object.create(null));Jn(this,"times",[])}time(en){en in this.started&&(0,$.warn)(`Timer is already running for ${en}`),this.started[en]=Date.now()}timeEnd(en){en in this.started||(0,$.warn)(`Timer has not been started for ${en}`),this.times.push({name:en,start:this.started[en],end:Date.now()}),delete this.started[en]}toString(){const en=[];let Jt=0;for(const{name:vn}of this.times)Jt=Math.max(vn.length,Jt);for(const{name:vn,start:$n,end:Mn}of this.times)en.push(`${vn.padEnd(Jt)} ${Mn-$n}ms
`);return en.join("")}}i.StatTimer=pt;function mt(hn,en){try{const{protocol:Jt}=en?new URL(hn,en):new URL(hn);return Jt==="http:"||Jt==="https:"}catch{return!1}}function ft(hn){hn.preventDefault()}function ht(hn,en=!1){return new Promise((Jt,vn)=>{const $n=document.createElement("script");$n.src=hn,$n.onload=function(Mn){en&&$n.remove(),Jt(Mn)},$n.onerror=function(){vn(new Error(`Cannot load script at: ${$n.src}`))},(document.head||document.documentElement).append($n)})}function yt(hn){console.log("Deprecated API usage: "+hn)}let bt;class gt{static toDateObject(en){if(!en||typeof en!="string")return null;bt||(bt=new RegExp("^D:(\\d{4})(\\d{2})?(\\d{2})?(\\d{2})?(\\d{2})?(\\d{2})?([Z|+|-])?(\\d{2})?'?(\\d{2})?'?"));const Jt=bt.exec(en);if(!Jt)return null;const vn=parseInt(Jt[1],10);let $n=parseInt(Jt[2],10);$n=$n>=1&&$n<=12?$n-1:0;let Mn=parseInt(Jt[3],10);Mn=Mn>=1&&Mn<=31?Mn:1;let On=parseInt(Jt[4],10);On=On>=0&&On<=23?On:0;let En=parseInt(Jt[5],10);En=En>=0&&En<=59?En:0;let Bn=parseInt(Jt[6],10);Bn=Bn>=0&&Bn<=59?Bn:0;const Hn=Jt[7]||"Z";let Wn=parseInt(Jt[8],10);Wn=Wn>=0&&Wn<=23?Wn:0;let _n=parseInt(Jt[9],10)||0;return _n=_n>=0&&_n<=59?_n:0,Hn==="-"?(On+=Wn,En+=_n):Hn==="+"&&(On-=Wn,En-=_n),new Date(Date.UTC(vn,$n,Mn,On,En,Bn))}}i.PDFDateString=gt;function xt(hn,{scale:en=1,rotation:Jt=0}){const{width:vn,height:$n}=hn.attributes.style,Mn=[0,0,parseInt(vn),parseInt($n)];return new lt({viewBox:Mn,scale:en,rotation:Jt})}function vt(hn){if(hn.startsWith("#")){const en=parseInt(hn.slice(1),16);return[(en&16711680)>>16,(en&65280)>>8,en&255]}return hn.startsWith("rgb(")?hn.slice(4,-1).split(",").map(en=>parseInt(en)):hn.startsWith("rgba(")?hn.slice(5,-1).split(",").map(en=>parseInt(en)).slice(0,3):((0,$.warn)(`Not a valid color format: "${hn}"`),[0,0,0])}function Lt(hn){const en=document.createElement("span");en.style.visibility="hidden",document.body.append(en);for(const Jt of hn.keys()){en.style.color=Jt;const vn=window.getComputedStyle(en).color;hn.set(Jt,vt(vn))}en.remove()}function $t(hn){const{a:en,b:Jt,c:vn,d:$n,e:Mn,f:On}=hn.getTransform();return[en,Jt,vn,$n,Mn,On]}function Tt(hn){const{a:en,b:Jt,c:vn,d:$n,e:Mn,f:On}=hn.getTransform().invertSelf();return[en,Jt,vn,$n,Mn,On]}function Et(hn,en,Jt=!1,vn=!0){if(en instanceof lt){const{pageWidth:$n,pageHeight:Mn}=en.rawDims,{style:On}=hn,En=$.FeatureTest.isCSSRoundSupported,Bn=`var(--scale-factor) * ${$n}px`,Hn=`var(--scale-factor) * ${Mn}px`,Wn=En?`round(${Bn}, 1px)`:`calc(${Bn})`,_n=En?`round(${Hn}, 1px)`:`calc(${Hn})`;!Jt||en.rotation%180===0?(On.width=Wn,On.height=_n):(On.width=_n,On.height=Wn)}vn&&hn.setAttribute("data-main-rotation",en.rotation)}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.BaseStandardFontDataFactory=i.BaseSVGFactory=i.BaseFilterFactory=i.BaseCanvasFactory=i.BaseCMapReaderFactory=void 0;var s=o(1);class ${constructor(){this.constructor===$&&(0,s.unreachable)("Cannot initialize BaseFilterFactory.")}addFilter(at){return"none"}addHCMFilter(at,it){return"none"}addHighlightHCMFilter(at,it,st,lt){return"none"}destroy(at=!1){}}i.BaseFilterFactory=$;class j{constructor(){this.constructor===j&&(0,s.unreachable)("Cannot initialize BaseCanvasFactory.")}create(at,it){if(at<=0||it<=0)throw new Error("Invalid canvas size");const st=this._createCanvas(at,it);return{canvas:st,context:st.getContext("2d")}}reset(at,it,st){if(!at.canvas)throw new Error("Canvas is not specified");if(it<=0||st<=0)throw new Error("Invalid canvas size");at.canvas.width=it,at.canvas.height=st}destroy(at){if(!at.canvas)throw new Error("Canvas is not specified");at.canvas.width=0,at.canvas.height=0,at.canvas=null,at.context=null}_createCanvas(at,it){(0,s.unreachable)("Abstract method `_createCanvas` called.")}}i.BaseCanvasFactory=j;class _e{constructor({baseUrl:at=null,isCompressed:it=!0}){this.constructor===_e&&(0,s.unreachable)("Cannot initialize BaseCMapReaderFactory."),this.baseUrl=at,this.isCompressed=it}async fetch({name:at}){if(!this.baseUrl)throw new Error('The CMap "baseUrl" parameter must be specified, ensure that the "cMapUrl" and "cMapPacked" API parameters are provided.');if(!at)throw new Error("CMap name must be specified.");const it=this.baseUrl+at+(this.isCompressed?".bcmap":""),st=this.isCompressed?s.CMapCompressionType.BINARY:s.CMapCompressionType.NONE;return this._fetchData(it,st).catch(lt=>{throw new Error(`Unable to load ${this.isCompressed?"binary ":""}CMap at: ${it}`)})}_fetchData(at,it){(0,s.unreachable)("Abstract method `_fetchData` called.")}}i.BaseCMapReaderFactory=_e;class et{constructor({baseUrl:at=null}){this.constructor===et&&(0,s.unreachable)("Cannot initialize BaseStandardFontDataFactory."),this.baseUrl=at}async fetch({filename:at}){if(!this.baseUrl)throw new Error('The standard font "baseUrl" parameter must be specified, ensure that the "standardFontDataUrl" API parameter is provided.');if(!at)throw new Error("Font filename must be specified.");const it=`${this.baseUrl}${at}`;return this._fetchData(it).catch(st=>{throw new Error(`Unable to load font data at: ${it}`)})}_fetchData(at){(0,s.unreachable)("Abstract method `_fetchData` called.")}}i.BaseStandardFontDataFactory=et;class tt{constructor(){this.constructor===tt&&(0,s.unreachable)("Cannot initialize BaseSVGFactory.")}create(at,it,st=!1){if(at<=0||it<=0)throw new Error("Invalid SVG dimensions");const lt=this._createSVG("svg:svg");return lt.setAttribute("version","1.1"),st||(lt.setAttribute("width",`${at}px`),lt.setAttribute("height",`${it}px`)),lt.setAttribute("preserveAspectRatio","none"),lt.setAttribute("viewBox",`0 0 ${at} ${it}`),lt}createElement(at){if(typeof at!="string")throw new Error("Invalid SVG element type");return this._createSVG(at)}_createSVG(at){(0,s.unreachable)("Abstract method `_createSVG` called.")}}i.BaseSVGFactory=tt},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.MurmurHash3_64=void 0;var s=o(1);const $=3285377520,j=4294901760,_e=65535;class et{constructor(nt){this.h1=nt?nt&4294967295:$,this.h2=nt?nt&4294967295:$}update(nt){let at,it;if(typeof nt=="string"){at=new Uint8Array(nt.length*2),it=0;for(let yt=0,bt=nt.length;yt<bt;yt++){const gt=nt.charCodeAt(yt);gt<=255?at[it++]=gt:(at[it++]=gt>>>8,at[it++]=gt&255)}}else if((0,s.isArrayBuffer)(nt))at=nt.slice(),it=at.byteLength;else throw new Error("Wrong data format in MurmurHash3_64_update. Input must be a string or array.");const st=it>>2,lt=it-st*4,ct=new Uint32Array(at.buffer,0,st);let rt=0,ut=0,ot=this.h1,dt=this.h2;const pt=3432918353,mt=461845907,ft=pt&_e,ht=mt&_e;for(let yt=0;yt<st;yt++)yt&1?(rt=ct[yt],rt=rt*pt&j|rt*ft&_e,rt=rt<<15|rt>>>17,rt=rt*mt&j|rt*ht&_e,ot^=rt,ot=ot<<13|ot>>>19,ot=ot*5+3864292196):(ut=ct[yt],ut=ut*pt&j|ut*ft&_e,ut=ut<<15|ut>>>17,ut=ut*mt&j|ut*ht&_e,dt^=ut,dt=dt<<13|dt>>>19,dt=dt*5+3864292196);switch(rt=0,lt){case 3:rt^=at[st*4+2]<<16;case 2:rt^=at[st*4+1]<<8;case 1:rt^=at[st*4],rt=rt*pt&j|rt*ft&_e,rt=rt<<15|rt>>>17,rt=rt*mt&j|rt*ht&_e,st&1?ot^=rt:dt^=rt}this.h1=ot,this.h2=dt}hexdigest(){let nt=this.h1,at=this.h2;return nt^=at>>>1,nt=nt*3981806797&j|nt*36045&_e,at=at*4283543511&j|((at<<16|nt>>>16)*2950163797&j)>>>16,nt^=at>>>1,nt=nt*444984403&j|nt*60499&_e,at=at*3301882366&j|((at<<16|nt>>>16)*3120437893&j)>>>16,nt^=at>>>1,(nt>>>0).toString(16).padStart(8,"0")+(at>>>0).toString(16).padStart(8,"0")}}i.MurmurHash3_64=et},(a,i,o)=>{var _e;Object.defineProperty(i,"__esModule",{value:!0}),i.FontLoader=i.FontFaceObject=void 0;var s=o(1);class ${constructor({ownerDocument:tt=globalThis.document,styleElement:nt=null}){Yt(this,_e,new Set);this._document=tt,this.nativeFontFaces=new Set,this.styleElement=null,this.loadingRequests=[],this.loadTestFontId=0}addNativeFontFace(tt){this.nativeFontFaces.add(tt),this._document.fonts.add(tt)}removeNativeFontFace(tt){this.nativeFontFaces.delete(tt),this._document.fonts.delete(tt)}insertRule(tt){this.styleElement||(this.styleElement=this._document.createElement("style"),this._document.documentElement.getElementsByTagName("head")[0].append(this.styleElement));const nt=this.styleElement.sheet;nt.insertRule(tt,nt.cssRules.length)}clear(){for(const tt of this.nativeFontFaces)this._document.fonts.delete(tt);this.nativeFontFaces.clear(),St(this,_e).clear(),this.styleElement&&(this.styleElement.remove(),this.styleElement=null)}async loadSystemFont(tt){if(!(!tt||St(this,_e).has(tt.loadedName))){if((0,s.assert)(!this.disableFontFace,"loadSystemFont shouldn't be called when `disableFontFace` is set."),this.isFontLoadingAPISupported){const{loadedName:nt,src:at,style:it}=tt,st=new FontFace(nt,at,it);this.addNativeFontFace(st);try{await st.load(),St(this,_e).add(nt)}catch{(0,s.warn)(`Cannot load system font: ${tt.baseFontName}, installing it could help to improve PDF rendering.`),this.removeNativeFontFace(st)}return}(0,s.unreachable)("Not implemented: loadSystemFont without the Font Loading API.")}}async bind(tt){if(tt.attached||tt.missingFile&&!tt.systemFontInfo)return;if(tt.attached=!0,tt.systemFontInfo){await this.loadSystemFont(tt.systemFontInfo);return}if(this.isFontLoadingAPISupported){const at=tt.createNativeFontFace();if(at){this.addNativeFontFace(at);try{await at.loaded}catch(it){throw(0,s.warn)(`Failed to load font '${at.family}': '${it}'.`),tt.disableFontFace=!0,it}}return}const nt=tt.createFontFaceRule();if(nt){if(this.insertRule(nt),this.isSyncFontLoadingSupported)return;await new Promise(at=>{const it=this._queueLoadingCallback(at);this._prepareFontLoadEvent(tt,it)})}}get isFontLoadingAPISupported(){var nt;const tt=!!((nt=this._document)!=null&&nt.fonts);return(0,s.shadow)(this,"isFontLoadingAPISupported",tt)}get isSyncFontLoadingSupported(){let tt=!1;return(s.isNodeJS||typeof navigator<"u"&&/Mozilla\/5.0.*?rv:\d+.*? Gecko/.test(navigator.userAgent))&&(tt=!0),(0,s.shadow)(this,"isSyncFontLoadingSupported",tt)}_queueLoadingCallback(tt){function nt(){for((0,s.assert)(!it.done,"completeRequest() cannot be called twice."),it.done=!0;at.length>0&&at[0].done;){const st=at.shift();setTimeout(st.callback,0)}}const{loadingRequests:at}=this,it={done:!1,complete:nt,callback:tt};return at.push(it),it}get _loadTestFont(){const tt=atob("T1RUTwALAIAAAwAwQ0ZGIDHtZg4AAAOYAAAAgUZGVE1lkzZwAAAEHAAAABxHREVGABQAFQAABDgAAAAeT1MvMlYNYwkAAAEgAAAAYGNtYXABDQLUAAACNAAAAUJoZWFk/xVFDQAAALwAAAA2aGhlYQdkA+oAAAD0AAAAJGhtdHgD6AAAAAAEWAAAAAZtYXhwAAJQAAAAARgAAAAGbmFtZVjmdH4AAAGAAAAAsXBvc3T/hgAzAAADeAAAACAAAQAAAAEAALZRFsRfDzz1AAsD6AAAAADOBOTLAAAAAM4KHDwAAAAAA+gDIQAAAAgAAgAAAAAAAAABAAADIQAAAFoD6AAAAAAD6AABAAAAAAAAAAAAAAAAAAAAAQAAUAAAAgAAAAQD6AH0AAUAAAKKArwAAACMAooCvAAAAeAAMQECAAACAAYJAAAAAAAAAAAAAQAAAAAAAAAAAAAAAFBmRWQAwAAuAC4DIP84AFoDIQAAAAAAAQAAAAAAAAAAACAAIAABAAAADgCuAAEAAAAAAAAAAQAAAAEAAAAAAAEAAQAAAAEAAAAAAAIAAQAAAAEAAAAAAAMAAQAAAAEAAAAAAAQAAQAAAAEAAAAAAAUAAQAAAAEAAAAAAAYAAQAAAAMAAQQJAAAAAgABAAMAAQQJAAEAAgABAAMAAQQJAAIAAgABAAMAAQQJAAMAAgABAAMAAQQJAAQAAgABAAMAAQQJAAUAAgABAAMAAQQJAAYAAgABWABYAAAAAAAAAwAAAAMAAAAcAAEAAAAAADwAAwABAAAAHAAEACAAAAAEAAQAAQAAAC7//wAAAC7////TAAEAAAAAAAABBgAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAAAAAAAD/gwAyAAAAAQAAAAAAAAAAAAAAAAAAAAABAAQEAAEBAQJYAAEBASH4DwD4GwHEAvgcA/gXBIwMAYuL+nz5tQXkD5j3CBLnEQACAQEBIVhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYAAABAQAADwACAQEEE/t3Dov6fAH6fAT+fPp8+nwHDosMCvm1Cvm1DAz6fBQAAAAAAAABAAAAAMmJbzEAAAAAzgTjFQAAAADOBOQpAAEAAAAAAAAADAAUAAQAAAABAAAAAgABAAAAAAAAAAAD6AAAAAAAAA==");return(0,s.shadow)(this,"_loadTestFont",tt)}_prepareFontLoadEvent(tt,nt){function at(vt,Lt){return vt.charCodeAt(Lt)<<24|vt.charCodeAt(Lt+1)<<16|vt.charCodeAt(Lt+2)<<8|vt.charCodeAt(Lt+3)&255}function it(vt,Lt,$t,Tt){const Et=vt.substring(0,Lt),Dt=vt.substring(Lt+$t);return Et+Tt+Dt}let st,lt;const ct=this._document.createElement("canvas");ct.width=1,ct.height=1;const rt=ct.getContext("2d");let ut=0;function ot(vt,Lt){if(++ut>30){(0,s.warn)("Load test font never loaded."),Lt();return}if(rt.font="30px "+vt,rt.fillText(".",0,20),rt.getImageData(0,0,1,1).data[3]>0){Lt();return}setTimeout(ot.bind(null,vt,Lt))}const dt=`lt${Date.now()}${this.loadTestFontId++}`;let pt=this._loadTestFont;pt=it(pt,976,dt.length,dt);const ft=16,ht=1482184792;let yt=at(pt,ft);for(st=0,lt=dt.length-3;st<lt;st+=4)yt=yt-ht+at(dt,st)|0;st<dt.length&&(yt=yt-ht+at(dt+"XXX",st)|0),pt=it(pt,ft,4,(0,s.string32)(yt));const bt=`url(data:font/opentype;base64,${btoa(pt)});`,gt=`@font-face {font-family:"${dt}";src:${bt}}`;this.insertRule(gt);const xt=this._document.createElement("div");xt.style.visibility="hidden",xt.style.width=xt.style.height="10px",xt.style.position="absolute",xt.style.top=xt.style.left="0px";for(const vt of[tt.loadedName,dt]){const Lt=this._document.createElement("span");Lt.textContent="Hi",Lt.style.fontFamily=vt,xt.append(Lt)}this._document.body.append(xt),ot(dt,()=>{xt.remove(),nt.complete()})}}_e=new WeakMap,i.FontLoader=$;class j{constructor(tt,{isEvalSupported:nt=!0,disableFontFace:at=!1,ignoreErrors:it=!1,inspectFont:st=null}){this.compiledGlyphs=Object.create(null);for(const lt in tt)this[lt]=tt[lt];this.isEvalSupported=nt!==!1,this.disableFontFace=at===!0,this.ignoreErrors=it===!0,this._inspectFont=st}createNativeFontFace(){var nt;if(!this.data||this.disableFontFace)return null;let tt;if(!this.cssFontInfo)tt=new FontFace(this.loadedName,this.data,{});else{const at={weight:this.cssFontInfo.fontWeight};this.cssFontInfo.italicAngle&&(at.style=`oblique ${this.cssFontInfo.italicAngle}deg`),tt=new FontFace(this.cssFontInfo.fontFamily,this.data,at)}return(nt=this._inspectFont)==null||nt.call(this,this),tt}createFontFaceRule(){var it;if(!this.data||this.disableFontFace)return null;const tt=(0,s.bytesToString)(this.data),nt=`url(data:${this.mimetype};base64,${btoa(tt)});`;let at;if(!this.cssFontInfo)at=`@font-face {font-family:"${this.loadedName}";src:${nt}}`;else{let st=`font-weight: ${this.cssFontInfo.fontWeight};`;this.cssFontInfo.italicAngle&&(st+=`font-style: oblique ${this.cssFontInfo.italicAngle}deg;`),at=`@font-face {font-family:"${this.cssFontInfo.fontFamily}";${st}src:${nt}}`}return(it=this._inspectFont)==null||it.call(this,this,nt),at}getPathGenerator(tt,nt){if(this.compiledGlyphs[nt]!==void 0)return this.compiledGlyphs[nt];let at;try{at=tt.get(this.loadedName+"_path_"+nt)}catch(it){if(!this.ignoreErrors)throw it;return(0,s.warn)(`getPathGenerator - ignoring character: "${it}".`),this.compiledGlyphs[nt]=function(st,lt){}}if(this.isEvalSupported&&s.FeatureTest.isEvalSupported){const it=[];for(const st of at){const lt=st.args!==void 0?st.args.join(","):"";it.push("c.",st.cmd,"(",lt,`);
`)}return this.compiledGlyphs[nt]=new Function("c","size",it.join(""))}return this.compiledGlyphs[nt]=function(it,st){for(const lt of at)lt.cmd==="scale"&&(lt.args=[st,-st]),it[lt.cmd].apply(it,lt.args)}}}i.FontFaceObject=j},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.NodeStandardFontDataFactory=i.NodeFilterFactory=i.NodeCanvasFactory=i.NodeCMapReaderFactory=void 0;var s=o(7);o(1);const $=function(nt){return new Promise((at,it)=>{require$$5.readFile(nt,(lt,ct)=>{if(lt||!ct){it(new Error(lt));return}at(new Uint8Array(ct))})})};class j extends s.BaseFilterFactory{}i.NodeFilterFactory=j;class _e extends s.BaseCanvasFactory{_createCanvas(at,it){return require$$5.createCanvas(at,it)}}i.NodeCanvasFactory=_e;class et extends s.BaseCMapReaderFactory{_fetchData(at,it){return $(at).then(st=>({cMapData:st,compressionType:it}))}}i.NodeCMapReaderFactory=et;class tt extends s.BaseStandardFontDataFactory{_fetchData(at){return $(at)}}i.NodeStandardFontDataFactory=tt},(a,i,o)=>{var Ct,Bo,Zt,Zo;Object.defineProperty(i,"__esModule",{value:!0}),i.CanvasGraphics=void 0;var s=o(1),$=o(6),j=o(12),_e=o(13);const et=16,tt=100,nt=4096,at=15,it=10,st=1e3,lt=16;function ct(Ft,wt){if(Ft._removeMirroring)throw new Error("Context is already forwarding operations.");Ft.__originalSave=Ft.save,Ft.__originalRestore=Ft.restore,Ft.__originalRotate=Ft.rotate,Ft.__originalScale=Ft.scale,Ft.__originalTranslate=Ft.translate,Ft.__originalTransform=Ft.transform,Ft.__originalSetTransform=Ft.setTransform,Ft.__originalResetTransform=Ft.resetTransform,Ft.__originalClip=Ft.clip,Ft.__originalMoveTo=Ft.moveTo,Ft.__originalLineTo=Ft.lineTo,Ft.__originalBezierCurveTo=Ft.bezierCurveTo,Ft.__originalRect=Ft.rect,Ft.__originalClosePath=Ft.closePath,Ft.__originalBeginPath=Ft.beginPath,Ft._removeMirroring=()=>{Ft.save=Ft.__originalSave,Ft.restore=Ft.__originalRestore,Ft.rotate=Ft.__originalRotate,Ft.scale=Ft.__originalScale,Ft.translate=Ft.__originalTranslate,Ft.transform=Ft.__originalTransform,Ft.setTransform=Ft.__originalSetTransform,Ft.resetTransform=Ft.__originalResetTransform,Ft.clip=Ft.__originalClip,Ft.moveTo=Ft.__originalMoveTo,Ft.lineTo=Ft.__originalLineTo,Ft.bezierCurveTo=Ft.__originalBezierCurveTo,Ft.rect=Ft.__originalRect,Ft.closePath=Ft.__originalClosePath,Ft.beginPath=Ft.__originalBeginPath,delete Ft._removeMirroring},Ft.save=function(){wt.save(),this.__originalSave()},Ft.restore=function(){wt.restore(),this.__originalRestore()},Ft.translate=function(At,Pt){wt.translate(At,Pt),this.__originalTranslate(At,Pt)},Ft.scale=function(At,Pt){wt.scale(At,Pt),this.__originalScale(At,Pt)},Ft.transform=function(At,Pt,Mt,Ot,Bt,zt){wt.transform(At,Pt,Mt,Ot,Bt,zt),this.__originalTransform(At,Pt,Mt,Ot,Bt,zt)},Ft.setTransform=function(At,Pt,Mt,Ot,Bt,zt){wt.setTransform(At,Pt,Mt,Ot,Bt,zt),this.__originalSetTransform(At,Pt,Mt,Ot,Bt,zt)},Ft.resetTransform=function(){wt.resetTransform(),this.__originalResetTransform()},Ft.rotate=function(At){wt.rotate(At),this.__originalRotate(At)},Ft.clip=function(At){wt.clip(At),this.__originalClip(At)},Ft.moveTo=function(kt,At){wt.moveTo(kt,At),this.__originalMoveTo(kt,At)},Ft.lineTo=function(kt,At){wt.lineTo(kt,At),this.__originalLineTo(kt,At)},Ft.bezierCurveTo=function(kt,At,Pt,Mt,Ot,Bt){wt.bezierCurveTo(kt,At,Pt,Mt,Ot,Bt),this.__originalBezierCurveTo(kt,At,Pt,Mt,Ot,Bt)},Ft.rect=function(kt,At,Pt,Mt){wt.rect(kt,At,Pt,Mt),this.__originalRect(kt,At,Pt,Mt)},Ft.closePath=function(){wt.closePath(),this.__originalClosePath()},Ft.beginPath=function(){wt.beginPath(),this.__originalBeginPath()}}class rt{constructor(wt){this.canvasFactory=wt,this.cache=Object.create(null)}getCanvas(wt,kt,At){let Pt;return this.cache[wt]!==void 0?(Pt=this.cache[wt],this.canvasFactory.reset(Pt,kt,At)):(Pt=this.canvasFactory.create(kt,At),this.cache[wt]=Pt),Pt}delete(wt){delete this.cache[wt]}clear(){for(const wt in this.cache){const kt=this.cache[wt];this.canvasFactory.destroy(kt),delete this.cache[wt]}}}function ut(Ft,wt,kt,At,Pt,Mt,Ot,Bt,zt,Gt){const[Wt,qt,tn,ln,gn,yn]=(0,$.getCurrentTransform)(Ft);if(qt===0&&tn===0){const xn=Ot*Wt+gn,hn=Math.round(xn),en=Bt*ln+yn,Jt=Math.round(en),vn=(Ot+zt)*Wt+gn,$n=Math.abs(Math.round(vn)-hn)||1,Mn=(Bt+Gt)*ln+yn,On=Math.abs(Math.round(Mn)-Jt)||1;return Ft.setTransform(Math.sign(Wt),0,0,Math.sign(ln),hn,Jt),Ft.drawImage(wt,kt,At,Pt,Mt,0,0,$n,On),Ft.setTransform(Wt,qt,tn,ln,gn,yn),[$n,On]}if(Wt===0&&ln===0){const xn=Bt*tn+gn,hn=Math.round(xn),en=Ot*qt+yn,Jt=Math.round(en),vn=(Bt+Gt)*tn+gn,$n=Math.abs(Math.round(vn)-hn)||1,Mn=(Ot+zt)*qt+yn,On=Math.abs(Math.round(Mn)-Jt)||1;return Ft.setTransform(0,Math.sign(qt),Math.sign(tn),0,hn,Jt),Ft.drawImage(wt,kt,At,Pt,Mt,0,0,On,$n),Ft.setTransform(Wt,qt,tn,ln,gn,yn),[On,$n]}Ft.drawImage(wt,kt,At,Pt,Mt,Ot,Bt,zt,Gt);const Pn=Math.hypot(Wt,qt),cn=Math.hypot(tn,ln);return[Pn*zt,cn*Gt]}function ot(Ft){const{width:wt,height:kt}=Ft;if(wt>st||kt>st)return null;const At=1e3,Pt=new Uint8Array([0,2,4,0,1,0,5,4,8,10,0,8,0,2,1,0]),Mt=wt+1;let Ot=new Uint8Array(Mt*(kt+1)),Bt,zt,Gt;const Wt=wt+7&-8;let qt=new Uint8Array(Wt*kt),tn=0;for(const cn of Ft.data){let xn=128;for(;xn>0;)qt[tn++]=cn&xn?0:255,xn>>=1}let ln=0;for(tn=0,qt[tn]!==0&&(Ot[0]=1,++ln),zt=1;zt<wt;zt++)qt[tn]!==qt[tn+1]&&(Ot[zt]=qt[tn]?2:1,++ln),tn++;for(qt[tn]!==0&&(Ot[zt]=2,++ln),Bt=1;Bt<kt;Bt++){tn=Bt*Wt,Gt=Bt*Mt,qt[tn-Wt]!==qt[tn]&&(Ot[Gt]=qt[tn]?1:8,++ln);let cn=(qt[tn]?4:0)+(qt[tn-Wt]?8:0);for(zt=1;zt<wt;zt++)cn=(cn>>2)+(qt[tn+1]?4:0)+(qt[tn-Wt+1]?8:0),Pt[cn]&&(Ot[Gt+zt]=Pt[cn],++ln),tn++;if(qt[tn-Wt]!==qt[tn]&&(Ot[Gt+zt]=qt[tn]?2:4,++ln),ln>At)return null}for(tn=Wt*(kt-1),Gt=Bt*Mt,qt[tn]!==0&&(Ot[Gt]=8,++ln),zt=1;zt<wt;zt++)qt[tn]!==qt[tn+1]&&(Ot[Gt+zt]=qt[tn]?4:8,++ln),tn++;if(qt[tn]!==0&&(Ot[Gt+zt]=4,++ln),ln>At)return null;const gn=new Int32Array([0,Mt,-1,0,-Mt,0,0,0,1]),yn=new Path2D;for(Bt=0;ln&&Bt<=kt;Bt++){let cn=Bt*Mt;const xn=cn+wt;for(;cn<xn&&!Ot[cn];)cn++;if(cn===xn)continue;yn.moveTo(cn%Mt,Bt);const hn=cn;let en=Ot[cn];do{const Jt=gn[en];do cn+=Jt;while(!Ot[cn]);const vn=Ot[cn];vn!==5&&vn!==10?(en=vn,Ot[cn]=0):(en=vn&51*en>>4,Ot[cn]&=en>>2|en<<2),yn.lineTo(cn%Mt,cn/Mt|0),Ot[cn]||--ln}while(hn!==cn);--Bt}return qt=null,Ot=null,function(cn){cn.save(),cn.scale(1/wt,-1/kt),cn.translate(0,-kt),cn.fill(yn),cn.beginPath(),cn.restore()}}class dt{constructor(wt,kt){this.alphaIsShape=!1,this.fontSize=0,this.fontSizeScale=1,this.textMatrix=s.IDENTITY_MATRIX,this.textMatrixScale=1,this.fontMatrix=s.FONT_IDENTITY_MATRIX,this.leading=0,this.x=0,this.y=0,this.lineX=0,this.lineY=0,this.charSpacing=0,this.wordSpacing=0,this.textHScale=1,this.textRenderingMode=s.TextRenderingMode.FILL,this.textRise=0,this.fillColor="#000000",this.strokeColor="#000000",this.patternFill=!1,this.fillAlpha=1,this.strokeAlpha=1,this.lineWidth=1,this.activeSMask=null,this.transferMaps="none",this.startNewPathAndClipBox([0,0,wt,kt])}clone(){const wt=Object.create(this);return wt.clipBox=this.clipBox.slice(),wt}setCurrentPoint(wt,kt){this.x=wt,this.y=kt}updatePathMinMax(wt,kt,At){[kt,At]=s.Util.applyTransform([kt,At],wt),this.minX=Math.min(this.minX,kt),this.minY=Math.min(this.minY,At),this.maxX=Math.max(this.maxX,kt),this.maxY=Math.max(this.maxY,At)}updateRectMinMax(wt,kt){const At=s.Util.applyTransform(kt,wt),Pt=s.Util.applyTransform(kt.slice(2),wt);this.minX=Math.min(this.minX,At[0],Pt[0]),this.minY=Math.min(this.minY,At[1],Pt[1]),this.maxX=Math.max(this.maxX,At[0],Pt[0]),this.maxY=Math.max(this.maxY,At[1],Pt[1])}updateScalingPathMinMax(wt,kt){s.Util.scaleMinMax(wt,kt),this.minX=Math.min(this.minX,kt[0]),this.maxX=Math.max(this.maxX,kt[1]),this.minY=Math.min(this.minY,kt[2]),this.maxY=Math.max(this.maxY,kt[3])}updateCurvePathMinMax(wt,kt,At,Pt,Mt,Ot,Bt,zt,Gt,Wt){const qt=s.Util.bezierBoundingBox(kt,At,Pt,Mt,Ot,Bt,zt,Gt);if(Wt){Wt[0]=Math.min(Wt[0],qt[0],qt[2]),Wt[1]=Math.max(Wt[1],qt[0],qt[2]),Wt[2]=Math.min(Wt[2],qt[1],qt[3]),Wt[3]=Math.max(Wt[3],qt[1],qt[3]);return}this.updateRectMinMax(wt,qt)}getPathBoundingBox(wt=j.PathType.FILL,kt=null){const At=[this.minX,this.minY,this.maxX,this.maxY];if(wt===j.PathType.STROKE){kt||(0,s.unreachable)("Stroke bounding box must include transform.");const Pt=s.Util.singularValueDecompose2dScale(kt),Mt=Pt[0]*this.lineWidth/2,Ot=Pt[1]*this.lineWidth/2;At[0]-=Mt,At[1]-=Ot,At[2]+=Mt,At[3]+=Ot}return At}updateClipFromPath(){const wt=s.Util.intersect(this.clipBox,this.getPathBoundingBox());this.startNewPathAndClipBox(wt||[0,0,0,0])}isEmptyClip(){return this.minX===1/0}startNewPathAndClipBox(wt){this.clipBox=wt,this.minX=1/0,this.minY=1/0,this.maxX=0,this.maxY=0}getClippedPathBoundingBox(wt=j.PathType.FILL,kt=null){return s.Util.intersect(this.clipBox,this.getPathBoundingBox(wt,kt))}}function pt(Ft,wt){if(typeof ImageData<"u"&&wt instanceof ImageData){Ft.putImageData(wt,0,0);return}const kt=wt.height,At=wt.width,Pt=kt%lt,Mt=(kt-Pt)/lt,Ot=Pt===0?Mt:Mt+1,Bt=Ft.createImageData(At,lt);let zt=0,Gt;const Wt=wt.data,qt=Bt.data;let tn,ln,gn,yn;if(wt.kind===s.ImageKind.GRAYSCALE_1BPP){const Pn=Wt.byteLength,cn=new Uint32Array(qt.buffer,0,qt.byteLength>>2),xn=cn.length,hn=At+7>>3,en=4294967295,Jt=s.FeatureTest.isLittleEndian?4278190080:255;for(tn=0;tn<Ot;tn++){for(gn=tn<Mt?lt:Pt,Gt=0,ln=0;ln<gn;ln++){const vn=Pn-zt;let $n=0;const Mn=vn>hn?At:vn*8-7,On=Mn&-8;let En=0,Bn=0;for(;$n<On;$n+=8)Bn=Wt[zt++],cn[Gt++]=Bn&128?en:Jt,cn[Gt++]=Bn&64?en:Jt,cn[Gt++]=Bn&32?en:Jt,cn[Gt++]=Bn&16?en:Jt,cn[Gt++]=Bn&8?en:Jt,cn[Gt++]=Bn&4?en:Jt,cn[Gt++]=Bn&2?en:Jt,cn[Gt++]=Bn&1?en:Jt;for(;$n<Mn;$n++)En===0&&(Bn=Wt[zt++],En=128),cn[Gt++]=Bn&En?en:Jt,En>>=1}for(;Gt<xn;)cn[Gt++]=0;Ft.putImageData(Bt,0,tn*lt)}}else if(wt.kind===s.ImageKind.RGBA_32BPP){for(ln=0,yn=At*lt*4,tn=0;tn<Mt;tn++)qt.set(Wt.subarray(zt,zt+yn)),zt+=yn,Ft.putImageData(Bt,0,ln),ln+=lt;tn<Ot&&(yn=At*Pt*4,qt.set(Wt.subarray(zt,zt+yn)),Ft.putImageData(Bt,0,ln))}else if(wt.kind===s.ImageKind.RGB_24BPP)for(gn=lt,yn=At*gn,tn=0;tn<Ot;tn++){for(tn>=Mt&&(gn=Pt,yn=At*gn),Gt=0,ln=yn;ln--;)qt[Gt++]=Wt[zt++],qt[Gt++]=Wt[zt++],qt[Gt++]=Wt[zt++],qt[Gt++]=255;Ft.putImageData(Bt,0,tn*lt)}else throw new Error(`bad image kind: ${wt.kind}`)}function mt(Ft,wt){if(wt.bitmap){Ft.drawImage(wt.bitmap,0,0);return}const kt=wt.height,At=wt.width,Pt=kt%lt,Mt=(kt-Pt)/lt,Ot=Pt===0?Mt:Mt+1,Bt=Ft.createImageData(At,lt);let zt=0;const Gt=wt.data,Wt=Bt.data;for(let qt=0;qt<Ot;qt++){const tn=qt<Mt?lt:Pt;({srcPos:zt}=(0,_e.convertBlackAndWhiteToRGBA)({src:Gt,srcPos:zt,dest:Wt,width:At,height:tn,nonBlackColor:0})),Ft.putImageData(Bt,0,qt*lt)}}function ft(Ft,wt){const kt=["strokeStyle","fillStyle","fillRule","globalAlpha","lineWidth","lineCap","lineJoin","miterLimit","globalCompositeOperation","font","filter"];for(const At of kt)Ft[At]!==void 0&&(wt[At]=Ft[At]);Ft.setLineDash!==void 0&&(wt.setLineDash(Ft.getLineDash()),wt.lineDashOffset=Ft.lineDashOffset)}function ht(Ft){if(Ft.strokeStyle=Ft.fillStyle="#000000",Ft.fillRule="nonzero",Ft.globalAlpha=1,Ft.lineWidth=1,Ft.lineCap="butt",Ft.lineJoin="miter",Ft.miterLimit=10,Ft.globalCompositeOperation="source-over",Ft.font="10px sans-serif",Ft.setLineDash!==void 0&&(Ft.setLineDash([]),Ft.lineDashOffset=0),!s.isNodeJS){const{filter:wt}=Ft;wt!=="none"&&wt!==""&&(Ft.filter="none")}}function yt(Ft,wt,kt,At){const Pt=Ft.length;for(let Mt=3;Mt<Pt;Mt+=4){const Ot=Ft[Mt];if(Ot===0)Ft[Mt-3]=wt,Ft[Mt-2]=kt,Ft[Mt-1]=At;else if(Ot<255){const Bt=255-Ot;Ft[Mt-3]=Ft[Mt-3]*Ot+wt*Bt>>8,Ft[Mt-2]=Ft[Mt-2]*Ot+kt*Bt>>8,Ft[Mt-1]=Ft[Mt-1]*Ot+At*Bt>>8}}}function bt(Ft,wt,kt){const At=Ft.length,Pt=1/255;for(let Mt=3;Mt<At;Mt+=4){const Ot=kt?kt[Ft[Mt]]:Ft[Mt];wt[Mt]=wt[Mt]*Ot*Pt|0}}function gt(Ft,wt,kt){const At=Ft.length;for(let Pt=3;Pt<At;Pt+=4){const Mt=Ft[Pt-3]*77+Ft[Pt-2]*152+Ft[Pt-1]*28;wt[Pt]=kt?wt[Pt]*kt[Mt>>8]>>8:wt[Pt]*Mt>>16}}function xt(Ft,wt,kt,At,Pt,Mt,Ot,Bt,zt,Gt,Wt){const qt=!!Mt,tn=qt?Mt[0]:0,ln=qt?Mt[1]:0,gn=qt?Mt[2]:0,yn=Pt==="Luminosity"?gt:bt,cn=Math.min(At,Math.ceil(1048576/kt));for(let xn=0;xn<At;xn+=cn){const hn=Math.min(cn,At-xn),en=Ft.getImageData(Bt-Gt,xn+(zt-Wt),kt,hn),Jt=wt.getImageData(Bt,xn+zt,kt,hn);qt&&yt(en.data,tn,ln,gn),yn(en.data,Jt.data,Ot),wt.putImageData(Jt,Bt,xn+zt)}}function vt(Ft,wt,kt,At){const Pt=At[0],Mt=At[1],Ot=At[2]-Pt,Bt=At[3]-Mt;Ot===0||Bt===0||(xt(wt.context,kt,Ot,Bt,wt.subtype,wt.backdrop,wt.transferMap,Pt,Mt,wt.offsetX,wt.offsetY),Ft.save(),Ft.globalAlpha=1,Ft.globalCompositeOperation="source-over",Ft.setTransform(1,0,0,1,0,0),Ft.drawImage(kt.canvas,0,0),Ft.restore())}function Lt(Ft,wt){const kt=s.Util.singularValueDecompose2dScale(Ft);kt[0]=Math.fround(kt[0]),kt[1]=Math.fround(kt[1]);const At=Math.fround((globalThis.devicePixelRatio||1)*$.PixelsPerInch.PDF_TO_CSS_UNITS);return wt!==void 0?wt:kt[0]<=At||kt[1]<=At}const $t=["butt","round","square"],Tt=["miter","round","bevel"],Et={},Dt={},sn=class sn{constructor(wt,kt,At,Pt,Mt,{optionalContentConfig:Ot,markedContentStack:Bt=null},zt,Gt){Yt(this,Ct);Yt(this,Zt);this.ctx=wt,this.current=new dt(this.ctx.canvas.width,this.ctx.canvas.height),this.stateStack=[],this.pendingClip=null,this.pendingEOFill=!1,this.res=null,this.xobjs=null,this.commonObjs=kt,this.objs=At,this.canvasFactory=Pt,this.filterFactory=Mt,this.groupStack=[],this.processingType3=null,this.baseTransform=null,this.baseTransformStack=[],this.groupLevel=0,this.smaskStack=[],this.smaskCounter=0,this.tempSMask=null,this.suspendedCtx=null,this.contentVisible=!0,this.markedContentStack=Bt||[],this.optionalContentConfig=Ot,this.cachedCanvases=new rt(this.canvasFactory),this.cachedPatterns=new Map,this.annotationCanvasMap=zt,this.viewportScale=1,this.outputScaleX=1,this.outputScaleY=1,this.pageColors=Gt,this._cachedScaleForStroking=[-1,0],this._cachedGetSinglePixelWidth=null,this._cachedBitmapsMap=new Map}getObject(wt,kt=null){return typeof wt=="string"?wt.startsWith("g_")?this.commonObjs.get(wt):this.objs.get(wt):kt}beginDrawing({transform:wt,viewport:kt,transparency:At=!1,background:Pt=null}){const Mt=this.ctx.canvas.width,Ot=this.ctx.canvas.height,Bt=this.ctx.fillStyle;if(this.ctx.fillStyle=Pt||"#ffffff",this.ctx.fillRect(0,0,Mt,Ot),this.ctx.fillStyle=Bt,At){const zt=this.cachedCanvases.getCanvas("transparent",Mt,Ot);this.compositeCtx=this.ctx,this.transparentCanvas=zt.canvas,this.ctx=zt.context,this.ctx.save(),this.ctx.transform(...(0,$.getCurrentTransform)(this.compositeCtx))}this.ctx.save(),ht(this.ctx),wt&&(this.ctx.transform(...wt),this.outputScaleX=wt[0],this.outputScaleY=wt[0]),this.ctx.transform(...kt.transform),this.viewportScale=kt.scale,this.baseTransform=(0,$.getCurrentTransform)(this.ctx)}executeOperatorList(wt,kt,At,Pt){const Mt=wt.argsArray,Ot=wt.fnArray;let Bt=kt||0;const zt=Mt.length;if(zt===Bt)return Bt;const Gt=zt-Bt>it&&typeof At=="function",Wt=Gt?Date.now()+at:0;let qt=0;const tn=this.commonObjs,ln=this.objs;let gn;for(;;){if(Pt!==void 0&&Bt===Pt.nextBreakPoint)return Pt.breakIt(Bt,At),Bt;if(gn=Ot[Bt],gn!==s.OPS.dependency)this[gn].apply(this,Mt[Bt]);else for(const yn of Mt[Bt]){const Pn=yn.startsWith("g_")?tn:ln;if(!Pn.has(yn))return Pn.get(yn,At),Bt}if(Bt++,Bt===zt)return Bt;if(Gt&&++qt>it){if(Date.now()>Wt)return At(),Bt;qt=0}}}endDrawing(){un(this,Ct,Bo).call(this),this.cachedCanvases.clear(),this.cachedPatterns.clear();for(const wt of this._cachedBitmapsMap.values()){for(const kt of wt.values())typeof HTMLCanvasElement<"u"&&kt instanceof HTMLCanvasElement&&(kt.width=kt.height=0);wt.clear()}this._cachedBitmapsMap.clear(),un(this,Zt,Zo).call(this)}_scaleImage(wt,kt){const At=wt.width,Pt=wt.height;let Mt=Math.max(Math.hypot(kt[0],kt[1]),1),Ot=Math.max(Math.hypot(kt[2],kt[3]),1),Bt=At,zt=Pt,Gt="prescale1",Wt,qt;for(;Mt>2&&Bt>1||Ot>2&&zt>1;){let tn=Bt,ln=zt;Mt>2&&Bt>1&&(tn=Bt>=16384?Math.floor(Bt/2)-1||1:Math.ceil(Bt/2),Mt/=Bt/tn),Ot>2&&zt>1&&(ln=zt>=16384?Math.floor(zt/2)-1||1:Math.ceil(zt)/2,Ot/=zt/ln),Wt=this.cachedCanvases.getCanvas(Gt,tn,ln),qt=Wt.context,qt.clearRect(0,0,tn,ln),qt.drawImage(wt,0,0,Bt,zt,0,0,tn,ln),wt=Wt.canvas,Bt=tn,zt=ln,Gt=Gt==="prescale1"?"prescale2":"prescale1"}return{img:wt,paintWidth:Bt,paintHeight:zt}}_createMaskCanvas(wt){const kt=this.ctx,{width:At,height:Pt}=wt,Mt=this.current.fillColor,Ot=this.current.patternFill,Bt=(0,$.getCurrentTransform)(kt);let zt,Gt,Wt,qt;if((wt.bitmap||wt.data)&&wt.count>1){const $n=wt.bitmap||wt.data.buffer;Gt=JSON.stringify(Ot?Bt:[Bt.slice(0,4),Mt]),zt=this._cachedBitmapsMap.get($n),zt||(zt=new Map,this._cachedBitmapsMap.set($n,zt));const Mn=zt.get(Gt);if(Mn&&!Ot){const On=Math.round(Math.min(Bt[0],Bt[2])+Bt[4]),En=Math.round(Math.min(Bt[1],Bt[3])+Bt[5]);return{canvas:Mn,offsetX:On,offsetY:En}}Wt=Mn}Wt||(qt=this.cachedCanvases.getCanvas("maskCanvas",At,Pt),mt(qt.context,wt));let tn=s.Util.transform(Bt,[1/At,0,0,-1/Pt,0,0]);tn=s.Util.transform(tn,[1,0,0,1,0,-Pt]);const ln=s.Util.applyTransform([0,0],tn),gn=s.Util.applyTransform([At,Pt],tn),yn=s.Util.normalizeRect([ln[0],ln[1],gn[0],gn[1]]),Pn=Math.round(yn[2]-yn[0])||1,cn=Math.round(yn[3]-yn[1])||1,xn=this.cachedCanvases.getCanvas("fillCanvas",Pn,cn),hn=xn.context,en=Math.min(ln[0],gn[0]),Jt=Math.min(ln[1],gn[1]);hn.translate(-en,-Jt),hn.transform(...tn),Wt||(Wt=this._scaleImage(qt.canvas,(0,$.getCurrentTransformInverse)(hn)),Wt=Wt.img,zt&&Ot&&zt.set(Gt,Wt)),hn.imageSmoothingEnabled=Lt((0,$.getCurrentTransform)(hn),wt.interpolate),ut(hn,Wt,0,0,Wt.width,Wt.height,0,0,At,Pt),hn.globalCompositeOperation="source-in";const vn=s.Util.transform((0,$.getCurrentTransformInverse)(hn),[1,0,0,1,-en,-Jt]);return hn.fillStyle=Ot?Mt.getPattern(kt,this,vn,j.PathType.FILL):Mt,hn.fillRect(0,0,At,Pt),zt&&!Ot&&(this.cachedCanvases.delete("fillCanvas"),zt.set(Gt,xn.canvas)),{canvas:xn.canvas,offsetX:Math.round(en),offsetY:Math.round(Jt)}}setLineWidth(wt){wt!==this.current.lineWidth&&(this._cachedScaleForStroking[0]=-1),this.current.lineWidth=wt,this.ctx.lineWidth=wt}setLineCap(wt){this.ctx.lineCap=$t[wt]}setLineJoin(wt){this.ctx.lineJoin=Tt[wt]}setMiterLimit(wt){this.ctx.miterLimit=wt}setDash(wt,kt){const At=this.ctx;At.setLineDash!==void 0&&(At.setLineDash(wt),At.lineDashOffset=kt)}setRenderingIntent(wt){}setFlatness(wt){}setGState(wt){for(const[kt,At]of wt)switch(kt){case"LW":this.setLineWidth(At);break;case"LC":this.setLineCap(At);break;case"LJ":this.setLineJoin(At);break;case"ML":this.setMiterLimit(At);break;case"D":this.setDash(At[0],At[1]);break;case"RI":this.setRenderingIntent(At);break;case"FL":this.setFlatness(At);break;case"Font":this.setFont(At[0],At[1]);break;case"CA":this.current.strokeAlpha=At;break;case"ca":this.current.fillAlpha=At,this.ctx.globalAlpha=At;break;case"BM":this.ctx.globalCompositeOperation=At;break;case"SMask":this.current.activeSMask=At?this.tempSMask:null,this.tempSMask=null,this.checkSMaskState();break;case"TR":this.ctx.filter=this.current.transferMaps=this.filterFactory.addFilter(At);break}}get inSMaskMode(){return!!this.suspendedCtx}checkSMaskState(){const wt=this.inSMaskMode;this.current.activeSMask&&!wt?this.beginSMaskMode():!this.current.activeSMask&&wt&&this.endSMaskMode()}beginSMaskMode(){if(this.inSMaskMode)throw new Error("beginSMaskMode called while already in smask mode");const wt=this.ctx.canvas.width,kt=this.ctx.canvas.height,At="smaskGroupAt"+this.groupLevel,Pt=this.cachedCanvases.getCanvas(At,wt,kt);this.suspendedCtx=this.ctx,this.ctx=Pt.context;const Mt=this.ctx;Mt.setTransform(...(0,$.getCurrentTransform)(this.suspendedCtx)),ft(this.suspendedCtx,Mt),ct(Mt,this.suspendedCtx),this.setGState([["BM","source-over"],["ca",1],["CA",1]])}endSMaskMode(){if(!this.inSMaskMode)throw new Error("endSMaskMode called while not in smask mode");this.ctx._removeMirroring(),ft(this.ctx,this.suspendedCtx),this.ctx=this.suspendedCtx,this.suspendedCtx=null}compose(wt){if(!this.current.activeSMask)return;wt?(wt[0]=Math.floor(wt[0]),wt[1]=Math.floor(wt[1]),wt[2]=Math.ceil(wt[2]),wt[3]=Math.ceil(wt[3])):wt=[0,0,this.ctx.canvas.width,this.ctx.canvas.height];const kt=this.current.activeSMask,At=this.suspendedCtx;vt(At,kt,this.ctx,wt),this.ctx.save(),this.ctx.setTransform(1,0,0,1,0,0),this.ctx.clearRect(0,0,this.ctx.canvas.width,this.ctx.canvas.height),this.ctx.restore()}save(){this.inSMaskMode?(ft(this.ctx,this.suspendedCtx),this.suspendedCtx.save()):this.ctx.save();const wt=this.current;this.stateStack.push(wt),this.current=wt.clone()}restore(){this.stateStack.length===0&&this.inSMaskMode&&this.endSMaskMode(),this.stateStack.length!==0&&(this.current=this.stateStack.pop(),this.inSMaskMode?(this.suspendedCtx.restore(),ft(this.suspendedCtx,this.ctx)):this.ctx.restore(),this.checkSMaskState(),this.pendingClip=null,this._cachedScaleForStroking[0]=-1,this._cachedGetSinglePixelWidth=null)}transform(wt,kt,At,Pt,Mt,Ot){this.ctx.transform(wt,kt,At,Pt,Mt,Ot),this._cachedScaleForStroking[0]=-1,this._cachedGetSinglePixelWidth=null}constructPath(wt,kt,At){const Pt=this.ctx,Mt=this.current;let Ot=Mt.x,Bt=Mt.y,zt,Gt;const Wt=(0,$.getCurrentTransform)(Pt),qt=Wt[0]===0&&Wt[3]===0||Wt[1]===0&&Wt[2]===0,tn=qt?At.slice(0):null;for(let ln=0,gn=0,yn=wt.length;ln<yn;ln++)switch(wt[ln]|0){case s.OPS.rectangle:Ot=kt[gn++],Bt=kt[gn++];const Pn=kt[gn++],cn=kt[gn++],xn=Ot+Pn,hn=Bt+cn;Pt.moveTo(Ot,Bt),Pn===0||cn===0?Pt.lineTo(xn,hn):(Pt.lineTo(xn,Bt),Pt.lineTo(xn,hn),Pt.lineTo(Ot,hn)),qt||Mt.updateRectMinMax(Wt,[Ot,Bt,xn,hn]),Pt.closePath();break;case s.OPS.moveTo:Ot=kt[gn++],Bt=kt[gn++],Pt.moveTo(Ot,Bt),qt||Mt.updatePathMinMax(Wt,Ot,Bt);break;case s.OPS.lineTo:Ot=kt[gn++],Bt=kt[gn++],Pt.lineTo(Ot,Bt),qt||Mt.updatePathMinMax(Wt,Ot,Bt);break;case s.OPS.curveTo:zt=Ot,Gt=Bt,Ot=kt[gn+4],Bt=kt[gn+5],Pt.bezierCurveTo(kt[gn],kt[gn+1],kt[gn+2],kt[gn+3],Ot,Bt),Mt.updateCurvePathMinMax(Wt,zt,Gt,kt[gn],kt[gn+1],kt[gn+2],kt[gn+3],Ot,Bt,tn),gn+=6;break;case s.OPS.curveTo2:zt=Ot,Gt=Bt,Pt.bezierCurveTo(Ot,Bt,kt[gn],kt[gn+1],kt[gn+2],kt[gn+3]),Mt.updateCurvePathMinMax(Wt,zt,Gt,Ot,Bt,kt[gn],kt[gn+1],kt[gn+2],kt[gn+3],tn),Ot=kt[gn+2],Bt=kt[gn+3],gn+=4;break;case s.OPS.curveTo3:zt=Ot,Gt=Bt,Ot=kt[gn+2],Bt=kt[gn+3],Pt.bezierCurveTo(kt[gn],kt[gn+1],Ot,Bt,Ot,Bt),Mt.updateCurvePathMinMax(Wt,zt,Gt,kt[gn],kt[gn+1],Ot,Bt,Ot,Bt,tn),gn+=4;break;case s.OPS.closePath:Pt.closePath();break}qt&&Mt.updateScalingPathMinMax(Wt,tn),Mt.setCurrentPoint(Ot,Bt)}closePath(){this.ctx.closePath()}stroke(wt=!0){const kt=this.ctx,At=this.current.strokeColor;kt.globalAlpha=this.current.strokeAlpha,this.contentVisible&&(typeof At=="object"&&(At!=null&&At.getPattern)?(kt.save(),kt.strokeStyle=At.getPattern(kt,this,(0,$.getCurrentTransformInverse)(kt),j.PathType.STROKE),this.rescaleAndStroke(!1),kt.restore()):this.rescaleAndStroke(!0)),wt&&this.consumePath(this.current.getClippedPathBoundingBox()),kt.globalAlpha=this.current.fillAlpha}closeStroke(){this.closePath(),this.stroke()}fill(wt=!0){const kt=this.ctx,At=this.current.fillColor,Pt=this.current.patternFill;let Mt=!1;Pt&&(kt.save(),kt.fillStyle=At.getPattern(kt,this,(0,$.getCurrentTransformInverse)(kt),j.PathType.FILL),Mt=!0);const Ot=this.current.getClippedPathBoundingBox();this.contentVisible&&Ot!==null&&(this.pendingEOFill?(kt.fill("evenodd"),this.pendingEOFill=!1):kt.fill()),Mt&&kt.restore(),wt&&this.consumePath(Ot)}eoFill(){this.pendingEOFill=!0,this.fill()}fillStroke(){this.fill(!1),this.stroke(!1),this.consumePath()}eoFillStroke(){this.pendingEOFill=!0,this.fillStroke()}closeFillStroke(){this.closePath(),this.fillStroke()}closeEOFillStroke(){this.pendingEOFill=!0,this.closePath(),this.fillStroke()}endPath(){this.consumePath()}clip(){this.pendingClip=Et}eoClip(){this.pendingClip=Dt}beginText(){this.current.textMatrix=s.IDENTITY_MATRIX,this.current.textMatrixScale=1,this.current.x=this.current.lineX=0,this.current.y=this.current.lineY=0}endText(){const wt=this.pendingTextPaths,kt=this.ctx;if(wt===void 0){kt.beginPath();return}kt.save(),kt.beginPath();for(const At of wt)kt.setTransform(...At.transform),kt.translate(At.x,At.y),At.addToPath(kt,At.fontSize);kt.restore(),kt.clip(),kt.beginPath(),delete this.pendingTextPaths}setCharSpacing(wt){this.current.charSpacing=wt}setWordSpacing(wt){this.current.wordSpacing=wt}setHScale(wt){this.current.textHScale=wt/100}setLeading(wt){this.current.leading=-wt}setFont(wt,kt){var Wt;const At=this.commonObjs.get(wt),Pt=this.current;if(!At)throw new Error(`Can't find font for ${wt}`);if(Pt.fontMatrix=At.fontMatrix||s.FONT_IDENTITY_MATRIX,(Pt.fontMatrix[0]===0||Pt.fontMatrix[3]===0)&&(0,s.warn)("Invalid font matrix for font "+wt),kt<0?(kt=-kt,Pt.fontDirection=-1):Pt.fontDirection=1,this.current.font=At,this.current.fontSize=kt,At.isType3Font)return;const Mt=At.loadedName||"sans-serif",Ot=((Wt=At.systemFontInfo)==null?void 0:Wt.css)||`"${Mt}", ${At.fallbackName}`;let Bt="normal";At.black?Bt="900":At.bold&&(Bt="bold");const zt=At.italic?"italic":"normal";let Gt=kt;kt<et?Gt=et:kt>tt&&(Gt=tt),this.current.fontSizeScale=kt/Gt,this.ctx.font=`${zt} ${Bt} ${Gt}px ${Ot}`}setTextRenderingMode(wt){this.current.textRenderingMode=wt}setTextRise(wt){this.current.textRise=wt}moveText(wt,kt){this.current.x=this.current.lineX+=wt,this.current.y=this.current.lineY+=kt}setLeadingMoveText(wt,kt){this.setLeading(-kt),this.moveText(wt,kt)}setTextMatrix(wt,kt,At,Pt,Mt,Ot){this.current.textMatrix=[wt,kt,At,Pt,Mt,Ot],this.current.textMatrixScale=Math.hypot(wt,kt),this.current.x=this.current.lineX=0,this.current.y=this.current.lineY=0}nextLine(){this.moveText(0,this.current.leading)}paintChar(wt,kt,At,Pt){const Mt=this.ctx,Ot=this.current,Bt=Ot.font,zt=Ot.textRenderingMode,Gt=Ot.fontSize/Ot.fontSizeScale,Wt=zt&s.TextRenderingMode.FILL_STROKE_MASK,qt=!!(zt&s.TextRenderingMode.ADD_TO_PATH_FLAG),tn=Ot.patternFill&&!Bt.missingFile;let ln;(Bt.disableFontFace||qt||tn)&&(ln=Bt.getPathGenerator(this.commonObjs,wt)),Bt.disableFontFace||tn?(Mt.save(),Mt.translate(kt,At),Mt.beginPath(),ln(Mt,Gt),Pt&&Mt.setTransform(...Pt),(Wt===s.TextRenderingMode.FILL||Wt===s.TextRenderingMode.FILL_STROKE)&&Mt.fill(),(Wt===s.TextRenderingMode.STROKE||Wt===s.TextRenderingMode.FILL_STROKE)&&Mt.stroke(),Mt.restore()):((Wt===s.TextRenderingMode.FILL||Wt===s.TextRenderingMode.FILL_STROKE)&&Mt.fillText(wt,kt,At),(Wt===s.TextRenderingMode.STROKE||Wt===s.TextRenderingMode.FILL_STROKE)&&Mt.strokeText(wt,kt,At)),qt&&(this.pendingTextPaths||(this.pendingTextPaths=[])).push({transform:(0,$.getCurrentTransform)(Mt),x:kt,y:At,fontSize:Gt,addToPath:ln})}get isFontSubpixelAAEnabled(){const{context:wt}=this.cachedCanvases.getCanvas("isFontSubpixelAAEnabled",10,10);wt.scale(1.5,1),wt.fillText("I",0,10);const kt=wt.getImageData(0,0,10,10).data;let At=!1;for(let Pt=3;Pt<kt.length;Pt+=4)if(kt[Pt]>0&&kt[Pt]<255){At=!0;break}return(0,s.shadow)(this,"isFontSubpixelAAEnabled",At)}showText(wt){const kt=this.current,At=kt.font;if(At.isType3Font)return this.showType3Text(wt);const Pt=kt.fontSize;if(Pt===0)return;const Mt=this.ctx,Ot=kt.fontSizeScale,Bt=kt.charSpacing,zt=kt.wordSpacing,Gt=kt.fontDirection,Wt=kt.textHScale*Gt,qt=wt.length,tn=At.vertical,ln=tn?1:-1,gn=At.defaultVMetrics,yn=Pt*kt.fontMatrix[0],Pn=kt.textRenderingMode===s.TextRenderingMode.FILL&&!At.disableFontFace&&!kt.patternFill;Mt.save(),Mt.transform(...kt.textMatrix),Mt.translate(kt.x,kt.y+kt.textRise),Gt>0?Mt.scale(Wt,-1):Mt.scale(Wt,1);let cn;if(kt.patternFill){Mt.save();const vn=kt.fillColor.getPattern(Mt,this,(0,$.getCurrentTransformInverse)(Mt),j.PathType.FILL);cn=(0,$.getCurrentTransform)(Mt),Mt.restore(),Mt.fillStyle=vn}let xn=kt.lineWidth;const hn=kt.textMatrixScale;if(hn===0||xn===0){const vn=kt.textRenderingMode&s.TextRenderingMode.FILL_STROKE_MASK;(vn===s.TextRenderingMode.STROKE||vn===s.TextRenderingMode.FILL_STROKE)&&(xn=this.getSinglePixelWidth())}else xn/=hn;if(Ot!==1&&(Mt.scale(Ot,Ot),xn/=Ot),Mt.lineWidth=xn,At.isInvalidPDFjsFont){const vn=[];let $n=0;for(const Mn of wt)vn.push(Mn.unicode),$n+=Mn.width;Mt.fillText(vn.join(""),0,0),kt.x+=$n*yn*Wt,Mt.restore(),this.compose();return}let en=0,Jt;for(Jt=0;Jt<qt;++Jt){const vn=wt[Jt];if(typeof vn=="number"){en+=ln*vn*Pt/1e3;continue}let $n=!1;const Mn=(vn.isSpace?zt:0)+Bt,On=vn.fontChar,En=vn.accent;let Bn,Hn,Wn=vn.width;if(tn){const Zn=vn.vmetric||gn,bn=-(vn.vmetric?Zn[1]:Wn*.5)*yn,dn=Zn[2]*yn;Wn=Zn?-Zn[0]:Wn,Bn=bn/Ot,Hn=(en+dn)/Ot}else Bn=en/Ot,Hn=0;if(At.remeasure&&Wn>0){const Zn=Mt.measureText(On).width*1e3/Pt*Ot;if(Wn<Zn&&this.isFontSubpixelAAEnabled){const bn=Wn/Zn;$n=!0,Mt.save(),Mt.scale(bn,1),Bn/=bn}else Wn!==Zn&&(Bn+=(Wn-Zn)/2e3*Pt/Ot)}if(this.contentVisible&&(vn.isInFont||At.missingFile)){if(Pn&&!En)Mt.fillText(On,Bn,Hn);else if(this.paintChar(On,Bn,Hn,cn),En){const Zn=Bn+Pt*En.offset.x/Ot,bn=Hn-Pt*En.offset.y/Ot;this.paintChar(En.fontChar,Zn,bn,cn)}}const _n=tn?Wn*yn-Mn*Gt:Wn*yn+Mn*Gt;en+=_n,$n&&Mt.restore()}tn?kt.y-=en:kt.x+=en*Wt,Mt.restore(),this.compose()}showType3Text(wt){const kt=this.ctx,At=this.current,Pt=At.font,Mt=At.fontSize,Ot=At.fontDirection,Bt=Pt.vertical?1:-1,zt=At.charSpacing,Gt=At.wordSpacing,Wt=At.textHScale*Ot,qt=At.fontMatrix||s.FONT_IDENTITY_MATRIX,tn=wt.length,ln=At.textRenderingMode===s.TextRenderingMode.INVISIBLE;let gn,yn,Pn,cn;if(!(ln||Mt===0)){for(this._cachedScaleForStroking[0]=-1,this._cachedGetSinglePixelWidth=null,kt.save(),kt.transform(...At.textMatrix),kt.translate(At.x,At.y),kt.scale(Wt,Ot),gn=0;gn<tn;++gn){if(yn=wt[gn],typeof yn=="number"){cn=Bt*yn*Mt/1e3,this.ctx.translate(cn,0),At.x+=cn*Wt;continue}const xn=(yn.isSpace?Gt:0)+zt,hn=Pt.charProcOperatorList[yn.operatorListId];if(!hn){(0,s.warn)(`Type3 character "${yn.operatorListId}" is not available.`);continue}this.contentVisible&&(this.processingType3=yn,this.save(),kt.scale(Mt,Mt),kt.transform(...qt),this.executeOperatorList(hn),this.restore()),Pn=s.Util.applyTransform([yn.width,0],qt)[0]*Mt+xn,kt.translate(Pn,0),At.x+=Pn*Wt}kt.restore(),this.processingType3=null}}setCharWidth(wt,kt){}setCharWidthAndBounds(wt,kt,At,Pt,Mt,Ot){this.ctx.rect(At,Pt,Mt-At,Ot-Pt),this.ctx.clip(),this.endPath()}getColorN_Pattern(wt){let kt;if(wt[0]==="TilingPattern"){const At=wt[1],Pt=this.baseTransform||(0,$.getCurrentTransform)(this.ctx),Mt={createCanvasGraphics:Ot=>new sn(Ot,this.commonObjs,this.objs,this.canvasFactory,this.filterFactory,{optionalContentConfig:this.optionalContentConfig,markedContentStack:this.markedContentStack})};kt=new j.TilingPattern(wt,At,this.ctx,Mt,Pt)}else kt=this._getPattern(wt[1],wt[2]);return kt}setStrokeColorN(){this.current.strokeColor=this.getColorN_Pattern(arguments)}setFillColorN(){this.current.fillColor=this.getColorN_Pattern(arguments),this.current.patternFill=!0}setStrokeRGBColor(wt,kt,At){const Pt=s.Util.makeHexColor(wt,kt,At);this.ctx.strokeStyle=Pt,this.current.strokeColor=Pt}setFillRGBColor(wt,kt,At){const Pt=s.Util.makeHexColor(wt,kt,At);this.ctx.fillStyle=Pt,this.current.fillColor=Pt,this.current.patternFill=!1}_getPattern(wt,kt=null){let At;return this.cachedPatterns.has(wt)?At=this.cachedPatterns.get(wt):(At=(0,j.getShadingPattern)(this.getObject(wt)),this.cachedPatterns.set(wt,At)),kt&&(At.matrix=kt),At}shadingFill(wt){if(!this.contentVisible)return;const kt=this.ctx;this.save();const At=this._getPattern(wt);kt.fillStyle=At.getPattern(kt,this,(0,$.getCurrentTransformInverse)(kt),j.PathType.SHADING);const Pt=(0,$.getCurrentTransformInverse)(kt);if(Pt){const{width:Mt,height:Ot}=kt.canvas,[Bt,zt,Gt,Wt]=s.Util.getAxialAlignedBoundingBox([0,0,Mt,Ot],Pt);this.ctx.fillRect(Bt,zt,Gt-Bt,Wt-zt)}else this.ctx.fillRect(-1e10,-1e10,2e10,2e10);this.compose(this.current.getClippedPathBoundingBox()),this.restore()}beginInlineImage(){(0,s.unreachable)("Should not call beginInlineImage")}beginImageData(){(0,s.unreachable)("Should not call beginImageData")}paintFormXObjectBegin(wt,kt){if(this.contentVisible&&(this.save(),this.baseTransformStack.push(this.baseTransform),Array.isArray(wt)&&wt.length===6&&this.transform(...wt),this.baseTransform=(0,$.getCurrentTransform)(this.ctx),kt)){const At=kt[2]-kt[0],Pt=kt[3]-kt[1];this.ctx.rect(kt[0],kt[1],At,Pt),this.current.updateRectMinMax((0,$.getCurrentTransform)(this.ctx),kt),this.clip(),this.endPath()}}paintFormXObjectEnd(){this.contentVisible&&(this.restore(),this.baseTransform=this.baseTransformStack.pop())}beginGroup(wt){if(!this.contentVisible)return;this.save(),this.inSMaskMode&&(this.endSMaskMode(),this.current.activeSMask=null);const kt=this.ctx;wt.isolated||(0,s.info)("TODO: Support non-isolated groups."),wt.knockout&&(0,s.warn)("Knockout groups not supported.");const At=(0,$.getCurrentTransform)(kt);if(wt.matrix&&kt.transform(...wt.matrix),!wt.bbox)throw new Error("Bounding box is required.");let Pt=s.Util.getAxialAlignedBoundingBox(wt.bbox,(0,$.getCurrentTransform)(kt));const Mt=[0,0,kt.canvas.width,kt.canvas.height];Pt=s.Util.intersect(Pt,Mt)||[0,0,0,0];const Ot=Math.floor(Pt[0]),Bt=Math.floor(Pt[1]);let zt=Math.max(Math.ceil(Pt[2])-Ot,1),Gt=Math.max(Math.ceil(Pt[3])-Bt,1),Wt=1,qt=1;zt>nt&&(Wt=zt/nt,zt=nt),Gt>nt&&(qt=Gt/nt,Gt=nt),this.current.startNewPathAndClipBox([0,0,zt,Gt]);let tn="groupAt"+this.groupLevel;wt.smask&&(tn+="_smask_"+this.smaskCounter++%2);const ln=this.cachedCanvases.getCanvas(tn,zt,Gt),gn=ln.context;gn.scale(1/Wt,1/qt),gn.translate(-Ot,-Bt),gn.transform(...At),wt.smask?this.smaskStack.push({canvas:ln.canvas,context:gn,offsetX:Ot,offsetY:Bt,scaleX:Wt,scaleY:qt,subtype:wt.smask.subtype,backdrop:wt.smask.backdrop,transferMap:wt.smask.transferMap||null,startTransformInverse:null}):(kt.setTransform(1,0,0,1,0,0),kt.translate(Ot,Bt),kt.scale(Wt,qt),kt.save()),ft(kt,gn),this.ctx=gn,this.setGState([["BM","source-over"],["ca",1],["CA",1]]),this.groupStack.push(kt),this.groupLevel++}endGroup(wt){if(!this.contentVisible)return;this.groupLevel--;const kt=this.ctx,At=this.groupStack.pop();if(this.ctx=At,this.ctx.imageSmoothingEnabled=!1,wt.smask)this.tempSMask=this.smaskStack.pop(),this.restore();else{this.ctx.restore();const Pt=(0,$.getCurrentTransform)(this.ctx);this.restore(),this.ctx.save(),this.ctx.setTransform(...Pt);const Mt=s.Util.getAxialAlignedBoundingBox([0,0,kt.canvas.width,kt.canvas.height],Pt);this.ctx.drawImage(kt.canvas,0,0),this.ctx.restore(),this.compose(Mt)}}beginAnnotation(wt,kt,At,Pt,Mt){if(un(this,Ct,Bo).call(this),ht(this.ctx),this.ctx.save(),this.save(),this.baseTransform&&this.ctx.setTransform(...this.baseTransform),Array.isArray(kt)&&kt.length===4){const Ot=kt[2]-kt[0],Bt=kt[3]-kt[1];if(Mt&&this.annotationCanvasMap){At=At.slice(),At[4]-=kt[0],At[5]-=kt[1],kt=kt.slice(),kt[0]=kt[1]=0,kt[2]=Ot,kt[3]=Bt;const[zt,Gt]=s.Util.singularValueDecompose2dScale((0,$.getCurrentTransform)(this.ctx)),{viewportScale:Wt}=this,qt=Math.ceil(Ot*this.outputScaleX*Wt),tn=Math.ceil(Bt*this.outputScaleY*Wt);this.annotationCanvas=this.canvasFactory.create(qt,tn);const{canvas:ln,context:gn}=this.annotationCanvas;this.annotationCanvasMap.set(wt,ln),this.annotationCanvas.savedCtx=this.ctx,this.ctx=gn,this.ctx.save(),this.ctx.setTransform(zt,0,0,-Gt,0,Bt*Gt),ht(this.ctx)}else ht(this.ctx),this.ctx.rect(kt[0],kt[1],Ot,Bt),this.ctx.clip(),this.endPath()}this.current=new dt(this.ctx.canvas.width,this.ctx.canvas.height),this.transform(...At),this.transform(...Pt)}endAnnotation(){this.annotationCanvas&&(this.ctx.restore(),un(this,Zt,Zo).call(this),this.ctx=this.annotationCanvas.savedCtx,delete this.annotationCanvas.savedCtx,delete this.annotationCanvas)}paintImageMaskXObject(wt){if(!this.contentVisible)return;const kt=wt.count;wt=this.getObject(wt.data,wt),wt.count=kt;const At=this.ctx,Pt=this.processingType3;if(Pt&&(Pt.compiled===void 0&&(Pt.compiled=ot(wt)),Pt.compiled)){Pt.compiled(At);return}const Mt=this._createMaskCanvas(wt),Ot=Mt.canvas;At.save(),At.setTransform(1,0,0,1,0,0),At.drawImage(Ot,Mt.offsetX,Mt.offsetY),At.restore(),this.compose()}paintImageMaskXObjectRepeat(wt,kt,At=0,Pt=0,Mt,Ot){if(!this.contentVisible)return;wt=this.getObject(wt.data,wt);const Bt=this.ctx;Bt.save();const zt=(0,$.getCurrentTransform)(Bt);Bt.transform(kt,At,Pt,Mt,0,0);const Gt=this._createMaskCanvas(wt);Bt.setTransform(1,0,0,1,Gt.offsetX-zt[4],Gt.offsetY-zt[5]);for(let Wt=0,qt=Ot.length;Wt<qt;Wt+=2){const tn=s.Util.transform(zt,[kt,At,Pt,Mt,Ot[Wt],Ot[Wt+1]]),[ln,gn]=s.Util.applyTransform([0,0],tn);Bt.drawImage(Gt.canvas,ln,gn)}Bt.restore(),this.compose()}paintImageMaskXObjectGroup(wt){if(!this.contentVisible)return;const kt=this.ctx,At=this.current.fillColor,Pt=this.current.patternFill;for(const Mt of wt){const{data:Ot,width:Bt,height:zt,transform:Gt}=Mt,Wt=this.cachedCanvases.getCanvas("maskCanvas",Bt,zt),qt=Wt.context;qt.save();const tn=this.getObject(Ot,Mt);mt(qt,tn),qt.globalCompositeOperation="source-in",qt.fillStyle=Pt?At.getPattern(qt,this,(0,$.getCurrentTransformInverse)(kt),j.PathType.FILL):At,qt.fillRect(0,0,Bt,zt),qt.restore(),kt.save(),kt.transform(...Gt),kt.scale(1,-1),ut(kt,Wt.canvas,0,0,Bt,zt,0,-1,1,1),kt.restore()}this.compose()}paintImageXObject(wt){if(!this.contentVisible)return;const kt=this.getObject(wt);if(!kt){(0,s.warn)("Dependent image isn't ready yet");return}this.paintInlineImageXObject(kt)}paintImageXObjectRepeat(wt,kt,At,Pt){if(!this.contentVisible)return;const Mt=this.getObject(wt);if(!Mt){(0,s.warn)("Dependent image isn't ready yet");return}const Ot=Mt.width,Bt=Mt.height,zt=[];for(let Gt=0,Wt=Pt.length;Gt<Wt;Gt+=2)zt.push({transform:[kt,0,0,At,Pt[Gt],Pt[Gt+1]],x:0,y:0,w:Ot,h:Bt});this.paintInlineImageXObjectGroup(Mt,zt)}applyTransferMapsToCanvas(wt){return this.current.transferMaps!=="none"&&(wt.filter=this.current.transferMaps,wt.drawImage(wt.canvas,0,0),wt.filter="none"),wt.canvas}applyTransferMapsToBitmap(wt){if(this.current.transferMaps==="none")return wt.bitmap;const{bitmap:kt,width:At,height:Pt}=wt,Mt=this.cachedCanvases.getCanvas("inlineImage",At,Pt),Ot=Mt.context;return Ot.filter=this.current.transferMaps,Ot.drawImage(kt,0,0),Ot.filter="none",Mt.canvas}paintInlineImageXObject(wt){if(!this.contentVisible)return;const kt=wt.width,At=wt.height,Pt=this.ctx;if(this.save(),!s.isNodeJS){const{filter:Bt}=Pt;Bt!=="none"&&Bt!==""&&(Pt.filter="none")}Pt.scale(1/kt,-1/At);let Mt;if(wt.bitmap)Mt=this.applyTransferMapsToBitmap(wt);else if(typeof HTMLElement=="function"&&wt instanceof HTMLElement||!wt.data)Mt=wt;else{const zt=this.cachedCanvases.getCanvas("inlineImage",kt,At).context;pt(zt,wt),Mt=this.applyTransferMapsToCanvas(zt)}const Ot=this._scaleImage(Mt,(0,$.getCurrentTransformInverse)(Pt));Pt.imageSmoothingEnabled=Lt((0,$.getCurrentTransform)(Pt),wt.interpolate),ut(Pt,Ot.img,0,0,Ot.paintWidth,Ot.paintHeight,0,-At,kt,At),this.compose(),this.restore()}paintInlineImageXObjectGroup(wt,kt){if(!this.contentVisible)return;const At=this.ctx;let Pt;if(wt.bitmap)Pt=wt.bitmap;else{const Mt=wt.width,Ot=wt.height,zt=this.cachedCanvases.getCanvas("inlineImage",Mt,Ot).context;pt(zt,wt),Pt=this.applyTransferMapsToCanvas(zt)}for(const Mt of kt)At.save(),At.transform(...Mt.transform),At.scale(1,-1),ut(At,Pt,Mt.x,Mt.y,Mt.w,Mt.h,0,-1,1,1),At.restore();this.compose()}paintSolidColorImageMask(){this.contentVisible&&(this.ctx.fillRect(0,0,1,1),this.compose())}markPoint(wt){}markPointProps(wt,kt){}beginMarkedContent(wt){this.markedContentStack.push({visible:!0})}beginMarkedContentProps(wt,kt){wt==="OC"?this.markedContentStack.push({visible:this.optionalContentConfig.isVisible(kt)}):this.markedContentStack.push({visible:!0}),this.contentVisible=this.isContentVisible()}endMarkedContent(){this.markedContentStack.pop(),this.contentVisible=this.isContentVisible()}beginCompat(){}endCompat(){}consumePath(wt){const kt=this.current.isEmptyClip();this.pendingClip&&this.current.updateClipFromPath(),this.pendingClip||this.compose(wt);const At=this.ctx;this.pendingClip&&(kt||(this.pendingClip===Dt?At.clip("evenodd"):At.clip()),this.pendingClip=null),this.current.startNewPathAndClipBox(this.current.clipBox),At.beginPath()}getSinglePixelWidth(){if(!this._cachedGetSinglePixelWidth){const wt=(0,$.getCurrentTransform)(this.ctx);if(wt[1]===0&&wt[2]===0)this._cachedGetSinglePixelWidth=1/Math.min(Math.abs(wt[0]),Math.abs(wt[3]));else{const kt=Math.abs(wt[0]*wt[3]-wt[2]*wt[1]),At=Math.hypot(wt[0],wt[2]),Pt=Math.hypot(wt[1],wt[3]);this._cachedGetSinglePixelWidth=Math.max(At,Pt)/kt}}return this._cachedGetSinglePixelWidth}getScaleForStroking(){if(this._cachedScaleForStroking[0]===-1){const{lineWidth:wt}=this.current,{a:kt,b:At,c:Pt,d:Mt}=this.ctx.getTransform();let Ot,Bt;if(At===0&&Pt===0){const zt=Math.abs(kt),Gt=Math.abs(Mt);if(zt===Gt)if(wt===0)Ot=Bt=1/zt;else{const Wt=zt*wt;Ot=Bt=Wt<1?1/Wt:1}else if(wt===0)Ot=1/zt,Bt=1/Gt;else{const Wt=zt*wt,qt=Gt*wt;Ot=Wt<1?1/Wt:1,Bt=qt<1?1/qt:1}}else{const zt=Math.abs(kt*Mt-At*Pt),Gt=Math.hypot(kt,At),Wt=Math.hypot(Pt,Mt);if(wt===0)Ot=Wt/zt,Bt=Gt/zt;else{const qt=wt*zt;Ot=Wt>qt?Wt/qt:1,Bt=Gt>qt?Gt/qt:1}}this._cachedScaleForStroking[0]=Ot,this._cachedScaleForStroking[1]=Bt}return this._cachedScaleForStroking}rescaleAndStroke(wt){const{ctx:kt}=this,{lineWidth:At}=this.current,[Pt,Mt]=this.getScaleForStroking();if(kt.lineWidth=At||1,Pt===1&&Mt===1){kt.stroke();return}const Ot=kt.getLineDash();if(wt&&kt.save(),kt.scale(Pt,Mt),Ot.length>0){const Bt=Math.max(Pt,Mt);kt.setLineDash(Ot.map(zt=>zt/Bt)),kt.lineDashOffset/=Bt}kt.stroke(),wt&&kt.restore()}isContentVisible(){for(let wt=this.markedContentStack.length-1;wt>=0;wt--)if(!this.markedContentStack[wt].visible)return!1;return!0}};Ct=new WeakSet,Bo=function(){for(;this.stateStack.length||this.inSMaskMode;)this.restore();this.ctx.restore(),this.transparentCanvas&&(this.ctx=this.compositeCtx,this.ctx.save(),this.ctx.setTransform(1,0,0,1,0,0),this.ctx.drawImage(this.transparentCanvas,0,0),this.ctx.restore(),this.transparentCanvas=null)},Zt=new WeakSet,Zo=function(){if(this.pageColors){const wt=this.filterFactory.addHCMFilter(this.pageColors.foreground,this.pageColors.background);if(wt!=="none"){const kt=this.ctx.filter;this.ctx.filter=wt,this.ctx.drawImage(this.ctx.canvas,0,0),this.ctx.filter=kt}}};let It=sn;i.CanvasGraphics=It;for(const Ft in s.OPS)It.prototype[Ft]!==void 0&&(It.prototype[s.OPS[Ft]]=It.prototype[Ft])},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.TilingPattern=i.PathType=void 0,i.getShadingPattern=lt;var s=o(1),$=o(6);const j={FILL:"Fill",STROKE:"Stroke",SHADING:"Shading"};i.PathType=j;function _e(ot,dt){if(!dt)return;const pt=dt[2]-dt[0],mt=dt[3]-dt[1],ft=new Path2D;ft.rect(dt[0],dt[1],pt,mt),ot.clip(ft)}class et{constructor(){this.constructor===et&&(0,s.unreachable)("Cannot initialize BaseShadingPattern.")}getPattern(){(0,s.unreachable)("Abstract method `getPattern` called.")}}class tt extends et{constructor(dt){super(),this._type=dt[1],this._bbox=dt[2],this._colorStops=dt[3],this._p0=dt[4],this._p1=dt[5],this._r0=dt[6],this._r1=dt[7],this.matrix=null}_createGradient(dt){let pt;this._type==="axial"?pt=dt.createLinearGradient(this._p0[0],this._p0[1],this._p1[0],this._p1[1]):this._type==="radial"&&(pt=dt.createRadialGradient(this._p0[0],this._p0[1],this._r0,this._p1[0],this._p1[1],this._r1));for(const mt of this._colorStops)pt.addColorStop(mt[0],mt[1]);return pt}getPattern(dt,pt,mt,ft){let ht;if(ft===j.STROKE||ft===j.FILL){const yt=pt.current.getClippedPathBoundingBox(ft,(0,$.getCurrentTransform)(dt))||[0,0,0,0],bt=Math.ceil(yt[2]-yt[0])||1,gt=Math.ceil(yt[3]-yt[1])||1,xt=pt.cachedCanvases.getCanvas("pattern",bt,gt,!0),vt=xt.context;vt.clearRect(0,0,vt.canvas.width,vt.canvas.height),vt.beginPath(),vt.rect(0,0,vt.canvas.width,vt.canvas.height),vt.translate(-yt[0],-yt[1]),mt=s.Util.transform(mt,[1,0,0,1,yt[0],yt[1]]),vt.transform(...pt.baseTransform),this.matrix&&vt.transform(...this.matrix),_e(vt,this._bbox),vt.fillStyle=this._createGradient(vt),vt.fill(),ht=dt.createPattern(xt.canvas,"no-repeat");const Lt=new DOMMatrix(mt);ht.setTransform(Lt)}else _e(dt,this._bbox),ht=this._createGradient(dt);return ht}}function nt(ot,dt,pt,mt,ft,ht,yt,bt){const gt=dt.coords,xt=dt.colors,vt=ot.data,Lt=ot.width*4;let $t;gt[pt+1]>gt[mt+1]&&($t=pt,pt=mt,mt=$t,$t=ht,ht=yt,yt=$t),gt[mt+1]>gt[ft+1]&&($t=mt,mt=ft,ft=$t,$t=yt,yt=bt,bt=$t),gt[pt+1]>gt[mt+1]&&($t=pt,pt=mt,mt=$t,$t=ht,ht=yt,yt=$t);const Tt=(gt[pt]+dt.offsetX)*dt.scaleX,Et=(gt[pt+1]+dt.offsetY)*dt.scaleY,Dt=(gt[mt]+dt.offsetX)*dt.scaleX,It=(gt[mt+1]+dt.offsetY)*dt.scaleY,Ct=(gt[ft]+dt.offsetX)*dt.scaleX,jt=(gt[ft+1]+dt.offsetY)*dt.scaleY;if(Et>=jt)return;const Zt=xt[ht],Xt=xt[ht+1],sn=xt[ht+2],Ft=xt[yt],wt=xt[yt+1],kt=xt[yt+2],At=xt[bt],Pt=xt[bt+1],Mt=xt[bt+2],Ot=Math.round(Et),Bt=Math.round(jt);let zt,Gt,Wt,qt,tn,ln,gn,yn;for(let Pn=Ot;Pn<=Bt;Pn++){if(Pn<It){const Jt=Pn<Et?0:(Et-Pn)/(Et-It);zt=Tt-(Tt-Dt)*Jt,Gt=Zt-(Zt-Ft)*Jt,Wt=Xt-(Xt-wt)*Jt,qt=sn-(sn-kt)*Jt}else{let Jt;Pn>jt?Jt=1:It===jt?Jt=0:Jt=(It-Pn)/(It-jt),zt=Dt-(Dt-Ct)*Jt,Gt=Ft-(Ft-At)*Jt,Wt=wt-(wt-Pt)*Jt,qt=kt-(kt-Mt)*Jt}let cn;Pn<Et?cn=0:Pn>jt?cn=1:cn=(Et-Pn)/(Et-jt),tn=Tt-(Tt-Ct)*cn,ln=Zt-(Zt-At)*cn,gn=Xt-(Xt-Pt)*cn,yn=sn-(sn-Mt)*cn;const xn=Math.round(Math.min(zt,tn)),hn=Math.round(Math.max(zt,tn));let en=Lt*Pn+xn*4;for(let Jt=xn;Jt<=hn;Jt++)cn=(zt-Jt)/(zt-tn),cn<0?cn=0:cn>1&&(cn=1),vt[en++]=Gt-(Gt-ln)*cn|0,vt[en++]=Wt-(Wt-gn)*cn|0,vt[en++]=qt-(qt-yn)*cn|0,vt[en++]=255}}function at(ot,dt,pt){const mt=dt.coords,ft=dt.colors;let ht,yt;switch(dt.type){case"lattice":const bt=dt.verticesPerRow,gt=Math.floor(mt.length/bt)-1,xt=bt-1;for(ht=0;ht<gt;ht++){let vt=ht*bt;for(let Lt=0;Lt<xt;Lt++,vt++)nt(ot,pt,mt[vt],mt[vt+1],mt[vt+bt],ft[vt],ft[vt+1],ft[vt+bt]),nt(ot,pt,mt[vt+bt+1],mt[vt+1],mt[vt+bt],ft[vt+bt+1],ft[vt+1],ft[vt+bt])}break;case"triangles":for(ht=0,yt=mt.length;ht<yt;ht+=3)nt(ot,pt,mt[ht],mt[ht+1],mt[ht+2],ft[ht],ft[ht+1],ft[ht+2]);break;default:throw new Error("illegal figure")}}class it extends et{constructor(dt){super(),this._coords=dt[2],this._colors=dt[3],this._figures=dt[4],this._bounds=dt[5],this._bbox=dt[7],this._background=dt[8],this.matrix=null}_createMeshCanvas(dt,pt,mt){const bt=Math.floor(this._bounds[0]),gt=Math.floor(this._bounds[1]),xt=Math.ceil(this._bounds[2])-bt,vt=Math.ceil(this._bounds[3])-gt,Lt=Math.min(Math.ceil(Math.abs(xt*dt[0]*1.1)),3e3),$t=Math.min(Math.ceil(Math.abs(vt*dt[1]*1.1)),3e3),Tt=xt/Lt,Et=vt/$t,Dt={coords:this._coords,colors:this._colors,offsetX:-bt,offsetY:-gt,scaleX:1/Tt,scaleY:1/Et},It=Lt+2*2,Ct=$t+2*2,jt=mt.getCanvas("mesh",It,Ct,!1),Zt=jt.context,Xt=Zt.createImageData(Lt,$t);if(pt){const Ft=Xt.data;for(let wt=0,kt=Ft.length;wt<kt;wt+=4)Ft[wt]=pt[0],Ft[wt+1]=pt[1],Ft[wt+2]=pt[2],Ft[wt+3]=255}for(const Ft of this._figures)at(Xt,Ft,Dt);return Zt.putImageData(Xt,2,2),{canvas:jt.canvas,offsetX:bt-2*Tt,offsetY:gt-2*Et,scaleX:Tt,scaleY:Et}}getPattern(dt,pt,mt,ft){_e(dt,this._bbox);let ht;if(ft===j.SHADING)ht=s.Util.singularValueDecompose2dScale((0,$.getCurrentTransform)(dt));else if(ht=s.Util.singularValueDecompose2dScale(pt.baseTransform),this.matrix){const bt=s.Util.singularValueDecompose2dScale(this.matrix);ht=[ht[0]*bt[0],ht[1]*bt[1]]}const yt=this._createMeshCanvas(ht,ft===j.SHADING?null:this._background,pt.cachedCanvases);return ft!==j.SHADING&&(dt.setTransform(...pt.baseTransform),this.matrix&&dt.transform(...this.matrix)),dt.translate(yt.offsetX,yt.offsetY),dt.scale(yt.scaleX,yt.scaleY),dt.createPattern(yt.canvas,"no-repeat")}}class st extends et{getPattern(){return"hotpink"}}function lt(ot){switch(ot[0]){case"RadialAxial":return new tt(ot);case"Mesh":return new it(ot);case"Dummy":return new st}throw new Error(`Unknown IR type: ${ot[0]}`)}const ct={COLORED:1,UNCOLORED:2},ut=class ut{constructor(dt,pt,mt,ft,ht){this.operatorList=dt[2],this.matrix=dt[3]||[1,0,0,1,0,0],this.bbox=dt[4],this.xstep=dt[5],this.ystep=dt[6],this.paintType=dt[7],this.tilingType=dt[8],this.color=pt,this.ctx=mt,this.canvasGraphicsFactory=ft,this.baseTransform=ht}createPatternCanvas(dt){const pt=this.operatorList,mt=this.bbox,ft=this.xstep,ht=this.ystep,yt=this.paintType,bt=this.tilingType,gt=this.color,xt=this.canvasGraphicsFactory;(0,s.info)("TilingType: "+bt);const vt=mt[0],Lt=mt[1],$t=mt[2],Tt=mt[3],Et=s.Util.singularValueDecompose2dScale(this.matrix),Dt=s.Util.singularValueDecompose2dScale(this.baseTransform),It=[Et[0]*Dt[0],Et[1]*Dt[1]],Ct=this.getSizeAndScale(ft,this.ctx.canvas.width,It[0]),jt=this.getSizeAndScale(ht,this.ctx.canvas.height,It[1]),Zt=dt.cachedCanvases.getCanvas("pattern",Ct.size,jt.size,!0),Xt=Zt.context,sn=xt.createCanvasGraphics(Xt);sn.groupLevel=dt.groupLevel,this.setFillAndStrokeStyleToContext(sn,yt,gt);let Ft=vt,wt=Lt,kt=$t,At=Tt;return vt<0&&(Ft=0,kt+=Math.abs(vt)),Lt<0&&(wt=0,At+=Math.abs(Lt)),Xt.translate(-(Ct.scale*Ft),-(jt.scale*wt)),sn.transform(Ct.scale,0,0,jt.scale,0,0),Xt.save(),this.clipBbox(sn,Ft,wt,kt,At),sn.baseTransform=(0,$.getCurrentTransform)(sn.ctx),sn.executeOperatorList(pt),sn.endDrawing(),{canvas:Zt.canvas,scaleX:Ct.scale,scaleY:jt.scale,offsetX:Ft,offsetY:wt}}getSizeAndScale(dt,pt,mt){dt=Math.abs(dt);const ft=Math.max(ut.MAX_PATTERN_SIZE,pt);let ht=Math.ceil(dt*mt);return ht>=ft?ht=ft:mt=ht/dt,{scale:mt,size:ht}}clipBbox(dt,pt,mt,ft,ht){const yt=ft-pt,bt=ht-mt;dt.ctx.rect(pt,mt,yt,bt),dt.current.updateRectMinMax((0,$.getCurrentTransform)(dt.ctx),[pt,mt,ft,ht]),dt.clip(),dt.endPath()}setFillAndStrokeStyleToContext(dt,pt,mt){const ft=dt.ctx,ht=dt.current;switch(pt){case ct.COLORED:const yt=this.ctx;ft.fillStyle=yt.fillStyle,ft.strokeStyle=yt.strokeStyle,ht.fillColor=yt.fillStyle,ht.strokeColor=yt.strokeStyle;break;case ct.UNCOLORED:const bt=s.Util.makeHexColor(mt[0],mt[1],mt[2]);ft.fillStyle=bt,ft.strokeStyle=bt,ht.fillColor=bt,ht.strokeColor=bt;break;default:throw new s.FormatError(`Unsupported paint type: ${pt}`)}}getPattern(dt,pt,mt,ft){let ht=mt;ft!==j.SHADING&&(ht=s.Util.transform(ht,pt.baseTransform),this.matrix&&(ht=s.Util.transform(ht,this.matrix)));const yt=this.createPatternCanvas(pt);let bt=new DOMMatrix(ht);bt=bt.translate(yt.offsetX,yt.offsetY),bt=bt.scale(1/yt.scaleX,1/yt.scaleY);const gt=dt.createPattern(yt.canvas,"repeat");return gt.setTransform(bt),gt}};Jn(ut,"MAX_PATTERN_SIZE",3e3);let rt=ut;i.TilingPattern=rt},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.convertBlackAndWhiteToRGBA=j,i.convertToRGBA=$,i.grayToRGBA=et;var s=o(1);function $(tt){switch(tt.kind){case s.ImageKind.GRAYSCALE_1BPP:return j(tt);case s.ImageKind.RGB_24BPP:return _e(tt)}return null}function j({src:tt,srcPos:nt=0,dest:at,width:it,height:st,nonBlackColor:lt=4294967295,inverseDecode:ct=!1}){const rt=s.FeatureTest.isLittleEndian?4278190080:255,[ut,ot]=ct?[lt,rt]:[rt,lt],dt=it>>3,pt=it&7,mt=tt.length;at=new Uint32Array(at.buffer);let ft=0;for(let ht=0;ht<st;ht++){for(const bt=nt+dt;nt<bt;nt++){const gt=nt<mt?tt[nt]:255;at[ft++]=gt&128?ot:ut,at[ft++]=gt&64?ot:ut,at[ft++]=gt&32?ot:ut,at[ft++]=gt&16?ot:ut,at[ft++]=gt&8?ot:ut,at[ft++]=gt&4?ot:ut,at[ft++]=gt&2?ot:ut,at[ft++]=gt&1?ot:ut}if(pt===0)continue;const yt=nt<mt?tt[nt++]:255;for(let bt=0;bt<pt;bt++)at[ft++]=yt&1<<7-bt?ot:ut}return{srcPos:nt,destPos:ft}}function _e({src:tt,srcPos:nt=0,dest:at,destPos:it=0,width:st,height:lt}){let ct=0;const rt=tt.length>>2,ut=new Uint32Array(tt.buffer,nt,rt);if(s.FeatureTest.isLittleEndian){for(;ct<rt-2;ct+=3,it+=4){const ot=ut[ct],dt=ut[ct+1],pt=ut[ct+2];at[it]=ot|4278190080,at[it+1]=ot>>>24|dt<<8|4278190080,at[it+2]=dt>>>16|pt<<16|4278190080,at[it+3]=pt>>>8|4278190080}for(let ot=ct*4,dt=tt.length;ot<dt;ot+=3)at[it++]=tt[ot]|tt[ot+1]<<8|tt[ot+2]<<16|4278190080}else{for(;ct<rt-2;ct+=3,it+=4){const ot=ut[ct],dt=ut[ct+1],pt=ut[ct+2];at[it]=ot|255,at[it+1]=ot<<24|dt>>>8|255,at[it+2]=dt<<16|pt>>>16|255,at[it+3]=pt<<8|255}for(let ot=ct*4,dt=tt.length;ot<dt;ot+=3)at[it++]=tt[ot]<<24|tt[ot+1]<<16|tt[ot+2]<<8|255}return{srcPos:nt,destPos:it}}function et(tt,nt){if(s.FeatureTest.isLittleEndian)for(let at=0,it=tt.length;at<it;at++)nt[at]=tt[at]*65793|4278190080;else for(let at=0,it=tt.length;at<it;at++)nt[at]=tt[at]*16843008|255}},(a,i)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.GlobalWorkerOptions=void 0;const o=Object.create(null);i.GlobalWorkerOptions=o,o.workerPort=null,o.workerSrc=""},(a,i,o)=>{var tt,yr,at,vr,st,yo;Object.defineProperty(i,"__esModule",{value:!0}),i.MessageHandler=void 0;var s=o(1);const $={UNKNOWN:0,DATA:1,ERROR:2},j={UNKNOWN:0,CANCEL:1,CANCEL_COMPLETE:2,CLOSE:3,ENQUEUE:4,ERROR:5,PULL:6,PULL_COMPLETE:7,START_COMPLETE:8};function _e(ct){switch(ct instanceof Error||typeof ct=="object"&&ct!==null||(0,s.unreachable)('wrapReason: Expected "reason" to be a (possibly cloned) Error.'),ct.name){case"AbortException":return new s.AbortException(ct.message);case"MissingPDFException":return new s.MissingPDFException(ct.message);case"PasswordException":return new s.PasswordException(ct.message,ct.code);case"UnexpectedResponseException":return new s.UnexpectedResponseException(ct.message,ct.status);case"UnknownErrorException":return new s.UnknownErrorException(ct.message,ct.details);default:return new s.UnknownErrorException(ct.message,ct.toString())}}class et{constructor(rt,ut,ot){Yt(this,tt);Yt(this,at);Yt(this,st);this.sourceName=rt,this.targetName=ut,this.comObj=ot,this.callbackId=1,this.streamId=1,this.streamSinks=Object.create(null),this.streamControllers=Object.create(null),this.callbackCapabilities=Object.create(null),this.actionHandler=Object.create(null),this._onComObjOnMessage=dt=>{const pt=dt.data;if(pt.targetName!==this.sourceName)return;if(pt.stream){un(this,at,vr).call(this,pt);return}if(pt.callback){const ft=pt.callbackId,ht=this.callbackCapabilities[ft];if(!ht)throw new Error(`Cannot resolve callback ${ft}`);if(delete this.callbackCapabilities[ft],pt.callback===$.DATA)ht.resolve(pt.data);else if(pt.callback===$.ERROR)ht.reject(_e(pt.reason));else throw new Error("Unexpected callback case");return}const mt=this.actionHandler[pt.action];if(!mt)throw new Error(`Unknown action from worker: ${pt.action}`);if(pt.callbackId){const ft=this.sourceName,ht=pt.sourceName;new Promise(function(yt){yt(mt(pt.data))}).then(function(yt){ot.postMessage({sourceName:ft,targetName:ht,callback:$.DATA,callbackId:pt.callbackId,data:yt})},function(yt){ot.postMessage({sourceName:ft,targetName:ht,callback:$.ERROR,callbackId:pt.callbackId,reason:_e(yt)})});return}if(pt.streamId){un(this,tt,yr).call(this,pt);return}mt(pt.data)},ot.addEventListener("message",this._onComObjOnMessage)}on(rt,ut){const ot=this.actionHandler;if(ot[rt])throw new Error(`There is already an actionName called "${rt}"`);ot[rt]=ut}send(rt,ut,ot){this.comObj.postMessage({sourceName:this.sourceName,targetName:this.targetName,action:rt,data:ut},ot)}sendWithPromise(rt,ut,ot){const dt=this.callbackId++,pt=new s.PromiseCapability;this.callbackCapabilities[dt]=pt;try{this.comObj.postMessage({sourceName:this.sourceName,targetName:this.targetName,action:rt,callbackId:dt,data:ut},ot)}catch(mt){pt.reject(mt)}return pt.promise}sendWithStream(rt,ut,ot,dt){const pt=this.streamId++,mt=this.sourceName,ft=this.targetName,ht=this.comObj;return new ReadableStream({start:yt=>{const bt=new s.PromiseCapability;return this.streamControllers[pt]={controller:yt,startCall:bt,pullCall:null,cancelCall:null,isClosed:!1},ht.postMessage({sourceName:mt,targetName:ft,action:rt,streamId:pt,data:ut,desiredSize:yt.desiredSize},dt),bt.promise},pull:yt=>{const bt=new s.PromiseCapability;return this.streamControllers[pt].pullCall=bt,ht.postMessage({sourceName:mt,targetName:ft,stream:j.PULL,streamId:pt,desiredSize:yt.desiredSize}),bt.promise},cancel:yt=>{(0,s.assert)(yt instanceof Error,"cancel must have a valid reason");const bt=new s.PromiseCapability;return this.streamControllers[pt].cancelCall=bt,this.streamControllers[pt].isClosed=!0,ht.postMessage({sourceName:mt,targetName:ft,stream:j.CANCEL,streamId:pt,reason:_e(yt)}),bt.promise}},ot)}destroy(){this.comObj.removeEventListener("message",this._onComObjOnMessage)}}tt=new WeakSet,yr=function(rt){const ut=rt.streamId,ot=this.sourceName,dt=rt.sourceName,pt=this.comObj,mt=this,ft=this.actionHandler[rt.action],ht={enqueue(yt,bt=1,gt){if(this.isCancelled)return;const xt=this.desiredSize;this.desiredSize-=bt,xt>0&&this.desiredSize<=0&&(this.sinkCapability=new s.PromiseCapability,this.ready=this.sinkCapability.promise),pt.postMessage({sourceName:ot,targetName:dt,stream:j.ENQUEUE,streamId:ut,chunk:yt},gt)},close(){this.isCancelled||(this.isCancelled=!0,pt.postMessage({sourceName:ot,targetName:dt,stream:j.CLOSE,streamId:ut}),delete mt.streamSinks[ut])},error(yt){(0,s.assert)(yt instanceof Error,"error must have a valid reason"),!this.isCancelled&&(this.isCancelled=!0,pt.postMessage({sourceName:ot,targetName:dt,stream:j.ERROR,streamId:ut,reason:_e(yt)}))},sinkCapability:new s.PromiseCapability,onPull:null,onCancel:null,isCancelled:!1,desiredSize:rt.desiredSize,ready:null};ht.sinkCapability.resolve(),ht.ready=ht.sinkCapability.promise,this.streamSinks[ut]=ht,new Promise(function(yt){yt(ft(rt.data,ht))}).then(function(){pt.postMessage({sourceName:ot,targetName:dt,stream:j.START_COMPLETE,streamId:ut,success:!0})},function(yt){pt.postMessage({sourceName:ot,targetName:dt,stream:j.START_COMPLETE,streamId:ut,reason:_e(yt)})})},at=new WeakSet,vr=function(rt){const ut=rt.streamId,ot=this.sourceName,dt=rt.sourceName,pt=this.comObj,mt=this.streamControllers[ut],ft=this.streamSinks[ut];switch(rt.stream){case j.START_COMPLETE:rt.success?mt.startCall.resolve():mt.startCall.reject(_e(rt.reason));break;case j.PULL_COMPLETE:rt.success?mt.pullCall.resolve():mt.pullCall.reject(_e(rt.reason));break;case j.PULL:if(!ft){pt.postMessage({sourceName:ot,targetName:dt,stream:j.PULL_COMPLETE,streamId:ut,success:!0});break}ft.desiredSize<=0&&rt.desiredSize>0&&ft.sinkCapability.resolve(),ft.desiredSize=rt.desiredSize,new Promise(function(ht){var yt;ht((yt=ft.onPull)==null?void 0:yt.call(ft))}).then(function(){pt.postMessage({sourceName:ot,targetName:dt,stream:j.PULL_COMPLETE,streamId:ut,success:!0})},function(ht){pt.postMessage({sourceName:ot,targetName:dt,stream:j.PULL_COMPLETE,streamId:ut,reason:_e(ht)})});break;case j.ENQUEUE:if((0,s.assert)(mt,"enqueue should have stream controller"),mt.isClosed)break;mt.controller.enqueue(rt.chunk);break;case j.CLOSE:if((0,s.assert)(mt,"close should have stream controller"),mt.isClosed)break;mt.isClosed=!0,mt.controller.close(),un(this,st,yo).call(this,mt,ut);break;case j.ERROR:(0,s.assert)(mt,"error should have stream controller"),mt.controller.error(_e(rt.reason)),un(this,st,yo).call(this,mt,ut);break;case j.CANCEL_COMPLETE:rt.success?mt.cancelCall.resolve():mt.cancelCall.reject(_e(rt.reason)),un(this,st,yo).call(this,mt,ut);break;case j.CANCEL:if(!ft)break;new Promise(function(ht){var yt;ht((yt=ft.onCancel)==null?void 0:yt.call(ft,_e(rt.reason)))}).then(function(){pt.postMessage({sourceName:ot,targetName:dt,stream:j.CANCEL_COMPLETE,streamId:ut,success:!0})},function(ht){pt.postMessage({sourceName:ot,targetName:dt,stream:j.CANCEL_COMPLETE,streamId:ut,reason:_e(ht)})}),ft.sinkCapability.reject(_e(rt.reason)),ft.isCancelled=!0,delete this.streamSinks[ut];break;default:throw new Error("Unexpected stream case")}},st=new WeakSet,yo=async function(rt,ut){var ot,dt,pt;await Promise.allSettled([(ot=rt.startCall)==null?void 0:ot.promise,(dt=rt.pullCall)==null?void 0:dt.promise,(pt=rt.cancelCall)==null?void 0:pt.promise]),delete this.streamControllers[ut]},i.MessageHandler=et},(a,i,o)=>{var j,_e;Object.defineProperty(i,"__esModule",{value:!0}),i.Metadata=void 0;var s=o(1);class ${constructor({parsedData:tt,rawData:nt}){Yt(this,j,void 0);Yt(this,_e,void 0);wn(this,j,tt),wn(this,_e,nt)}getRaw(){return St(this,_e)}get(tt){return St(this,j).get(tt)??null}getAll(){return(0,s.objectFromMap)(St(this,j))}has(tt){return St(this,j).has(tt)}}j=new WeakMap,_e=new WeakMap,i.Metadata=$},(a,i,o)=>{var tt,nt,at,it,st,lt,Wo;Object.defineProperty(i,"__esModule",{value:!0}),i.OptionalContentConfig=void 0;var s=o(1),$=o(8);const j=Symbol("INTERNAL");class _e{constructor(ut,ot){Yt(this,tt,!0);this.name=ut,this.intent=ot}get visible(){return St(this,tt)}_setVisible(ut,ot){ut!==j&&(0,s.unreachable)("Internal method `_setVisible` called."),wn(this,tt,ot)}}tt=new WeakMap;class et{constructor(ut){Yt(this,lt);Yt(this,nt,null);Yt(this,at,new Map);Yt(this,it,null);Yt(this,st,null);if(this.name=null,this.creator=null,ut!==null){this.name=ut.name,this.creator=ut.creator,wn(this,st,ut.order);for(const ot of ut.groups)St(this,at).set(ot.id,new _e(ot.name,ot.intent));if(ut.baseState==="OFF")for(const ot of St(this,at).values())ot._setVisible(j,!1);for(const ot of ut.on)St(this,at).get(ot)._setVisible(j,!0);for(const ot of ut.off)St(this,at).get(ot)._setVisible(j,!1);wn(this,it,this.getHash())}}isVisible(ut){if(St(this,at).size===0)return!0;if(!ut)return(0,s.warn)("Optional content group not defined."),!0;if(ut.type==="OCG")return St(this,at).has(ut.id)?St(this,at).get(ut.id).visible:((0,s.warn)(`Optional content group not found: ${ut.id}`),!0);if(ut.type==="OCMD"){if(ut.expression)return un(this,lt,Wo).call(this,ut.expression);if(!ut.policy||ut.policy==="AnyOn"){for(const ot of ut.ids){if(!St(this,at).has(ot))return(0,s.warn)(`Optional content group not found: ${ot}`),!0;if(St(this,at).get(ot).visible)return!0}return!1}else if(ut.policy==="AllOn"){for(const ot of ut.ids){if(!St(this,at).has(ot))return(0,s.warn)(`Optional content group not found: ${ot}`),!0;if(!St(this,at).get(ot).visible)return!1}return!0}else if(ut.policy==="AnyOff"){for(const ot of ut.ids){if(!St(this,at).has(ot))return(0,s.warn)(`Optional content group not found: ${ot}`),!0;if(!St(this,at).get(ot).visible)return!0}return!1}else if(ut.policy==="AllOff"){for(const ot of ut.ids){if(!St(this,at).has(ot))return(0,s.warn)(`Optional content group not found: ${ot}`),!0;if(St(this,at).get(ot).visible)return!1}return!0}return(0,s.warn)(`Unknown optional content policy ${ut.policy}.`),!0}return(0,s.warn)(`Unknown group type ${ut.type}.`),!0}setVisibility(ut,ot=!0){if(!St(this,at).has(ut)){(0,s.warn)(`Optional content group not found: ${ut}`);return}St(this,at).get(ut)._setVisible(j,!!ot),wn(this,nt,null)}get hasInitialVisibility(){return St(this,it)===null||this.getHash()===St(this,it)}getOrder(){return St(this,at).size?St(this,st)?St(this,st).slice():[...St(this,at).keys()]:null}getGroups(){return St(this,at).size>0?(0,s.objectFromMap)(St(this,at)):null}getGroup(ut){return St(this,at).get(ut)||null}getHash(){if(St(this,nt)!==null)return St(this,nt);const ut=new $.MurmurHash3_64;for(const[ot,dt]of St(this,at))ut.update(`${ot}:${dt.visible}`);return wn(this,nt,ut.hexdigest())}}nt=new WeakMap,at=new WeakMap,it=new WeakMap,st=new WeakMap,lt=new WeakSet,Wo=function(ut){const ot=ut.length;if(ot<2)return!0;const dt=ut[0];for(let pt=1;pt<ot;pt++){const mt=ut[pt];let ft;if(Array.isArray(mt))ft=un(this,lt,Wo).call(this,mt);else if(St(this,at).has(mt))ft=St(this,at).get(mt).visible;else return(0,s.warn)(`Optional content group not found: ${mt}`),!0;switch(dt){case"And":if(!ft)return!1;break;case"Or":if(ft)return!0;break;case"Not":return!ft;default:return!0}}return dt==="And"},i.OptionalContentConfig=et},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.PDFDataTransportStream=void 0;var s=o(1),$=o(6);class j{constructor({length:nt,initialData:at,progressiveDone:it=!1,contentDispositionFilename:st=null,disableRange:lt=!1,disableStream:ct=!1},rt){if((0,s.assert)(rt,'PDFDataTransportStream - missing required "pdfDataRangeTransport" argument.'),this._queuedChunks=[],this._progressiveDone=it,this._contentDispositionFilename=st,(at==null?void 0:at.length)>0){const ut=at instanceof Uint8Array&&at.byteLength===at.buffer.byteLength?at.buffer:new Uint8Array(at).buffer;this._queuedChunks.push(ut)}this._pdfDataRangeTransport=rt,this._isStreamingSupported=!ct,this._isRangeSupported=!lt,this._contentLength=nt,this._fullRequestReader=null,this._rangeReaders=[],this._pdfDataRangeTransport.addRangeListener((ut,ot)=>{this._onReceiveData({begin:ut,chunk:ot})}),this._pdfDataRangeTransport.addProgressListener((ut,ot)=>{this._onProgress({loaded:ut,total:ot})}),this._pdfDataRangeTransport.addProgressiveReadListener(ut=>{this._onReceiveData({chunk:ut})}),this._pdfDataRangeTransport.addProgressiveDoneListener(()=>{this._onProgressiveDone()}),this._pdfDataRangeTransport.transportReady()}_onReceiveData({begin:nt,chunk:at}){const it=at instanceof Uint8Array&&at.byteLength===at.buffer.byteLength?at.buffer:new Uint8Array(at).buffer;if(nt===void 0)this._fullRequestReader?this._fullRequestReader._enqueue(it):this._queuedChunks.push(it);else{const st=this._rangeReaders.some(function(lt){return lt._begin!==nt?!1:(lt._enqueue(it),!0)});(0,s.assert)(st,"_onReceiveData - no `PDFDataTransportStreamRangeReader` instance found.")}}get _progressiveDataLength(){var nt;return((nt=this._fullRequestReader)==null?void 0:nt._loaded)??0}_onProgress(nt){var at,it,st,lt;nt.total===void 0?(it=(at=this._rangeReaders[0])==null?void 0:at.onProgress)==null||it.call(at,{loaded:nt.loaded}):(lt=(st=this._fullRequestReader)==null?void 0:st.onProgress)==null||lt.call(st,{loaded:nt.loaded,total:nt.total})}_onProgressiveDone(){var nt;(nt=this._fullRequestReader)==null||nt.progressiveDone(),this._progressiveDone=!0}_removeRangeReader(nt){const at=this._rangeReaders.indexOf(nt);at>=0&&this._rangeReaders.splice(at,1)}getFullReader(){(0,s.assert)(!this._fullRequestReader,"PDFDataTransportStream.getFullReader can only be called once.");const nt=this._queuedChunks;return this._queuedChunks=null,new _e(this,nt,this._progressiveDone,this._contentDispositionFilename)}getRangeReader(nt,at){if(at<=this._progressiveDataLength)return null;const it=new et(this,nt,at);return this._pdfDataRangeTransport.requestDataRange(nt,at),this._rangeReaders.push(it),it}cancelAllRequests(nt){var at;(at=this._fullRequestReader)==null||at.cancel(nt);for(const it of this._rangeReaders.slice(0))it.cancel(nt);this._pdfDataRangeTransport.abort()}}i.PDFDataTransportStream=j;class _e{constructor(nt,at,it=!1,st=null){this._stream=nt,this._done=it||!1,this._filename=(0,$.isPdfFile)(st)?st:null,this._queuedChunks=at||[],this._loaded=0;for(const lt of this._queuedChunks)this._loaded+=lt.byteLength;this._requests=[],this._headersReady=Promise.resolve(),nt._fullRequestReader=this,this.onProgress=null}_enqueue(nt){this._done||(this._requests.length>0?this._requests.shift().resolve({value:nt,done:!1}):this._queuedChunks.push(nt),this._loaded+=nt.byteLength)}get headersReady(){return this._headersReady}get filename(){return this._filename}get isRangeSupported(){return this._stream._isRangeSupported}get isStreamingSupported(){return this._stream._isStreamingSupported}get contentLength(){return this._stream._contentLength}async read(){if(this._queuedChunks.length>0)return{value:this._queuedChunks.shift(),done:!1};if(this._done)return{value:void 0,done:!0};const nt=new s.PromiseCapability;return this._requests.push(nt),nt.promise}cancel(nt){this._done=!0;for(const at of this._requests)at.resolve({value:void 0,done:!0});this._requests.length=0}progressiveDone(){this._done||(this._done=!0)}}class et{constructor(nt,at,it){this._stream=nt,this._begin=at,this._end=it,this._queuedChunk=null,this._requests=[],this._done=!1,this.onProgress=null}_enqueue(nt){if(!this._done){if(this._requests.length===0)this._queuedChunk=nt;else{this._requests.shift().resolve({value:nt,done:!1});for(const it of this._requests)it.resolve({value:void 0,done:!0});this._requests.length=0}this._done=!0,this._stream._removeRangeReader(this)}}get isStreamingSupported(){return!1}async read(){if(this._queuedChunk){const at=this._queuedChunk;return this._queuedChunk=null,{value:at,done:!1}}if(this._done)return{value:void 0,done:!0};const nt=new s.PromiseCapability;return this._requests.push(nt),nt.promise}cancel(nt){this._done=!0;for(const at of this._requests)at.resolve({value:void 0,done:!0});this._requests.length=0,this._stream._removeRangeReader(this)}}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.PDFFetchStream=void 0;var s=o(1),$=o(20);function j(it,st,lt){return{method:"GET",headers:it,signal:lt.signal,mode:"cors",credentials:st?"include":"same-origin",redirect:"follow"}}function _e(it){const st=new Headers;for(const lt in it){const ct=it[lt];ct!==void 0&&st.append(lt,ct)}return st}function et(it){return it instanceof Uint8Array?it.buffer:it instanceof ArrayBuffer?it:((0,s.warn)(`getArrayBuffer - unexpected data format: ${it}`),new Uint8Array(it).buffer)}class tt{constructor(st){this.source=st,this.isHttp=/^https?:/i.test(st.url),this.httpHeaders=this.isHttp&&st.httpHeaders||{},this._fullRequestReader=null,this._rangeRequestReaders=[]}get _progressiveDataLength(){var st;return((st=this._fullRequestReader)==null?void 0:st._loaded)??0}getFullReader(){return(0,s.assert)(!this._fullRequestReader,"PDFFetchStream.getFullReader can only be called once."),this._fullRequestReader=new nt(this),this._fullRequestReader}getRangeReader(st,lt){if(lt<=this._progressiveDataLength)return null;const ct=new at(this,st,lt);return this._rangeRequestReaders.push(ct),ct}cancelAllRequests(st){var lt;(lt=this._fullRequestReader)==null||lt.cancel(st);for(const ct of this._rangeRequestReaders.slice(0))ct.cancel(st)}}i.PDFFetchStream=tt;class nt{constructor(st){this._stream=st,this._reader=null,this._loaded=0,this._filename=null;const lt=st.source;this._withCredentials=lt.withCredentials||!1,this._contentLength=lt.length,this._headersCapability=new s.PromiseCapability,this._disableRange=lt.disableRange||!1,this._rangeChunkSize=lt.rangeChunkSize,!this._rangeChunkSize&&!this._disableRange&&(this._disableRange=!0),this._abortController=new AbortController,this._isStreamingSupported=!lt.disableStream,this._isRangeSupported=!lt.disableRange,this._headers=_e(this._stream.httpHeaders);const ct=lt.url;fetch(ct,j(this._headers,this._withCredentials,this._abortController)).then(rt=>{if(!(0,$.validateResponseStatus)(rt.status))throw(0,$.createResponseStatusError)(rt.status,ct);this._reader=rt.body.getReader(),this._headersCapability.resolve();const ut=pt=>rt.headers.get(pt),{allowRangeRequests:ot,suggestedLength:dt}=(0,$.validateRangeRequestCapabilities)({getResponseHeader:ut,isHttp:this._stream.isHttp,rangeChunkSize:this._rangeChunkSize,disableRange:this._disableRange});this._isRangeSupported=ot,this._contentLength=dt||this._contentLength,this._filename=(0,$.extractFilenameFromHeader)(ut),!this._isStreamingSupported&&this._isRangeSupported&&this.cancel(new s.AbortException("Streaming is disabled."))}).catch(this._headersCapability.reject),this.onProgress=null}get headersReady(){return this._headersCapability.promise}get filename(){return this._filename}get contentLength(){return this._contentLength}get isRangeSupported(){return this._isRangeSupported}get isStreamingSupported(){return this._isStreamingSupported}async read(){var ct;await this._headersCapability.promise;const{value:st,done:lt}=await this._reader.read();return lt?{value:st,done:lt}:(this._loaded+=st.byteLength,(ct=this.onProgress)==null||ct.call(this,{loaded:this._loaded,total:this._contentLength}),{value:et(st),done:!1})}cancel(st){var lt;(lt=this._reader)==null||lt.cancel(st),this._abortController.abort()}}class at{constructor(st,lt,ct){this._stream=st,this._reader=null,this._loaded=0;const rt=st.source;this._withCredentials=rt.withCredentials||!1,this._readCapability=new s.PromiseCapability,this._isStreamingSupported=!rt.disableStream,this._abortController=new AbortController,this._headers=_e(this._stream.httpHeaders),this._headers.append("Range",`bytes=${lt}-${ct-1}`);const ut=rt.url;fetch(ut,j(this._headers,this._withCredentials,this._abortController)).then(ot=>{if(!(0,$.validateResponseStatus)(ot.status))throw(0,$.createResponseStatusError)(ot.status,ut);this._readCapability.resolve(),this._reader=ot.body.getReader()}).catch(this._readCapability.reject),this.onProgress=null}get isStreamingSupported(){return this._isStreamingSupported}async read(){var ct;await this._readCapability.promise;const{value:st,done:lt}=await this._reader.read();return lt?{value:st,done:lt}:(this._loaded+=st.byteLength,(ct=this.onProgress)==null||ct.call(this,{loaded:this._loaded}),{value:et(st),done:!1})}cancel(st){var lt;(lt=this._reader)==null||lt.cancel(st),this._abortController.abort()}}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.createResponseStatusError=tt,i.extractFilenameFromHeader=et,i.validateRangeRequestCapabilities=_e,i.validateResponseStatus=nt;var s=o(1),$=o(21),j=o(6);function _e({getResponseHeader:at,isHttp:it,rangeChunkSize:st,disableRange:lt}){const ct={allowRangeRequests:!1,suggestedLength:void 0},rt=parseInt(at("Content-Length"),10);return!Number.isInteger(rt)||(ct.suggestedLength=rt,rt<=2*st)||lt||!it||at("Accept-Ranges")!=="bytes"||(at("Content-Encoding")||"identity")!=="identity"||(ct.allowRangeRequests=!0),ct}function et(at){const it=at("Content-Disposition");if(it){let st=(0,$.getFilenameFromContentDispositionHeader)(it);if(st.includes("%"))try{st=decodeURIComponent(st)}catch{}if((0,j.isPdfFile)(st))return st}return null}function tt(at,it){return at===404||at===0&&it.startsWith("file:")?new s.MissingPDFException('Missing PDF "'+it+'".'):new s.UnexpectedResponseException(`Unexpected server response (${at}) while retrieving PDF "${it}".`,at)}function nt(at){return at===200||at===206}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.getFilenameFromContentDispositionHeader=$;var s=o(1);function $(j){let _e=!0,et=tt("filename\\*","i").exec(j);if(et){et=et[1];let rt=st(et);return rt=unescape(rt),rt=lt(rt),rt=ct(rt),at(rt)}if(et=it(j),et){const rt=ct(et);return at(rt)}if(et=tt("filename","i").exec(j),et){et=et[1];let rt=st(et);return rt=ct(rt),at(rt)}function tt(rt,ut){return new RegExp("(?:^|;)\\s*"+rt+'\\s*=\\s*([^";\\s][^;\\s]*|"(?:[^"\\\\]|\\\\"?)+"?)',ut)}function nt(rt,ut){if(rt){if(!/^[\x00-\xFF]+$/.test(ut))return ut;try{const ot=new TextDecoder(rt,{fatal:!0}),dt=(0,s.stringToBytes)(ut);ut=ot.decode(dt),_e=!1}catch{}}return ut}function at(rt){return _e&&/[\x80-\xff]/.test(rt)&&(rt=nt("utf-8",rt),_e&&(rt=nt("iso-8859-1",rt))),rt}function it(rt){const ut=[];let ot;const dt=tt("filename\\*((?!0\\d)\\d+)(\\*?)","ig");for(;(ot=dt.exec(rt))!==null;){let[,mt,ft,ht]=ot;if(mt=parseInt(mt,10),mt in ut){if(mt===0)break;continue}ut[mt]=[ft,ht]}const pt=[];for(let mt=0;mt<ut.length&&mt in ut;++mt){let[ft,ht]=ut[mt];ht=st(ht),ft&&(ht=unescape(ht),mt===0&&(ht=lt(ht))),pt.push(ht)}return pt.join("")}function st(rt){if(rt.startsWith('"')){const ut=rt.slice(1).split('\\"');for(let ot=0;ot<ut.length;++ot){const dt=ut[ot].indexOf('"');dt!==-1&&(ut[ot]=ut[ot].slice(0,dt),ut.length=ot+1),ut[ot]=ut[ot].replaceAll(/\\(.)/g,"$1")}rt=ut.join('"')}return rt}function lt(rt){const ut=rt.indexOf("'");if(ut===-1)return rt;const ot=rt.slice(0,ut),pt=rt.slice(ut+1).replace(/^[^']*'/,"");return nt(ot,pt)}function ct(rt){return!rt.startsWith("=?")||/[\x00-\x19\x80-\xff]/.test(rt)?rt:rt.replaceAll(/=\?([\w-]*)\?([QqBb])\?((?:[^?]|\?(?!=))*)\?=/g,function(ut,ot,dt,pt){if(dt==="q"||dt==="Q")return pt=pt.replaceAll("_"," "),pt=pt.replaceAll(/=([0-9a-fA-F]{2})/g,function(mt,ft){return String.fromCharCode(parseInt(ft,16))}),nt(ot,pt);try{pt=atob(pt)}catch{}return nt(ot,pt)})}return""}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.PDFNetworkStream=void 0;var s=o(1),$=o(20);const j=200,_e=206;function et(st){const lt=st.response;return typeof lt!="string"?lt:(0,s.stringToBytes)(lt).buffer}class tt{constructor(lt,ct={}){this.url=lt,this.isHttp=/^https?:/i.test(lt),this.httpHeaders=this.isHttp&&ct.httpHeaders||Object.create(null),this.withCredentials=ct.withCredentials||!1,this.currXhrId=0,this.pendingRequests=Object.create(null)}requestRange(lt,ct,rt){const ut={begin:lt,end:ct};for(const ot in rt)ut[ot]=rt[ot];return this.request(ut)}requestFull(lt){return this.request(lt)}request(lt){const ct=new XMLHttpRequest,rt=this.currXhrId++,ut=this.pendingRequests[rt]={xhr:ct};ct.open("GET",this.url),ct.withCredentials=this.withCredentials;for(const ot in this.httpHeaders){const dt=this.httpHeaders[ot];dt!==void 0&&ct.setRequestHeader(ot,dt)}return this.isHttp&&"begin"in lt&&"end"in lt?(ct.setRequestHeader("Range",`bytes=${lt.begin}-${lt.end-1}`),ut.expectedStatus=_e):ut.expectedStatus=j,ct.responseType="arraybuffer",lt.onError&&(ct.onerror=function(ot){lt.onError(ct.status)}),ct.onreadystatechange=this.onStateChange.bind(this,rt),ct.onprogress=this.onProgress.bind(this,rt),ut.onHeadersReceived=lt.onHeadersReceived,ut.onDone=lt.onDone,ut.onError=lt.onError,ut.onProgress=lt.onProgress,ct.send(null),rt}onProgress(lt,ct){var ut;const rt=this.pendingRequests[lt];rt&&((ut=rt.onProgress)==null||ut.call(rt,ct))}onStateChange(lt,ct){var mt,ft,ht;const rt=this.pendingRequests[lt];if(!rt)return;const ut=rt.xhr;if(ut.readyState>=2&&rt.onHeadersReceived&&(rt.onHeadersReceived(),delete rt.onHeadersReceived),ut.readyState!==4||!(lt in this.pendingRequests))return;if(delete this.pendingRequests[lt],ut.status===0&&this.isHttp){(mt=rt.onError)==null||mt.call(rt,ut.status);return}const ot=ut.status||j;if(!(ot===j&&rt.expectedStatus===_e)&&ot!==rt.expectedStatus){(ft=rt.onError)==null||ft.call(rt,ut.status);return}const pt=et(ut);if(ot===_e){const yt=ut.getResponseHeader("Content-Range"),bt=/bytes (\d+)-(\d+)\/(\d+)/.exec(yt);rt.onDone({begin:parseInt(bt[1],10),chunk:pt})}else pt?rt.onDone({begin:0,chunk:pt}):(ht=rt.onError)==null||ht.call(rt,ut.status)}getRequestXhr(lt){return this.pendingRequests[lt].xhr}isPendingRequest(lt){return lt in this.pendingRequests}abortRequest(lt){const ct=this.pendingRequests[lt].xhr;delete this.pendingRequests[lt],ct.abort()}}class nt{constructor(lt){this._source=lt,this._manager=new tt(lt.url,{httpHeaders:lt.httpHeaders,withCredentials:lt.withCredentials}),this._rangeChunkSize=lt.rangeChunkSize,this._fullRequestReader=null,this._rangeRequestReaders=[]}_onRangeRequestReaderClosed(lt){const ct=this._rangeRequestReaders.indexOf(lt);ct>=0&&this._rangeRequestReaders.splice(ct,1)}getFullReader(){return(0,s.assert)(!this._fullRequestReader,"PDFNetworkStream.getFullReader can only be called once."),this._fullRequestReader=new at(this._manager,this._source),this._fullRequestReader}getRangeReader(lt,ct){const rt=new it(this._manager,lt,ct);return rt.onClosed=this._onRangeRequestReaderClosed.bind(this),this._rangeRequestReaders.push(rt),rt}cancelAllRequests(lt){var ct;(ct=this._fullRequestReader)==null||ct.cancel(lt);for(const rt of this._rangeRequestReaders.slice(0))rt.cancel(lt)}}i.PDFNetworkStream=nt;class at{constructor(lt,ct){this._manager=lt;const rt={onHeadersReceived:this._onHeadersReceived.bind(this),onDone:this._onDone.bind(this),onError:this._onError.bind(this),onProgress:this._onProgress.bind(this)};this._url=ct.url,this._fullRequestId=lt.requestFull(rt),this._headersReceivedCapability=new s.PromiseCapability,this._disableRange=ct.disableRange||!1,this._contentLength=ct.length,this._rangeChunkSize=ct.rangeChunkSize,!this._rangeChunkSize&&!this._disableRange&&(this._disableRange=!0),this._isStreamingSupported=!1,this._isRangeSupported=!1,this._cachedChunks=[],this._requests=[],this._done=!1,this._storedError=void 0,this._filename=null,this.onProgress=null}_onHeadersReceived(){const lt=this._fullRequestId,ct=this._manager.getRequestXhr(lt),rt=dt=>ct.getResponseHeader(dt),{allowRangeRequests:ut,suggestedLength:ot}=(0,$.validateRangeRequestCapabilities)({getResponseHeader:rt,isHttp:this._manager.isHttp,rangeChunkSize:this._rangeChunkSize,disableRange:this._disableRange});ut&&(this._isRangeSupported=!0),this._contentLength=ot||this._contentLength,this._filename=(0,$.extractFilenameFromHeader)(rt),this._isRangeSupported&&this._manager.abortRequest(lt),this._headersReceivedCapability.resolve()}_onDone(lt){if(lt&&(this._requests.length>0?this._requests.shift().resolve({value:lt.chunk,done:!1}):this._cachedChunks.push(lt.chunk)),this._done=!0,!(this._cachedChunks.length>0)){for(const ct of this._requests)ct.resolve({value:void 0,done:!0});this._requests.length=0}}_onError(lt){this._storedError=(0,$.createResponseStatusError)(lt,this._url),this._headersReceivedCapability.reject(this._storedError);for(const ct of this._requests)ct.reject(this._storedError);this._requests.length=0,this._cachedChunks.length=0}_onProgress(lt){var ct;(ct=this.onProgress)==null||ct.call(this,{loaded:lt.loaded,total:lt.lengthComputable?lt.total:this._contentLength})}get filename(){return this._filename}get isRangeSupported(){return this._isRangeSupported}get isStreamingSupported(){return this._isStreamingSupported}get contentLength(){return this._contentLength}get headersReady(){return this._headersReceivedCapability.promise}async read(){if(this._storedError)throw this._storedError;if(this._cachedChunks.length>0)return{value:this._cachedChunks.shift(),done:!1};if(this._done)return{value:void 0,done:!0};const lt=new s.PromiseCapability;return this._requests.push(lt),lt.promise}cancel(lt){this._done=!0,this._headersReceivedCapability.reject(lt);for(const ct of this._requests)ct.resolve({value:void 0,done:!0});this._requests.length=0,this._manager.isPendingRequest(this._fullRequestId)&&this._manager.abortRequest(this._fullRequestId),this._fullRequestReader=null}}class it{constructor(lt,ct,rt){this._manager=lt;const ut={onDone:this._onDone.bind(this),onError:this._onError.bind(this),onProgress:this._onProgress.bind(this)};this._url=lt.url,this._requestId=lt.requestRange(ct,rt,ut),this._requests=[],this._queuedChunk=null,this._done=!1,this._storedError=void 0,this.onProgress=null,this.onClosed=null}_close(){var lt;(lt=this.onClosed)==null||lt.call(this,this)}_onDone(lt){const ct=lt.chunk;this._requests.length>0?this._requests.shift().resolve({value:ct,done:!1}):this._queuedChunk=ct,this._done=!0;for(const rt of this._requests)rt.resolve({value:void 0,done:!0});this._requests.length=0,this._close()}_onError(lt){this._storedError=(0,$.createResponseStatusError)(lt,this._url);for(const ct of this._requests)ct.reject(this._storedError);this._requests.length=0,this._queuedChunk=null}_onProgress(lt){var ct;this.isStreamingSupported||(ct=this.onProgress)==null||ct.call(this,{loaded:lt.loaded})}get isStreamingSupported(){return!1}async read(){if(this._storedError)throw this._storedError;if(this._queuedChunk!==null){const ct=this._queuedChunk;return this._queuedChunk=null,{value:ct,done:!1}}if(this._done)return{value:void 0,done:!0};const lt=new s.PromiseCapability;return this._requests.push(lt),lt.promise}cancel(lt){this._done=!0;for(const ct of this._requests)ct.resolve({value:void 0,done:!0});this._requests.length=0,this._manager.isPendingRequest(this._requestId)&&this._manager.abortRequest(this._requestId),this._close()}}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.PDFNodeStream=void 0;var s=o(1),$=o(20);const j=/^file:\/\/\/[a-zA-Z]:\//;function _e(rt){const ut=require$$5,ot=ut.parse(rt);return ot.protocol==="file:"||ot.host?ot:/^[a-z]:[/\\]/i.test(rt)?ut.parse(`file:///${rt}`):(ot.host||(ot.protocol="file:"),ot)}class et{constructor(ut){this.source=ut,this.url=_e(ut.url),this.isHttp=this.url.protocol==="http:"||this.url.protocol==="https:",this.isFsUrl=this.url.protocol==="file:",this.httpHeaders=this.isHttp&&ut.httpHeaders||{},this._fullRequestReader=null,this._rangeRequestReaders=[]}get _progressiveDataLength(){var ut;return((ut=this._fullRequestReader)==null?void 0:ut._loaded)??0}getFullReader(){return(0,s.assert)(!this._fullRequestReader,"PDFNodeStream.getFullReader can only be called once."),this._fullRequestReader=this.isFsUrl?new lt(this):new it(this),this._fullRequestReader}getRangeReader(ut,ot){if(ot<=this._progressiveDataLength)return null;const dt=this.isFsUrl?new ct(this,ut,ot):new st(this,ut,ot);return this._rangeRequestReaders.push(dt),dt}cancelAllRequests(ut){var ot;(ot=this._fullRequestReader)==null||ot.cancel(ut);for(const dt of this._rangeRequestReaders.slice(0))dt.cancel(ut)}}i.PDFNodeStream=et;class tt{constructor(ut){this._url=ut.url,this._done=!1,this._storedError=null,this.onProgress=null;const ot=ut.source;this._contentLength=ot.length,this._loaded=0,this._filename=null,this._disableRange=ot.disableRange||!1,this._rangeChunkSize=ot.rangeChunkSize,!this._rangeChunkSize&&!this._disableRange&&(this._disableRange=!0),this._isStreamingSupported=!ot.disableStream,this._isRangeSupported=!ot.disableRange,this._readableStream=null,this._readCapability=new s.PromiseCapability,this._headersCapability=new s.PromiseCapability}get headersReady(){return this._headersCapability.promise}get filename(){return this._filename}get contentLength(){return this._contentLength}get isRangeSupported(){return this._isRangeSupported}get isStreamingSupported(){return this._isStreamingSupported}async read(){var dt;if(await this._readCapability.promise,this._done)return{value:void 0,done:!0};if(this._storedError)throw this._storedError;const ut=this._readableStream.read();return ut===null?(this._readCapability=new s.PromiseCapability,this.read()):(this._loaded+=ut.length,(dt=this.onProgress)==null||dt.call(this,{loaded:this._loaded,total:this._contentLength}),{value:new Uint8Array(ut).buffer,done:!1})}cancel(ut){if(!this._readableStream){this._error(ut);return}this._readableStream.destroy(ut)}_error(ut){this._storedError=ut,this._readCapability.resolve()}_setReadableStream(ut){this._readableStream=ut,ut.on("readable",()=>{this._readCapability.resolve()}),ut.on("end",()=>{ut.destroy(),this._done=!0,this._readCapability.resolve()}),ut.on("error",ot=>{this._error(ot)}),!this._isStreamingSupported&&this._isRangeSupported&&this._error(new s.AbortException("streaming is disabled")),this._storedError&&this._readableStream.destroy(this._storedError)}}class nt{constructor(ut){this._url=ut.url,this._done=!1,this._storedError=null,this.onProgress=null,this._loaded=0,this._readableStream=null,this._readCapability=new s.PromiseCapability;const ot=ut.source;this._isStreamingSupported=!ot.disableStream}get isStreamingSupported(){return this._isStreamingSupported}async read(){var dt;if(await this._readCapability.promise,this._done)return{value:void 0,done:!0};if(this._storedError)throw this._storedError;const ut=this._readableStream.read();return ut===null?(this._readCapability=new s.PromiseCapability,this.read()):(this._loaded+=ut.length,(dt=this.onProgress)==null||dt.call(this,{loaded:this._loaded}),{value:new Uint8Array(ut).buffer,done:!1})}cancel(ut){if(!this._readableStream){this._error(ut);return}this._readableStream.destroy(ut)}_error(ut){this._storedError=ut,this._readCapability.resolve()}_setReadableStream(ut){this._readableStream=ut,ut.on("readable",()=>{this._readCapability.resolve()}),ut.on("end",()=>{ut.destroy(),this._done=!0,this._readCapability.resolve()}),ut.on("error",ot=>{this._error(ot)}),this._storedError&&this._readableStream.destroy(this._storedError)}}function at(rt,ut){return{protocol:rt.protocol,auth:rt.auth,host:rt.hostname,port:rt.port,path:rt.path,method:"GET",headers:ut}}class it extends tt{constructor(ut){super(ut);const ot=dt=>{if(dt.statusCode===404){const ht=new s.MissingPDFException(`Missing PDF "${this._url}".`);this._storedError=ht,this._headersCapability.reject(ht);return}this._headersCapability.resolve(),this._setReadableStream(dt);const pt=ht=>this._readableStream.headers[ht.toLowerCase()],{allowRangeRequests:mt,suggestedLength:ft}=(0,$.validateRangeRequestCapabilities)({getResponseHeader:pt,isHttp:ut.isHttp,rangeChunkSize:this._rangeChunkSize,disableRange:this._disableRange});this._isRangeSupported=mt,this._contentLength=ft||this._contentLength,this._filename=(0,$.extractFilenameFromHeader)(pt)};if(this._request=null,this._url.protocol==="http:"){const dt=require$$5;this._request=dt.request(at(this._url,ut.httpHeaders),ot)}else{const dt=require$$5;this._request=dt.request(at(this._url,ut.httpHeaders),ot)}this._request.on("error",dt=>{this._storedError=dt,this._headersCapability.reject(dt)}),this._request.end()}}class st extends nt{constructor(ut,ot,dt){super(ut),this._httpHeaders={};for(const mt in ut.httpHeaders){const ft=ut.httpHeaders[mt];ft!==void 0&&(this._httpHeaders[mt]=ft)}this._httpHeaders.Range=`bytes=${ot}-${dt-1}`;const pt=mt=>{if(mt.statusCode===404){const ft=new s.MissingPDFException(`Missing PDF "${this._url}".`);this._storedError=ft;return}this._setReadableStream(mt)};if(this._request=null,this._url.protocol==="http:"){const mt=require$$5;this._request=mt.request(at(this._url,this._httpHeaders),pt)}else{const mt=require$$5;this._request=mt.request(at(this._url,this._httpHeaders),pt)}this._request.on("error",mt=>{this._storedError=mt}),this._request.end()}}class lt extends tt{constructor(ut){super(ut);let ot=decodeURIComponent(this._url.path);j.test(this._url.href)&&(ot=ot.replace(/^\//,""));const dt=require$$5;dt.lstat(ot,(pt,mt)=>{if(pt){pt.code==="ENOENT"&&(pt=new s.MissingPDFException(`Missing PDF "${ot}".`)),this._storedError=pt,this._headersCapability.reject(pt);return}this._contentLength=mt.size,this._setReadableStream(dt.createReadStream(ot)),this._headersCapability.resolve()})}}class ct extends nt{constructor(ut,ot,dt){super(ut);let pt=decodeURIComponent(this._url.path);j.test(this._url.href)&&(pt=pt.replace(/^\//,""));const mt=require$$5;this._setReadableStream(mt.createReadStream(pt,{start:ot,end:dt-1}))}}},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.SVGGraphics=void 0;var s=o(6),$=o(1);const j={fontStyle:"normal",fontWeight:"normal",fillColor:"#000000"},_e="http://www.w3.org/XML/1998/namespace",et="http://www.w3.org/1999/xlink",tt=["butt","round","square"],nt=["miter","round","bevel"],at=function(mt,ft="",ht=!1){if(URL.createObjectURL&&typeof Blob<"u"&&!ht)return URL.createObjectURL(new Blob([mt],{type:ft}));const yt="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=";let bt=`data:${ft};base64,`;for(let gt=0,xt=mt.length;gt<xt;gt+=3){const vt=mt[gt]&255,Lt=mt[gt+1]&255,$t=mt[gt+2]&255,Tt=vt>>2,Et=(vt&3)<<4|Lt>>4,Dt=gt+1<xt?(Lt&15)<<2|$t>>6:64,It=gt+2<xt?$t&63:64;bt+=yt[Tt]+yt[Et]+yt[Dt]+yt[It]}return bt},it=function(){const mt=new Uint8Array([137,80,78,71,13,10,26,10]),ft=12,ht=new Int32Array(256);for(let $t=0;$t<256;$t++){let Tt=$t;for(let Et=0;Et<8;Et++)Tt=Tt&1?3988292384^Tt>>1&2147483647:Tt>>1&2147483647;ht[$t]=Tt}function yt($t,Tt,Et){let Dt=-1;for(let It=Tt;It<Et;It++){const Ct=(Dt^$t[It])&255,jt=ht[Ct];Dt=Dt>>>8^jt}return Dt^-1}function bt($t,Tt,Et,Dt){let It=Dt;const Ct=Tt.length;Et[It]=Ct>>24&255,Et[It+1]=Ct>>16&255,Et[It+2]=Ct>>8&255,Et[It+3]=Ct&255,It+=4,Et[It]=$t.charCodeAt(0)&255,Et[It+1]=$t.charCodeAt(1)&255,Et[It+2]=$t.charCodeAt(2)&255,Et[It+3]=$t.charCodeAt(3)&255,It+=4,Et.set(Tt,It),It+=Tt.length;const jt=yt(Et,Dt+4,It);Et[It]=jt>>24&255,Et[It+1]=jt>>16&255,Et[It+2]=jt>>8&255,Et[It+3]=jt&255}function gt($t,Tt,Et){let Dt=1,It=0;for(let Ct=Tt;Ct<Et;++Ct)Dt=(Dt+($t[Ct]&255))%65521,It=(It+Dt)%65521;return It<<16|Dt}function xt($t){if(!$.isNodeJS)return vt($t);try{const Tt=parseInt(process.versions.node)>=8?$t:Buffer.from($t),Et=require$$5.deflateSync(Tt,{level:9});return Et instanceof Uint8Array?Et:new Uint8Array(Et)}catch(Tt){(0,$.warn)("Not compressing PNG because zlib.deflateSync is unavailable: "+Tt)}return vt($t)}function vt($t){let Tt=$t.length;const Et=65535,Dt=Math.ceil(Tt/Et),It=new Uint8Array(2+Tt+Dt*5+4);let Ct=0;It[Ct++]=120,It[Ct++]=156;let jt=0;for(;Tt>Et;)It[Ct++]=0,It[Ct++]=255,It[Ct++]=255,It[Ct++]=0,It[Ct++]=0,It.set($t.subarray(jt,jt+Et),Ct),Ct+=Et,jt+=Et,Tt-=Et;It[Ct++]=1,It[Ct++]=Tt&255,It[Ct++]=Tt>>8&255,It[Ct++]=~Tt&65535&255,It[Ct++]=(~Tt&65535)>>8&255,It.set($t.subarray(jt),Ct),Ct+=$t.length-jt;const Zt=gt($t,0,$t.length);return It[Ct++]=Zt>>24&255,It[Ct++]=Zt>>16&255,It[Ct++]=Zt>>8&255,It[Ct++]=Zt&255,It}function Lt($t,Tt,Et,Dt){const It=$t.width,Ct=$t.height;let jt,Zt,Xt;const sn=$t.data;switch(Tt){case $.ImageKind.GRAYSCALE_1BPP:Zt=0,jt=1,Xt=It+7>>3;break;case $.ImageKind.RGB_24BPP:Zt=2,jt=8,Xt=It*3;break;case $.ImageKind.RGBA_32BPP:Zt=6,jt=8,Xt=It*4;break;default:throw new Error("invalid format")}const Ft=new Uint8Array((1+Xt)*Ct);let wt=0,kt=0;for(let zt=0;zt<Ct;++zt)Ft[wt++]=0,Ft.set(sn.subarray(kt,kt+Xt),wt),kt+=Xt,wt+=Xt;if(Tt===$.ImageKind.GRAYSCALE_1BPP&&Dt){wt=0;for(let zt=0;zt<Ct;zt++){wt++;for(let Gt=0;Gt<Xt;Gt++)Ft[wt++]^=255}}const At=new Uint8Array([It>>24&255,It>>16&255,It>>8&255,It&255,Ct>>24&255,Ct>>16&255,Ct>>8&255,Ct&255,jt,Zt,0,0,0]),Pt=xt(Ft),Mt=mt.length+ft*3+At.length+Pt.length,Ot=new Uint8Array(Mt);let Bt=0;return Ot.set(mt,Bt),Bt+=mt.length,bt("IHDR",At,Ot,Bt),Bt+=ft+At.length,bt("IDATA",Pt,Ot,Bt),Bt+=ft+Pt.length,bt("IEND",new Uint8Array(0),Ot,Bt),at(Ot,"image/png",Et)}return function(Tt,Et,Dt){const It=Tt.kind===void 0?$.ImageKind.GRAYSCALE_1BPP:Tt.kind;return Lt(Tt,It,Et,Dt)}}();class st{constructor(){this.fontSizeScale=1,this.fontWeight=j.fontWeight,this.fontSize=0,this.textMatrix=$.IDENTITY_MATRIX,this.fontMatrix=$.FONT_IDENTITY_MATRIX,this.leading=0,this.textRenderingMode=$.TextRenderingMode.FILL,this.textMatrixScale=1,this.x=0,this.y=0,this.lineX=0,this.lineY=0,this.charSpacing=0,this.wordSpacing=0,this.textHScale=1,this.textRise=0,this.fillColor=j.fillColor,this.strokeColor="#000000",this.fillAlpha=1,this.strokeAlpha=1,this.lineWidth=1,this.lineJoin="",this.lineCap="",this.miterLimit=0,this.dashArray=[],this.dashPhase=0,this.dependencies=[],this.activeClipUrl=null,this.clipGroup=null,this.maskId=""}clone(){return Object.create(this)}setCurrentPoint(ft,ht){this.x=ft,this.y=ht}}function lt(mt){let ft=[];const ht=[];for(const yt of mt){if(yt.fn==="save"){ft.push({fnId:92,fn:"group",items:[]}),ht.push(ft),ft=ft.at(-1).items;continue}yt.fn==="restore"?ft=ht.pop():ft.push(yt)}return ft}function ct(mt){if(Number.isInteger(mt))return mt.toString();const ft=mt.toFixed(10);let ht=ft.length-1;if(ft[ht]!=="0")return ft;do ht--;while(ft[ht]==="0");return ft.substring(0,ft[ht]==="."?ht:ht+1)}function rt(mt){if(mt[4]===0&&mt[5]===0){if(mt[1]===0&&mt[2]===0)return mt[0]===1&&mt[3]===1?"":`scale(${ct(mt[0])} ${ct(mt[3])})`;if(mt[0]===mt[3]&&mt[1]===-mt[2]){const ft=Math.acos(mt[0])*180/Math.PI;return`rotate(${ct(ft)})`}}else if(mt[0]===1&&mt[1]===0&&mt[2]===0&&mt[3]===1)return`translate(${ct(mt[4])} ${ct(mt[5])})`;return`matrix(${ct(mt[0])} ${ct(mt[1])} ${ct(mt[2])} ${ct(mt[3])} ${ct(mt[4])} ${ct(mt[5])})`}let ut=0,ot=0,dt=0;class pt{constructor(ft,ht,yt=!1){(0,s.deprecated)("The SVG back-end is no longer maintained and *may* be removed in the future."),this.svgFactory=new s.DOMSVGFactory,this.current=new st,this.transformMatrix=$.IDENTITY_MATRIX,this.transformStack=[],this.extraStack=[],this.commonObjs=ft,this.objs=ht,this.pendingClip=null,this.pendingEOFill=!1,this.embedFonts=!1,this.embeddedFonts=Object.create(null),this.cssStyle=null,this.forceDataSchema=!!yt,this._operatorIdMapping=[];for(const bt in $.OPS)this._operatorIdMapping[$.OPS[bt]]=bt}getObject(ft,ht=null){return typeof ft=="string"?ft.startsWith("g_")?this.commonObjs.get(ft):this.objs.get(ft):ht}save(){this.transformStack.push(this.transformMatrix);const ft=this.current;this.extraStack.push(ft),this.current=ft.clone()}restore(){this.transformMatrix=this.transformStack.pop(),this.current=this.extraStack.pop(),this.pendingClip=null,this.tgrp=null}group(ft){this.save(),this.executeOpTree(ft),this.restore()}loadDependencies(ft){const ht=ft.fnArray,yt=ft.argsArray;for(let bt=0,gt=ht.length;bt<gt;bt++)if(ht[bt]===$.OPS.dependency)for(const xt of yt[bt]){const vt=xt.startsWith("g_")?this.commonObjs:this.objs,Lt=new Promise($t=>{vt.get(xt,$t)});this.current.dependencies.push(Lt)}return Promise.all(this.current.dependencies)}transform(ft,ht,yt,bt,gt,xt){const vt=[ft,ht,yt,bt,gt,xt];this.transformMatrix=$.Util.transform(this.transformMatrix,vt),this.tgrp=null}getSVG(ft,ht){this.viewport=ht;const yt=this._initialize(ht);return this.loadDependencies(ft).then(()=>(this.transformMatrix=$.IDENTITY_MATRIX,this.executeOpTree(this.convertOpList(ft)),yt))}convertOpList(ft){const ht=this._operatorIdMapping,yt=ft.argsArray,bt=ft.fnArray,gt=[];for(let xt=0,vt=bt.length;xt<vt;xt++){const Lt=bt[xt];gt.push({fnId:Lt,fn:ht[Lt],args:yt[xt]})}return lt(gt)}executeOpTree(ft){for(const ht of ft){const yt=ht.fn,bt=ht.fnId,gt=ht.args;switch(bt|0){case $.OPS.beginText:this.beginText();break;case $.OPS.dependency:break;case $.OPS.setLeading:this.setLeading(gt);break;case $.OPS.setLeadingMoveText:this.setLeadingMoveText(gt[0],gt[1]);break;case $.OPS.setFont:this.setFont(gt);break;case $.OPS.showText:this.showText(gt[0]);break;case $.OPS.showSpacedText:this.showText(gt[0]);break;case $.OPS.endText:this.endText();break;case $.OPS.moveText:this.moveText(gt[0],gt[1]);break;case $.OPS.setCharSpacing:this.setCharSpacing(gt[0]);break;case $.OPS.setWordSpacing:this.setWordSpacing(gt[0]);break;case $.OPS.setHScale:this.setHScale(gt[0]);break;case $.OPS.setTextMatrix:this.setTextMatrix(gt[0],gt[1],gt[2],gt[3],gt[4],gt[5]);break;case $.OPS.setTextRise:this.setTextRise(gt[0]);break;case $.OPS.setTextRenderingMode:this.setTextRenderingMode(gt[0]);break;case $.OPS.setLineWidth:this.setLineWidth(gt[0]);break;case $.OPS.setLineJoin:this.setLineJoin(gt[0]);break;case $.OPS.setLineCap:this.setLineCap(gt[0]);break;case $.OPS.setMiterLimit:this.setMiterLimit(gt[0]);break;case $.OPS.setFillRGBColor:this.setFillRGBColor(gt[0],gt[1],gt[2]);break;case $.OPS.setStrokeRGBColor:this.setStrokeRGBColor(gt[0],gt[1],gt[2]);break;case $.OPS.setStrokeColorN:this.setStrokeColorN(gt);break;case $.OPS.setFillColorN:this.setFillColorN(gt);break;case $.OPS.shadingFill:this.shadingFill(gt[0]);break;case $.OPS.setDash:this.setDash(gt[0],gt[1]);break;case $.OPS.setRenderingIntent:this.setRenderingIntent(gt[0]);break;case $.OPS.setFlatness:this.setFlatness(gt[0]);break;case $.OPS.setGState:this.setGState(gt[0]);break;case $.OPS.fill:this.fill();break;case $.OPS.eoFill:this.eoFill();break;case $.OPS.stroke:this.stroke();break;case $.OPS.fillStroke:this.fillStroke();break;case $.OPS.eoFillStroke:this.eoFillStroke();break;case $.OPS.clip:this.clip("nonzero");break;case $.OPS.eoClip:this.clip("evenodd");break;case $.OPS.paintSolidColorImageMask:this.paintSolidColorImageMask();break;case $.OPS.paintImageXObject:this.paintImageXObject(gt[0]);break;case $.OPS.paintInlineImageXObject:this.paintInlineImageXObject(gt[0]);break;case $.OPS.paintImageMaskXObject:this.paintImageMaskXObject(gt[0]);break;case $.OPS.paintFormXObjectBegin:this.paintFormXObjectBegin(gt[0],gt[1]);break;case $.OPS.paintFormXObjectEnd:this.paintFormXObjectEnd();break;case $.OPS.closePath:this.closePath();break;case $.OPS.closeStroke:this.closeStroke();break;case $.OPS.closeFillStroke:this.closeFillStroke();break;case $.OPS.closeEOFillStroke:this.closeEOFillStroke();break;case $.OPS.nextLine:this.nextLine();break;case $.OPS.transform:this.transform(gt[0],gt[1],gt[2],gt[3],gt[4],gt[5]);break;case $.OPS.constructPath:this.constructPath(gt[0],gt[1]);break;case $.OPS.endPath:this.endPath();break;case 92:this.group(ht.items);break;default:(0,$.warn)(`Unimplemented operator ${yt}`);break}}}setWordSpacing(ft){this.current.wordSpacing=ft}setCharSpacing(ft){this.current.charSpacing=ft}nextLine(){this.moveText(0,this.current.leading)}setTextMatrix(ft,ht,yt,bt,gt,xt){const vt=this.current;vt.textMatrix=vt.lineMatrix=[ft,ht,yt,bt,gt,xt],vt.textMatrixScale=Math.hypot(ft,ht),vt.x=vt.lineX=0,vt.y=vt.lineY=0,vt.xcoords=[],vt.ycoords=[],vt.tspan=this.svgFactory.createElement("svg:tspan"),vt.tspan.setAttributeNS(null,"font-family",vt.fontFamily),vt.tspan.setAttributeNS(null,"font-size",`${ct(vt.fontSize)}px`),vt.tspan.setAttributeNS(null,"y",ct(-vt.y)),vt.txtElement=this.svgFactory.createElement("svg:text"),vt.txtElement.append(vt.tspan)}beginText(){const ft=this.current;ft.x=ft.lineX=0,ft.y=ft.lineY=0,ft.textMatrix=$.IDENTITY_MATRIX,ft.lineMatrix=$.IDENTITY_MATRIX,ft.textMatrixScale=1,ft.tspan=this.svgFactory.createElement("svg:tspan"),ft.txtElement=this.svgFactory.createElement("svg:text"),ft.txtgrp=this.svgFactory.createElement("svg:g"),ft.xcoords=[],ft.ycoords=[]}moveText(ft,ht){const yt=this.current;yt.x=yt.lineX+=ft,yt.y=yt.lineY+=ht,yt.xcoords=[],yt.ycoords=[],yt.tspan=this.svgFactory.createElement("svg:tspan"),yt.tspan.setAttributeNS(null,"font-family",yt.fontFamily),yt.tspan.setAttributeNS(null,"font-size",`${ct(yt.fontSize)}px`),yt.tspan.setAttributeNS(null,"y",ct(-yt.y))}showText(ft){const ht=this.current,yt=ht.font,bt=ht.fontSize;if(bt===0)return;const gt=ht.fontSizeScale,xt=ht.charSpacing,vt=ht.wordSpacing,Lt=ht.fontDirection,$t=ht.textHScale*Lt,Tt=yt.vertical,Et=Tt?1:-1,Dt=yt.defaultVMetrics,It=bt*ht.fontMatrix[0];let Ct=0;for(const Xt of ft){if(Xt===null){Ct+=Lt*vt;continue}else if(typeof Xt=="number"){Ct+=Et*Xt*bt/1e3;continue}const sn=(Xt.isSpace?vt:0)+xt,Ft=Xt.fontChar;let wt,kt,At=Xt.width;if(Tt){let Mt;const Ot=Xt.vmetric||Dt;Mt=Xt.vmetric?Ot[1]:At*.5,Mt=-Mt*It;const Bt=Ot[2]*It;At=Ot?-Ot[0]:At,wt=Mt/gt,kt=(Ct+Bt)/gt}else wt=Ct/gt,kt=0;(Xt.isInFont||yt.missingFile)&&(ht.xcoords.push(ht.x+wt),Tt&&ht.ycoords.push(-ht.y+kt),ht.tspan.textContent+=Ft);const Pt=Tt?At*It-sn*Lt:At*It+sn*Lt;Ct+=Pt}ht.tspan.setAttributeNS(null,"x",ht.xcoords.map(ct).join(" ")),Tt?ht.tspan.setAttributeNS(null,"y",ht.ycoords.map(ct).join(" ")):ht.tspan.setAttributeNS(null,"y",ct(-ht.y)),Tt?ht.y-=Ct:ht.x+=Ct*$t,ht.tspan.setAttributeNS(null,"font-family",ht.fontFamily),ht.tspan.setAttributeNS(null,"font-size",`${ct(ht.fontSize)}px`),ht.fontStyle!==j.fontStyle&&ht.tspan.setAttributeNS(null,"font-style",ht.fontStyle),ht.fontWeight!==j.fontWeight&&ht.tspan.setAttributeNS(null,"font-weight",ht.fontWeight);const jt=ht.textRenderingMode&$.TextRenderingMode.FILL_STROKE_MASK;if(jt===$.TextRenderingMode.FILL||jt===$.TextRenderingMode.FILL_STROKE?(ht.fillColor!==j.fillColor&&ht.tspan.setAttributeNS(null,"fill",ht.fillColor),ht.fillAlpha<1&&ht.tspan.setAttributeNS(null,"fill-opacity",ht.fillAlpha)):ht.textRenderingMode===$.TextRenderingMode.ADD_TO_PATH?ht.tspan.setAttributeNS(null,"fill","transparent"):ht.tspan.setAttributeNS(null,"fill","none"),jt===$.TextRenderingMode.STROKE||jt===$.TextRenderingMode.FILL_STROKE){const Xt=1/(ht.textMatrixScale||1);this._setStrokeAttributes(ht.tspan,Xt)}let Zt=ht.textMatrix;ht.textRise!==0&&(Zt=Zt.slice(),Zt[5]+=ht.textRise),ht.txtElement.setAttributeNS(null,"transform",`${rt(Zt)} scale(${ct($t)}, -1)`),ht.txtElement.setAttributeNS(_e,"xml:space","preserve"),ht.txtElement.append(ht.tspan),ht.txtgrp.append(ht.txtElement),this._ensureTransformGroup().append(ht.txtElement)}setLeadingMoveText(ft,ht){this.setLeading(-ht),this.moveText(ft,ht)}addFontStyle(ft){if(!ft.data)throw new Error('addFontStyle: No font data available, ensure that the "fontExtraProperties" API parameter is set.');this.cssStyle||(this.cssStyle=this.svgFactory.createElement("svg:style"),this.cssStyle.setAttributeNS(null,"type","text/css"),this.defs.append(this.cssStyle));const ht=at(ft.data,ft.mimetype,this.forceDataSchema);this.cssStyle.textContent+=`@font-face { font-family: "${ft.loadedName}"; src: url(${ht}); }
`}setFont(ft){const ht=this.current,yt=this.commonObjs.get(ft[0]);let bt=ft[1];ht.font=yt,this.embedFonts&&!yt.missingFile&&!this.embeddedFonts[yt.loadedName]&&(this.addFontStyle(yt),this.embeddedFonts[yt.loadedName]=yt),ht.fontMatrix=yt.fontMatrix||$.FONT_IDENTITY_MATRIX;let gt="normal";yt.black?gt="900":yt.bold&&(gt="bold");const xt=yt.italic?"italic":"normal";bt<0?(bt=-bt,ht.fontDirection=-1):ht.fontDirection=1,ht.fontSize=bt,ht.fontFamily=yt.loadedName,ht.fontWeight=gt,ht.fontStyle=xt,ht.tspan=this.svgFactory.createElement("svg:tspan"),ht.tspan.setAttributeNS(null,"y",ct(-ht.y)),ht.xcoords=[],ht.ycoords=[]}endText(){var ht;const ft=this.current;ft.textRenderingMode&$.TextRenderingMode.ADD_TO_PATH_FLAG&&((ht=ft.txtElement)!=null&&ht.hasChildNodes())&&(ft.element=ft.txtElement,this.clip("nonzero"),this.endPath())}setLineWidth(ft){ft>0&&(this.current.lineWidth=ft)}setLineCap(ft){this.current.lineCap=tt[ft]}setLineJoin(ft){this.current.lineJoin=nt[ft]}setMiterLimit(ft){this.current.miterLimit=ft}setStrokeAlpha(ft){this.current.strokeAlpha=ft}setStrokeRGBColor(ft,ht,yt){this.current.strokeColor=$.Util.makeHexColor(ft,ht,yt)}setFillAlpha(ft){this.current.fillAlpha=ft}setFillRGBColor(ft,ht,yt){this.current.fillColor=$.Util.makeHexColor(ft,ht,yt),this.current.tspan=this.svgFactory.createElement("svg:tspan"),this.current.xcoords=[],this.current.ycoords=[]}setStrokeColorN(ft){this.current.strokeColor=this._makeColorN_Pattern(ft)}setFillColorN(ft){this.current.fillColor=this._makeColorN_Pattern(ft)}shadingFill(ft){const{width:ht,height:yt}=this.viewport,bt=$.Util.inverseTransform(this.transformMatrix),[gt,xt,vt,Lt]=$.Util.getAxialAlignedBoundingBox([0,0,ht,yt],bt),$t=this.svgFactory.createElement("svg:rect");$t.setAttributeNS(null,"x",gt),$t.setAttributeNS(null,"y",xt),$t.setAttributeNS(null,"width",vt-gt),$t.setAttributeNS(null,"height",Lt-xt),$t.setAttributeNS(null,"fill",this._makeShadingPattern(ft)),this.current.fillAlpha<1&&$t.setAttributeNS(null,"fill-opacity",this.current.fillAlpha),this._ensureTransformGroup().append($t)}_makeColorN_Pattern(ft){return ft[0]==="TilingPattern"?this._makeTilingPattern(ft):this._makeShadingPattern(ft)}_makeTilingPattern(ft){const ht=ft[1],yt=ft[2],bt=ft[3]||$.IDENTITY_MATRIX,[gt,xt,vt,Lt]=ft[4],$t=ft[5],Tt=ft[6],Et=ft[7],Dt=`shading${dt++}`,[It,Ct,jt,Zt]=$.Util.normalizeRect([...$.Util.applyTransform([gt,xt],bt),...$.Util.applyTransform([vt,Lt],bt)]),[Xt,sn]=$.Util.singularValueDecompose2dScale(bt),Ft=$t*Xt,wt=Tt*sn,kt=this.svgFactory.createElement("svg:pattern");kt.setAttributeNS(null,"id",Dt),kt.setAttributeNS(null,"patternUnits","userSpaceOnUse"),kt.setAttributeNS(null,"width",Ft),kt.setAttributeNS(null,"height",wt),kt.setAttributeNS(null,"x",`${It}`),kt.setAttributeNS(null,"y",`${Ct}`);const At=this.svg,Pt=this.transformMatrix,Mt=this.current.fillColor,Ot=this.current.strokeColor,Bt=this.svgFactory.create(jt-It,Zt-Ct);if(this.svg=Bt,this.transformMatrix=bt,Et===2){const zt=$.Util.makeHexColor(...ht);this.current.fillColor=zt,this.current.strokeColor=zt}return this.executeOpTree(this.convertOpList(yt)),this.svg=At,this.transformMatrix=Pt,this.current.fillColor=Mt,this.current.strokeColor=Ot,kt.append(Bt.childNodes[0]),this.defs.append(kt),`url(#${Dt})`}_makeShadingPattern(ft){switch(typeof ft=="string"&&(ft=this.objs.get(ft)),ft[0]){case"RadialAxial":const ht=`shading${dt++}`,yt=ft[3];let bt;switch(ft[1]){case"axial":const gt=ft[4],xt=ft[5];bt=this.svgFactory.createElement("svg:linearGradient"),bt.setAttributeNS(null,"id",ht),bt.setAttributeNS(null,"gradientUnits","userSpaceOnUse"),bt.setAttributeNS(null,"x1",gt[0]),bt.setAttributeNS(null,"y1",gt[1]),bt.setAttributeNS(null,"x2",xt[0]),bt.setAttributeNS(null,"y2",xt[1]);break;case"radial":const vt=ft[4],Lt=ft[5],$t=ft[6],Tt=ft[7];bt=this.svgFactory.createElement("svg:radialGradient"),bt.setAttributeNS(null,"id",ht),bt.setAttributeNS(null,"gradientUnits","userSpaceOnUse"),bt.setAttributeNS(null,"cx",Lt[0]),bt.setAttributeNS(null,"cy",Lt[1]),bt.setAttributeNS(null,"r",Tt),bt.setAttributeNS(null,"fx",vt[0]),bt.setAttributeNS(null,"fy",vt[1]),bt.setAttributeNS(null,"fr",$t);break;default:throw new Error(`Unknown RadialAxial type: ${ft[1]}`)}for(const gt of yt){const xt=this.svgFactory.createElement("svg:stop");xt.setAttributeNS(null,"offset",gt[0]),xt.setAttributeNS(null,"stop-color",gt[1]),bt.append(xt)}return this.defs.append(bt),`url(#${ht})`;case"Mesh":return(0,$.warn)("Unimplemented pattern Mesh"),null;case"Dummy":return"hotpink";default:throw new Error(`Unknown IR type: ${ft[0]}`)}}setDash(ft,ht){this.current.dashArray=ft,this.current.dashPhase=ht}constructPath(ft,ht){const yt=this.current;let bt=yt.x,gt=yt.y,xt=[],vt=0;for(const Lt of ft)switch(Lt|0){case $.OPS.rectangle:bt=ht[vt++],gt=ht[vt++];const $t=ht[vt++],Tt=ht[vt++],Et=bt+$t,Dt=gt+Tt;xt.push("M",ct(bt),ct(gt),"L",ct(Et),ct(gt),"L",ct(Et),ct(Dt),"L",ct(bt),ct(Dt),"Z");break;case $.OPS.moveTo:bt=ht[vt++],gt=ht[vt++],xt.push("M",ct(bt),ct(gt));break;case $.OPS.lineTo:bt=ht[vt++],gt=ht[vt++],xt.push("L",ct(bt),ct(gt));break;case $.OPS.curveTo:bt=ht[vt+4],gt=ht[vt+5],xt.push("C",ct(ht[vt]),ct(ht[vt+1]),ct(ht[vt+2]),ct(ht[vt+3]),ct(bt),ct(gt)),vt+=6;break;case $.OPS.curveTo2:xt.push("C",ct(bt),ct(gt),ct(ht[vt]),ct(ht[vt+1]),ct(ht[vt+2]),ct(ht[vt+3])),bt=ht[vt+2],gt=ht[vt+3],vt+=4;break;case $.OPS.curveTo3:bt=ht[vt+2],gt=ht[vt+3],xt.push("C",ct(ht[vt]),ct(ht[vt+1]),ct(bt),ct(gt),ct(bt),ct(gt)),vt+=4;break;case $.OPS.closePath:xt.push("Z");break}xt=xt.join(" "),yt.path&&ft.length>0&&ft[0]!==$.OPS.rectangle&&ft[0]!==$.OPS.moveTo?xt=yt.path.getAttributeNS(null,"d")+xt:(yt.path=this.svgFactory.createElement("svg:path"),this._ensureTransformGroup().append(yt.path)),yt.path.setAttributeNS(null,"d",xt),yt.path.setAttributeNS(null,"fill","none"),yt.element=yt.path,yt.setCurrentPoint(bt,gt)}endPath(){const ft=this.current;if(ft.path=null,!this.pendingClip)return;if(!ft.element){this.pendingClip=null;return}const ht=`clippath${ut++}`,yt=this.svgFactory.createElement("svg:clipPath");yt.setAttributeNS(null,"id",ht),yt.setAttributeNS(null,"transform",rt(this.transformMatrix));const bt=ft.element.cloneNode(!0);if(this.pendingClip==="evenodd"?bt.setAttributeNS(null,"clip-rule","evenodd"):bt.setAttributeNS(null,"clip-rule","nonzero"),this.pendingClip=null,yt.append(bt),this.defs.append(yt),ft.activeClipUrl){ft.clipGroup=null;for(const gt of this.extraStack)gt.clipGroup=null;yt.setAttributeNS(null,"clip-path",ft.activeClipUrl)}ft.activeClipUrl=`url(#${ht})`,this.tgrp=null}clip(ft){this.pendingClip=ft}closePath(){const ft=this.current;if(ft.path){const ht=`${ft.path.getAttributeNS(null,"d")}Z`;ft.path.setAttributeNS(null,"d",ht)}}setLeading(ft){this.current.leading=-ft}setTextRise(ft){this.current.textRise=ft}setTextRenderingMode(ft){this.current.textRenderingMode=ft}setHScale(ft){this.current.textHScale=ft/100}setRenderingIntent(ft){}setFlatness(ft){}setGState(ft){for(const[ht,yt]of ft)switch(ht){case"LW":this.setLineWidth(yt);break;case"LC":this.setLineCap(yt);break;case"LJ":this.setLineJoin(yt);break;case"ML":this.setMiterLimit(yt);break;case"D":this.setDash(yt[0],yt[1]);break;case"RI":this.setRenderingIntent(yt);break;case"FL":this.setFlatness(yt);break;case"Font":this.setFont(yt);break;case"CA":this.setStrokeAlpha(yt);break;case"ca":this.setFillAlpha(yt);break;default:(0,$.warn)(`Unimplemented graphic state operator ${ht}`);break}}fill(){const ft=this.current;ft.element&&(ft.element.setAttributeNS(null,"fill",ft.fillColor),ft.element.setAttributeNS(null,"fill-opacity",ft.fillAlpha),this.endPath())}stroke(){const ft=this.current;ft.element&&(this._setStrokeAttributes(ft.element),ft.element.setAttributeNS(null,"fill","none"),this.endPath())}_setStrokeAttributes(ft,ht=1){const yt=this.current;let bt=yt.dashArray;ht!==1&&bt.length>0&&(bt=bt.map(function(gt){return ht*gt})),ft.setAttributeNS(null,"stroke",yt.strokeColor),ft.setAttributeNS(null,"stroke-opacity",yt.strokeAlpha),ft.setAttributeNS(null,"stroke-miterlimit",ct(yt.miterLimit)),ft.setAttributeNS(null,"stroke-linecap",yt.lineCap),ft.setAttributeNS(null,"stroke-linejoin",yt.lineJoin),ft.setAttributeNS(null,"stroke-width",ct(ht*yt.lineWidth)+"px"),ft.setAttributeNS(null,"stroke-dasharray",bt.map(ct).join(" ")),ft.setAttributeNS(null,"stroke-dashoffset",ct(ht*yt.dashPhase)+"px")}eoFill(){var ft;(ft=this.current.element)==null||ft.setAttributeNS(null,"fill-rule","evenodd"),this.fill()}fillStroke(){this.stroke(),this.fill()}eoFillStroke(){var ft;(ft=this.current.element)==null||ft.setAttributeNS(null,"fill-rule","evenodd"),this.fillStroke()}closeStroke(){this.closePath(),this.stroke()}closeFillStroke(){this.closePath(),this.fillStroke()}closeEOFillStroke(){this.closePath(),this.eoFillStroke()}paintSolidColorImageMask(){const ft=this.svgFactory.createElement("svg:rect");ft.setAttributeNS(null,"x","0"),ft.setAttributeNS(null,"y","0"),ft.setAttributeNS(null,"width","1px"),ft.setAttributeNS(null,"height","1px"),ft.setAttributeNS(null,"fill",this.current.fillColor),this._ensureTransformGroup().append(ft)}paintImageXObject(ft){const ht=this.getObject(ft);if(!ht){(0,$.warn)(`Dependent image with object ID ${ft} is not ready yet`);return}this.paintInlineImageXObject(ht)}paintInlineImageXObject(ft,ht){const yt=ft.width,bt=ft.height,gt=it(ft,this.forceDataSchema,!!ht),xt=this.svgFactory.createElement("svg:rect");xt.setAttributeNS(null,"x","0"),xt.setAttributeNS(null,"y","0"),xt.setAttributeNS(null,"width",ct(yt)),xt.setAttributeNS(null,"height",ct(bt)),this.current.element=xt,this.clip("nonzero");const vt=this.svgFactory.createElement("svg:image");vt.setAttributeNS(et,"xlink:href",gt),vt.setAttributeNS(null,"x","0"),vt.setAttributeNS(null,"y",ct(-bt)),vt.setAttributeNS(null,"width",ct(yt)+"px"),vt.setAttributeNS(null,"height",ct(bt)+"px"),vt.setAttributeNS(null,"transform",`scale(${ct(1/yt)} ${ct(-1/bt)})`),ht?ht.append(vt):this._ensureTransformGroup().append(vt)}paintImageMaskXObject(ft){const ht=this.getObject(ft.data,ft);if(ht.bitmap){(0,$.warn)("paintImageMaskXObject: ImageBitmap support is not implemented, ensure that the `isOffscreenCanvasSupported` API parameter is disabled.");return}const yt=this.current,bt=ht.width,gt=ht.height,xt=yt.fillColor;yt.maskId=`mask${ot++}`;const vt=this.svgFactory.createElement("svg:mask");vt.setAttributeNS(null,"id",yt.maskId);const Lt=this.svgFactory.createElement("svg:rect");Lt.setAttributeNS(null,"x","0"),Lt.setAttributeNS(null,"y","0"),Lt.setAttributeNS(null,"width",ct(bt)),Lt.setAttributeNS(null,"height",ct(gt)),Lt.setAttributeNS(null,"fill",xt),Lt.setAttributeNS(null,"mask",`url(#${yt.maskId})`),this.defs.append(vt),this._ensureTransformGroup().append(Lt),this.paintInlineImageXObject(ht,vt)}paintFormXObjectBegin(ft,ht){if(Array.isArray(ft)&&ft.length===6&&this.transform(ft[0],ft[1],ft[2],ft[3],ft[4],ft[5]),ht){const yt=ht[2]-ht[0],bt=ht[3]-ht[1],gt=this.svgFactory.createElement("svg:rect");gt.setAttributeNS(null,"x",ht[0]),gt.setAttributeNS(null,"y",ht[1]),gt.setAttributeNS(null,"width",ct(yt)),gt.setAttributeNS(null,"height",ct(bt)),this.current.element=gt,this.clip("nonzero"),this.endPath()}}paintFormXObjectEnd(){}_initialize(ft){const ht=this.svgFactory.create(ft.width,ft.height),yt=this.svgFactory.createElement("svg:defs");ht.append(yt),this.defs=yt;const bt=this.svgFactory.createElement("svg:g");return bt.setAttributeNS(null,"transform",rt(ft.transform)),ht.append(bt),this.svg=bt,ht}_ensureClipGroup(){if(!this.current.clipGroup){const ft=this.svgFactory.createElement("svg:g");ft.setAttributeNS(null,"clip-path",this.current.activeClipUrl),this.svg.append(ft),this.current.clipGroup=ft}return this.current.clipGroup}_ensureTransformGroup(){return this.tgrp||(this.tgrp=this.svgFactory.createElement("svg:g"),this.tgrp.setAttributeNS(null,"transform",rt(this.transformMatrix)),this.current.activeClipUrl?this._ensureClipGroup().append(this.tgrp):this.svg.append(this.tgrp)),this.tgrp}}i.SVGGraphics=pt},(a,i)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.XfaText=void 0;class o{static textContent($){const j=[],_e={items:j,styles:Object.create(null)};function et(tt){var it;if(!tt)return;let nt=null;const at=tt.name;if(at==="#text")nt=tt.value;else if(o.shouldBuildText(at))(it=tt==null?void 0:tt.attributes)!=null&&it.textContent?nt=tt.attributes.textContent:tt.value&&(nt=tt.value);else return;if(nt!==null&&j.push({str:nt}),!!tt.children)for(const st of tt.children)et(st)}return et($),_e}static shouldBuildText($){return!($==="textarea"||$==="input"||$==="option"||$==="select")}}i.XfaText=o},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.TextLayerRenderTask=void 0,i.renderTextLayer=rt,i.updateTextLayer=ut;var s=o(1),$=o(6);const j=1e5,_e=30,et=.8,tt=new Map;function nt(ot,dt){let pt;if(dt&&s.FeatureTest.isOffscreenCanvasSupported)pt=new OffscreenCanvas(ot,ot).getContext("2d",{alpha:!1});else{const mt=document.createElement("canvas");mt.width=mt.height=ot,pt=mt.getContext("2d",{alpha:!1})}return pt}function at(ot,dt){const pt=tt.get(ot);if(pt)return pt;const mt=nt(_e,dt);mt.font=`${_e}px ${ot}`;const ft=mt.measureText("");let ht=ft.fontBoundingBoxAscent,yt=Math.abs(ft.fontBoundingBoxDescent);if(ht){const gt=ht/(ht+yt);return tt.set(ot,gt),mt.canvas.width=mt.canvas.height=0,gt}mt.strokeStyle="red",mt.clearRect(0,0,_e,_e),mt.strokeText("g",0,0);let bt=mt.getImageData(0,0,_e,_e).data;yt=0;for(let gt=bt.length-1-3;gt>=0;gt-=4)if(bt[gt]>0){yt=Math.ceil(gt/4/_e);break}mt.clearRect(0,0,_e,_e),mt.strokeText("A",0,_e),bt=mt.getImageData(0,0,_e,_e).data,ht=0;for(let gt=0,xt=bt.length;gt<xt;gt+=4)if(bt[gt]>0){ht=_e-Math.floor(gt/4/_e);break}if(mt.canvas.width=mt.canvas.height=0,ht){const gt=ht/(ht+yt);return tt.set(ot,gt),gt}return tt.set(ot,et),et}function it(ot,dt,pt){const mt=document.createElement("span"),ft={angle:0,canvasWidth:0,hasText:dt.str!=="",hasEOL:dt.hasEOL,fontSize:0};ot._textDivs.push(mt);const ht=s.Util.transform(ot._transform,dt.transform);let yt=Math.atan2(ht[1],ht[0]);const bt=pt[dt.fontName];bt.vertical&&(yt+=Math.PI/2);const gt=Math.hypot(ht[2],ht[3]),xt=gt*at(bt.fontFamily,ot._isOffscreenCanvasSupported);let vt,Lt;yt===0?(vt=ht[4],Lt=ht[5]-xt):(vt=ht[4]+xt*Math.sin(yt),Lt=ht[5]-xt*Math.cos(yt));const $t="calc(var(--scale-factor)*",Tt=mt.style;ot._container===ot._rootContainer?(Tt.left=`${(100*vt/ot._pageWidth).toFixed(2)}%`,Tt.top=`${(100*Lt/ot._pageHeight).toFixed(2)}%`):(Tt.left=`${$t}${vt.toFixed(2)}px)`,Tt.top=`${$t}${Lt.toFixed(2)}px)`),Tt.fontSize=`${$t}${gt.toFixed(2)}px)`,Tt.fontFamily=bt.fontFamily,ft.fontSize=gt,mt.setAttribute("role","presentation"),mt.textContent=dt.str,mt.dir=dt.dir,ot._fontInspectorEnabled&&(mt.dataset.fontName=dt.fontName),yt!==0&&(ft.angle=yt*(180/Math.PI));let Et=!1;if(dt.str.length>1)Et=!0;else if(dt.str!==" "&&dt.transform[0]!==dt.transform[3]){const Dt=Math.abs(dt.transform[0]),It=Math.abs(dt.transform[3]);Dt!==It&&Math.max(Dt,It)/Math.min(Dt,It)>1.5&&(Et=!0)}Et&&(ft.canvasWidth=bt.vertical?dt.height:dt.width),ot._textDivProperties.set(mt,ft),ot._isReadableStream&&ot._layoutText(mt)}function st(ot){const{div:dt,scale:pt,properties:mt,ctx:ft,prevFontSize:ht,prevFontFamily:yt}=ot,{style:bt}=dt;let gt="";if(mt.canvasWidth!==0&&mt.hasText){const{fontFamily:xt}=bt,{canvasWidth:vt,fontSize:Lt}=mt;(ht!==Lt||yt!==xt)&&(ft.font=`${Lt*pt}px ${xt}`,ot.prevFontSize=Lt,ot.prevFontFamily=xt);const{width:$t}=ft.measureText(dt.textContent);$t>0&&(gt=`scaleX(${vt*pt/$t})`)}mt.angle!==0&&(gt=`rotate(${mt.angle}deg) ${gt}`),gt.length>0&&(bt.transform=gt)}function lt(ot){if(ot._canceled)return;const dt=ot._textDivs,pt=ot._capability;if(dt.length>j){pt.resolve();return}if(!ot._isReadableStream)for(const ft of dt)ot._layoutText(ft);pt.resolve()}class ct{constructor({textContentSource:dt,container:pt,viewport:mt,textDivs:ft,textDivProperties:ht,textContentItemsStr:yt,isOffscreenCanvasSupported:bt}){var $t;this._textContentSource=dt,this._isReadableStream=dt instanceof ReadableStream,this._container=this._rootContainer=pt,this._textDivs=ft||[],this._textContentItemsStr=yt||[],this._isOffscreenCanvasSupported=bt,this._fontInspectorEnabled=!!(($t=globalThis.FontInspector)!=null&&$t.enabled),this._reader=null,this._textDivProperties=ht||new WeakMap,this._canceled=!1,this._capability=new s.PromiseCapability,this._layoutTextParams={prevFontSize:null,prevFontFamily:null,div:null,scale:mt.scale*(globalThis.devicePixelRatio||1),properties:null,ctx:nt(0,bt)};const{pageWidth:gt,pageHeight:xt,pageX:vt,pageY:Lt}=mt.rawDims;this._transform=[1,0,0,-1,-vt,Lt+xt],this._pageWidth=gt,this._pageHeight=xt,(0,$.setLayerDimensions)(pt,mt),this._capability.promise.finally(()=>{this._layoutTextParams=null}).catch(()=>{})}get promise(){return this._capability.promise}cancel(){this._canceled=!0,this._reader&&(this._reader.cancel(new s.AbortException("TextLayer task cancelled.")).catch(()=>{}),this._reader=null),this._capability.reject(new s.AbortException("TextLayer task cancelled."))}_processItems(dt,pt){for(const mt of dt){if(mt.str===void 0){if(mt.type==="beginMarkedContentProps"||mt.type==="beginMarkedContent"){const ft=this._container;this._container=document.createElement("span"),this._container.classList.add("markedContent"),mt.id!==null&&this._container.setAttribute("id",`${mt.id}`),ft.append(this._container)}else mt.type==="endMarkedContent"&&(this._container=this._container.parentNode);continue}this._textContentItemsStr.push(mt.str),it(this,mt,pt)}}_layoutText(dt){const pt=this._layoutTextParams.properties=this._textDivProperties.get(dt);if(this._layoutTextParams.div=dt,st(this._layoutTextParams),pt.hasText&&this._container.append(dt),pt.hasEOL){const mt=document.createElement("br");mt.setAttribute("role","presentation"),this._container.append(mt)}}_render(){const dt=new s.PromiseCapability;let pt=Object.create(null);if(this._isReadableStream){const mt=()=>{this._reader.read().then(({value:ft,done:ht})=>{if(ht){dt.resolve();return}Object.assign(pt,ft.styles),this._processItems(ft.items,pt),mt()},dt.reject)};this._reader=this._textContentSource.getReader(),mt()}else if(this._textContentSource){const{items:mt,styles:ft}=this._textContentSource;this._processItems(mt,ft),dt.resolve()}else throw new Error('No "textContentSource" parameter specified.');dt.promise.then(()=>{pt=null,lt(this)},this._capability.reject)}}i.TextLayerRenderTask=ct;function rt(ot){!ot.textContentSource&&(ot.textContent||ot.textContentStream)&&((0,$.deprecated)("The TextLayerRender `textContent`/`textContentStream` parameters will be removed in the future, please use `textContentSource` instead."),ot.textContentSource=ot.textContent||ot.textContentStream);const{container:dt,viewport:pt}=ot,mt=getComputedStyle(dt),ft=mt.getPropertyValue("visibility"),ht=parseFloat(mt.getPropertyValue("--scale-factor"));ft==="visible"&&(!ht||Math.abs(ht-pt.scale)>1e-5)&&console.error("The `--scale-factor` CSS-variable must be set, to the same value as `viewport.scale`, either on the `container`-element itself or higher up in the DOM.");const yt=new ct(ot);return yt._render(),yt}function ut({container:ot,viewport:dt,textDivs:pt,textDivProperties:mt,isOffscreenCanvasSupported:ft,mustRotate:ht=!0,mustRescale:yt=!0}){if(ht&&(0,$.setLayerDimensions)(ot,{rotation:dt.rotation}),yt){const bt=nt(0,ft),xt={prevFontSize:null,prevFontFamily:null,div:null,scale:dt.scale*(globalThis.devicePixelRatio||1),properties:null,ctx:bt};for(const vt of pt)xt.properties=mt.get(vt),xt.div=vt,st(xt)}}},(a,i,o)=>{var at,it,st,lt,ct,rt,ut,ot,dt,pt,mt,Uo,ht,vo,bt,Vo,xt,Ho;Object.defineProperty(i,"__esModule",{value:!0}),i.AnnotationEditorLayer=void 0;var s=o(1),$=o(4),j=o(28),_e=o(33),et=o(6),tt=o(34);const Lt=class Lt{constructor({uiManager:Tt,pageIndex:Et,div:Dt,accessibilityManager:It,annotationLayer:Ct,viewport:jt,l10n:Zt}){Yt(this,mt);Yt(this,ht);Yt(this,bt);Yt(this,xt);Yt(this,at,void 0);Yt(this,it,!1);Yt(this,st,null);Yt(this,lt,this.pointerup.bind(this));Yt(this,ct,this.pointerdown.bind(this));Yt(this,rt,new Map);Yt(this,ut,!1);Yt(this,ot,!1);Yt(this,dt,!1);Yt(this,pt,void 0);const Xt=[j.FreeTextEditor,_e.InkEditor,tt.StampEditor];if(!Lt._initialized){Lt._initialized=!0;for(const sn of Xt)sn.initialize(Zt)}Tt.registerEditorTypes(Xt),wn(this,pt,Tt),this.pageIndex=Et,this.div=Dt,wn(this,at,It),wn(this,st,Ct),this.viewport=jt,St(this,pt).addLayer(this)}get isEmpty(){return St(this,rt).size===0}updateToolbar(Tt){St(this,pt).updateToolbar(Tt)}updateMode(Tt=St(this,pt).getMode()){un(this,xt,Ho).call(this),Tt===s.AnnotationEditorType.INK?(this.addInkEditorIfNeeded(!1),this.disableClick()):this.enableClick(),Tt!==s.AnnotationEditorType.NONE&&(this.div.classList.toggle("freeTextEditing",Tt===s.AnnotationEditorType.FREETEXT),this.div.classList.toggle("inkEditing",Tt===s.AnnotationEditorType.INK),this.div.classList.toggle("stampEditing",Tt===s.AnnotationEditorType.STAMP),this.div.hidden=!1)}addInkEditorIfNeeded(Tt){if(!Tt&&St(this,pt).getMode()!==s.AnnotationEditorType.INK)return;if(!Tt){for(const Dt of St(this,rt).values())if(Dt.isEmpty()){Dt.setInBackground();return}}un(this,ht,vo).call(this,{offsetX:0,offsetY:0},!1).setInBackground()}setEditingState(Tt){St(this,pt).setEditingState(Tt)}addCommands(Tt){St(this,pt).addCommands(Tt)}enable(){this.div.style.pointerEvents="auto";const Tt=new Set;for(const Dt of St(this,rt).values())Dt.enableEditing(),Dt.annotationElementId&&Tt.add(Dt.annotationElementId);if(!St(this,st))return;const Et=St(this,st).getEditableAnnotations();for(const Dt of Et){if(Dt.hide(),St(this,pt).isDeletedAnnotationElement(Dt.data.id)||Tt.has(Dt.data.id))continue;const It=this.deserialize(Dt);It&&(this.addOrRebuild(It),It.enableEditing())}}disable(){var Et;wn(this,dt,!0),this.div.style.pointerEvents="none";const Tt=new Set;for(const Dt of St(this,rt).values()){if(Dt.disableEditing(),!Dt.annotationElementId||Dt.serialize()!==null){Tt.add(Dt.annotationElementId);continue}(Et=this.getEditableAnnotation(Dt.annotationElementId))==null||Et.show(),Dt.remove()}if(St(this,st)){const Dt=St(this,st).getEditableAnnotations();for(const It of Dt){const{id:Ct}=It.data;Tt.has(Ct)||St(this,pt).isDeletedAnnotationElement(Ct)||It.show()}}un(this,xt,Ho).call(this),this.isEmpty&&(this.div.hidden=!0),wn(this,dt,!1)}getEditableAnnotation(Tt){var Et;return((Et=St(this,st))==null?void 0:Et.getEditableAnnotation(Tt))||null}setActiveEditor(Tt){St(this,pt).getActive()!==Tt&&St(this,pt).setActiveEditor(Tt)}enableClick(){this.div.addEventListener("pointerdown",St(this,ct)),this.div.addEventListener("pointerup",St(this,lt))}disableClick(){this.div.removeEventListener("pointerdown",St(this,ct)),this.div.removeEventListener("pointerup",St(this,lt))}attach(Tt){St(this,rt).set(Tt.id,Tt);const{annotationElementId:Et}=Tt;Et&&St(this,pt).isDeletedAnnotationElement(Et)&&St(this,pt).removeDeletedAnnotationElement(Tt)}detach(Tt){var Et;St(this,rt).delete(Tt.id),(Et=St(this,at))==null||Et.removePointerInTextLayer(Tt.contentDiv),!St(this,dt)&&Tt.annotationElementId&&St(this,pt).addDeletedAnnotationElement(Tt)}remove(Tt){this.detach(Tt),St(this,pt).removeEditor(Tt),Tt.div.contains(document.activeElement)&&setTimeout(()=>{St(this,pt).focusMainContainer()},0),Tt.div.remove(),Tt.isAttachedToDOM=!1,St(this,ot)||this.addInkEditorIfNeeded(!1)}changeParent(Tt){var Et;Tt.parent!==this&&(Tt.annotationElementId&&(St(this,pt).addDeletedAnnotationElement(Tt.annotationElementId),$.AnnotationEditor.deleteAnnotationElement(Tt),Tt.annotationElementId=null),this.attach(Tt),(Et=Tt.parent)==null||Et.detach(Tt),Tt.setParent(this),Tt.div&&Tt.isAttachedToDOM&&(Tt.div.remove(),this.div.append(Tt.div)))}add(Tt){if(this.changeParent(Tt),St(this,pt).addEditor(Tt),this.attach(Tt),!Tt.isAttachedToDOM){const Et=Tt.render();this.div.append(Et),Tt.isAttachedToDOM=!0}Tt.fixAndSetPosition(),Tt.onceAdded(),St(this,pt).addToAnnotationStorage(Tt)}moveEditorInDOM(Tt){var Dt;if(!Tt.isAttachedToDOM)return;const{activeElement:Et}=document;Tt.div.contains(Et)&&(Tt._focusEventsAllowed=!1,setTimeout(()=>{Tt.div.contains(document.activeElement)?Tt._focusEventsAllowed=!0:(Tt.div.addEventListener("focusin",()=>{Tt._focusEventsAllowed=!0},{once:!0}),Et.focus())},0)),Tt._structTreeParentId=(Dt=St(this,at))==null?void 0:Dt.moveElementInDOM(this.div,Tt.div,Tt.contentDiv,!0)}addOrRebuild(Tt){Tt.needsToBeRebuilt()?Tt.rebuild():this.add(Tt)}addUndoableEditor(Tt){const Et=()=>Tt._uiManager.rebuild(Tt),Dt=()=>{Tt.remove()};this.addCommands({cmd:Et,undo:Dt,mustExec:!1})}getNextId(){return St(this,pt).getId()}pasteEditor(Tt,Et){St(this,pt).updateToolbar(Tt),St(this,pt).updateMode(Tt);const{offsetX:Dt,offsetY:It}=un(this,bt,Vo).call(this),Ct=this.getNextId(),jt=un(this,mt,Uo).call(this,{parent:this,id:Ct,x:Dt,y:It,uiManager:St(this,pt),isCentered:!0,...Et});jt&&this.add(jt)}deserialize(Tt){switch(Tt.annotationType??Tt.annotationEditorType){case s.AnnotationEditorType.FREETEXT:return j.FreeTextEditor.deserialize(Tt,this,St(this,pt));case s.AnnotationEditorType.INK:return _e.InkEditor.deserialize(Tt,this,St(this,pt));case s.AnnotationEditorType.STAMP:return tt.StampEditor.deserialize(Tt,this,St(this,pt))}return null}addNewEditor(){un(this,ht,vo).call(this,un(this,bt,Vo).call(this),!0)}setSelected(Tt){St(this,pt).setSelected(Tt)}toggleSelected(Tt){St(this,pt).toggleSelected(Tt)}isSelected(Tt){return St(this,pt).isSelected(Tt)}unselect(Tt){St(this,pt).unselect(Tt)}pointerup(Tt){const{isMac:Et}=s.FeatureTest.platform;if(!(Tt.button!==0||Tt.ctrlKey&&Et)&&Tt.target===this.div&&St(this,ut)){if(wn(this,ut,!1),!St(this,it)){wn(this,it,!0);return}if(St(this,pt).getMode()===s.AnnotationEditorType.STAMP){St(this,pt).unselectAll();return}un(this,ht,vo).call(this,Tt,!1)}}pointerdown(Tt){if(St(this,ut)){wn(this,ut,!1);return}const{isMac:Et}=s.FeatureTest.platform;if(Tt.button!==0||Tt.ctrlKey&&Et||Tt.target!==this.div)return;wn(this,ut,!0);const Dt=St(this,pt).getActive();wn(this,it,!Dt||Dt.isEmpty())}findNewParent(Tt,Et,Dt){const It=St(this,pt).findParent(Et,Dt);return It===null||It===this?!1:(It.changeParent(Tt),!0)}destroy(){var Tt,Et;((Tt=St(this,pt).getActive())==null?void 0:Tt.parent)===this&&(St(this,pt).commitOrRemove(),St(this,pt).setActiveEditor(null));for(const Dt of St(this,rt).values())(Et=St(this,at))==null||Et.removePointerInTextLayer(Dt.contentDiv),Dt.setParent(null),Dt.isAttachedToDOM=!1,Dt.div.remove();this.div=null,St(this,rt).clear(),St(this,pt).removeLayer(this)}render({viewport:Tt}){this.viewport=Tt,(0,et.setLayerDimensions)(this.div,Tt);for(const Et of St(this,pt).getEditors(this.pageIndex))this.add(Et);this.updateMode()}update({viewport:Tt}){St(this,pt).commitOrRemove(),this.viewport=Tt,(0,et.setLayerDimensions)(this.div,{rotation:Tt.rotation}),this.updateMode()}get pageDimensions(){const{pageWidth:Tt,pageHeight:Et}=this.viewport.rawDims;return[Tt,Et]}};at=new WeakMap,it=new WeakMap,st=new WeakMap,lt=new WeakMap,ct=new WeakMap,rt=new WeakMap,ut=new WeakMap,ot=new WeakMap,dt=new WeakMap,pt=new WeakMap,mt=new WeakSet,Uo=function(Tt){switch(St(this,pt).getMode()){case s.AnnotationEditorType.FREETEXT:return new j.FreeTextEditor(Tt);case s.AnnotationEditorType.INK:return new _e.InkEditor(Tt);case s.AnnotationEditorType.STAMP:return new tt.StampEditor(Tt)}return null},ht=new WeakSet,vo=function(Tt,Et){const Dt=this.getNextId(),It=un(this,mt,Uo).call(this,{parent:this,id:Dt,x:Tt.offsetX,y:Tt.offsetY,uiManager:St(this,pt),isCentered:Et});return It&&this.add(It),It},bt=new WeakSet,Vo=function(){const{x:Tt,y:Et,width:Dt,height:It}=this.div.getBoundingClientRect(),Ct=Math.max(0,Tt),jt=Math.max(0,Et),Zt=Math.min(window.innerWidth,Tt+Dt),Xt=Math.min(window.innerHeight,Et+It),sn=(Ct+Zt)/2-Tt,Ft=(jt+Xt)/2-Et,[wt,kt]=this.viewport.rotation%180===0?[sn,Ft]:[Ft,sn];return{offsetX:wt,offsetY:kt}},xt=new WeakSet,Ho=function(){wn(this,ot,!0);for(const Tt of St(this,rt).values())Tt.isEmpty()&&Tt.remove();wn(this,ot,!1)},Jn(Lt,"_initialized",!1);let nt=Lt;i.AnnotationEditorLayer=nt},(a,i,o)=>{var tt,nt,at,it,st,lt,ct,rt,ut,ot,br,pt,xr,ft,wr,yt,lo,gt,Go,vt,Tr,$t,qo;Object.defineProperty(i,"__esModule",{value:!0}),i.FreeTextEditor=void 0;var s=o(1),$=o(5),j=o(4),_e=o(29);const Et=class Et extends j.AnnotationEditor{constructor(Ct){super({...Ct,name:"freeTextEditor"});Yt(this,ot);Yt(this,pt);Yt(this,ft);Yt(this,yt);Yt(this,gt);Yt(this,vt);Yt(this,$t);Yt(this,tt,this.editorDivBlur.bind(this));Yt(this,nt,this.editorDivFocus.bind(this));Yt(this,at,this.editorDivInput.bind(this));Yt(this,it,this.editorDivKeydown.bind(this));Yt(this,st,void 0);Yt(this,lt,"");Yt(this,ct,`${this.id}-editor`);Yt(this,rt,void 0);Yt(this,ut,null);wn(this,st,Ct.color||Et._defaultColor||j.AnnotationEditor._defaultLineColor),wn(this,rt,Ct.fontSize||Et._defaultFontSize)}static get _keyboardManager(){const Ct=Et.prototype,jt=sn=>sn.isEmpty(),Zt=$.AnnotationEditorUIManager.TRANSLATE_SMALL,Xt=$.AnnotationEditorUIManager.TRANSLATE_BIG;return(0,s.shadow)(this,"_keyboardManager",new $.KeyboardManager([[["ctrl+s","mac+meta+s","ctrl+p","mac+meta+p"],Ct.commitOrRemove,{bubbles:!0}],[["ctrl+Enter","mac+meta+Enter","Escape","mac+Escape"],Ct.commitOrRemove],[["ArrowLeft","mac+ArrowLeft"],Ct._translateEmpty,{args:[-Zt,0],checker:jt}],[["ctrl+ArrowLeft","mac+shift+ArrowLeft"],Ct._translateEmpty,{args:[-Xt,0],checker:jt}],[["ArrowRight","mac+ArrowRight"],Ct._translateEmpty,{args:[Zt,0],checker:jt}],[["ctrl+ArrowRight","mac+shift+ArrowRight"],Ct._translateEmpty,{args:[Xt,0],checker:jt}],[["ArrowUp","mac+ArrowUp"],Ct._translateEmpty,{args:[0,-Zt],checker:jt}],[["ctrl+ArrowUp","mac+shift+ArrowUp"],Ct._translateEmpty,{args:[0,-Xt],checker:jt}],[["ArrowDown","mac+ArrowDown"],Ct._translateEmpty,{args:[0,Zt],checker:jt}],[["ctrl+ArrowDown","mac+shift+ArrowDown"],Ct._translateEmpty,{args:[0,Xt],checker:jt}]]))}static initialize(Ct){j.AnnotationEditor.initialize(Ct,{strings:["free_text2_default_content","editor_free_text2_aria_label"]});const jt=getComputedStyle(document.documentElement);this._internalPadding=parseFloat(jt.getPropertyValue("--freetext-padding"))}static updateDefaultParams(Ct,jt){switch(Ct){case s.AnnotationEditorParamsType.FREETEXT_SIZE:Et._defaultFontSize=jt;break;case s.AnnotationEditorParamsType.FREETEXT_COLOR:Et._defaultColor=jt;break}}updateParams(Ct,jt){switch(Ct){case s.AnnotationEditorParamsType.FREETEXT_SIZE:un(this,ot,br).call(this,jt);break;case s.AnnotationEditorParamsType.FREETEXT_COLOR:un(this,pt,xr).call(this,jt);break}}static get defaultPropertiesToUpdate(){return[[s.AnnotationEditorParamsType.FREETEXT_SIZE,Et._defaultFontSize],[s.AnnotationEditorParamsType.FREETEXT_COLOR,Et._defaultColor||j.AnnotationEditor._defaultLineColor]]}get propertiesToUpdate(){return[[s.AnnotationEditorParamsType.FREETEXT_SIZE,St(this,rt)],[s.AnnotationEditorParamsType.FREETEXT_COLOR,St(this,st)]]}_translateEmpty(Ct,jt){this._uiManager.translateSelectedEditors(Ct,jt,!0)}getInitialTranslation(){const Ct=this.parentScale;return[-Et._internalPadding*Ct,-(Et._internalPadding+St(this,rt))*Ct]}rebuild(){this.parent&&(super.rebuild(),this.div!==null&&(this.isAttachedToDOM||this.parent.add(this)))}enableEditMode(){this.isInEditMode()||(this.parent.setEditingState(!1),this.parent.updateToolbar(s.AnnotationEditorType.FREETEXT),super.enableEditMode(),this.overlayDiv.classList.remove("enabled"),this.editorDiv.contentEditable=!0,this._isDraggable=!1,this.div.removeAttribute("aria-activedescendant"),this.editorDiv.addEventListener("keydown",St(this,it)),this.editorDiv.addEventListener("focus",St(this,nt)),this.editorDiv.addEventListener("blur",St(this,tt)),this.editorDiv.addEventListener("input",St(this,at)))}disableEditMode(){this.isInEditMode()&&(this.parent.setEditingState(!0),super.disableEditMode(),this.overlayDiv.classList.add("enabled"),this.editorDiv.contentEditable=!1,this.div.setAttribute("aria-activedescendant",St(this,ct)),this._isDraggable=!0,this.editorDiv.removeEventListener("keydown",St(this,it)),this.editorDiv.removeEventListener("focus",St(this,nt)),this.editorDiv.removeEventListener("blur",St(this,tt)),this.editorDiv.removeEventListener("input",St(this,at)),this.div.focus({preventScroll:!0}),this.isEditing=!1,this.parent.div.classList.add("freeTextEditing"))}focusin(Ct){this._focusEventsAllowed&&(super.focusin(Ct),Ct.target!==this.editorDiv&&this.editorDiv.focus())}onceAdded(){var Ct;if(this.width){un(this,$t,qo).call(this);return}this.enableEditMode(),this.editorDiv.focus(),(Ct=this._initialOptions)!=null&&Ct.isCentered&&this.center(),this._initialOptions=null}isEmpty(){return!this.editorDiv||this.editorDiv.innerText.trim()===""}remove(){this.isEditing=!1,this.parent&&(this.parent.setEditingState(!0),this.parent.div.classList.add("freeTextEditing")),super.remove()}commit(){if(!this.isInEditMode())return;super.commit(),this.disableEditMode();const Ct=St(this,lt),jt=wn(this,lt,un(this,ft,wr).call(this).trimEnd());if(Ct===jt)return;const Zt=Xt=>{if(wn(this,lt,Xt),!Xt){this.remove();return}un(this,gt,Go).call(this),this._uiManager.rebuild(this),un(this,yt,lo).call(this)};this.addCommands({cmd:()=>{Zt(jt)},undo:()=>{Zt(Ct)},mustExec:!1}),un(this,yt,lo).call(this)}shouldGetKeyboardEvents(){return this.isInEditMode()}enterInEditMode(){this.enableEditMode(),this.editorDiv.focus()}dblclick(Ct){this.enterInEditMode()}keydown(Ct){Ct.target===this.div&&Ct.key==="Enter"&&(this.enterInEditMode(),Ct.preventDefault())}editorDivKeydown(Ct){Et._keyboardManager.exec(this,Ct)}editorDivFocus(Ct){this.isEditing=!0}editorDivBlur(Ct){this.isEditing=!1}editorDivInput(Ct){this.parent.div.classList.toggle("freeTextEditing",this.isEmpty())}disableEditing(){this.editorDiv.setAttribute("role","comment"),this.editorDiv.removeAttribute("aria-multiline")}enableEditing(){this.editorDiv.setAttribute("role","textbox"),this.editorDiv.setAttribute("aria-multiline",!0)}render(){if(this.div)return this.div;let Ct,jt;this.width&&(Ct=this.x,jt=this.y),super.render(),this.editorDiv=document.createElement("div"),this.editorDiv.className="internal",this.editorDiv.setAttribute("id",St(this,ct)),this.enableEditing(),j.AnnotationEditor._l10nPromise.get("editor_free_text2_aria_label").then(Xt=>{var sn;return(sn=this.editorDiv)==null?void 0:sn.setAttribute("aria-label",Xt)}),j.AnnotationEditor._l10nPromise.get("free_text2_default_content").then(Xt=>{var sn;return(sn=this.editorDiv)==null?void 0:sn.setAttribute("default-content",Xt)}),this.editorDiv.contentEditable=!0;const{style:Zt}=this.editorDiv;if(Zt.fontSize=`calc(${St(this,rt)}px * var(--scale-factor))`,Zt.color=St(this,st),this.div.append(this.editorDiv),this.overlayDiv=document.createElement("div"),this.overlayDiv.classList.add("overlay","enabled"),this.div.append(this.overlayDiv),(0,$.bindEvents)(this,this.div,["dblclick","keydown"]),this.width){const[Xt,sn]=this.parentDimensions;if(this.annotationElementId){const{position:Ft}=St(this,ut);let[wt,kt]=this.getInitialTranslation();[wt,kt]=this.pageTranslationToScreen(wt,kt);const[At,Pt]=this.pageDimensions,[Mt,Ot]=this.pageTranslation;let Bt,zt;switch(this.rotation){case 0:Bt=Ct+(Ft[0]-Mt)/At,zt=jt+this.height-(Ft[1]-Ot)/Pt;break;case 90:Bt=Ct+(Ft[0]-Mt)/At,zt=jt-(Ft[1]-Ot)/Pt,[wt,kt]=[kt,-wt];break;case 180:Bt=Ct-this.width+(Ft[0]-Mt)/At,zt=jt-(Ft[1]-Ot)/Pt,[wt,kt]=[-wt,-kt];break;case 270:Bt=Ct+(Ft[0]-Mt-this.height*Pt)/At,zt=jt+(Ft[1]-Ot-this.width*At)/Pt,[wt,kt]=[-kt,wt];break}this.setAt(Bt*Xt,zt*sn,wt,kt)}else this.setAt(Ct*Xt,jt*sn,this.width*Xt,this.height*sn);un(this,gt,Go).call(this),this._isDraggable=!0,this.editorDiv.contentEditable=!1}else this._isDraggable=!1,this.editorDiv.contentEditable=!0;return this.div}get contentDiv(){return this.editorDiv}static deserialize(Ct,jt,Zt){let Xt=null;if(Ct instanceof _e.FreeTextAnnotationElement){const{data:{defaultAppearanceData:{fontSize:Ft,fontColor:wt},rect:kt,rotation:At,id:Pt},textContent:Mt,textPosition:Ot,parent:{page:{pageNumber:Bt}}}=Ct;if(!Mt||Mt.length===0)return null;Xt=Ct={annotationType:s.AnnotationEditorType.FREETEXT,color:Array.from(wt),fontSize:Ft,value:Mt.join(`
`),position:Ot,pageIndex:Bt-1,rect:kt,rotation:At,id:Pt,deleted:!1}}const sn=super.deserialize(Ct,jt,Zt);return wn(sn,rt,Ct.fontSize),wn(sn,st,s.Util.makeHexColor(...Ct.color)),wn(sn,lt,Ct.value),sn.annotationElementId=Ct.id||null,wn(sn,ut,Xt),sn}serialize(Ct=!1){if(this.isEmpty())return null;if(this.deleted)return{pageIndex:this.pageIndex,id:this.annotationElementId,deleted:!0};const jt=Et._internalPadding*this.parentScale,Zt=this.getRect(jt,jt),Xt=j.AnnotationEditor._colorManager.convert(this.isAttachedToDOM?getComputedStyle(this.editorDiv).color:St(this,st)),sn={annotationType:s.AnnotationEditorType.FREETEXT,color:Xt,fontSize:St(this,rt),value:St(this,lt),pageIndex:this.pageIndex,rect:Zt,rotation:this.rotation,structTreeParentId:this._structTreeParentId};return Ct?sn:this.annotationElementId&&!un(this,vt,Tr).call(this,sn)?null:(sn.id=this.annotationElementId,sn)}};tt=new WeakMap,nt=new WeakMap,at=new WeakMap,it=new WeakMap,st=new WeakMap,lt=new WeakMap,ct=new WeakMap,rt=new WeakMap,ut=new WeakMap,ot=new WeakSet,br=function(Ct){const jt=Xt=>{this.editorDiv.style.fontSize=`calc(${Xt}px * var(--scale-factor))`,this.translate(0,-(Xt-St(this,rt))*this.parentScale),wn(this,rt,Xt),un(this,yt,lo).call(this)},Zt=St(this,rt);this.addCommands({cmd:()=>{jt(Ct)},undo:()=>{jt(Zt)},mustExec:!0,type:s.AnnotationEditorParamsType.FREETEXT_SIZE,overwriteIfSameType:!0,keepUndo:!0})},pt=new WeakSet,xr=function(Ct){const jt=St(this,st);this.addCommands({cmd:()=>{wn(this,st,this.editorDiv.style.color=Ct)},undo:()=>{wn(this,st,this.editorDiv.style.color=jt)},mustExec:!0,type:s.AnnotationEditorParamsType.FREETEXT_COLOR,overwriteIfSameType:!0,keepUndo:!0})},ft=new WeakSet,wr=function(){const Ct=this.editorDiv.getElementsByTagName("div");if(Ct.length===0)return this.editorDiv.innerText;const jt=[];for(const Zt of Ct)jt.push(Zt.innerText.replace(/\r\n?|\n/,""));return jt.join(`
`)},yt=new WeakSet,lo=function(){const[Ct,jt]=this.parentDimensions;let Zt;if(this.isAttachedToDOM)Zt=this.div.getBoundingClientRect();else{const{currentLayer:Xt,div:sn}=this,Ft=sn.style.display;sn.style.display="hidden",Xt.div.append(this.div),Zt=sn.getBoundingClientRect(),sn.remove(),sn.style.display=Ft}this.rotation%180===this.parentRotation%180?(this.width=Zt.width/Ct,this.height=Zt.height/jt):(this.width=Zt.height/Ct,this.height=Zt.width/jt),this.fixAndSetPosition()},gt=new WeakSet,Go=function(){if(this.editorDiv.replaceChildren(),!!St(this,lt))for(const Ct of St(this,lt).split(`
`)){const jt=document.createElement("div");jt.append(Ct?document.createTextNode(Ct):document.createElement("br")),this.editorDiv.append(jt)}},vt=new WeakSet,Tr=function(Ct){const{value:jt,fontSize:Zt,color:Xt,rect:sn,pageIndex:Ft}=St(this,ut);return Ct.value!==jt||Ct.fontSize!==Zt||Ct.rect.some((wt,kt)=>Math.abs(wt-sn[kt])>=1)||Ct.color.some((wt,kt)=>wt!==Xt[kt])||Ct.pageIndex!==Ft},$t=new WeakSet,qo=function(Ct=!1){if(!this.annotationElementId)return;if(un(this,yt,lo).call(this),!Ct&&(this.width===0||this.height===0)){setTimeout(()=>un(this,$t,qo).call(this,!0),0);return}const jt=Et._internalPadding*this.parentScale;St(this,ut).rect=this.getRect(jt,jt)},Jn(Et,"_freeTextDefaultContent",""),Jn(Et,"_internalPadding",0),Jn(Et,"_defaultColor",null),Jn(Et,"_defaultFontSize",10),Jn(Et,"_type","freetext");let et=Et;i.FreeTextEditor=et},(a,i,o)=>{var kt,Pt,_a,Ot,Ar,zt,Gt,Wt,qt,tn,ln,gn,yn,Pn,cn,xn,hn,en,Jt,vn,$n,Mn,On,kr,Bn,bo,Wn,Yo,Zn,Ko,dn,an,In,Dn,Xn,Yn,pn,Xo,Kt,nn,kn,Rn,Sr,Tn,Jo;Object.defineProperty(i,"__esModule",{value:!0}),i.StampAnnotationElement=i.InkAnnotationElement=i.FreeTextAnnotationElement=i.AnnotationLayer=void 0;var s=o(1),$=o(6),j=o(3),_e=o(30),et=o(31),tt=o(32);const nt=1e3,at=9,it=new WeakSet;function st(Cn){return{width:Cn[2]-Cn[0],height:Cn[3]-Cn[1]}}class lt{static create(Ut){switch(Ut.data.annotationType){case s.AnnotationType.LINK:return new rt(Ut);case s.AnnotationType.TEXT:return new ut(Ut);case s.AnnotationType.WIDGET:switch(Ut.data.fieldType){case"Tx":return new dt(Ut);case"Btn":return Ut.data.radioButton?new ft(Ut):Ut.data.checkBox?new mt(Ut):new ht(Ut);case"Ch":return new yt(Ut);case"Sig":return new pt(Ut)}return new ot(Ut);case s.AnnotationType.POPUP:return new bt(Ut);case s.AnnotationType.FREETEXT:return new xt(Ut);case s.AnnotationType.LINE:return new vt(Ut);case s.AnnotationType.SQUARE:return new Lt(Ut);case s.AnnotationType.CIRCLE:return new $t(Ut);case s.AnnotationType.POLYLINE:return new Tt(Ut);case s.AnnotationType.CARET:return new Dt(Ut);case s.AnnotationType.INK:return new It(Ut);case s.AnnotationType.POLYGON:return new Et(Ut);case s.AnnotationType.HIGHLIGHT:return new Ct(Ut);case s.AnnotationType.UNDERLINE:return new jt(Ut);case s.AnnotationType.SQUIGGLY:return new Zt(Ut);case s.AnnotationType.STRIKEOUT:return new Xt(Ut);case s.AnnotationType.STAMP:return new sn(Ut);case s.AnnotationType.FILEATTACHMENT:return new Ft(Ut);default:return new ct(Ut)}}}const At=class At{constructor(Ut,{isRenderable:Rt=!1,ignoreBorder:Nt=!1,createQuadrilaterals:Vt=!1}={}){Yt(this,kt,!1);this.isRenderable=Rt,this.data=Ut.data,this.layer=Ut.layer,this.linkService=Ut.linkService,this.downloadManager=Ut.downloadManager,this.imageResourcesPath=Ut.imageResourcesPath,this.renderForms=Ut.renderForms,this.svgFactory=Ut.svgFactory,this.annotationStorage=Ut.annotationStorage,this.enableScripting=Ut.enableScripting,this.hasJSActions=Ut.hasJSActions,this._fieldObjects=Ut.fieldObjects,this.parent=Ut.parent,Rt&&(this.container=this._createContainer(Nt)),Vt&&this._createQuadrilaterals()}static _hasPopupData({titleObj:Ut,contentsObj:Rt,richText:Nt}){return!!(Ut!=null&&Ut.str||Rt!=null&&Rt.str||Nt!=null&&Nt.str)}get hasPopupData(){return At._hasPopupData(this.data)}_createContainer(Ut){const{data:Rt,parent:{page:Nt,viewport:Vt}}=this,Qt=document.createElement("section");Qt.setAttribute("data-annotation-id",Rt.id),this instanceof ot||(Qt.tabIndex=nt),Qt.style.zIndex=this.parent.zIndex++,this.data.popupRef&&Qt.setAttribute("aria-haspopup","dialog"),Rt.noRotate&&Qt.classList.add("norotate");const{pageWidth:rn,pageHeight:fn,pageX:Ln,pageY:zn}=Vt.rawDims;if(!Rt.rect||this instanceof bt){const{rotation:Fn}=Rt;return!Rt.hasOwnCanvas&&Fn!==0&&this.setRotation(Fn,Qt),Qt}const{width:on,height:mn}=st(Rt.rect),Sn=s.Util.normalizeRect([Rt.rect[0],Nt.view[3]-Rt.rect[1]+Nt.view[1],Rt.rect[2],Nt.view[3]-Rt.rect[3]+Nt.view[1]]);if(!Ut&&Rt.borderStyle.width>0){Qt.style.borderWidth=`${Rt.borderStyle.width}px`;const Fn=Rt.borderStyle.horizontalCornerRadius,jn=Rt.borderStyle.verticalCornerRadius;if(Fn>0||jn>0){const Kn=`calc(${Fn}px * var(--scale-factor)) / calc(${jn}px * var(--scale-factor))`;Qt.style.borderRadius=Kn}else if(this instanceof ft){const Kn=`calc(${on}px * var(--scale-factor)) / calc(${mn}px * var(--scale-factor))`;Qt.style.borderRadius=Kn}switch(Rt.borderStyle.style){case s.AnnotationBorderStyleType.SOLID:Qt.style.borderStyle="solid";break;case s.AnnotationBorderStyleType.DASHED:Qt.style.borderStyle="dashed";break;case s.AnnotationBorderStyleType.BEVELED:(0,s.warn)("Unimplemented border style: beveled");break;case s.AnnotationBorderStyleType.INSET:(0,s.warn)("Unimplemented border style: inset");break;case s.AnnotationBorderStyleType.UNDERLINE:Qt.style.borderBottomStyle="solid";break}const Vn=Rt.borderColor||null;Vn?(wn(this,kt,!0),Qt.style.borderColor=s.Util.makeHexColor(Vn[0]|0,Vn[1]|0,Vn[2]|0)):Qt.style.borderWidth=0}Qt.style.left=`${100*(Sn[0]-Ln)/rn}%`,Qt.style.top=`${100*(Sn[1]-zn)/fn}%`;const{rotation:An}=Rt;return Rt.hasOwnCanvas||An===0?(Qt.style.width=`${100*on/rn}%`,Qt.style.height=`${100*mn/fn}%`):this.setRotation(An,Qt),Qt}setRotation(Ut,Rt=this.container){if(!this.data.rect)return;const{pageWidth:Nt,pageHeight:Vt}=this.parent.viewport.rawDims,{width:Qt,height:rn}=st(this.data.rect);let fn,Ln;Ut%180===0?(fn=100*Qt/Nt,Ln=100*rn/Vt):(fn=100*rn/Nt,Ln=100*Qt/Vt),Rt.style.width=`${fn}%`,Rt.style.height=`${Ln}%`,Rt.setAttribute("data-main-rotation",(360-Ut)%360)}get _commonActions(){const Ut=(Rt,Nt,Vt)=>{const Qt=Vt.detail[Rt],rn=Qt[0],fn=Qt.slice(1);Vt.target.style[Nt]=_e.ColorConverters[`${rn}_HTML`](fn),this.annotationStorage.setValue(this.data.id,{[Nt]:_e.ColorConverters[`${rn}_rgb`](fn)})};return(0,s.shadow)(this,"_commonActions",{display:Rt=>{const{display:Nt}=Rt.detail,Vt=Nt%2===1;this.container.style.visibility=Vt?"hidden":"visible",this.annotationStorage.setValue(this.data.id,{noView:Vt,noPrint:Nt===1||Nt===2})},print:Rt=>{this.annotationStorage.setValue(this.data.id,{noPrint:!Rt.detail.print})},hidden:Rt=>{const{hidden:Nt}=Rt.detail;this.container.style.visibility=Nt?"hidden":"visible",this.annotationStorage.setValue(this.data.id,{noPrint:Nt,noView:Nt})},focus:Rt=>{setTimeout(()=>Rt.target.focus({preventScroll:!1}),0)},userName:Rt=>{Rt.target.title=Rt.detail.userName},readonly:Rt=>{Rt.target.disabled=Rt.detail.readonly},required:Rt=>{this._setRequired(Rt.target,Rt.detail.required)},bgColor:Rt=>{Ut("bgColor","backgroundColor",Rt)},fillColor:Rt=>{Ut("fillColor","backgroundColor",Rt)},fgColor:Rt=>{Ut("fgColor","color",Rt)},textColor:Rt=>{Ut("textColor","color",Rt)},borderColor:Rt=>{Ut("borderColor","borderColor",Rt)},strokeColor:Rt=>{Ut("strokeColor","borderColor",Rt)},rotation:Rt=>{const Nt=Rt.detail.rotation;this.setRotation(Nt),this.annotationStorage.setValue(this.data.id,{rotation:Nt})}})}_dispatchEventFromSandbox(Ut,Rt){const Nt=this._commonActions;for(const Vt of Object.keys(Rt.detail)){const Qt=Ut[Vt]||Nt[Vt];Qt==null||Qt(Rt)}}_setDefaultPropertiesFromJS(Ut){if(!this.enableScripting)return;const Rt=this.annotationStorage.getRawValue(this.data.id);if(!Rt)return;const Nt=this._commonActions;for(const[Vt,Qt]of Object.entries(Rt)){const rn=Nt[Vt];if(rn){const fn={detail:{[Vt]:Qt},target:Ut};rn(fn),delete Rt[Vt]}}}_createQuadrilaterals(){if(!this.container)return;const{quadPoints:Ut}=this.data;if(!Ut)return;const[Rt,Nt,Vt,Qt]=this.data.rect;if(Ut.length===1){const[,{x:jn,y:Vn},{x:Kn,y:Gn}]=Ut[0];if(Vt===jn&&Qt===Vn&&Rt===Kn&&Nt===Gn)return}const{style:rn}=this.container;let fn;if(St(this,kt)){const{borderColor:jn,borderWidth:Vn}=rn;rn.borderWidth=0,fn=["url('data:image/svg+xml;utf8,",'<svg xmlns="http://www.w3.org/2000/svg"',' preserveAspectRatio="none" viewBox="0 0 1 1">',`<g fill="transparent" stroke="${jn}" stroke-width="${Vn}">`],this.container.classList.add("hasBorder")}const Ln=Vt-Rt,zn=Qt-Nt,{svgFactory:on}=this,mn=on.createElement("svg");mn.classList.add("quadrilateralsContainer"),mn.setAttribute("width",0),mn.setAttribute("height",0);const Sn=on.createElement("defs");mn.append(Sn);const An=on.createElement("clipPath"),Fn=`clippath_${this.data.id}`;An.setAttribute("id",Fn),An.setAttribute("clipPathUnits","objectBoundingBox"),Sn.append(An);for(const[,{x:jn,y:Vn},{x:Kn,y:Gn}]of Ut){const qn=on.createElement("rect"),Qn=(Kn-Rt)/Ln,na=(Qt-Vn)/zn,ba=(jn-Kn)/Ln,sr=(Vn-Gn)/zn;qn.setAttribute("x",Qn),qn.setAttribute("y",na),qn.setAttribute("width",ba),qn.setAttribute("height",sr),An.append(qn),fn==null||fn.push(`<rect vector-effect="non-scaling-stroke" x="${Qn}" y="${na}" width="${ba}" height="${sr}"/>`)}St(this,kt)&&(fn.push("</g></svg>')"),rn.backgroundImage=fn.join("")),this.container.append(mn),this.container.style.clipPath=`url(#${Fn})`}_createPopup(){const{container:Ut,data:Rt}=this;Ut.setAttribute("aria-haspopup","dialog");const Nt=new bt({data:{color:Rt.color,titleObj:Rt.titleObj,modificationDate:Rt.modificationDate,contentsObj:Rt.contentsObj,richText:Rt.richText,parentRect:Rt.rect,borderStyle:0,id:`popup_${Rt.id}`,rotation:Rt.rotation},parent:this.parent,elements:[this]});this.parent.div.append(Nt.render())}render(){(0,s.unreachable)("Abstract method `AnnotationElement.render` called")}_getElementsByName(Ut,Rt=null){const Nt=[];if(this._fieldObjects){const Vt=this._fieldObjects[Ut];if(Vt)for(const{page:Qt,id:rn,exportValues:fn}of Vt){if(Qt===-1||rn===Rt)continue;const Ln=typeof fn=="string"?fn:null,zn=document.querySelector(`[data-element-id="${rn}"]`);if(zn&&!it.has(zn)){(0,s.warn)(`_getElementsByName - element not allowed: ${rn}`);continue}Nt.push({id:rn,exportValue:Ln,domElement:zn})}return Nt}for(const Vt of document.getElementsByName(Ut)){const{exportValue:Qt}=Vt,rn=Vt.getAttribute("data-element-id");rn!==Rt&&it.has(Vt)&&Nt.push({id:rn,exportValue:Qt,domElement:Vt})}return Nt}show(){var Ut;this.container&&(this.container.hidden=!1),(Ut=this.popup)==null||Ut.maybeShow()}hide(){var Ut;this.container&&(this.container.hidden=!0),(Ut=this.popup)==null||Ut.forceHide()}getElementsToTriggerPopup(){return this.container}addHighlightArea(){const Ut=this.getElementsToTriggerPopup();if(Array.isArray(Ut))for(const Rt of Ut)Rt.classList.add("highlightArea");else Ut.classList.add("highlightArea")}_editOnDoubleClick(){const{annotationEditorType:Ut,data:{id:Rt}}=this;this.container.addEventListener("dblclick",()=>{var Nt;(Nt=this.linkService.eventBus)==null||Nt.dispatch("switchannotationeditormode",{source:this,mode:Ut,editId:Rt})})}};kt=new WeakMap;let ct=At;class rt extends ct{constructor(Rt,Nt=null){super(Rt,{isRenderable:!0,ignoreBorder:!!(Nt!=null&&Nt.ignoreBorder),createQuadrilaterals:!0});Yt(this,Pt);Yt(this,Ot);this.isTooltipOnly=Rt.data.isTooltipOnly}render(){const{data:Rt,linkService:Nt}=this,Vt=document.createElement("a");Vt.setAttribute("data-element-id",Rt.id);let Qt=!1;return Rt.url?(Nt.addLinkAttributes(Vt,Rt.url,Rt.newWindow),Qt=!0):Rt.action?(this._bindNamedAction(Vt,Rt.action),Qt=!0):Rt.attachment?(this._bindAttachment(Vt,Rt.attachment),Qt=!0):Rt.setOCGState?(un(this,Ot,Ar).call(this,Vt,Rt.setOCGState),Qt=!0):Rt.dest?(this._bindLink(Vt,Rt.dest),Qt=!0):(Rt.actions&&(Rt.actions.Action||Rt.actions["Mouse Up"]||Rt.actions["Mouse Down"])&&this.enableScripting&&this.hasJSActions&&(this._bindJSAction(Vt,Rt),Qt=!0),Rt.resetForm?(this._bindResetFormAction(Vt,Rt.resetForm),Qt=!0):this.isTooltipOnly&&!Qt&&(this._bindLink(Vt,""),Qt=!0)),this.container.classList.add("linkAnnotation"),Qt&&this.container.append(Vt),this.container}_bindLink(Rt,Nt){Rt.href=this.linkService.getDestinationHash(Nt),Rt.onclick=()=>(Nt&&this.linkService.goToDestination(Nt),!1),(Nt||Nt==="")&&un(this,Pt,_a).call(this)}_bindNamedAction(Rt,Nt){Rt.href=this.linkService.getAnchorUrl(""),Rt.onclick=()=>(this.linkService.executeNamedAction(Nt),!1),un(this,Pt,_a).call(this)}_bindAttachment(Rt,Nt){Rt.href=this.linkService.getAnchorUrl(""),Rt.onclick=()=>{var Vt;return(Vt=this.downloadManager)==null||Vt.openOrDownloadData(this.container,Nt.content,Nt.filename),!1},un(this,Pt,_a).call(this)}_bindJSAction(Rt,Nt){Rt.href=this.linkService.getAnchorUrl("");const Vt=new Map([["Action","onclick"],["Mouse Up","onmouseup"],["Mouse Down","onmousedown"]]);for(const Qt of Object.keys(Nt.actions)){const rn=Vt.get(Qt);rn&&(Rt[rn]=()=>{var fn;return(fn=this.linkService.eventBus)==null||fn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:Nt.id,name:Qt}}),!1})}Rt.onclick||(Rt.onclick=()=>!1),un(this,Pt,_a).call(this)}_bindResetFormAction(Rt,Nt){const Vt=Rt.onclick;if(Vt||(Rt.href=this.linkService.getAnchorUrl("")),un(this,Pt,_a).call(this),!this._fieldObjects){(0,s.warn)('_bindResetFormAction - "resetForm" action not supported, ensure that the `fieldObjects` parameter is provided.'),Vt||(Rt.onclick=()=>!1);return}Rt.onclick=()=>{var mn;Vt==null||Vt();const{fields:Qt,refs:rn,include:fn}=Nt,Ln=[];if(Qt.length!==0||rn.length!==0){const Sn=new Set(rn);for(const An of Qt){const Fn=this._fieldObjects[An]||[];for(const{id:jn}of Fn)Sn.add(jn)}for(const An of Object.values(this._fieldObjects))for(const Fn of An)Sn.has(Fn.id)===fn&&Ln.push(Fn)}else for(const Sn of Object.values(this._fieldObjects))Ln.push(...Sn);const zn=this.annotationStorage,on=[];for(const Sn of Ln){const{id:An}=Sn;switch(on.push(An),Sn.type){case"text":{const jn=Sn.defaultValue||"";zn.setValue(An,{value:jn});break}case"checkbox":case"radiobutton":{const jn=Sn.defaultValue===Sn.exportValues;zn.setValue(An,{value:jn});break}case"combobox":case"listbox":{const jn=Sn.defaultValue||"";zn.setValue(An,{value:jn});break}default:continue}const Fn=document.querySelector(`[data-element-id="${An}"]`);if(Fn){if(!it.has(Fn)){(0,s.warn)(`_bindResetFormAction - element not allowed: ${An}`);continue}}else continue;Fn.dispatchEvent(new Event("resetform"))}return this.enableScripting&&((mn=this.linkService.eventBus)==null||mn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:"app",ids:on,name:"ResetForm"}})),!1}}}Pt=new WeakSet,_a=function(){this.container.setAttribute("data-internal-link","")},Ot=new WeakSet,Ar=function(Rt,Nt){Rt.href=this.linkService.getAnchorUrl(""),Rt.onclick=()=>(this.linkService.executeSetOCGState(Nt),!1),un(this,Pt,_a).call(this)};class ut extends ct{constructor(Ut){super(Ut,{isRenderable:!0})}render(){this.container.classList.add("textAnnotation");const Ut=document.createElement("img");return Ut.src=this.imageResourcesPath+"annotation-"+this.data.name.toLowerCase()+".svg",Ut.alt="[{{type}} Annotation]",Ut.dataset.l10nId="text_annotation_type",Ut.dataset.l10nArgs=JSON.stringify({type:this.data.name}),!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container.append(Ut),this.container}}class ot extends ct{render(){return this.data.alternativeText&&(this.container.title=this.data.alternativeText),this.container}showElementAndHideCanvas(Ut){var Rt;this.data.hasOwnCanvas&&(((Rt=Ut.previousSibling)==null?void 0:Rt.nodeName)==="CANVAS"&&(Ut.previousSibling.hidden=!0),Ut.hidden=!1)}_getKeyModifier(Ut){const{isWin:Rt,isMac:Nt}=s.FeatureTest.platform;return Rt&&Ut.ctrlKey||Nt&&Ut.metaKey}_setEventListener(Ut,Rt,Nt,Vt,Qt){Nt.includes("mouse")?Ut.addEventListener(Nt,rn=>{var fn;(fn=this.linkService.eventBus)==null||fn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:this.data.id,name:Vt,value:Qt(rn),shift:rn.shiftKey,modifier:this._getKeyModifier(rn)}})}):Ut.addEventListener(Nt,rn=>{var fn;if(Nt==="blur"){if(!Rt.focused||!rn.relatedTarget)return;Rt.focused=!1}else if(Nt==="focus"){if(Rt.focused)return;Rt.focused=!0}Qt&&((fn=this.linkService.eventBus)==null||fn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:this.data.id,name:Vt,value:Qt(rn)}}))})}_setEventListeners(Ut,Rt,Nt,Vt){var Qt,rn,fn;for(const[Ln,zn]of Nt)(zn==="Action"||(Qt=this.data.actions)!=null&&Qt[zn])&&((zn==="Focus"||zn==="Blur")&&(Rt||(Rt={focused:!1})),this._setEventListener(Ut,Rt,Ln,zn,Vt),zn==="Focus"&&!((rn=this.data.actions)!=null&&rn.Blur)?this._setEventListener(Ut,Rt,"blur","Blur",null):zn==="Blur"&&!((fn=this.data.actions)!=null&&fn.Focus)&&this._setEventListener(Ut,Rt,"focus","Focus",null))}_setBackgroundColor(Ut){const Rt=this.data.backgroundColor||null;Ut.style.backgroundColor=Rt===null?"transparent":s.Util.makeHexColor(Rt[0],Rt[1],Rt[2])}_setTextStyle(Ut){const Rt=["left","center","right"],{fontColor:Nt}=this.data.defaultAppearanceData,Vt=this.data.defaultAppearanceData.fontSize||at,Qt=Ut.style;let rn;const fn=2,Ln=zn=>Math.round(10*zn)/10;if(this.data.multiLine){const zn=Math.abs(this.data.rect[3]-this.data.rect[1]-fn),on=Math.round(zn/(s.LINE_FACTOR*Vt))||1,mn=zn/on;rn=Math.min(Vt,Ln(mn/s.LINE_FACTOR))}else{const zn=Math.abs(this.data.rect[3]-this.data.rect[1]-fn);rn=Math.min(Vt,Ln(zn/s.LINE_FACTOR))}Qt.fontSize=`calc(${rn}px * var(--scale-factor))`,Qt.color=s.Util.makeHexColor(Nt[0],Nt[1],Nt[2]),this.data.textAlignment!==null&&(Qt.textAlign=Rt[this.data.textAlignment])}_setRequired(Ut,Rt){Rt?Ut.setAttribute("required",!0):Ut.removeAttribute("required"),Ut.setAttribute("aria-required",Rt)}}class dt extends ot{constructor(Ut){const Rt=Ut.renderForms||!Ut.data.hasAppearance&&!!Ut.data.fieldValue;super(Ut,{isRenderable:Rt})}setPropertyOnSiblings(Ut,Rt,Nt,Vt){const Qt=this.annotationStorage;for(const rn of this._getElementsByName(Ut.name,Ut.id))rn.domElement&&(rn.domElement[Rt]=Nt),Qt.setValue(rn.id,{[Vt]:Nt})}render(){var Vt,Qt;const Ut=this.annotationStorage,Rt=this.data.id;this.container.classList.add("textWidgetAnnotation");let Nt=null;if(this.renderForms){const rn=Ut.getValue(Rt,{value:this.data.fieldValue});let fn=rn.value||"";const Ln=Ut.getValue(Rt,{charLimit:this.data.maxLen}).charLimit;Ln&&fn.length>Ln&&(fn=fn.slice(0,Ln));let zn=rn.formattedValue||((Vt=this.data.textContent)==null?void 0:Vt.join(`
`))||null;zn&&this.data.comb&&(zn=zn.replaceAll(/\s+/g,""));const on={userValue:fn,formattedValue:zn,lastCommittedValue:null,commitKey:1,focused:!1};this.data.multiLine?(Nt=document.createElement("textarea"),Nt.textContent=zn??fn,this.data.doNotScroll&&(Nt.style.overflowY="hidden")):(Nt=document.createElement("input"),Nt.type="text",Nt.setAttribute("value",zn??fn),this.data.doNotScroll&&(Nt.style.overflowX="hidden")),this.data.hasOwnCanvas&&(Nt.hidden=!0),it.add(Nt),Nt.setAttribute("data-element-id",Rt),Nt.disabled=this.data.readOnly,Nt.name=this.data.fieldName,Nt.tabIndex=nt,this._setRequired(Nt,this.data.required),Ln&&(Nt.maxLength=Ln),Nt.addEventListener("input",Sn=>{Ut.setValue(Rt,{value:Sn.target.value}),this.setPropertyOnSiblings(Nt,"value",Sn.target.value,"value"),on.formattedValue=null}),Nt.addEventListener("resetform",Sn=>{const An=this.data.defaultFieldValue??"";Nt.value=on.userValue=An,on.formattedValue=null});let mn=Sn=>{const{formattedValue:An}=on;An!=null&&(Sn.target.value=An),Sn.target.scrollLeft=0};if(this.enableScripting&&this.hasJSActions){Nt.addEventListener("focus",An=>{if(on.focused)return;const{target:Fn}=An;on.userValue&&(Fn.value=on.userValue),on.lastCommittedValue=Fn.value,on.commitKey=1,on.focused=!0}),Nt.addEventListener("updatefromsandbox",An=>{this.showElementAndHideCanvas(An.target);const Fn={value(jn){on.userValue=jn.detail.value??"",Ut.setValue(Rt,{value:on.userValue.toString()}),jn.target.value=on.userValue},formattedValue(jn){const{formattedValue:Vn}=jn.detail;on.formattedValue=Vn,Vn!=null&&jn.target!==document.activeElement&&(jn.target.value=Vn),Ut.setValue(Rt,{formattedValue:Vn})},selRange(jn){jn.target.setSelectionRange(...jn.detail.selRange)},charLimit:jn=>{var qn;const{charLimit:Vn}=jn.detail,{target:Kn}=jn;if(Vn===0){Kn.removeAttribute("maxLength");return}Kn.setAttribute("maxLength",Vn);let Gn=on.userValue;!Gn||Gn.length<=Vn||(Gn=Gn.slice(0,Vn),Kn.value=on.userValue=Gn,Ut.setValue(Rt,{value:Gn}),(qn=this.linkService.eventBus)==null||qn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:Rt,name:"Keystroke",value:Gn,willCommit:!0,commitKey:1,selStart:Kn.selectionStart,selEnd:Kn.selectionEnd}}))}};this._dispatchEventFromSandbox(Fn,An)}),Nt.addEventListener("keydown",An=>{var Vn;on.commitKey=1;let Fn=-1;if(An.key==="Escape"?Fn=0:An.key==="Enter"&&!this.data.multiLine?Fn=2:An.key==="Tab"&&(on.commitKey=3),Fn===-1)return;const{value:jn}=An.target;on.lastCommittedValue!==jn&&(on.lastCommittedValue=jn,on.userValue=jn,(Vn=this.linkService.eventBus)==null||Vn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:Rt,name:"Keystroke",value:jn,willCommit:!0,commitKey:Fn,selStart:An.target.selectionStart,selEnd:An.target.selectionEnd}}))});const Sn=mn;mn=null,Nt.addEventListener("blur",An=>{var jn;if(!on.focused||!An.relatedTarget)return;on.focused=!1;const{value:Fn}=An.target;on.userValue=Fn,on.lastCommittedValue!==Fn&&((jn=this.linkService.eventBus)==null||jn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:Rt,name:"Keystroke",value:Fn,willCommit:!0,commitKey:on.commitKey,selStart:An.target.selectionStart,selEnd:An.target.selectionEnd}})),Sn(An)}),(Qt=this.data.actions)!=null&&Qt.Keystroke&&Nt.addEventListener("beforeinput",An=>{var na;on.lastCommittedValue=null;const{data:Fn,target:jn}=An,{value:Vn,selectionStart:Kn,selectionEnd:Gn}=jn;let qn=Kn,Qn=Gn;switch(An.inputType){case"deleteWordBackward":{const ba=Vn.substring(0,Kn).match(/\w*[^\w]*$/);ba&&(qn-=ba[0].length);break}case"deleteWordForward":{const ba=Vn.substring(Kn).match(/^[^\w]*\w*/);ba&&(Qn+=ba[0].length);break}case"deleteContentBackward":Kn===Gn&&(qn-=1);break;case"deleteContentForward":Kn===Gn&&(Qn+=1);break}An.preventDefault(),(na=this.linkService.eventBus)==null||na.dispatch("dispatcheventinsandbox",{source:this,detail:{id:Rt,name:"Keystroke",value:Vn,change:Fn||"",willCommit:!1,selStart:qn,selEnd:Qn}})}),this._setEventListeners(Nt,on,[["focus","Focus"],["blur","Blur"],["mousedown","Mouse Down"],["mouseenter","Mouse Enter"],["mouseleave","Mouse Exit"],["mouseup","Mouse Up"]],An=>An.target.value)}if(mn&&Nt.addEventListener("blur",mn),this.data.comb){const An=(this.data.rect[2]-this.data.rect[0])/Ln;Nt.classList.add("comb"),Nt.style.letterSpacing=`calc(${An}px * var(--scale-factor) - 1ch)`}}else Nt=document.createElement("div"),Nt.textContent=this.data.fieldValue,Nt.style.verticalAlign="middle",Nt.style.display="table-cell";return this._setTextStyle(Nt),this._setBackgroundColor(Nt),this._setDefaultPropertiesFromJS(Nt),this.container.append(Nt),this.container}}class pt extends ot{constructor(Ut){super(Ut,{isRenderable:!!Ut.data.hasOwnCanvas})}}class mt extends ot{constructor(Ut){super(Ut,{isRenderable:Ut.renderForms})}render(){const Ut=this.annotationStorage,Rt=this.data,Nt=Rt.id;let Vt=Ut.getValue(Nt,{value:Rt.exportValue===Rt.fieldValue}).value;typeof Vt=="string"&&(Vt=Vt!=="Off",Ut.setValue(Nt,{value:Vt})),this.container.classList.add("buttonWidgetAnnotation","checkBox");const Qt=document.createElement("input");return it.add(Qt),Qt.setAttribute("data-element-id",Nt),Qt.disabled=Rt.readOnly,this._setRequired(Qt,this.data.required),Qt.type="checkbox",Qt.name=Rt.fieldName,Vt&&Qt.setAttribute("checked",!0),Qt.setAttribute("exportValue",Rt.exportValue),Qt.tabIndex=nt,Qt.addEventListener("change",rn=>{const{name:fn,checked:Ln}=rn.target;for(const zn of this._getElementsByName(fn,Nt)){const on=Ln&&zn.exportValue===Rt.exportValue;zn.domElement&&(zn.domElement.checked=on),Ut.setValue(zn.id,{value:on})}Ut.setValue(Nt,{value:Ln})}),Qt.addEventListener("resetform",rn=>{const fn=Rt.defaultFieldValue||"Off";rn.target.checked=fn===Rt.exportValue}),this.enableScripting&&this.hasJSActions&&(Qt.addEventListener("updatefromsandbox",rn=>{const fn={value(Ln){Ln.target.checked=Ln.detail.value!=="Off",Ut.setValue(Nt,{value:Ln.target.checked})}};this._dispatchEventFromSandbox(fn,rn)}),this._setEventListeners(Qt,null,[["change","Validate"],["change","Action"],["focus","Focus"],["blur","Blur"],["mousedown","Mouse Down"],["mouseenter","Mouse Enter"],["mouseleave","Mouse Exit"],["mouseup","Mouse Up"]],rn=>rn.target.checked)),this._setBackgroundColor(Qt),this._setDefaultPropertiesFromJS(Qt),this.container.append(Qt),this.container}}class ft extends ot{constructor(Ut){super(Ut,{isRenderable:Ut.renderForms})}render(){this.container.classList.add("buttonWidgetAnnotation","radioButton");const Ut=this.annotationStorage,Rt=this.data,Nt=Rt.id;let Vt=Ut.getValue(Nt,{value:Rt.fieldValue===Rt.buttonValue}).value;typeof Vt=="string"&&(Vt=Vt!==Rt.buttonValue,Ut.setValue(Nt,{value:Vt}));const Qt=document.createElement("input");if(it.add(Qt),Qt.setAttribute("data-element-id",Nt),Qt.disabled=Rt.readOnly,this._setRequired(Qt,this.data.required),Qt.type="radio",Qt.name=Rt.fieldName,Vt&&Qt.setAttribute("checked",!0),Qt.tabIndex=nt,Qt.addEventListener("change",rn=>{const{name:fn,checked:Ln}=rn.target;for(const zn of this._getElementsByName(fn,Nt))Ut.setValue(zn.id,{value:!1});Ut.setValue(Nt,{value:Ln})}),Qt.addEventListener("resetform",rn=>{const fn=Rt.defaultFieldValue;rn.target.checked=fn!=null&&fn===Rt.buttonValue}),this.enableScripting&&this.hasJSActions){const rn=Rt.buttonValue;Qt.addEventListener("updatefromsandbox",fn=>{const Ln={value:zn=>{const on=rn===zn.detail.value;for(const mn of this._getElementsByName(zn.target.name)){const Sn=on&&mn.id===Nt;mn.domElement&&(mn.domElement.checked=Sn),Ut.setValue(mn.id,{value:Sn})}}};this._dispatchEventFromSandbox(Ln,fn)}),this._setEventListeners(Qt,null,[["change","Validate"],["change","Action"],["focus","Focus"],["blur","Blur"],["mousedown","Mouse Down"],["mouseenter","Mouse Enter"],["mouseleave","Mouse Exit"],["mouseup","Mouse Up"]],fn=>fn.target.checked)}return this._setBackgroundColor(Qt),this._setDefaultPropertiesFromJS(Qt),this.container.append(Qt),this.container}}class ht extends rt{constructor(Ut){super(Ut,{ignoreBorder:Ut.data.hasAppearance})}render(){const Ut=super.render();Ut.classList.add("buttonWidgetAnnotation","pushButton"),this.data.alternativeText&&(Ut.title=this.data.alternativeText);const Rt=Ut.lastChild;return this.enableScripting&&this.hasJSActions&&Rt&&(this._setDefaultPropertiesFromJS(Rt),Rt.addEventListener("updatefromsandbox",Nt=>{this._dispatchEventFromSandbox({},Nt)})),Ut}}class yt extends ot{constructor(Ut){super(Ut,{isRenderable:Ut.renderForms})}render(){this.container.classList.add("choiceWidgetAnnotation");const Ut=this.annotationStorage,Rt=this.data.id,Nt=Ut.getValue(Rt,{value:this.data.fieldValue}),Vt=document.createElement("select");it.add(Vt),Vt.setAttribute("data-element-id",Rt),Vt.disabled=this.data.readOnly,this._setRequired(Vt,this.data.required),Vt.name=this.data.fieldName,Vt.tabIndex=nt;let Qt=this.data.combo&&this.data.options.length>0;this.data.combo||(Vt.size=this.data.options.length,this.data.multiSelect&&(Vt.multiple=!0)),Vt.addEventListener("resetform",on=>{const mn=this.data.defaultFieldValue;for(const Sn of Vt.options)Sn.selected=Sn.value===mn});for(const on of this.data.options){const mn=document.createElement("option");mn.textContent=on.displayValue,mn.value=on.exportValue,Nt.value.includes(on.exportValue)&&(mn.setAttribute("selected",!0),Qt=!1),Vt.append(mn)}let rn=null;if(Qt){const on=document.createElement("option");on.value=" ",on.setAttribute("hidden",!0),on.setAttribute("selected",!0),Vt.prepend(on),rn=()=>{on.remove(),Vt.removeEventListener("input",rn),rn=null},Vt.addEventListener("input",rn)}const fn=on=>{const mn=on?"value":"textContent",{options:Sn,multiple:An}=Vt;return An?Array.prototype.filter.call(Sn,Fn=>Fn.selected).map(Fn=>Fn[mn]):Sn.selectedIndex===-1?null:Sn[Sn.selectedIndex][mn]};let Ln=fn(!1);const zn=on=>{const mn=on.target.options;return Array.prototype.map.call(mn,Sn=>({displayValue:Sn.textContent,exportValue:Sn.value}))};return this.enableScripting&&this.hasJSActions?(Vt.addEventListener("updatefromsandbox",on=>{const mn={value(Sn){rn==null||rn();const An=Sn.detail.value,Fn=new Set(Array.isArray(An)?An:[An]);for(const jn of Vt.options)jn.selected=Fn.has(jn.value);Ut.setValue(Rt,{value:fn(!0)}),Ln=fn(!1)},multipleSelection(Sn){Vt.multiple=!0},remove(Sn){const An=Vt.options,Fn=Sn.detail.remove;An[Fn].selected=!1,Vt.remove(Fn),An.length>0&&Array.prototype.findIndex.call(An,Vn=>Vn.selected)===-1&&(An[0].selected=!0),Ut.setValue(Rt,{value:fn(!0),items:zn(Sn)}),Ln=fn(!1)},clear(Sn){for(;Vt.length!==0;)Vt.remove(0);Ut.setValue(Rt,{value:null,items:[]}),Ln=fn(!1)},insert(Sn){const{index:An,displayValue:Fn,exportValue:jn}=Sn.detail.insert,Vn=Vt.children[An],Kn=document.createElement("option");Kn.textContent=Fn,Kn.value=jn,Vn?Vn.before(Kn):Vt.append(Kn),Ut.setValue(Rt,{value:fn(!0),items:zn(Sn)}),Ln=fn(!1)},items(Sn){const{items:An}=Sn.detail;for(;Vt.length!==0;)Vt.remove(0);for(const Fn of An){const{displayValue:jn,exportValue:Vn}=Fn,Kn=document.createElement("option");Kn.textContent=jn,Kn.value=Vn,Vt.append(Kn)}Vt.options.length>0&&(Vt.options[0].selected=!0),Ut.setValue(Rt,{value:fn(!0),items:zn(Sn)}),Ln=fn(!1)},indices(Sn){const An=new Set(Sn.detail.indices);for(const Fn of Sn.target.options)Fn.selected=An.has(Fn.index);Ut.setValue(Rt,{value:fn(!0)}),Ln=fn(!1)},editable(Sn){Sn.target.disabled=!Sn.detail.editable}};this._dispatchEventFromSandbox(mn,on)}),Vt.addEventListener("input",on=>{var Sn;const mn=fn(!0);Ut.setValue(Rt,{value:mn}),on.preventDefault(),(Sn=this.linkService.eventBus)==null||Sn.dispatch("dispatcheventinsandbox",{source:this,detail:{id:Rt,name:"Keystroke",value:Ln,changeEx:mn,willCommit:!1,commitKey:1,keyDown:!1}})}),this._setEventListeners(Vt,null,[["focus","Focus"],["blur","Blur"],["mousedown","Mouse Down"],["mouseenter","Mouse Enter"],["mouseleave","Mouse Exit"],["mouseup","Mouse Up"],["input","Action"],["input","Validate"]],on=>on.target.value)):Vt.addEventListener("input",function(on){Ut.setValue(Rt,{value:fn(!0)})}),this.data.combo&&this._setTextStyle(Vt),this._setBackgroundColor(Vt),this._setDefaultPropertiesFromJS(Vt),this.container.append(Vt),this.container}}class bt extends ct{constructor(Ut){const{data:Rt,elements:Nt}=Ut;super(Ut,{isRenderable:ct._hasPopupData(Rt)}),this.elements=Nt}render(){this.container.classList.add("popupAnnotation");const Ut=new gt({container:this.container,color:this.data.color,titleObj:this.data.titleObj,modificationDate:this.data.modificationDate,contentsObj:this.data.contentsObj,richText:this.data.richText,rect:this.data.rect,parentRect:this.data.parentRect||null,parent:this.parent,elements:this.elements,open:this.data.open}),Rt=[];for(const Nt of this.elements)Nt.popup=Ut,Rt.push(Nt.data.id),Nt.addHighlightArea();return this.container.setAttribute("aria-controls",Rt.map(Nt=>`${s.AnnotationPrefix}${Nt}`).join(",")),this.container}}class gt{constructor({container:Ut,color:Rt,elements:Nt,titleObj:Vt,modificationDate:Qt,contentsObj:rn,richText:fn,parent:Ln,rect:zn,parentRect:on,open:mn}){Yt(this,On);Yt(this,Bn);Yt(this,Wn);Yt(this,Zn);Yt(this,zt,null);Yt(this,Gt,un(this,On,kr).bind(this));Yt(this,Wt,un(this,Zn,Ko).bind(this));Yt(this,qt,un(this,Wn,Yo).bind(this));Yt(this,tn,un(this,Bn,bo).bind(this));Yt(this,ln,null);Yt(this,gn,null);Yt(this,yn,null);Yt(this,Pn,null);Yt(this,cn,null);Yt(this,xn,null);Yt(this,hn,!1);Yt(this,en,null);Yt(this,Jt,null);Yt(this,vn,null);Yt(this,$n,null);Yt(this,Mn,!1);var An;wn(this,gn,Ut),wn(this,$n,Vt),wn(this,yn,rn),wn(this,vn,fn),wn(this,cn,Ln),wn(this,ln,Rt),wn(this,Jt,zn),wn(this,xn,on),wn(this,Pn,Nt);const Sn=$.PDFDateString.toDateObject(Qt);Sn&&wn(this,zt,Ln.l10n.get("annotation_date_string",{date:Sn.toLocaleDateString(),time:Sn.toLocaleTimeString()})),this.trigger=Nt.flatMap(Fn=>Fn.getElementsToTriggerPopup());for(const Fn of this.trigger)Fn.addEventListener("click",St(this,tn)),Fn.addEventListener("mouseenter",St(this,qt)),Fn.addEventListener("mouseleave",St(this,Wt)),Fn.classList.add("popupTriggerArea");for(const Fn of Nt)(An=Fn.container)==null||An.addEventListener("keydown",St(this,Gt));St(this,gn).hidden=!0,mn&&un(this,Bn,bo).call(this)}render(){if(St(this,en))return;const{page:{view:Ut},viewport:{rawDims:{pageWidth:Rt,pageHeight:Nt,pageX:Vt,pageY:Qt}}}=St(this,cn),rn=wn(this,en,document.createElement("div"));if(rn.className="popup",St(this,ln)){const qn=rn.style.outlineColor=s.Util.makeHexColor(...St(this,ln));CSS.supports("background-color","color-mix(in srgb, red 30%, white)")?rn.style.backgroundColor=`color-mix(in srgb, ${qn} 30%, white)`:rn.style.backgroundColor=s.Util.makeHexColor(...St(this,ln).map(na=>Math.floor(.7*(255-na)+na)))}const fn=document.createElement("span");fn.className="header";const Ln=document.createElement("h1");if(fn.append(Ln),{dir:Ln.dir,str:Ln.textContent}=St(this,$n),rn.append(fn),St(this,zt)){const qn=document.createElement("span");qn.classList.add("popupDate"),St(this,zt).then(Qn=>{qn.textContent=Qn}),fn.append(qn)}const zn=St(this,yn),on=St(this,vn);if(on!=null&&on.str&&(!(zn!=null&&zn.str)||zn.str===on.str))tt.XfaLayer.render({xfaHtml:on.html,intent:"richText",div:rn}),rn.lastChild.classList.add("richText","popupContent");else{const qn=this._formatContents(zn);rn.append(qn)}let mn=!!St(this,xn),Sn=mn?St(this,xn):St(this,Jt);for(const qn of St(this,Pn))if(!Sn||s.Util.intersect(qn.data.rect,Sn)!==null){Sn=qn.data.rect,mn=!0;break}const An=s.Util.normalizeRect([Sn[0],Ut[3]-Sn[1]+Ut[1],Sn[2],Ut[3]-Sn[3]+Ut[1]]),jn=mn?Sn[2]-Sn[0]+5:0,Vn=An[0]+jn,Kn=An[1],{style:Gn}=St(this,gn);Gn.left=`${100*(Vn-Vt)/Rt}%`,Gn.top=`${100*(Kn-Qt)/Nt}%`,St(this,gn).append(rn)}_formatContents({str:Ut,dir:Rt}){const Nt=document.createElement("p");Nt.classList.add("popupContent"),Nt.dir=Rt;const Vt=Ut.split(/(?:\r\n?|\n)/);for(let Qt=0,rn=Vt.length;Qt<rn;++Qt){const fn=Vt[Qt];Nt.append(document.createTextNode(fn)),Qt<rn-1&&Nt.append(document.createElement("br"))}return Nt}forceHide(){wn(this,Mn,this.isVisible),St(this,Mn)&&(St(this,gn).hidden=!0)}maybeShow(){St(this,Mn)&&(wn(this,Mn,!1),St(this,gn).hidden=!1)}get isVisible(){return St(this,gn).hidden===!1}}zt=new WeakMap,Gt=new WeakMap,Wt=new WeakMap,qt=new WeakMap,tn=new WeakMap,ln=new WeakMap,gn=new WeakMap,yn=new WeakMap,Pn=new WeakMap,cn=new WeakMap,xn=new WeakMap,hn=new WeakMap,en=new WeakMap,Jt=new WeakMap,vn=new WeakMap,$n=new WeakMap,Mn=new WeakMap,On=new WeakSet,kr=function(Ut){Ut.altKey||Ut.shiftKey||Ut.ctrlKey||Ut.metaKey||(Ut.key==="Enter"||Ut.key==="Escape"&&St(this,hn))&&un(this,Bn,bo).call(this)},Bn=new WeakSet,bo=function(){wn(this,hn,!St(this,hn)),St(this,hn)?(un(this,Wn,Yo).call(this),St(this,gn).addEventListener("click",St(this,tn)),St(this,gn).addEventListener("keydown",St(this,Gt))):(un(this,Zn,Ko).call(this),St(this,gn).removeEventListener("click",St(this,tn)),St(this,gn).removeEventListener("keydown",St(this,Gt)))},Wn=new WeakSet,Yo=function(){St(this,en)||this.render(),this.isVisible?St(this,hn)&&St(this,gn).classList.add("focused"):(St(this,gn).hidden=!1,St(this,gn).style.zIndex=parseInt(St(this,gn).style.zIndex)+1e3)},Zn=new WeakSet,Ko=function(){St(this,gn).classList.remove("focused"),!(St(this,hn)||!this.isVisible)&&(St(this,gn).hidden=!0,St(this,gn).style.zIndex=parseInt(St(this,gn).style.zIndex)-1e3)};class xt extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0}),this.textContent=Ut.data.textContent,this.textPosition=Ut.data.textPosition,this.annotationEditorType=s.AnnotationEditorType.FREETEXT}render(){if(this.container.classList.add("freeTextAnnotation"),this.textContent){const Ut=document.createElement("div");Ut.classList.add("annotationTextContent"),Ut.setAttribute("role","comment");for(const Rt of this.textContent){const Nt=document.createElement("span");Nt.textContent=Rt,Ut.append(Nt)}this.container.append(Ut)}return!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this._editOnDoubleClick(),this.container}}i.FreeTextAnnotationElement=xt;class vt extends ct{constructor(Rt){super(Rt,{isRenderable:!0,ignoreBorder:!0});Yt(this,dn,null)}render(){this.container.classList.add("lineAnnotation");const Rt=this.data,{width:Nt,height:Vt}=st(Rt.rect),Qt=this.svgFactory.create(Nt,Vt,!0),rn=wn(this,dn,this.svgFactory.createElement("svg:line"));return rn.setAttribute("x1",Rt.rect[2]-Rt.lineCoordinates[0]),rn.setAttribute("y1",Rt.rect[3]-Rt.lineCoordinates[1]),rn.setAttribute("x2",Rt.rect[2]-Rt.lineCoordinates[2]),rn.setAttribute("y2",Rt.rect[3]-Rt.lineCoordinates[3]),rn.setAttribute("stroke-width",Rt.borderStyle.width||1),rn.setAttribute("stroke","transparent"),rn.setAttribute("fill","transparent"),Qt.append(rn),this.container.append(Qt),!Rt.popupRef&&this.hasPopupData&&this._createPopup(),this.container}getElementsToTriggerPopup(){return St(this,dn)}addHighlightArea(){this.container.classList.add("highlightArea")}}dn=new WeakMap;class Lt extends ct{constructor(Rt){super(Rt,{isRenderable:!0,ignoreBorder:!0});Yt(this,an,null)}render(){this.container.classList.add("squareAnnotation");const Rt=this.data,{width:Nt,height:Vt}=st(Rt.rect),Qt=this.svgFactory.create(Nt,Vt,!0),rn=Rt.borderStyle.width,fn=wn(this,an,this.svgFactory.createElement("svg:rect"));return fn.setAttribute("x",rn/2),fn.setAttribute("y",rn/2),fn.setAttribute("width",Nt-rn),fn.setAttribute("height",Vt-rn),fn.setAttribute("stroke-width",rn||1),fn.setAttribute("stroke","transparent"),fn.setAttribute("fill","transparent"),Qt.append(fn),this.container.append(Qt),!Rt.popupRef&&this.hasPopupData&&this._createPopup(),this.container}getElementsToTriggerPopup(){return St(this,an)}addHighlightArea(){this.container.classList.add("highlightArea")}}an=new WeakMap;class $t extends ct{constructor(Rt){super(Rt,{isRenderable:!0,ignoreBorder:!0});Yt(this,In,null)}render(){this.container.classList.add("circleAnnotation");const Rt=this.data,{width:Nt,height:Vt}=st(Rt.rect),Qt=this.svgFactory.create(Nt,Vt,!0),rn=Rt.borderStyle.width,fn=wn(this,In,this.svgFactory.createElement("svg:ellipse"));return fn.setAttribute("cx",Nt/2),fn.setAttribute("cy",Vt/2),fn.setAttribute("rx",Nt/2-rn/2),fn.setAttribute("ry",Vt/2-rn/2),fn.setAttribute("stroke-width",rn||1),fn.setAttribute("stroke","transparent"),fn.setAttribute("fill","transparent"),Qt.append(fn),this.container.append(Qt),!Rt.popupRef&&this.hasPopupData&&this._createPopup(),this.container}getElementsToTriggerPopup(){return St(this,In)}addHighlightArea(){this.container.classList.add("highlightArea")}}In=new WeakMap;class Tt extends ct{constructor(Rt){super(Rt,{isRenderable:!0,ignoreBorder:!0});Yt(this,Dn,null);this.containerClassName="polylineAnnotation",this.svgElementName="svg:polyline"}render(){this.container.classList.add(this.containerClassName);const Rt=this.data,{width:Nt,height:Vt}=st(Rt.rect),Qt=this.svgFactory.create(Nt,Vt,!0);let rn=[];for(const Ln of Rt.vertices){const zn=Ln.x-Rt.rect[0],on=Rt.rect[3]-Ln.y;rn.push(zn+","+on)}rn=rn.join(" ");const fn=wn(this,Dn,this.svgFactory.createElement(this.svgElementName));return fn.setAttribute("points",rn),fn.setAttribute("stroke-width",Rt.borderStyle.width||1),fn.setAttribute("stroke","transparent"),fn.setAttribute("fill","transparent"),Qt.append(fn),this.container.append(Qt),!Rt.popupRef&&this.hasPopupData&&this._createPopup(),this.container}getElementsToTriggerPopup(){return St(this,Dn)}addHighlightArea(){this.container.classList.add("highlightArea")}}Dn=new WeakMap;class Et extends Tt{constructor(Ut){super(Ut),this.containerClassName="polygonAnnotation",this.svgElementName="svg:polygon"}}class Dt extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0})}render(){return this.container.classList.add("caretAnnotation"),!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container}}class It extends ct{constructor(Rt){super(Rt,{isRenderable:!0,ignoreBorder:!0});Yt(this,Xn,[]);this.containerClassName="inkAnnotation",this.svgElementName="svg:polyline",this.annotationEditorType=s.AnnotationEditorType.INK}render(){this.container.classList.add(this.containerClassName);const Rt=this.data,{width:Nt,height:Vt}=st(Rt.rect),Qt=this.svgFactory.create(Nt,Vt,!0);for(const rn of Rt.inkLists){let fn=[];for(const zn of rn){const on=zn.x-Rt.rect[0],mn=Rt.rect[3]-zn.y;fn.push(`${on},${mn}`)}fn=fn.join(" ");const Ln=this.svgFactory.createElement(this.svgElementName);St(this,Xn).push(Ln),Ln.setAttribute("points",fn),Ln.setAttribute("stroke-width",Rt.borderStyle.width||1),Ln.setAttribute("stroke","transparent"),Ln.setAttribute("fill","transparent"),!Rt.popupRef&&this.hasPopupData&&this._createPopup(),Qt.append(Ln)}return this.container.append(Qt),this.container}getElementsToTriggerPopup(){return St(this,Xn)}addHighlightArea(){this.container.classList.add("highlightArea")}}Xn=new WeakMap,i.InkAnnotationElement=It;class Ct extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0,createQuadrilaterals:!0})}render(){return!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container.classList.add("highlightAnnotation"),this.container}}class jt extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0,createQuadrilaterals:!0})}render(){return!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container.classList.add("underlineAnnotation"),this.container}}class Zt extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0,createQuadrilaterals:!0})}render(){return!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container.classList.add("squigglyAnnotation"),this.container}}class Xt extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0,createQuadrilaterals:!0})}render(){return!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container.classList.add("strikeoutAnnotation"),this.container}}class sn extends ct{constructor(Ut){super(Ut,{isRenderable:!0,ignoreBorder:!0})}render(){return this.container.classList.add("stampAnnotation"),!this.data.popupRef&&this.hasPopupData&&this._createPopup(),this.container}}i.StampAnnotationElement=sn;class Ft extends ct{constructor(Rt){var Qt;super(Rt,{isRenderable:!0});Yt(this,pn);Yt(this,Yn,null);const{filename:Nt,content:Vt}=this.data.file;this.filename=(0,$.getFilenameFromUrl)(Nt,!0),this.content=Vt,(Qt=this.linkService.eventBus)==null||Qt.dispatch("fileattachmentannotation",{source:this,filename:Nt,content:Vt})}render(){this.container.classList.add("fileAttachmentAnnotation");const{container:Rt,data:Nt}=this;let Vt;Nt.hasAppearance||Nt.fillAlpha===0?Vt=document.createElement("div"):(Vt=document.createElement("img"),Vt.src=`${this.imageResourcesPath}annotation-${/paperclip/i.test(Nt.name)?"paperclip":"pushpin"}.svg`,Nt.fillAlpha&&Nt.fillAlpha<1&&(Vt.style=`filter: opacity(${Math.round(Nt.fillAlpha*100)}%);`)),Vt.addEventListener("dblclick",un(this,pn,Xo).bind(this)),wn(this,Yn,Vt);const{isMac:Qt}=s.FeatureTest.platform;return Rt.addEventListener("keydown",rn=>{rn.key==="Enter"&&(Qt?rn.metaKey:rn.ctrlKey)&&un(this,pn,Xo).call(this)}),!Nt.popupRef&&this.hasPopupData?this._createPopup():Vt.classList.add("popupTriggerArea"),Rt.append(Vt),Rt}getElementsToTriggerPopup(){return St(this,Yn)}addHighlightArea(){this.container.classList.add("highlightArea")}}Yn=new WeakMap,pn=new WeakSet,Xo=function(){var Rt;(Rt=this.downloadManager)==null||Rt.openOrDownloadData(this.container,this.content,this.filename)};class wt{constructor({div:Ut,accessibilityManager:Rt,annotationCanvasMap:Nt,l10n:Vt,page:Qt,viewport:rn}){Yt(this,Rn);Yt(this,Tn);Yt(this,Kt,null);Yt(this,nn,null);Yt(this,kn,new Map);this.div=Ut,wn(this,Kt,Rt),wn(this,nn,Nt),this.l10n=Vt,this.page=Qt,this.viewport=rn,this.zIndex=0,this.l10n||(this.l10n=et.NullL10n)}async render(Ut){const{annotations:Rt}=Ut,Nt=this.div;(0,$.setLayerDimensions)(Nt,this.viewport);const Vt=new Map,Qt={data:null,layer:Nt,linkService:Ut.linkService,downloadManager:Ut.downloadManager,imageResourcesPath:Ut.imageResourcesPath||"",renderForms:Ut.renderForms!==!1,svgFactory:new $.DOMSVGFactory,annotationStorage:Ut.annotationStorage||new j.AnnotationStorage,enableScripting:Ut.enableScripting===!0,hasJSActions:Ut.hasJSActions,fieldObjects:Ut.fieldObjects,parent:this,elements:null};for(const rn of Rt){if(rn.noHTML)continue;const fn=rn.annotationType===s.AnnotationType.POPUP;if(fn){const on=Vt.get(rn.id);if(!on)continue;Qt.elements=on}else{const{width:on,height:mn}=st(rn.rect);if(on<=0||mn<=0)continue}Qt.data=rn;const Ln=lt.create(Qt);if(!Ln.isRenderable)continue;if(!fn&&rn.popupRef){const on=Vt.get(rn.popupRef);on?on.push(Ln):Vt.set(rn.popupRef,[Ln])}Ln.annotationEditorType>0&&St(this,kn).set(Ln.data.id,Ln);const zn=Ln.render();rn.hidden&&(zn.style.visibility="hidden"),un(this,Rn,Sr).call(this,zn,rn.id)}un(this,Tn,Jo).call(this),await this.l10n.translate(Nt)}update({viewport:Ut}){const Rt=this.div;this.viewport=Ut,(0,$.setLayerDimensions)(Rt,{rotation:Ut.rotation}),un(this,Tn,Jo).call(this),Rt.hidden=!1}getEditableAnnotations(){return Array.from(St(this,kn).values())}getEditableAnnotation(Ut){return St(this,kn).get(Ut)}}Kt=new WeakMap,nn=new WeakMap,kn=new WeakMap,Rn=new WeakSet,Sr=function(Ut,Rt){var Vt;const Nt=Ut.firstChild||Ut;Nt.id=`${s.AnnotationPrefix}${Rt}`,this.div.append(Ut),(Vt=St(this,Kt))==null||Vt.moveElementInDOM(this.div,Ut,Nt,!1)},Tn=new WeakSet,Jo=function(){if(!St(this,nn))return;const Ut=this.div;for(const[Rt,Nt]of St(this,nn)){const Vt=Ut.querySelector(`[data-annotation-id="${Rt}"]`);if(!Vt)continue;const{firstChild:Qt}=Vt;Qt?Qt.nodeName==="CANVAS"?Qt.replaceWith(Nt):Qt.before(Nt):Vt.append(Nt)}St(this,nn).clear()},i.AnnotationLayer=wt},(a,i)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.ColorConverters=void 0;function o(j){return Math.floor(Math.max(0,Math.min(1,j))*255).toString(16).padStart(2,"0")}function s(j){return Math.max(0,Math.min(255,255*j))}class ${static CMYK_G([_e,et,tt,nt]){return["G",1-Math.min(1,.3*_e+.59*tt+.11*et+nt)]}static G_CMYK([_e]){return["CMYK",0,0,0,1-_e]}static G_RGB([_e]){return["RGB",_e,_e,_e]}static G_rgb([_e]){return _e=s(_e),[_e,_e,_e]}static G_HTML([_e]){const et=o(_e);return`#${et}${et}${et}`}static RGB_G([_e,et,tt]){return["G",.3*_e+.59*et+.11*tt]}static RGB_rgb(_e){return _e.map(s)}static RGB_HTML(_e){return`#${_e.map(o).join("")}`}static T_HTML(){return"#00000000"}static T_rgb(){return[null]}static CMYK_RGB([_e,et,tt,nt]){return["RGB",1-Math.min(1,_e+nt),1-Math.min(1,tt+nt),1-Math.min(1,et+nt)]}static CMYK_rgb([_e,et,tt,nt]){return[s(1-Math.min(1,_e+nt)),s(1-Math.min(1,tt+nt)),s(1-Math.min(1,et+nt))]}static CMYK_HTML(_e){const et=this.CMYK_RGB(_e).slice(1);return this.RGB_HTML(et)}static RGB_CMYK([_e,et,tt]){const nt=1-_e,at=1-et,it=1-tt,st=Math.min(nt,at,it);return["CMYK",nt,at,it,st]}}i.ColorConverters=$},(a,i)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.NullL10n=void 0,i.getL10nFallback=s;const o={of_pages:"of {{pagesCount}}",page_of_pages:"({{pageNumber}} of {{pagesCount}})",document_properties_kb:"{{size_kb}} KB ({{size_b}} bytes)",document_properties_mb:"{{size_mb}} MB ({{size_b}} bytes)",document_properties_date_string:"{{date}}, {{time}}",document_properties_page_size_unit_inches:"in",document_properties_page_size_unit_millimeters:"mm",document_properties_page_size_orientation_portrait:"portrait",document_properties_page_size_orientation_landscape:"landscape",document_properties_page_size_name_a3:"A3",document_properties_page_size_name_a4:"A4",document_properties_page_size_name_letter:"Letter",document_properties_page_size_name_legal:"Legal",document_properties_page_size_dimension_string:"{{width}} × {{height}} {{unit}} ({{orientation}})",document_properties_page_size_dimension_name_string:"{{width}} × {{height}} {{unit}} ({{name}}, {{orientation}})",document_properties_linearized_yes:"Yes",document_properties_linearized_no:"No",additional_layers:"Additional Layers",page_landmark:"Page {{page}}",thumb_page_title:"Page {{page}}",thumb_page_canvas:"Thumbnail of Page {{page}}",find_reached_top:"Reached top of document, continued from bottom",find_reached_bottom:"Reached end of document, continued from top","find_match_count[one]":"{{current}} of {{total}} match","find_match_count[other]":"{{current}} of {{total}} matches","find_match_count_limit[one]":"More than {{limit}} match","find_match_count_limit[other]":"More than {{limit}} matches",find_not_found:"Phrase not found",page_scale_width:"Page Width",page_scale_fit:"Page Fit",page_scale_auto:"Automatic Zoom",page_scale_actual:"Actual Size",page_scale_percent:"{{scale}}%",loading_error:"An error occurred while loading the PDF.",invalid_file_error:"Invalid or corrupted PDF file.",missing_file_error:"Missing PDF file.",unexpected_response_error:"Unexpected server response.",rendering_error:"An error occurred while rendering the page.",annotation_date_string:"{{date}}, {{time}}",printing_not_supported:"Warning: Printing is not fully supported by this browser.",printing_not_ready:"Warning: The PDF is not fully loaded for printing.",web_fonts_disabled:"Web fonts are disabled: unable to use embedded PDF fonts.",free_text2_default_content:"Start typing…",editor_free_text2_aria_label:"Text Editor",editor_ink2_aria_label:"Draw Editor",editor_ink_canvas_aria_label:"User-created image",editor_alt_text_button_label:"Alt text",editor_alt_text_edit_button_label:"Edit alt text",editor_alt_text_decorative_tooltip:"Marked as decorative"};o.print_progress_percent="{{progress}}%";function s(_e,et){switch(_e){case"find_match_count":_e=`find_match_count[${et.total===1?"one":"other"}]`;break;case"find_match_count_limit":_e=`find_match_count_limit[${et.limit===1?"one":"other"}]`;break}return o[_e]||""}function $(_e,et){return et?_e.replaceAll(/\{\{\s*(\w+)\s*\}\}/g,(tt,nt)=>nt in et?et[nt]:"{{"+nt+"}}"):_e}const j={async getLanguage(){return"en-us"},async getDirection(){return"ltr"},async get(_e,et=null,tt=s(_e,et)){return $(tt,et)},async translate(_e){}};i.NullL10n=j},(a,i,o)=>{Object.defineProperty(i,"__esModule",{value:!0}),i.XfaLayer=void 0;var s=o(25);class ${static setupStorage(_e,et,tt,nt,at){const it=nt.getValue(et,{value:null});switch(tt.name){case"textarea":if(it.value!==null&&(_e.textContent=it.value),at==="print")break;_e.addEventListener("input",st=>{nt.setValue(et,{value:st.target.value})});break;case"input":if(tt.attributes.type==="radio"||tt.attributes.type==="checkbox"){if(it.value===tt.attributes.xfaOn?_e.setAttribute("checked",!0):it.value===tt.attributes.xfaOff&&_e.removeAttribute("checked"),at==="print")break;_e.addEventListener("change",st=>{nt.setValue(et,{value:st.target.checked?st.target.getAttribute("xfaOn"):st.target.getAttribute("xfaOff")})})}else{if(it.value!==null&&_e.setAttribute("value",it.value),at==="print")break;_e.addEventListener("input",st=>{nt.setValue(et,{value:st.target.value})})}break;case"select":if(it.value!==null){_e.setAttribute("value",it.value);for(const st of tt.children)st.attributes.value===it.value?st.attributes.selected=!0:st.attributes.hasOwnProperty("selected")&&delete st.attributes.selected}_e.addEventListener("input",st=>{const lt=st.target.options,ct=lt.selectedIndex===-1?"":lt[lt.selectedIndex].value;nt.setValue(et,{value:ct})});break}}static setAttributes({html:_e,element:et,storage:tt=null,intent:nt,linkService:at}){const{attributes:it}=et,st=_e instanceof HTMLAnchorElement;it.type==="radio"&&(it.name=`${it.name}-${nt}`);for(const[lt,ct]of Object.entries(it))if(ct!=null)switch(lt){case"class":ct.length&&_e.setAttribute(lt,ct.join(" "));break;case"dataId":break;case"id":_e.setAttribute("data-element-id",ct);break;case"style":Object.assign(_e.style,ct);break;case"textContent":_e.textContent=ct;break;default:(!st||lt!=="href"&&lt!=="newWindow")&&_e.setAttribute(lt,ct)}st&&at.addLinkAttributes(_e,it.href,it.newWindow),tt&&it.dataId&&this.setupStorage(_e,it.dataId,et,tt)}static render(_e){var rt;const et=_e.annotationStorage,tt=_e.linkService,nt=_e.xfaHtml,at=_e.intent||"display",it=document.createElement(nt.name);nt.attributes&&this.setAttributes({html:it,element:nt,intent:at,linkService:tt});const st=[[nt,-1,it]],lt=_e.div;if(lt.append(it),_e.viewport){const ut=`matrix(${_e.viewport.transform.join(",")})`;lt.style.transform=ut}at!=="richText"&&lt.setAttribute("class","xfaLayer xfaFont");const ct=[];for(;st.length>0;){const[ut,ot,dt]=st.at(-1);if(ot+1===ut.children.length){st.pop();continue}const pt=ut.children[++st.at(-1)[1]];if(pt===null)continue;const{name:mt}=pt;if(mt==="#text"){const ht=document.createTextNode(pt.value);ct.push(ht),dt.append(ht);continue}const ft=(rt=pt==null?void 0:pt.attributes)!=null&&rt.xmlns?document.createElementNS(pt.attributes.xmlns,mt):document.createElement(mt);if(dt.append(ft),pt.attributes&&this.setAttributes({html:ft,element:pt,storage:et,intent:at,linkService:tt}),pt.children&&pt.children.length>0)st.push([pt,-1,ft]);else if(pt.value){const ht=document.createTextNode(pt.value);s.XfaText.shouldBuildText(mt)&&ct.push(ht),ft.append(ht)}}for(const ut of lt.querySelectorAll(".xfaNonInteractive input, .xfaNonInteractive textarea"))ut.setAttribute("readOnly",!0);return{textDivs:ct}}static update(_e){const et=`matrix(${_e.viewport.transform.join(",")})`;_e.div.style.transform=et,_e.div.hidden=!1}}i.XfaLayer=$},(a,i,o)=>{var nt,at,it,st,lt,ct,rt,ut,ot,dt,pt,mt,ft,ht,yt,Cr,gt,Er,vt,Rr,$t,Pr,Et,Qo,It,Lr,jt,er,Xt,Mr,Ft,_r,kt,Dr,Pt,$r,Ot,Ir,zt,xa,Wt,tr,tn,xo,gn,wo,Pn,to,xn,nr,en,To,vn,jr,Mn,ar,En,Or,Hn,Fr,_n,ir,bn,Ao,an,no;Object.defineProperty(i,"__esModule",{value:!0}),i.InkEditor=void 0;var s=o(1),$=o(4),j=o(29),_e=o(6),et=o(5);const Dn=class Dn extends $.AnnotationEditor{constructor(pn){super({...pn,name:"inkEditor"});Yt(this,yt);Yt(this,gt);Yt(this,vt);Yt(this,$t);Yt(this,Et);Yt(this,It);Yt(this,jt);Yt(this,Xt);Yt(this,Ft);Yt(this,kt);Yt(this,Pt);Yt(this,Ot);Yt(this,zt);Yt(this,Wt);Yt(this,tn);Yt(this,gn);Yt(this,Pn);Yt(this,xn);Yt(this,en);Yt(this,Hn);Yt(this,_n);Yt(this,bn);Yt(this,an);Yt(this,nt,0);Yt(this,at,0);Yt(this,it,this.canvasPointermove.bind(this));Yt(this,st,this.canvasPointerleave.bind(this));Yt(this,lt,this.canvasPointerup.bind(this));Yt(this,ct,this.canvasPointerdown.bind(this));Yt(this,rt,new Path2D);Yt(this,ut,!1);Yt(this,ot,!1);Yt(this,dt,!1);Yt(this,pt,null);Yt(this,mt,0);Yt(this,ft,0);Yt(this,ht,null);this.color=pn.color||null,this.thickness=pn.thickness||null,this.opacity=pn.opacity||null,this.paths=[],this.bezierPath2D=[],this.allRawPaths=[],this.currentPath=[],this.scaleFactor=1,this.translationX=this.translationY=0,this.x=0,this.y=0,this._willKeepAspectRatio=!0}static initialize(pn){$.AnnotationEditor.initialize(pn,{strings:["editor_ink_canvas_aria_label","editor_ink2_aria_label"]})}static updateDefaultParams(pn,Ht){switch(pn){case s.AnnotationEditorParamsType.INK_THICKNESS:Dn._defaultThickness=Ht;break;case s.AnnotationEditorParamsType.INK_COLOR:Dn._defaultColor=Ht;break;case s.AnnotationEditorParamsType.INK_OPACITY:Dn._defaultOpacity=Ht/100;break}}updateParams(pn,Ht){switch(pn){case s.AnnotationEditorParamsType.INK_THICKNESS:un(this,yt,Cr).call(this,Ht);break;case s.AnnotationEditorParamsType.INK_COLOR:un(this,gt,Er).call(this,Ht);break;case s.AnnotationEditorParamsType.INK_OPACITY:un(this,vt,Rr).call(this,Ht);break}}static get defaultPropertiesToUpdate(){return[[s.AnnotationEditorParamsType.INK_THICKNESS,Dn._defaultThickness],[s.AnnotationEditorParamsType.INK_COLOR,Dn._defaultColor||$.AnnotationEditor._defaultLineColor],[s.AnnotationEditorParamsType.INK_OPACITY,Math.round(Dn._defaultOpacity*100)]]}get propertiesToUpdate(){return[[s.AnnotationEditorParamsType.INK_THICKNESS,this.thickness||Dn._defaultThickness],[s.AnnotationEditorParamsType.INK_COLOR,this.color||Dn._defaultColor||$.AnnotationEditor._defaultLineColor],[s.AnnotationEditorParamsType.INK_OPACITY,Math.round(100*(this.opacity??Dn._defaultOpacity))]]}rebuild(){this.parent&&(super.rebuild(),this.div!==null&&(this.canvas||(un(this,tn,xo).call(this),un(this,gn,wo).call(this)),this.isAttachedToDOM||(this.parent.add(this),un(this,Pn,to).call(this)),un(this,an,no).call(this)))}remove(){this.canvas!==null&&(this.isEmpty()||this.commit(),this.canvas.width=this.canvas.height=0,this.canvas.remove(),this.canvas=null,St(this,pt).disconnect(),wn(this,pt,null),super.remove())}setParent(pn){!this.parent&&pn?this._uiManager.removeShouldRescale(this):this.parent&&pn===null&&this._uiManager.addShouldRescale(this),super.setParent(pn)}onScaleChanging(){const[pn,Ht]=this.parentDimensions,Kt=this.width*pn,nn=this.height*Ht;this.setDimensions(Kt,nn)}enableEditMode(){St(this,ut)||this.canvas===null||(super.enableEditMode(),this._isDraggable=!1,this.canvas.addEventListener("pointerdown",St(this,ct)))}disableEditMode(){!this.isInEditMode()||this.canvas===null||(super.disableEditMode(),this._isDraggable=!this.isEmpty(),this.div.classList.remove("editing"),this.canvas.removeEventListener("pointerdown",St(this,ct)))}onceAdded(){this._isDraggable=!this.isEmpty()}isEmpty(){return this.paths.length===0||this.paths.length===1&&this.paths[0].length===0}commit(){St(this,ut)||(super.commit(),this.isEditing=!1,this.disableEditMode(),this.setInForeground(),wn(this,ut,!0),this.div.classList.add("disabled"),un(this,an,no).call(this,!0),this.makeResizable(),this.parent.addInkEditorIfNeeded(!0),this.moveInDOM(),this.div.focus({preventScroll:!0}))}focusin(pn){this._focusEventsAllowed&&(super.focusin(pn),this.enableEditMode())}canvasPointerdown(pn){pn.button!==0||!this.isInEditMode()||St(this,ut)||(this.setInForeground(),pn.preventDefault(),pn.type!=="mouse"&&this.div.focus(),un(this,It,Lr).call(this,pn.offsetX,pn.offsetY))}canvasPointermove(pn){pn.preventDefault(),un(this,jt,er).call(this,pn.offsetX,pn.offsetY)}canvasPointerup(pn){pn.preventDefault(),un(this,Wt,tr).call(this,pn)}canvasPointerleave(pn){un(this,Wt,tr).call(this,pn)}get isResizable(){return!this.isEmpty()&&St(this,ut)}render(){if(this.div)return this.div;let pn,Ht;this.width&&(pn=this.x,Ht=this.y),super.render(),$.AnnotationEditor._l10nPromise.get("editor_ink2_aria_label").then(Un=>{var Tn;return(Tn=this.div)==null?void 0:Tn.setAttribute("aria-label",Un)});const[Kt,nn,kn,Rn]=un(this,$t,Pr).call(this);if(this.setAt(Kt,nn,0,0),this.setDims(kn,Rn),un(this,tn,xo).call(this),this.width){const[Un,Tn]=this.parentDimensions;this.setAspectRatio(this.width*Un,this.height*Tn),this.setAt(pn*Un,Ht*Tn,this.width*Un,this.height*Tn),wn(this,dt,!0),un(this,Pn,to).call(this),this.setDims(this.width*Un,this.height*Tn),un(this,zt,xa).call(this),this.div.classList.add("disabled")}else this.div.classList.add("editing"),this.enableEditMode();return un(this,gn,wo).call(this),this.div}setDimensions(pn,Ht){const Kt=Math.round(pn),nn=Math.round(Ht);if(St(this,mt)===Kt&&St(this,ft)===nn)return;wn(this,mt,Kt),wn(this,ft,nn),this.canvas.style.visibility="hidden";const[kn,Rn]=this.parentDimensions;this.width=pn/kn,this.height=Ht/Rn,this.fixAndSetPosition(),St(this,ut)&&un(this,xn,nr).call(this,pn,Ht),un(this,Pn,to).call(this),un(this,zt,xa).call(this),this.canvas.style.visibility="visible",this.fixDims()}static deserialize(pn,Ht,Kt){var Qt,rn,fn;if(pn instanceof j.InkAnnotationElement)return null;const nn=super.deserialize(pn,Ht,Kt);nn.thickness=pn.thickness,nn.color=s.Util.makeHexColor(...pn.color),nn.opacity=pn.opacity;const[kn,Rn]=nn.pageDimensions,Un=nn.width*kn,Tn=nn.height*Rn,Nn=nn.parentScale,Cn=pn.thickness/2;wn(nn,ut,!0),wn(nn,mt,Math.round(Un)),wn(nn,ft,Math.round(Tn));const{paths:Ut,rect:Rt,rotation:Nt}=pn;for(let{bezier:Ln}of Ut){Ln=un(Qt=Dn,En,Or).call(Qt,Ln,Rt,Nt);const zn=[];nn.paths.push(zn);let on=Nn*(Ln[0]-Cn),mn=Nn*(Ln[1]-Cn);for(let An=2,Fn=Ln.length;An<Fn;An+=6){const jn=Nn*(Ln[An]-Cn),Vn=Nn*(Ln[An+1]-Cn),Kn=Nn*(Ln[An+2]-Cn),Gn=Nn*(Ln[An+3]-Cn),qn=Nn*(Ln[An+4]-Cn),Qn=Nn*(Ln[An+5]-Cn);zn.push([[on,mn],[jn,Vn],[Kn,Gn],[qn,Qn]]),on=qn,mn=Qn}const Sn=un(this,vn,jr).call(this,zn);nn.bezierPath2D.push(Sn)}const Vt=un(rn=nn,_n,ir).call(rn);return wn(nn,at,Math.max($.AnnotationEditor.MIN_SIZE,Vt[2]-Vt[0])),wn(nn,nt,Math.max($.AnnotationEditor.MIN_SIZE,Vt[3]-Vt[1])),un(fn=nn,xn,nr).call(fn,Un,Tn),nn}serialize(){if(this.isEmpty())return null;const pn=this.getRect(0,0),Ht=$.AnnotationEditor._colorManager.convert(this.ctx.strokeStyle);return{annotationType:s.AnnotationEditorType.INK,color:Ht,thickness:this.thickness,opacity:this.opacity,paths:un(this,Hn,Fr).call(this,this.scaleFactor/this.parentScale,this.translationX,this.translationY,pn),pageIndex:this.pageIndex,rect:pn,rotation:this.rotation,structTreeParentId:this._structTreeParentId}}};nt=new WeakMap,at=new WeakMap,it=new WeakMap,st=new WeakMap,lt=new WeakMap,ct=new WeakMap,rt=new WeakMap,ut=new WeakMap,ot=new WeakMap,dt=new WeakMap,pt=new WeakMap,mt=new WeakMap,ft=new WeakMap,ht=new WeakMap,yt=new WeakSet,Cr=function(pn){const Ht=this.thickness;this.addCommands({cmd:()=>{this.thickness=pn,un(this,an,no).call(this)},undo:()=>{this.thickness=Ht,un(this,an,no).call(this)},mustExec:!0,type:s.AnnotationEditorParamsType.INK_THICKNESS,overwriteIfSameType:!0,keepUndo:!0})},gt=new WeakSet,Er=function(pn){const Ht=this.color;this.addCommands({cmd:()=>{this.color=pn,un(this,zt,xa).call(this)},undo:()=>{this.color=Ht,un(this,zt,xa).call(this)},mustExec:!0,type:s.AnnotationEditorParamsType.INK_COLOR,overwriteIfSameType:!0,keepUndo:!0})},vt=new WeakSet,Rr=function(pn){pn/=100;const Ht=this.opacity;this.addCommands({cmd:()=>{this.opacity=pn,un(this,zt,xa).call(this)},undo:()=>{this.opacity=Ht,un(this,zt,xa).call(this)},mustExec:!0,type:s.AnnotationEditorParamsType.INK_OPACITY,overwriteIfSameType:!0,keepUndo:!0})},$t=new WeakSet,Pr=function(){const{parentRotation:pn,parentDimensions:[Ht,Kt]}=this;switch(pn){case 90:return[0,Kt,Kt,Ht];case 180:return[Ht,Kt,Ht,Kt];case 270:return[Ht,0,Kt,Ht];default:return[0,0,Ht,Kt]}},Et=new WeakSet,Qo=function(){const{ctx:pn,color:Ht,opacity:Kt,thickness:nn,parentScale:kn,scaleFactor:Rn}=this;pn.lineWidth=nn*kn/Rn,pn.lineCap="round",pn.lineJoin="round",pn.miterLimit=10,pn.strokeStyle=`${Ht}${(0,et.opacityToHex)(Kt)}`},It=new WeakSet,Lr=function(pn,Ht){this.canvas.addEventListener("contextmenu",_e.noContextMenu),this.canvas.addEventListener("pointerleave",St(this,st)),this.canvas.addEventListener("pointermove",St(this,it)),this.canvas.addEventListener("pointerup",St(this,lt)),this.canvas.removeEventListener("pointerdown",St(this,ct)),this.isEditing=!0,St(this,dt)||(wn(this,dt,!0),un(this,Pn,to).call(this),this.thickness||(this.thickness=Dn._defaultThickness),this.color||(this.color=Dn._defaultColor||$.AnnotationEditor._defaultLineColor),this.opacity??(this.opacity=Dn._defaultOpacity)),this.currentPath.push([pn,Ht]),wn(this,ot,!1),un(this,Et,Qo).call(this),wn(this,ht,()=>{un(this,kt,Dr).call(this),St(this,ht)&&window.requestAnimationFrame(St(this,ht))}),window.requestAnimationFrame(St(this,ht))},jt=new WeakSet,er=function(pn,Ht){const[Kt,nn]=this.currentPath.at(-1);if(this.currentPath.length>1&&pn===Kt&&Ht===nn)return;const kn=this.currentPath;let Rn=St(this,rt);if(kn.push([pn,Ht]),wn(this,ot,!0),kn.length<=2){Rn.moveTo(...kn[0]),Rn.lineTo(pn,Ht);return}kn.length===3&&(wn(this,rt,Rn=new Path2D),Rn.moveTo(...kn[0])),un(this,Pt,$r).call(this,Rn,...kn.at(-3),...kn.at(-2),pn,Ht)},Xt=new WeakSet,Mr=function(){if(this.currentPath.length===0)return;const pn=this.currentPath.at(-1);St(this,rt).lineTo(...pn)},Ft=new WeakSet,_r=function(pn,Ht){wn(this,ht,null),pn=Math.min(Math.max(pn,0),this.canvas.width),Ht=Math.min(Math.max(Ht,0),this.canvas.height),un(this,jt,er).call(this,pn,Ht),un(this,Xt,Mr).call(this);let Kt;if(this.currentPath.length!==1)Kt=un(this,Ot,Ir).call(this);else{const Tn=[pn,Ht];Kt=[[Tn,Tn.slice(),Tn.slice(),Tn]]}const nn=St(this,rt),kn=this.currentPath;this.currentPath=[],wn(this,rt,new Path2D);const Rn=()=>{this.allRawPaths.push(kn),this.paths.push(Kt),this.bezierPath2D.push(nn),this.rebuild()},Un=()=>{this.allRawPaths.pop(),this.paths.pop(),this.bezierPath2D.pop(),this.paths.length===0?this.remove():(this.canvas||(un(this,tn,xo).call(this),un(this,gn,wo).call(this)),un(this,an,no).call(this))};this.addCommands({cmd:Rn,undo:Un,mustExec:!0})},kt=new WeakSet,Dr=function(){if(!St(this,ot))return;wn(this,ot,!1);const pn=Math.ceil(this.thickness*this.parentScale),Ht=this.currentPath.slice(-3),Kt=Ht.map(Rn=>Rn[0]),nn=Ht.map(Rn=>Rn[1]);Math.min(...Kt)-pn,Math.max(...Kt)+pn,Math.min(...nn)-pn,Math.max(...nn)+pn;const{ctx:kn}=this;kn.save(),kn.clearRect(0,0,this.canvas.width,this.canvas.height);for(const Rn of this.bezierPath2D)kn.stroke(Rn);kn.stroke(St(this,rt)),kn.restore()},Pt=new WeakSet,$r=function(pn,Ht,Kt,nn,kn,Rn,Un){const Tn=(Ht+nn)/2,Nn=(Kt+kn)/2,Cn=(nn+Rn)/2,Ut=(kn+Un)/2;pn.bezierCurveTo(Tn+2*(nn-Tn)/3,Nn+2*(kn-Nn)/3,Cn+2*(nn-Cn)/3,Ut+2*(kn-Ut)/3,Cn,Ut)},Ot=new WeakSet,Ir=function(){const pn=this.currentPath;if(pn.length<=2)return[[pn[0],pn[0],pn.at(-1),pn.at(-1)]];const Ht=[];let Kt,[nn,kn]=pn[0];for(Kt=1;Kt<pn.length-2;Kt++){const[Rt,Nt]=pn[Kt],[Vt,Qt]=pn[Kt+1],rn=(Rt+Vt)/2,fn=(Nt+Qt)/2,Ln=[nn+2*(Rt-nn)/3,kn+2*(Nt-kn)/3],zn=[rn+2*(Rt-rn)/3,fn+2*(Nt-fn)/3];Ht.push([[nn,kn],Ln,zn,[rn,fn]]),[nn,kn]=[rn,fn]}const[Rn,Un]=pn[Kt],[Tn,Nn]=pn[Kt+1],Cn=[nn+2*(Rn-nn)/3,kn+2*(Un-kn)/3],Ut=[Tn+2*(Rn-Tn)/3,Nn+2*(Un-Nn)/3];return Ht.push([[nn,kn],Cn,Ut,[Tn,Nn]]),Ht},zt=new WeakSet,xa=function(){if(this.isEmpty()){un(this,en,To).call(this);return}un(this,Et,Qo).call(this);const{canvas:pn,ctx:Ht}=this;Ht.setTransform(1,0,0,1,0,0),Ht.clearRect(0,0,pn.width,pn.height),un(this,en,To).call(this);for(const Kt of this.bezierPath2D)Ht.stroke(Kt)},Wt=new WeakSet,tr=function(pn){this.canvas.removeEventListener("pointerleave",St(this,st)),this.canvas.removeEventListener("pointermove",St(this,it)),this.canvas.removeEventListener("pointerup",St(this,lt)),this.canvas.addEventListener("pointerdown",St(this,ct)),setTimeout(()=>{this.canvas.removeEventListener("contextmenu",_e.noContextMenu)},10),un(this,Ft,_r).call(this,pn.offsetX,pn.offsetY),this.addToAnnotationStorage(),this.setInBackground()},tn=new WeakSet,xo=function(){this.canvas=document.createElement("canvas"),this.canvas.width=this.canvas.height=0,this.canvas.className="inkEditorCanvas",$.AnnotationEditor._l10nPromise.get("editor_ink_canvas_aria_label").then(pn=>{var Ht;return(Ht=this.canvas)==null?void 0:Ht.setAttribute("aria-label",pn)}),this.div.append(this.canvas),this.ctx=this.canvas.getContext("2d")},gn=new WeakSet,wo=function(){wn(this,pt,new ResizeObserver(pn=>{const Ht=pn[0].contentRect;Ht.width&&Ht.height&&this.setDimensions(Ht.width,Ht.height)})),St(this,pt).observe(this.div)},Pn=new WeakSet,to=function(){if(!St(this,dt))return;const[pn,Ht]=this.parentDimensions;this.canvas.width=Math.ceil(this.width*pn),this.canvas.height=Math.ceil(this.height*Ht),un(this,en,To).call(this)},xn=new WeakSet,nr=function(pn,Ht){const Kt=un(this,bn,Ao).call(this),nn=(pn-Kt)/St(this,at),kn=(Ht-Kt)/St(this,nt);this.scaleFactor=Math.min(nn,kn)},en=new WeakSet,To=function(){const pn=un(this,bn,Ao).call(this)/2;this.ctx.setTransform(this.scaleFactor,0,0,this.scaleFactor,this.translationX*this.scaleFactor+pn,this.translationY*this.scaleFactor+pn)},vn=new WeakSet,jr=function(pn){const Ht=new Path2D;for(let Kt=0,nn=pn.length;Kt<nn;Kt++){const[kn,Rn,Un,Tn]=pn[Kt];Kt===0&&Ht.moveTo(...kn),Ht.bezierCurveTo(Rn[0],Rn[1],Un[0],Un[1],Tn[0],Tn[1])}return Ht},Mn=new WeakSet,ar=function(pn,Ht,Kt){const[nn,kn,Rn,Un]=Ht;switch(Kt){case 0:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2)pn[Tn]+=nn,pn[Tn+1]=Un-pn[Tn+1];break;case 90:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2){const Cn=pn[Tn];pn[Tn]=pn[Tn+1]+nn,pn[Tn+1]=Cn+kn}break;case 180:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2)pn[Tn]=Rn-pn[Tn],pn[Tn+1]+=kn;break;case 270:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2){const Cn=pn[Tn];pn[Tn]=Rn-pn[Tn+1],pn[Tn+1]=Un-Cn}break;default:throw new Error("Invalid rotation")}return pn},En=new WeakSet,Or=function(pn,Ht,Kt){const[nn,kn,Rn,Un]=Ht;switch(Kt){case 0:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2)pn[Tn]-=nn,pn[Tn+1]=Un-pn[Tn+1];break;case 90:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2){const Cn=pn[Tn];pn[Tn]=pn[Tn+1]-kn,pn[Tn+1]=Cn-nn}break;case 180:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2)pn[Tn]=Rn-pn[Tn],pn[Tn+1]-=kn;break;case 270:for(let Tn=0,Nn=pn.length;Tn<Nn;Tn+=2){const Cn=pn[Tn];pn[Tn]=Un-pn[Tn+1],pn[Tn+1]=Rn-Cn}break;default:throw new Error("Invalid rotation")}return pn},Hn=new WeakSet,Fr=function(pn,Ht,Kt,nn){var Nn,Cn;const kn=[],Rn=this.thickness/2,Un=pn*Ht+Rn,Tn=pn*Kt+Rn;for(const Ut of this.paths){const Rt=[],Nt=[];for(let Vt=0,Qt=Ut.length;Vt<Qt;Vt++){const[rn,fn,Ln,zn]=Ut[Vt],on=pn*rn[0]+Un,mn=pn*rn[1]+Tn,Sn=pn*fn[0]+Un,An=pn*fn[1]+Tn,Fn=pn*Ln[0]+Un,jn=pn*Ln[1]+Tn,Vn=pn*zn[0]+Un,Kn=pn*zn[1]+Tn;Vt===0&&(Rt.push(on,mn),Nt.push(on,mn)),Rt.push(Sn,An,Fn,jn,Vn,Kn),Nt.push(Sn,An),Vt===Qt-1&&Nt.push(Vn,Kn)}kn.push({bezier:un(Nn=Dn,Mn,ar).call(Nn,Rt,nn,this.rotation),points:un(Cn=Dn,Mn,ar).call(Cn,Nt,nn,this.rotation)})}return kn},_n=new WeakSet,ir=function(){let pn=1/0,Ht=-1/0,Kt=1/0,nn=-1/0;for(const kn of this.paths)for(const[Rn,Un,Tn,Nn]of kn){const Cn=s.Util.bezierBoundingBox(...Rn,...Un,...Tn,...Nn);pn=Math.min(pn,Cn[0]),Kt=Math.min(Kt,Cn[1]),Ht=Math.max(Ht,Cn[2]),nn=Math.max(nn,Cn[3])}return[pn,Kt,Ht,nn]},bn=new WeakSet,Ao=function(){return St(this,ut)?Math.ceil(this.thickness*this.parentScale):0},an=new WeakSet,no=function(pn=!1){if(this.isEmpty())return;if(!St(this,ut)){un(this,zt,xa).call(this);return}const Ht=un(this,_n,ir).call(this),Kt=un(this,bn,Ao).call(this);wn(this,at,Math.max($.AnnotationEditor.MIN_SIZE,Ht[2]-Ht[0])),wn(this,nt,Math.max($.AnnotationEditor.MIN_SIZE,Ht[3]-Ht[1]));const nn=Math.ceil(Kt+St(this,at)*this.scaleFactor),kn=Math.ceil(Kt+St(this,nt)*this.scaleFactor),[Rn,Un]=this.parentDimensions;this.width=nn/Rn,this.height=kn/Un,this.setAspectRatio(nn,kn);const Tn=this.translationX,Nn=this.translationY;this.translationX=-Ht[0],this.translationY=-Ht[1],un(this,Pn,to).call(this),un(this,zt,xa).call(this),wn(this,mt,nn),wn(this,ft,kn),this.setDims(nn,kn);const Cn=pn?Kt/this.scaleFactor/2:0;this.translate(Tn-this.translationX-Cn,Nn-this.translationY-Cn)},Yt(Dn,vn),Yt(Dn,Mn),Yt(Dn,En),Jn(Dn,"_defaultColor",null),Jn(Dn,"_defaultOpacity",1),Jn(Dn,"_defaultThickness",1),Jn(Dn,"_type","ink");let tt=Dn;i.InkEditor=tt},(a,i,o)=>{var tt,nt,at,it,st,lt,ct,rt,ut,ot,dt,co,mt,uo,ht,ko,bt,or,xt,Nr,Lt,zr,Tt,rr,Dt,So,Ct,Br;Object.defineProperty(i,"__esModule",{value:!0}),i.StampEditor=void 0;var s=o(1),$=o(4),j=o(6),_e=o(29);const Zt=class Zt extends $.AnnotationEditor{constructor(Ft){super({...Ft,name:"stampEditor"});Yt(this,dt);Yt(this,mt);Yt(this,ht);Yt(this,bt);Yt(this,xt);Yt(this,Lt);Yt(this,Tt);Yt(this,Dt);Yt(this,Ct);Yt(this,tt,null);Yt(this,nt,null);Yt(this,at,null);Yt(this,it,null);Yt(this,st,null);Yt(this,lt,null);Yt(this,ct,null);Yt(this,rt,null);Yt(this,ut,!1);Yt(this,ot,!1);wn(this,it,Ft.bitmapUrl),wn(this,st,Ft.bitmapFile)}static initialize(Ft){$.AnnotationEditor.initialize(Ft)}static get supportedTypes(){const Ft=["apng","avif","bmp","gif","jpeg","png","svg+xml","webp","x-icon"];return(0,s.shadow)(this,"supportedTypes",Ft.map(wt=>`image/${wt}`))}static get supportedTypesStr(){return(0,s.shadow)(this,"supportedTypesStr",this.supportedTypes.join(","))}static isHandlingMimeForPasting(Ft){return this.supportedTypes.includes(Ft)}static paste(Ft,wt){wt.pasteEditor(s.AnnotationEditorType.STAMP,{bitmapFile:Ft.getAsFile()})}remove(){var Ft,wt;St(this,nt)&&(wn(this,tt,null),this._uiManager.imageManager.deleteId(St(this,nt)),(Ft=St(this,lt))==null||Ft.remove(),wn(this,lt,null),(wt=St(this,ct))==null||wt.disconnect(),wn(this,ct,null)),super.remove()}rebuild(){if(!this.parent){St(this,nt)&&un(this,ht,ko).call(this);return}super.rebuild(),this.div!==null&&(St(this,nt)&&un(this,ht,ko).call(this),this.isAttachedToDOM||this.parent.add(this))}onceAdded(){this._isDraggable=!0,this.div.focus()}isEmpty(){return!(St(this,at)||St(this,tt)||St(this,it)||St(this,st))}get isResizable(){return!0}render(){if(this.div)return this.div;let Ft,wt;if(this.width&&(Ft=this.x,wt=this.y),super.render(),this.div.hidden=!0,St(this,tt)?un(this,bt,or).call(this):un(this,ht,ko).call(this),this.width){const[kt,At]=this.parentDimensions;this.setAt(Ft*kt,wt*At,this.width*kt,this.height*At)}return this.div}static deserialize(Ft,wt,kt){if(Ft instanceof _e.StampAnnotationElement)return null;const At=super.deserialize(Ft,wt,kt),{rect:Pt,bitmapUrl:Mt,bitmapId:Ot,isSvg:Bt,accessibilityData:zt}=Ft;Ot&&kt.imageManager.isValidId(Ot)?wn(At,nt,Ot):wn(At,it,Mt),wn(At,ut,Bt);const[Gt,Wt]=At.pageDimensions;return At.width=(Pt[2]-Pt[0])/Gt,At.height=(Pt[3]-Pt[1])/Wt,zt&&(At.altTextData=zt),At}serialize(Ft=!1,wt=null){if(this.isEmpty())return null;const kt={annotationType:s.AnnotationEditorType.STAMP,bitmapId:St(this,nt),pageIndex:this.pageIndex,rect:this.getRect(0,0),rotation:this.rotation,isSvg:St(this,ut),structTreeParentId:this._structTreeParentId};if(Ft)return kt.bitmapUrl=un(this,Dt,So).call(this,!0),kt.accessibilityData=this.altTextData,kt;const{decorative:At,altText:Pt}=this.altTextData;if(!At&&Pt&&(kt.accessibilityData={type:"Figure",alt:Pt}),wt===null)return kt;wt.stamps||(wt.stamps=new Map);const Mt=St(this,ut)?(kt.rect[2]-kt.rect[0])*(kt.rect[3]-kt.rect[1]):null;if(!wt.stamps.has(St(this,nt)))wt.stamps.set(St(this,nt),{area:Mt,serialized:kt}),kt.bitmap=un(this,Dt,So).call(this,!1);else if(St(this,ut)){const Ot=wt.stamps.get(St(this,nt));Mt>Ot.area&&(Ot.area=Mt,Ot.serialized.bitmap.close(),Ot.serialized.bitmap=un(this,Dt,So).call(this,!1))}return kt}};tt=new WeakMap,nt=new WeakMap,at=new WeakMap,it=new WeakMap,st=new WeakMap,lt=new WeakMap,ct=new WeakMap,rt=new WeakMap,ut=new WeakMap,ot=new WeakMap,dt=new WeakSet,co=function(Ft,wt=!1){if(!Ft){this.remove();return}wn(this,tt,Ft.bitmap),wt||(wn(this,nt,Ft.id),wn(this,ut,Ft.isSvg)),un(this,bt,or).call(this)},mt=new WeakSet,uo=function(){wn(this,at,null),this._uiManager.enableWaiting(!1),St(this,lt)&&this.div.focus()},ht=new WeakSet,ko=function(){if(St(this,nt)){this._uiManager.enableWaiting(!0),this._uiManager.imageManager.getFromId(St(this,nt)).then(wt=>un(this,dt,co).call(this,wt,!0)).finally(()=>un(this,mt,uo).call(this));return}if(St(this,it)){const wt=St(this,it);wn(this,it,null),this._uiManager.enableWaiting(!0),wn(this,at,this._uiManager.imageManager.getFromUrl(wt).then(kt=>un(this,dt,co).call(this,kt)).finally(()=>un(this,mt,uo).call(this)));return}if(St(this,st)){const wt=St(this,st);wn(this,st,null),this._uiManager.enableWaiting(!0),wn(this,at,this._uiManager.imageManager.getFromFile(wt).then(kt=>un(this,dt,co).call(this,kt)).finally(()=>un(this,mt,uo).call(this)));return}const Ft=document.createElement("input");Ft.type="file",Ft.accept=Zt.supportedTypesStr,wn(this,at,new Promise(wt=>{Ft.addEventListener("change",async()=>{if(!Ft.files||Ft.files.length===0)this.remove();else{this._uiManager.enableWaiting(!0);const kt=await this._uiManager.imageManager.getFromFile(Ft.files[0]);un(this,dt,co).call(this,kt)}wt()}),Ft.addEventListener("cancel",()=>{this.remove(),wt()})}).finally(()=>un(this,mt,uo).call(this))),Ft.click()},bt=new WeakSet,or=function(){const{div:Ft}=this;let{width:wt,height:kt}=St(this,tt);const[At,Pt]=this.pageDimensions,Mt=.75;if(this.width)wt=this.width*At,kt=this.height*Pt;else if(wt>Mt*At||kt>Mt*Pt){const Gt=Math.min(Mt*At/wt,Mt*Pt/kt);wt*=Gt,kt*=Gt}const[Ot,Bt]=this.parentDimensions;this.setDims(wt*Ot/At,kt*Bt/Pt),this._uiManager.enableWaiting(!1);const zt=wn(this,lt,document.createElement("canvas"));Ft.append(zt),Ft.hidden=!1,un(this,Tt,rr).call(this,wt,kt),un(this,Ct,Br).call(this),St(this,ot)||(this.parent.addUndoableEditor(this),wn(this,ot,!0)),this._uiManager._eventBus.dispatch("reporttelemetry",{source:this,details:{type:"editing",subtype:this.editorType,data:{action:"inserted_image"}}}),this.addAltTextButton()},xt=new WeakSet,Nr=function(Ft,wt){var Mt;const[kt,At]=this.parentDimensions;this.width=Ft/kt,this.height=wt/At,this.setDims(Ft,wt),(Mt=this._initialOptions)!=null&&Mt.isCentered?this.center():this.fixAndSetPosition(),this._initialOptions=null,St(this,rt)!==null&&clearTimeout(St(this,rt)),wn(this,rt,setTimeout(()=>{wn(this,rt,null),un(this,Tt,rr).call(this,Ft,wt)},200))},Lt=new WeakSet,zr=function(Ft,wt){const{width:kt,height:At}=St(this,tt);let Pt=kt,Mt=At,Ot=St(this,tt);for(;Pt>2*Ft||Mt>2*wt;){const Bt=Pt,zt=Mt;Pt>2*Ft&&(Pt=Pt>=16384?Math.floor(Pt/2)-1:Math.ceil(Pt/2)),Mt>2*wt&&(Mt=Mt>=16384?Math.floor(Mt/2)-1:Math.ceil(Mt/2));const Gt=new OffscreenCanvas(Pt,Mt);Gt.getContext("2d").drawImage(Ot,0,0,Bt,zt,0,0,Pt,Mt),Ot=Gt.transferToImageBitmap()}return Ot},Tt=new WeakSet,rr=function(Ft,wt){Ft=Math.ceil(Ft),wt=Math.ceil(wt);const kt=St(this,lt);if(!kt||kt.width===Ft&&kt.height===wt)return;kt.width=Ft,kt.height=wt;const At=St(this,ut)?St(this,tt):un(this,Lt,zr).call(this,Ft,wt),Pt=kt.getContext("2d");Pt.filter=this._uiManager.hcmFilter,Pt.drawImage(At,0,0,At.width,At.height,0,0,Ft,wt)},Dt=new WeakSet,So=function(Ft){if(Ft){if(St(this,ut)){const At=this._uiManager.imageManager.getSvgUrl(St(this,nt));if(At)return At}const wt=document.createElement("canvas");return{width:wt.width,height:wt.height}=St(this,tt),wt.getContext("2d").drawImage(St(this,tt),0,0),wt.toDataURL()}if(St(this,ut)){const[wt,kt]=this.pageDimensions,At=Math.round(this.width*wt*j.PixelsPerInch.PDF_TO_CSS_UNITS),Pt=Math.round(this.height*kt*j.PixelsPerInch.PDF_TO_CSS_UNITS),Mt=new OffscreenCanvas(At,Pt);return Mt.getContext("2d").drawImage(St(this,tt),0,0,St(this,tt).width,St(this,tt).height,0,0,At,Pt),Mt.transferToImageBitmap()}return structuredClone(St(this,tt))},Ct=new WeakSet,Br=function(){wn(this,ct,new ResizeObserver(Ft=>{const wt=Ft[0].contentRect;wt.width&&wt.height&&un(this,xt,Nr).call(this,wt.width,wt.height)})),St(this,ct).observe(this.div)},Jn(Zt,"_type","stamp");let et=Zt;i.StampEditor=et}],__webpack_module_cache__={};function __w_pdfjs_require__(a){var i=__webpack_module_cache__[a];if(i!==void 0)return i.exports;var o=__webpack_module_cache__[a]={exports:{}};return __webpack_modules__[a](o,o.exports,__w_pdfjs_require__),o.exports}var __webpack_exports__={};return(()=>{var a=__webpack_exports__;Object.defineProperty(a,"__esModule",{value:!0}),Object.defineProperty(a,"AbortException",{enumerable:!0,get:function(){return i.AbortException}}),Object.defineProperty(a,"AnnotationEditorLayer",{enumerable:!0,get:function(){return j.AnnotationEditorLayer}}),Object.defineProperty(a,"AnnotationEditorParamsType",{enumerable:!0,get:function(){return i.AnnotationEditorParamsType}}),Object.defineProperty(a,"AnnotationEditorType",{enumerable:!0,get:function(){return i.AnnotationEditorType}}),Object.defineProperty(a,"AnnotationEditorUIManager",{enumerable:!0,get:function(){return _e.AnnotationEditorUIManager}}),Object.defineProperty(a,"AnnotationLayer",{enumerable:!0,get:function(){return et.AnnotationLayer}}),Object.defineProperty(a,"AnnotationMode",{enumerable:!0,get:function(){return i.AnnotationMode}}),Object.defineProperty(a,"CMapCompressionType",{enumerable:!0,get:function(){return i.CMapCompressionType}}),Object.defineProperty(a,"DOMSVGFactory",{enumerable:!0,get:function(){return s.DOMSVGFactory}}),Object.defineProperty(a,"FeatureTest",{enumerable:!0,get:function(){return i.FeatureTest}}),Object.defineProperty(a,"GlobalWorkerOptions",{enumerable:!0,get:function(){return tt.GlobalWorkerOptions}}),Object.defineProperty(a,"ImageKind",{enumerable:!0,get:function(){return i.ImageKind}}),Object.defineProperty(a,"InvalidPDFException",{enumerable:!0,get:function(){return i.InvalidPDFException}}),Object.defineProperty(a,"MissingPDFException",{enumerable:!0,get:function(){return i.MissingPDFException}}),Object.defineProperty(a,"OPS",{enumerable:!0,get:function(){return i.OPS}}),Object.defineProperty(a,"PDFDataRangeTransport",{enumerable:!0,get:function(){return o.PDFDataRangeTransport}}),Object.defineProperty(a,"PDFDateString",{enumerable:!0,get:function(){return s.PDFDateString}}),Object.defineProperty(a,"PDFWorker",{enumerable:!0,get:function(){return o.PDFWorker}}),Object.defineProperty(a,"PasswordResponses",{enumerable:!0,get:function(){return i.PasswordResponses}}),Object.defineProperty(a,"PermissionFlag",{enumerable:!0,get:function(){return i.PermissionFlag}}),Object.defineProperty(a,"PixelsPerInch",{enumerable:!0,get:function(){return s.PixelsPerInch}}),Object.defineProperty(a,"PromiseCapability",{enumerable:!0,get:function(){return i.PromiseCapability}}),Object.defineProperty(a,"RenderingCancelledException",{enumerable:!0,get:function(){return s.RenderingCancelledException}}),Object.defineProperty(a,"SVGGraphics",{enumerable:!0,get:function(){return o.SVGGraphics}}),Object.defineProperty(a,"UnexpectedResponseException",{enumerable:!0,get:function(){return i.UnexpectedResponseException}}),Object.defineProperty(a,"Util",{enumerable:!0,get:function(){return i.Util}}),Object.defineProperty(a,"VerbosityLevel",{enumerable:!0,get:function(){return i.VerbosityLevel}}),Object.defineProperty(a,"XfaLayer",{enumerable:!0,get:function(){return nt.XfaLayer}}),Object.defineProperty(a,"build",{enumerable:!0,get:function(){return o.build}}),Object.defineProperty(a,"createValidAbsoluteUrl",{enumerable:!0,get:function(){return i.createValidAbsoluteUrl}}),Object.defineProperty(a,"getDocument",{enumerable:!0,get:function(){return o.getDocument}}),Object.defineProperty(a,"getFilenameFromUrl",{enumerable:!0,get:function(){return s.getFilenameFromUrl}}),Object.defineProperty(a,"getPdfFilenameFromUrl",{enumerable:!0,get:function(){return s.getPdfFilenameFromUrl}}),Object.defineProperty(a,"getXfaPageViewport",{enumerable:!0,get:function(){return s.getXfaPageViewport}}),Object.defineProperty(a,"isDataScheme",{enumerable:!0,get:function(){return s.isDataScheme}}),Object.defineProperty(a,"isPdfFile",{enumerable:!0,get:function(){return s.isPdfFile}}),Object.defineProperty(a,"loadScript",{enumerable:!0,get:function(){return s.loadScript}}),Object.defineProperty(a,"noContextMenu",{enumerable:!0,get:function(){return s.noContextMenu}}),Object.defineProperty(a,"normalizeUnicode",{enumerable:!0,get:function(){return i.normalizeUnicode}}),Object.defineProperty(a,"renderTextLayer",{enumerable:!0,get:function(){return $.renderTextLayer}}),Object.defineProperty(a,"setLayerDimensions",{enumerable:!0,get:function(){return s.setLayerDimensions}}),Object.defineProperty(a,"shadow",{enumerable:!0,get:function(){return i.shadow}}),Object.defineProperty(a,"updateTextLayer",{enumerable:!0,get:function(){return $.updateTextLayer}}),Object.defineProperty(a,"version",{enumerable:!0,get:function(){return o.version}});var i=__w_pdfjs_require__(1),o=__w_pdfjs_require__(2),s=__w_pdfjs_require__(6),$=__w_pdfjs_require__(26),j=__w_pdfjs_require__(27),_e=__w_pdfjs_require__(5),et=__w_pdfjs_require__(29),tt=__w_pdfjs_require__(14),nt=__w_pdfjs_require__(32)})(),__webpack_exports__})())})(pdf$1);var pdfExports=pdf$1.exports;const pdf=getDefaultExportFromCjs(pdfExports),pdfjsModule=_mergeNamespaces({__proto__:null,default:pdf},[pdfExports]),pdfjs="default"in pdfjsModule?pdf:pdfjsModule;var __spreadArray=function(a,i,o){if(o||arguments.length===2)for(var s=0,$=i.length,j;s<$;s++)(j||!(s in i))&&(j||(j=Array.prototype.slice.call(i,0,s)),j[s]=i[s]);return a.concat(j||Array.prototype.slice.call(i))},clipboardEvents=["onCopy","onCut","onPaste"],compositionEvents=["onCompositionEnd","onCompositionStart","onCompositionUpdate"],focusEvents=["onFocus","onBlur"],formEvents=["onInput","onInvalid","onReset","onSubmit"],imageEvents=["onLoad","onError"],keyboardEvents=["onKeyDown","onKeyPress","onKeyUp"],mediaEvents=["onAbort","onCanPlay","onCanPlayThrough","onDurationChange","onEmptied","onEncrypted","onEnded","onError","onLoadedData","onLoadedMetadata","onLoadStart","onPause","onPlay","onPlaying","onProgress","onRateChange","onSeeked","onSeeking","onStalled","onSuspend","onTimeUpdate","onVolumeChange","onWaiting"],mouseEvents=["onClick","onContextMenu","onDoubleClick","onMouseDown","onMouseEnter","onMouseLeave","onMouseMove","onMouseOut","onMouseOver","onMouseUp"],dragEvents=["onDrag","onDragEnd","onDragEnter","onDragExit","onDragLeave","onDragOver","onDragStart","onDrop"],selectionEvents=["onSelect"],touchEvents=["onTouchCancel","onTouchEnd","onTouchMove","onTouchStart"],pointerEvents=["onPointerDown","onPointerMove","onPointerUp","onPointerCancel","onGotPointerCapture","onLostPointerCapture","onPointerEnter","onPointerLeave","onPointerOver","onPointerOut"],uiEvents=["onScroll"],wheelEvents=["onWheel"],animationEvents=["onAnimationStart","onAnimationEnd","onAnimationIteration"],transitionEvents=["onTransitionEnd"],otherEvents=["onToggle"],changeEvents=["onChange"],allEvents=__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray(__spreadArray([],clipboardEvents,!0),compositionEvents,!0),focusEvents,!0),formEvents,!0),imageEvents,!0),keyboardEvents,!0),mediaEvents,!0),mouseEvents,!0),dragEvents,!0),selectionEvents,!0),touchEvents,!0),pointerEvents,!0),uiEvents,!0),wheelEvents,!0),animationEvents,!0),transitionEvents,!0),changeEvents,!0),otherEvents,!0);function makeEventProps(a,i){var o={};return allEvents.forEach(function(s){var $=a[s];$&&(i?o[s]=function(j){return $(j,i(s))}:o[s]=$)}),o}function makeCancellablePromise(a){var i=!1,o=new Promise(function(s,$){a.then(function(j){return!i&&s(j)}).catch(function(j){return!i&&$(j)})});return{promise:o,cancel:function(){i=!0}}}var isProduction=!0,prefix="Invariant failed";function invariant(a,i){if(!a){if(isProduction)throw new Error(prefix);var o=typeof i=="function"?i():i,s=o?"".concat(prefix,": ").concat(o):prefix;throw new Error(s)}}var warning=function(){},warning_1=warning;const warning$1=getDefaultExportFromCjs(warning_1);var has=Object.prototype.hasOwnProperty;function find(a,i,o){for(o of a.keys())if(dequal(o,i))return o}function dequal(a,i){var o,s,$;if(a===i)return!0;if(a&&i&&(o=a.constructor)===i.constructor){if(o===Date)return a.getTime()===i.getTime();if(o===RegExp)return a.toString()===i.toString();if(o===Array){if((s=a.length)===i.length)for(;s--&&dequal(a[s],i[s]););return s===-1}if(o===Set){if(a.size!==i.size)return!1;for(s of a)if($=s,$&&typeof $=="object"&&($=find(i,$),!$)||!i.has($))return!1;return!0}if(o===Map){if(a.size!==i.size)return!1;for(s of a)if($=s[0],$&&typeof $=="object"&&($=find(i,$),!$)||!dequal(s[1],i.get($)))return!1;return!0}if(o===ArrayBuffer)a=new Uint8Array(a),i=new Uint8Array(i);else if(o===DataView){if((s=a.byteLength)===i.byteLength)for(;s--&&a.getInt8(s)===i.getInt8(s););return s===-1}if(ArrayBuffer.isView(a)){if((s=a.byteLength)===i.byteLength)for(;s--&&a[s]===i[s];);return s===-1}if(!o||typeof a=="object"){s=0;for(o in a)if(has.call(a,o)&&++s&&!has.call(i,o)||!(o in i)||!dequal(a[o],i[o]))return!1;return Object.keys(i).length===s}}return a!==a&&i!==i}const DocumentContext=reactExports.createContext(null);function Message$1({children:a,type:i}){return React$1.createElement("div",{className:`react-pdf__message react-pdf__message--${i}`},a)}const DEFAULT_LINK_REL="noopener noreferrer nofollow";class LinkService{constructor(){this.externalLinkEnabled=!0,this.externalLinkRel=void 0,this.externalLinkTarget=void 0,this.isInPresentationMode=!1,this.pdfDocument=void 0,this.pdfViewer=void 0}setDocument(i){this.pdfDocument=i}setViewer(i){this.pdfViewer=i}setExternalLinkRel(i){this.externalLinkRel=i}setExternalLinkTarget(i){this.externalLinkTarget=i}setHistory(){}get pagesCount(){return this.pdfDocument?this.pdfDocument.numPages:0}get page(){return invariant(this.pdfViewer,"PDF viewer is not initialized."),this.pdfViewer.currentPageNumber||0}set page(i){invariant(this.pdfViewer,"PDF viewer is not initialized."),this.pdfViewer.currentPageNumber=i}get rotation(){return 0}set rotation(i){}goToDestination(i){return new Promise(o=>{invariant(this.pdfDocument,"PDF document not loaded."),invariant(i,"Destination is not specified."),typeof i=="string"?this.pdfDocument.getDestination(i).then(o):Array.isArray(i)?o(i):i.then(o)}).then(o=>{invariant(Array.isArray(o),`"${o}" is not a valid destination array.`);const s=o[0];new Promise($=>{invariant(this.pdfDocument,"PDF document not loaded."),s instanceof Object?this.pdfDocument.getPageIndex(s).then(j=>{$(j)}).catch(()=>{invariant(!1,`"${s}" is not a valid page reference.`)}):typeof s=="number"?$(s):invariant(!1,`"${s}" is not a valid destination reference.`)}).then($=>{const j=$+1;invariant(this.pdfViewer,"PDF viewer is not initialized."),invariant(j>=1&&j<=this.pagesCount,`"${j}" is not a valid page number.`),this.pdfViewer.scrollPageIntoView({dest:o,pageIndex:$,pageNumber:j})})})}navigateTo(i){this.goToDestination(i)}goToPage(i){const o=i-1;invariant(this.pdfViewer,"PDF viewer is not initialized."),invariant(i>=1&&i<=this.pagesCount,`"${i}" is not a valid page number.`),this.pdfViewer.scrollPageIntoView({pageIndex:o,pageNumber:i})}addLinkAttributes(i,o,s){i.href=o,i.rel=this.externalLinkRel||DEFAULT_LINK_REL,i.target=s?"_blank":this.externalLinkTarget||""}getDestinationHash(){return"#"}getAnchorUrl(){return"#"}setHash(){}executeNamedAction(){}cachePageRef(){}isPageVisible(){return!0}isPageCached(){return!0}executeSetOCGState(){}}const PasswordResponses={NEED_PASSWORD:1,INCORRECT_PASSWORD:2},isBrowser=typeof document<"u",isLocalFileSystem=isBrowser&&window.location.protocol==="file:";function isDefined(a){return typeof a<"u"}function isProvided(a){return isDefined(a)&&a!==null}function isString(a){return typeof a=="string"}function isArrayBuffer(a){return a instanceof ArrayBuffer}function isBlob(a){return invariant(isBrowser,"isBlob can only be used in a browser environment"),a instanceof Blob}function isDataURI(a){return isString(a)&&/^data:/.test(a)}function dataURItoByteString(a){invariant(isDataURI(a),"Invalid data URI.");const[i="",o=""]=a.split(",");return i.split(";").indexOf("base64")!==-1?atob(o):unescape(o)}function getDevicePixelRatio(){return isBrowser&&window.devicePixelRatio||1}const allowFileAccessFromFilesTip="On Chromium based browsers, you can use --allow-file-access-from-files flag for debugging purposes.";function displayCORSWarning(){warning$1(!isLocalFileSystem,`Loading PDF as base64 strings/URLs may not work on protocols other than HTTP/HTTPS. ${allowFileAccessFromFilesTip}`)}function displayWorkerWarning(){warning$1(!isLocalFileSystem,`Loading PDF.js worker may not work on protocols other than HTTP/HTTPS. ${allowFileAccessFromFilesTip}`)}function cancelRunningTask(a){a&&a.cancel&&a.cancel()}function makePageCallback(a,i){return Object.defineProperty(a,"width",{get(){return this.view[2]*i},configurable:!0}),Object.defineProperty(a,"height",{get(){return this.view[3]*i},configurable:!0}),Object.defineProperty(a,"originalWidth",{get(){return this.view[2]},configurable:!0}),Object.defineProperty(a,"originalHeight",{get(){return this.view[3]},configurable:!0}),a}function isCancelException(a){return a.name==="RenderingCancelledException"}function loadFromFile(a){return new Promise((i,o)=>{const s=new FileReader;s.onload=()=>{if(!s.result)return o(new Error("Error while reading a file."));i(s.result)},s.onerror=$=>{if(!$.target)return o(new Error("Error while reading a file."));const{error:j}=$.target;if(!j)return o(new Error("Error while reading a file."));switch(j.code){case j.NOT_FOUND_ERR:return o(new Error("Error while reading a file: File not found."));case j.SECURITY_ERR:return o(new Error("Error while reading a file: Security error."));case j.ABORT_ERR:return o(new Error("Error while reading a file: Aborted."));default:return o(new Error("Error while reading a file."))}},s.readAsArrayBuffer(a)})}function reducer(a,i){switch(i.type){case"RESOLVE":return{value:i.value,error:void 0};case"REJECT":return{value:!1,error:i.error};case"RESET":return{value:void 0,error:void 0};default:return a}}function useResolver(){return reactExports.useReducer(reducer,{value:void 0,error:void 0})}const{PDFDataRangeTransport:PDFDataRangeTransport$1}=pdfjs,eventProps=(()=>{const a={};return allEvents.forEach(i=>{a[i]=PropTypes.func}),a})(),isTypedArray=PropTypes.oneOfType([PropTypes.instanceOf(Int8Array),PropTypes.instanceOf(Uint8Array),PropTypes.instanceOf(Uint8ClampedArray),PropTypes.instanceOf(Int16Array),PropTypes.instanceOf(Uint16Array),PropTypes.instanceOf(Int32Array),PropTypes.instanceOf(Uint32Array),PropTypes.instanceOf(Float32Array),PropTypes.instanceOf(Float64Array)]),fileTypes=[PropTypes.string,PropTypes.instanceOf(ArrayBuffer),PropTypes.shape({data:PropTypes.oneOfType([PropTypes.string,PropTypes.instanceOf(ArrayBuffer),PropTypes.arrayOf(PropTypes.number.isRequired),isTypedArray]).isRequired}),PropTypes.shape({range:PropTypes.instanceOf(PDFDataRangeTransport$1).isRequired}),PropTypes.shape({url:PropTypes.string.isRequired})];typeof Blob<"u"&&fileTypes.push(PropTypes.instanceOf(Blob));const isClassName=PropTypes.oneOfType([PropTypes.string,PropTypes.arrayOf(PropTypes.string)]),isFile=PropTypes.oneOfType(fileTypes);PropTypes.instanceOf(LinkService);PropTypes.oneOf(["_self","_blank","_parent","_top"]);PropTypes.shape({commonObjs:PropTypes.shape({}).isRequired,getAnnotations:PropTypes.func.isRequired,getTextContent:PropTypes.func.isRequired,getViewport:PropTypes.func.isRequired,render:PropTypes.func.isRequired});const isPageIndex=function a(i,o,s){const{[o]:$,pageNumber:j,pdf:_e}=i;if(!isDefined(_e))return null;if(isDefined($)){if(typeof $!="number")return new Error(`\`${o}\` of type \`${typeof $}\` supplied to \`${s}\`, expected \`number\`.`);if($<0)return new Error(`Expected \`${o}\` to be greater or equal to 0.`);const{numPages:et}=_e;if($+1>et)return new Error(`Expected \`${o}\` to be less or equal to ${et-1}.`)}else if(!isDefined(j))return new Error(`\`${o}\` not supplied. Either pageIndex or pageNumber must be supplied to \`${s}\`.`);return null},isPageNumber=function a(i,o,s){const{[o]:$,pageIndex:j,pdf:_e}=i;if(!isDefined(_e))return null;if(isDefined($)){if(typeof $!="number")return new Error(`\`${o}\` of type \`${typeof $}\` supplied to \`${s}\`, expected \`number\`.`);if($<1)return new Error(`Expected \`${o}\` to be greater or equal to 1.`);const{numPages:et}=_e;if($>et)return new Error(`Expected \`${o}\` to be less or equal to ${et}.`)}else if(!isDefined(j))return new Error(`\`${o}\` not supplied. Either pageIndex or pageNumber must be supplied to \`${s}\`.`);return null},isPdf=PropTypes.oneOfType([PropTypes.any,PropTypes.oneOf([!1])]),isRef=PropTypes.oneOfType([PropTypes.func,PropTypes.exact({current:PropTypes.any})]),isRenderMode=PropTypes.oneOf(["canvas","custom","none","svg"]),isRotate=PropTypes.oneOf([0,90,180,270]);var __awaiter=function(a,i,o,s){function $(j){return j instanceof o?j:new o(function(_e){_e(j)})}return new(o||(o=Promise))(function(j,_e){function et(at){try{nt(s.next(at))}catch(it){_e(it)}}function tt(at){try{nt(s.throw(at))}catch(it){_e(it)}}function nt(at){at.done?j(at.value):$(at.value).then(et,tt)}nt((s=s.apply(a,i||[])).next())})},__rest$1=function(a,i){var o={};for(var s in a)Object.prototype.hasOwnProperty.call(a,s)&&i.indexOf(s)<0&&(o[s]=a[s]);if(a!=null&&typeof Object.getOwnPropertySymbols=="function")for(var $=0,s=Object.getOwnPropertySymbols(a);$<s.length;$++)i.indexOf(s[$])<0&&Object.prototype.propertyIsEnumerable.call(a,s[$])&&(o[s[$]]=a[s[$]]);return o};const{PDFDataRangeTransport}=pdfjs,defaultOnPassword=(a,i)=>{switch(i){case PasswordResponses.NEED_PASSWORD:{const o=prompt("Enter the password to open this PDF file.");a(o);break}case PasswordResponses.INCORRECT_PASSWORD:{const o=prompt("Invalid password. Please try again.");a(o);break}}};function isParameterObject(a){return typeof a=="object"&&a!==null&&("data"in a||"range"in a||"url"in a)}const Document=reactExports.forwardRef(function a(i,o){var{children:s,className:$,error:j="Failed to load PDF file.",externalLinkRel:_e,externalLinkTarget:et,file:tt,inputRef:nt,imageResourcesPath:at,loading:it="Loading PDF…",noData:st="No PDF file specified.",onItemClick:lt,onLoadError:ct,onLoadProgress:rt,onLoadSuccess:ut,onPassword:ot=defaultOnPassword,onSourceError:dt,onSourceSuccess:pt,options:mt,renderMode:ft,rotate:ht}=i,yt=__rest$1(i,["children","className","error","externalLinkRel","externalLinkTarget","file","inputRef","imageResourcesPath","loading","noData","onItemClick","onLoadError","onLoadProgress","onLoadSuccess","onPassword","onSourceError","onSourceSuccess","options","renderMode","rotate"]);const[bt,gt]=useResolver(),{value:xt,error:vt}=bt,[Lt,$t]=useResolver(),{value:Tt,error:Et}=Lt,Dt=reactExports.useRef(new LinkService),It=reactExports.useRef([]),Ct=reactExports.useRef(),jt=reactExports.useRef();reactExports.useEffect(()=>{tt&&tt!==Ct.current&&isParameterObject(tt)&&(warning$1(!dequal(tt,Ct.current),`File prop passed to <Document /> changed, but it's equal to previous one. This might result in unnecessary reloads. Consider memoizing the value passed to "file" prop.`),Ct.current=tt)},[tt]),reactExports.useEffect(()=>{mt&&mt!==jt.current&&(warning$1(!dequal(mt,jt.current),`Options prop passed to <Document /> changed, but it's equal to previous one. This might result in unnecessary reloads. Consider memoizing the value passed to "options" prop.`),jt.current=mt)},[mt]);const Zt=reactExports.useRef({scrollPageIntoView:ln=>{const{dest:gn,pageNumber:yn,pageIndex:Pn=yn-1}=ln;if(lt){lt({dest:gn,pageIndex:Pn,pageNumber:yn});return}const cn=It.current[Pn];if(cn){cn.scrollIntoView();return}warning$1(!1,`An internal link leading to page ${yn} was clicked, but neither <Document> was provided with onItemClick nor it was able to find the page within itself. Either provide onItemClick to <Document> and handle navigating by yourself or ensure that all pages are rendered within <Document>.`)}});reactExports.useImperativeHandle(o,()=>({linkService:Dt,pages:It,viewer:Zt}),[]);function Xt(){pt&&pt()}function sn(){vt&&(warning$1(!1,vt.toString()),dt&&dt(vt))}function Ft(){gt({type:"RESET"})}reactExports.useEffect(Ft,[tt,gt]);const wt=reactExports.useCallback(()=>__awaiter(this,void 0,void 0,function*(){if(!tt)return null;if(typeof tt=="string")return isDataURI(tt)?{data:dataURItoByteString(tt)}:(displayCORSWarning(),{url:tt});if(tt instanceof PDFDataRangeTransport)return{range:tt};if(isArrayBuffer(tt))return{data:tt};if(isBrowser&&isBlob(tt))return{data:yield loadFromFile(tt)};if(invariant(typeof tt=="object","Invalid parameter in file, need either Uint8Array, string or a parameter object"),invariant(isParameterObject(tt),"Invalid parameter object: need either .data, .range or .url"),"url"in tt&&typeof tt.url=="string"){if(isDataURI(tt.url)){const{url:ln}=tt,gn=__rest$1(tt,["url"]),yn=dataURItoByteString(ln);return Object.assign({data:yn},gn)}displayCORSWarning()}return tt}),[tt]);reactExports.useEffect(()=>{const ln=makeCancellablePromise(wt());return ln.promise.then(gn=>{gt({type:"RESOLVE",value:gn})}).catch(gn=>{gt({type:"REJECT",error:gn})}),()=>{cancelRunningTask(ln)}},[wt,gt]),reactExports.useEffect(()=>{if(!(typeof xt>"u")){if(xt===!1){sn();return}Xt()}},[xt]);function kt(){Tt&&(ut&&ut(Tt),It.current=new Array(Tt.numPages),Dt.current.setDocument(Tt))}function At(){Et&&(warning$1(!1,Et.toString()),ct&&ct(Et))}function Pt(){$t({type:"RESET"})}reactExports.useEffect(Pt,[$t,xt]);function Mt(){if(!xt)return;const ln=mt?Object.assign(Object.assign({},xt),mt):xt,gn=pdfjs.getDocument(ln);rt&&(gn.onProgress=rt),ot&&(gn.onPassword=ot);const yn=gn;return yn.promise.then(Pn=>{$t({type:"RESOLVE",value:Pn})}).catch(Pn=>{yn.destroyed||$t({type:"REJECT",error:Pn})}),()=>{yn.destroy()}}reactExports.useEffect(Mt,[mt,$t,xt]),reactExports.useEffect(()=>{if(!(typeof Tt>"u")){if(Tt===!1){At();return}kt()}},[Tt]);function Ot(){Dt.current.setViewer(Zt.current),Dt.current.setExternalLinkRel(_e),Dt.current.setExternalLinkTarget(et)}reactExports.useEffect(Ot,[_e,et]);function Bt(ln,gn){It.current[ln]=gn}function zt(ln){delete It.current[ln]}const Gt=reactExports.useMemo(()=>({imageResourcesPath:at,linkService:Dt.current,onItemClick:lt,pdf:Tt,registerPage:Bt,renderMode:ft,rotate:ht,unregisterPage:zt}),[at,lt,Tt,ft,ht]),Wt=reactExports.useMemo(()=>makeEventProps(yt,()=>Tt),[yt,Tt]);function qt(){return React$1.createElement(DocumentContext.Provider,{value:Gt},s)}function tn(){return tt?Tt==null?React$1.createElement(Message$1,{type:"loading"},typeof it=="function"?it():it):Tt===!1?React$1.createElement(Message$1,{type:"error"},typeof j=="function"?j():j):qt():React$1.createElement(Message$1,{type:"no-data"},typeof st=="function"?st():st)}return React$1.createElement("div",Object.assign({className:clsx("react-pdf__Document",$),ref:nt,style:{"--scale-factor":"1"}},Wt),tn())}),isFunctionOrNode$1=PropTypes.oneOfType([PropTypes.func,PropTypes.node]);Document.propTypes=Object.assign(Object.assign({},eventProps),{children:PropTypes.node,className:isClassName,error:isFunctionOrNode$1,externalLinkRel:PropTypes.string,externalLinkTarget:PropTypes.oneOf(["_self","_blank","_parent","_top"]),file:isFile,imageResourcesPath:PropTypes.string,inputRef:isRef,loading:isFunctionOrNode$1,noData:isFunctionOrNode$1,onItemClick:PropTypes.func,onLoadError:PropTypes.func,onLoadProgress:PropTypes.func,onLoadSuccess:PropTypes.func,onPassword:PropTypes.func,onSourceError:PropTypes.func,onSourceSuccess:PropTypes.func,options:PropTypes.shape({canvasFactory:PropTypes.any,canvasMaxAreaInBytes:PropTypes.number,cMapPacked:PropTypes.bool,CMapReaderFactory:PropTypes.any,cMapUrl:PropTypes.string,disableAutoFetch:PropTypes.bool,disableFontFace:PropTypes.bool,disableRange:PropTypes.bool,disableStream:PropTypes.bool,docBaseUrl:PropTypes.string,enableXfa:PropTypes.bool,filterFactory:PropTypes.any,fontExtraProperties:PropTypes.bool,httpHeaders:PropTypes.object,isEvalSupported:PropTypes.bool,isOffscreenCanvasSupported:PropTypes.bool,length:PropTypes.number,maxImageSize:PropTypes.number,ownerDocument:PropTypes.any,password:PropTypes.string,pdfBug:PropTypes.bool,rangeChunkSize:PropTypes.number,StandardFontDataFactory:PropTypes.any,standardFontDataUrl:PropTypes.string,stopAtErrors:PropTypes.bool,useSystemFonts:PropTypes.bool,useWorkerFetch:PropTypes.bool,verbosity:PropTypes.number,withCredentials:PropTypes.bool,worker:PropTypes.any}),rotate:PropTypes.number});const Document$1=Document;function useDocumentContext(){return reactExports.useContext(DocumentContext)}function mergeRefs(){for(var a=[],i=0;i<arguments.length;i++)a[i]=arguments[i];var o=a.filter(Boolean);if(o.length<=1){var s=o[0];return s||null}return function(j){o.forEach(function(_e){typeof _e=="function"?_e(j):_e&&(_e.current=j)})}}const PageContext=reactExports.createContext(null),PDF_ROLE_TO_HTML_ROLE={Document:null,DocumentFragment:null,Part:"group",Sect:"group",Div:"group",Aside:"note",NonStruct:"none",P:null,H:"heading",Title:null,FENote:"note",Sub:"group",Lbl:null,Span:null,Em:null,Strong:null,Link:"link",Annot:"note",Form:"form",Ruby:null,RB:null,RT:null,RP:null,Warichu:null,WT:null,WP:null,L:"list",LI:"listitem",LBody:null,Table:"table",TR:"row",TH:"columnheader",TD:"cell",THead:"columnheader",TBody:null,TFoot:null,Caption:null,Figure:"figure",Formula:null,Artifact:null},HEADING_PATTERN=/^H(\d+)$/;function isPdfRole(a){return a in PDF_ROLE_TO_HTML_ROLE}function isStructTreeNode(a){return"children"in a}function isStructTreeNodeWithOnlyContentChild(a){return isStructTreeNode(a)?a.children.length===1&&0 in a.children&&"id"in a.children[0]:!1}function getRoleAttributes(a){const i={};if(isStructTreeNode(a)){const{role:o}=a,s=o.match(HEADING_PATTERN);if(s)i.role="heading",i["aria-level"]=Number(s[1]);else if(isPdfRole(o)){const $=PDF_ROLE_TO_HTML_ROLE[o];$&&(i.role=$)}}return i}function getBaseAttributes(a){const i={};if(isStructTreeNode(a)){if(a.alt!==void 0&&(i["aria-label"]=a.alt),a.lang!==void 0&&(i.lang=a.lang),isStructTreeNodeWithOnlyContentChild(a)){const[o]=a.children;if(o){const s=getBaseAttributes(o);return Object.assign(Object.assign({},i),s)}}}else"id"in a&&(i["aria-owns"]=a.id);return i}function getAttributes(a){return a?Object.assign(Object.assign({},getRoleAttributes(a)),getBaseAttributes(a)):null}function StructTreeItem({className:a,node:i}){const o=reactExports.useMemo(()=>getAttributes(i),[i]),s=reactExports.useMemo(()=>!isStructTreeNode(i)||isStructTreeNodeWithOnlyContentChild(i)?null:i.children.map(($,j)=>React$1.createElement(StructTreeItem,{key:j,node:$})),[i]);return React$1.createElement("span",Object.assign({className:a},o),s)}function usePageContext(){return reactExports.useContext(PageContext)}function StructTree(){const a=usePageContext();invariant(a,"Unable to find Page context.");const{onGetStructTreeError:i,onGetStructTreeSuccess:o}=a,[s,$]=useResolver(),{value:j,error:_e}=s,{customTextRenderer:et,page:tt}=a;function nt(){j&&o&&o(j)}function at(){_e&&(warning$1(!1,_e.toString()),i&&i(_e))}function it(){$({type:"RESET"})}reactExports.useEffect(it,[$,tt]);function st(){if(et||!tt)return;const lt=makeCancellablePromise(tt.getStructTree()),ct=lt;return lt.promise.then(rt=>{$({type:"RESOLVE",value:rt})}).catch(rt=>{$({type:"REJECT",error:rt})}),()=>cancelRunningTask(ct)}return reactExports.useEffect(st,[et,tt,$]),reactExports.useEffect(()=>{if(j!==void 0){if(j===!1){at();return}nt()}},[j]),j?React$1.createElement(StructTreeItem,{className:"react-pdf__Page__structTree structTree",node:j}):null}const ANNOTATION_MODE=pdfjs.AnnotationMode;function PageCanvas(a){const i=usePageContext();invariant(i,"Unable to find Page context.");const o=Object.assign(Object.assign({},i),a),{_className:s,canvasBackground:$,devicePixelRatio:j=getDevicePixelRatio(),onRenderError:_e,onRenderSuccess:et,page:tt,renderForms:nt,renderTextLayer:at,rotate:it,scale:st}=o,{canvasRef:lt}=a;invariant(tt,"Attempted to render page canvas, but no page was specified.");const ct=reactExports.useRef(null);function rt(){tt&&et&&et(makePageCallback(tt,st))}function ut(ft){isCancelException(ft)||(warning$1(!1,ft.toString()),_e&&_e(ft))}const ot=reactExports.useMemo(()=>tt.getViewport({scale:st*j,rotation:it}),[j,tt,it,st]),dt=reactExports.useMemo(()=>tt.getViewport({scale:st,rotation:it}),[tt,it,st]);function pt(){if(!tt)return;tt.cleanup();const{current:ft}=ct;if(!ft)return;ft.width=ot.width,ft.height=ot.height,ft.style.width=`${Math.floor(dt.width)}px`,ft.style.height=`${Math.floor(dt.height)}px`,ft.style.visibility="hidden";const ht={annotationMode:nt?ANNOTATION_MODE.ENABLE_FORMS:ANNOTATION_MODE.ENABLE,canvasContext:ft.getContext("2d",{alpha:!1}),viewport:ot};$&&(ht.background=$);const yt=tt.render(ht),bt=yt;return yt.promise.then(()=>{ft.style.visibility="",rt()}).catch(ut),()=>cancelRunningTask(bt)}reactExports.useEffect(pt,[$,ct,j,tt,nt,ot,dt]);const mt=reactExports.useCallback(()=>{const{current:ft}=ct;ft&&(ft.width=0,ft.height=0)},[ct]);return reactExports.useEffect(()=>mt,[mt]),React$1.createElement("canvas",{className:`${s}__canvas`,dir:"ltr",ref:mergeRefs(lt,ct),style:{display:"block",userSelect:"none"}},at?React$1.createElement(StructTree,null):null)}function PageSVG(){const a=usePageContext();invariant(a,"Unable to find Page context.");const{_className:i,onRenderSuccess:o,onRenderError:s,page:$,rotate:j,scale:_e}=a;invariant($,"Attempted to render page SVG, but no page was specified.");const[et,tt]=useResolver(),{value:nt,error:at}=et;function it(){$&&o&&o(makePageCallback($,_e))}function st(){at&&(isCancelException(at)||(warning$1(!1,at.toString()),s&&s(at)))}const lt=reactExports.useMemo(()=>$.getViewport({scale:_e,rotation:j}),[$,j,_e]);function ct(){tt({type:"RESET"})}reactExports.useEffect(ct,[$,tt,lt]);function rt(){if(!$)return;const pt=makeCancellablePromise($.getOperatorList());return pt.promise.then(mt=>{new pdfjs.SVGGraphics($.commonObjs,$.objs).getSVG(mt,lt).then(ht=>{if(!(ht instanceof SVGElement))throw new Error("getSVG returned unexpected result.");tt({type:"RESOLVE",value:ht})}).catch(ht=>{tt({type:"REJECT",error:ht})})}).catch(mt=>{tt({type:"REJECT",error:mt})}),()=>cancelRunningTask(pt)}reactExports.useEffect(rt,[$,tt,lt]),reactExports.useEffect(()=>{if(nt!==void 0){if(nt===!1){st();return}it()}},[nt]);function ut(pt){if(!pt||!nt)return;pt.firstElementChild||pt.appendChild(nt);const{width:mt,height:ft}=lt;nt.setAttribute("width",`${mt}`),nt.setAttribute("height",`${ft}`)}const{width:ot,height:dt}=lt;return React$1.createElement("div",{className:`${i}__svg`,ref:pt=>ut(pt),style:{display:"block",backgroundColor:"white",overflow:"hidden",width:ot,height:dt,userSelect:"none"}})}function isTextItem(a){return"str"in a}function TextLayer(){const a=usePageContext();invariant(a,"Unable to find Page context.");const{customTextRenderer:i,onGetTextError:o,onGetTextSuccess:s,onRenderTextLayerError:$,onRenderTextLayerSuccess:j,page:_e,pageIndex:et,pageNumber:tt,rotate:nt,scale:at}=a;invariant(_e,"Attempted to load page text content, but no page was specified.");const[it,st]=useResolver(),{value:lt,error:ct}=it,rt=reactExports.useRef(null),ut=reactExports.useRef();warning$1(parseInt(window.getComputedStyle(document.body).getPropertyValue("--react-pdf-text-layer"),10)===1,"TextLayer styles not found. Read more: https://github.com/wojtekmaj/react-pdf#support-for-text-layer");function ot(){lt&&s&&s(lt)}function dt(){ct&&(warning$1(!1,ct.toString()),o&&o(ct))}function pt(){st({type:"RESET"})}reactExports.useEffect(pt,[_e,st]);function mt(){if(!_e)return;const vt=makeCancellablePromise(_e.getTextContent()),Lt=vt;return vt.promise.then($t=>{st({type:"RESOLVE",value:$t})}).catch($t=>{st({type:"REJECT",error:$t})}),()=>cancelRunningTask(Lt)}reactExports.useEffect(mt,[_e,st]),reactExports.useEffect(()=>{if(lt!==void 0){if(lt===!1){dt();return}ot()}},[lt]);const ft=reactExports.useCallback(()=>{j&&j()},[j]),ht=reactExports.useCallback(vt=>{warning$1(!1,vt.toString()),$&&$(vt)},[$]);function yt(){const vt=ut.current;vt&&vt.classList.add("active")}function bt(){const vt=ut.current;vt&&vt.classList.remove("active")}const gt=reactExports.useMemo(()=>_e.getViewport({scale:at,rotation:nt}),[_e,nt,at]);function xt(){if(!_e||!lt)return;const{current:vt}=rt;if(!vt)return;vt.innerHTML="";const Lt=_e.streamTextContent({includeMarkedContent:!0}),$t={container:vt,textContentSource:Lt,viewport:gt},Tt=pdfjs.renderTextLayer($t),Et=Tt;return Tt.promise.then(()=>{const Dt=document.createElement("div");Dt.className="endOfContent",vt.append(Dt),ut.current=Dt;const It=vt.querySelectorAll('[role="presentation"]');if(i){let Ct=0;lt.items.forEach((jt,Zt)=>{if(!isTextItem(jt))return;const Xt=It[Ct];if(!Xt)return;const sn=i(Object.assign({pageIndex:et,pageNumber:tt,itemIndex:Zt},jt));Xt.innerHTML=sn,Ct+=jt.str&&jt.hasEOL?2:1})}ft()}).catch(ht),()=>cancelRunningTask(Et)}return reactExports.useLayoutEffect(xt,[i,ht,ft,_e,et,tt,lt,gt]),React$1.createElement("div",{className:clsx("react-pdf__Page__textContent","textLayer"),onMouseUp:bt,onMouseDown:yt,ref:rt})}function AnnotationLayer(){const a=useDocumentContext(),i=usePageContext();invariant(i,"Unable to find Page context.");const o=Object.assign(Object.assign({},a),i),{imageResourcesPath:s,linkService:$,onGetAnnotationsError:j,onGetAnnotationsSuccess:_e,onRenderAnnotationLayerError:et,onRenderAnnotationLayerSuccess:tt,page:nt,pdf:at,renderForms:it,rotate:st,scale:lt=1}=o;invariant(at,"Attempted to load page annotations, but no document was specified. Wrap <Page /> in a <Document /> or pass explicit `pdf` prop."),invariant(nt,"Attempted to load page annotations, but no page was specified."),invariant($,"Attempted to load page annotations, but no linkService was specified.");const[ct,rt]=useResolver(),{value:ut,error:ot}=ct,dt=reactExports.useRef(null);warning$1(parseInt(window.getComputedStyle(document.body).getPropertyValue("--react-pdf-annotation-layer"),10)===1,"AnnotationLayer styles not found. Read more: https://github.com/wojtekmaj/react-pdf#support-for-annotations");function pt(){ut&&_e&&_e(ut)}function mt(){ot&&(warning$1(!1,ot.toString()),j&&j(ot))}function ft(){rt({type:"RESET"})}reactExports.useEffect(ft,[rt,nt]);function ht(){if(!nt)return;const vt=makeCancellablePromise(nt.getAnnotations()),Lt=vt;return vt.promise.then($t=>{rt({type:"RESOLVE",value:$t})}).catch($t=>{rt({type:"REJECT",error:$t})}),()=>{cancelRunningTask(Lt)}}reactExports.useEffect(ht,[rt,nt,it]),reactExports.useEffect(()=>{if(ut!==void 0){if(ut===!1){mt();return}pt()}},[ut]);function yt(){tt&&tt()}function bt(vt){warning$1(!1,`${vt}`),et&&et(vt)}const gt=reactExports.useMemo(()=>nt.getViewport({scale:lt,rotation:st}),[nt,st,lt]);function xt(){if(!at||!nt||!$||!ut)return;const{current:vt}=dt;if(!vt)return;const Lt=gt.clone({dontFlip:!0}),$t={accessibilityManager:null,annotationCanvasMap:null,div:vt,l10n:null,page:nt,viewport:Lt},Tt={annotations:ut,annotationStorage:at.annotationStorage,div:vt,downloadManager:null,imageResourcesPath:s,linkService:$,page:nt,renderForms:it,viewport:Lt};vt.innerHTML="";try{new pdfjs.AnnotationLayer($t).render(Tt),yt()}catch(Et){bt(Et)}return()=>{}}return reactExports.useEffect(xt,[ut,s,$,nt,it,gt]),React$1.createElement("div",{className:clsx("react-pdf__Page__annotations","annotationLayer"),ref:dt})}var __rest=function(a,i){var o={};for(var s in a)Object.prototype.hasOwnProperty.call(a,s)&&i.indexOf(s)<0&&(o[s]=a[s]);if(a!=null&&typeof Object.getOwnPropertySymbols=="function")for(var $=0,s=Object.getOwnPropertySymbols(a);$<s.length;$++)i.indexOf(s[$])<0&&Object.prototype.propertyIsEnumerable.call(a,s[$])&&(o[s[$]]=a[s[$]]);return o};const defaultScale=1,Page=function a(i){const o=useDocumentContext(),s=Object.assign(Object.assign({},o),i),{_className:$="react-pdf__Page",_enableRegisterUnregisterPage:j=!0,canvasBackground:_e,canvasRef:et,children:tt,className:nt,customRenderer:at,customTextRenderer:it,devicePixelRatio:st,error:lt="Failed to load the page.",height:ct,inputRef:rt,loading:ut="Loading page…",noData:ot="No page specified.",onGetAnnotationsError:dt,onGetAnnotationsSuccess:pt,onGetStructTreeError:mt,onGetStructTreeSuccess:ft,onGetTextError:ht,onGetTextSuccess:yt,onLoadError:bt,onLoadSuccess:gt,onRenderAnnotationLayerError:xt,onRenderAnnotationLayerSuccess:vt,onRenderError:Lt,onRenderSuccess:$t,onRenderTextLayerError:Tt,onRenderTextLayerSuccess:Et,pageIndex:Dt,pageNumber:It,pdf:Ct,registerPage:jt,renderAnnotationLayer:Zt=!0,renderForms:Xt=!1,renderMode:sn="canvas",renderTextLayer:Ft=!0,rotate:wt,scale:kt=defaultScale,unregisterPage:At,width:Pt}=s,Mt=__rest(s,["_className","_enableRegisterUnregisterPage","canvasBackground","canvasRef","children","className","customRenderer","customTextRenderer","devicePixelRatio","error","height","inputRef","loading","noData","onGetAnnotationsError","onGetAnnotationsSuccess","onGetStructTreeError","onGetStructTreeSuccess","onGetTextError","onGetTextSuccess","onLoadError","onLoadSuccess","onRenderAnnotationLayerError","onRenderAnnotationLayerSuccess","onRenderError","onRenderSuccess","onRenderTextLayerError","onRenderTextLayerSuccess","pageIndex","pageNumber","pdf","registerPage","renderAnnotationLayer","renderForms","renderMode","renderTextLayer","rotate","scale","unregisterPage","width"]),[Ot,Bt]=useResolver(),{value:zt,error:Gt}=Ot,Wt=reactExports.useRef(null);invariant(Ct,"Attempted to load a page, but no document was specified. Wrap <Page /> in a <Document /> or pass explicit `pdf` prop.");const qt=isProvided(It)?It-1:Dt??null,tn=It??(isProvided(Dt)?Dt+1:null),ln=wt??(zt?zt.rotate:null),gn=reactExports.useMemo(()=>{if(!zt)return null;let Wn=1;const _n=kt??defaultScale;if(Pt||ct){const Zn=zt.getViewport({scale:1,rotation:ln});Pt?Wn=Pt/Zn.width:ct&&(Wn=ct/Zn.height)}return _n*Wn},[ct,zt,ln,kt,Pt]);function yn(){return()=>{isProvided(qt)&&j&&At&&At(qt)}}reactExports.useEffect(yn,[j,Ct,qt,At]);function Pn(){if(gt){if(!zt||!gn)return;gt(makePageCallback(zt,gn))}if(j&&jt){if(!isProvided(qt)||!Wt.current)return;jt(qt,Wt.current)}}function cn(){Gt&&(warning$1(!1,Gt.toString()),bt&&bt(Gt))}function xn(){Bt({type:"RESET"})}reactExports.useEffect(xn,[Bt,Ct,qt]);function hn(){if(!Ct||!tn)return;const Wn=makeCancellablePromise(Ct.getPage(tn)),_n=Wn;return Wn.promise.then(Zn=>{Bt({type:"RESOLVE",value:Zn})}).catch(Zn=>{Bt({type:"REJECT",error:Zn})}),()=>cancelRunningTask(_n)}reactExports.useEffect(hn,[Bt,Ct,qt,tn,jt]),reactExports.useEffect(()=>{if(zt!==void 0){if(zt===!1){cn();return}Pn()}},[zt,gn]);const en=reactExports.useMemo(()=>zt&&isProvided(qt)&&tn&&isProvided(ln)&&isProvided(gn)?{_className:$,canvasBackground:_e,customTextRenderer:it,devicePixelRatio:st,onGetAnnotationsError:dt,onGetAnnotationsSuccess:pt,onGetStructTreeError:mt,onGetStructTreeSuccess:ft,onGetTextError:ht,onGetTextSuccess:yt,onRenderAnnotationLayerError:xt,onRenderAnnotationLayerSuccess:vt,onRenderError:Lt,onRenderSuccess:$t,onRenderTextLayerError:Tt,onRenderTextLayerSuccess:Et,page:zt,pageIndex:qt,pageNumber:tn,renderForms:Xt,renderTextLayer:Ft,rotate:ln,scale:gn}:null,[$,_e,it,st,dt,pt,mt,ft,ht,yt,xt,vt,Lt,$t,Tt,Et,zt,qt,tn,Xt,Ft,ln,gn]),Jt=reactExports.useMemo(()=>makeEventProps(Mt,()=>zt&&(gn?makePageCallback(zt,gn):void 0)),[Mt,zt,gn]),vn=`${qt}@${gn}/${ln}`,$n=`${qt}/${ln}`;function Mn(){switch(sn){case"custom":return invariant(at,'renderMode was set to "custom", but no customRenderer was passed.'),React$1.createElement(at,{key:`${vn}_custom`});case"none":return null;case"svg":return React$1.createElement(PageSVG,{key:`${$n}_svg`});case"canvas":default:return React$1.createElement(PageCanvas,{key:`${vn}_canvas`,canvasRef:et})}}function On(){return Ft?React$1.createElement(TextLayer,{key:`${vn}_text`}):null}function En(){return Zt?React$1.createElement(AnnotationLayer,{key:`${vn}_annotations`}):null}function Bn(){return React$1.createElement(PageContext.Provider,{value:en},Mn(),On(),En(),tt)}function Hn(){return tn?Ct===null||zt===void 0||zt===null?React$1.createElement(Message$1,{type:"loading"},typeof ut=="function"?ut():ut):Ct===!1||zt===!1?React$1.createElement(Message$1,{type:"error"},typeof lt=="function"?lt():lt):Bn():React$1.createElement(Message$1,{type:"no-data"},typeof ot=="function"?ot():ot)}return React$1.createElement("div",Object.assign({className:clsx($,nt),"data-page-number":tn,ref:mergeRefs(rt,Wt),style:{"--scale-factor":`${gn}`,backgroundColor:_e||"white",position:"relative",minWidth:"min-content",minHeight:"min-content"}},Jt),Hn())},isFunctionOrNode=PropTypes.oneOfType([PropTypes.func,PropTypes.node]);Page.propTypes=Object.assign(Object.assign({},eventProps),{canvasBackground:PropTypes.string,canvasRef:isRef,children:PropTypes.node,className:isClassName,customRenderer:PropTypes.func,customTextRenderer:PropTypes.func,devicePixelRatio:PropTypes.number,error:isFunctionOrNode,height:PropTypes.number,imageResourcesPath:PropTypes.string,inputRef:isRef,loading:isFunctionOrNode,noData:isFunctionOrNode,onGetTextError:PropTypes.func,onGetTextSuccess:PropTypes.func,onLoadError:PropTypes.func,onLoadSuccess:PropTypes.func,onRenderError:PropTypes.func,onRenderSuccess:PropTypes.func,onRenderTextLayerError:PropTypes.func,onRenderTextLayerSuccess:PropTypes.func,pageIndex:isPageIndex,pageNumber:isPageNumber,pdf:isPdf,renderAnnotationLayer:PropTypes.bool,renderForms:PropTypes.bool,renderMode:isRenderMode,renderTextLayer:PropTypes.bool,rotate:isRotate,scale:PropTypes.number,width:PropTypes.number});const Page$1=Page;displayWorkerWarning();pdfjs.GlobalWorkerOptions.workerSrc="pdf.worker.js";pdfjs.GlobalWorkerOptions.workerSrc=new URL("/assets/pdf.worker.min-DKQKFyKK.js",import.meta.url).toString();const options={cMapUrl:"/cmaps/",standardFontDataUrl:"/standard_fonts/"};function PdfViewer({paperId:a,width:i}){const[o,s]=reactExports.useState(null),[$,j]=reactExports.useState(1),[_e,et]=reactExports.useState("");reactExports.useEffect(()=>{(async lt=>{try{const ct=await fetchPdf(lt),rt=new Blob([ct.data],{type:"application/pdf"}),ut=URL.createObjectURL(rt);et(ut)}catch(ct){console.error("Error fetching PDF:",ct)}})(a)},[a]);function tt({numPages:st}){s(st)}function nt(st){j(lt=>lt+st)}function at(st){st.preventDefault(),nt(-1)}function it(st){st.preventDefault(),nt(1)}return jsxRuntimeExports.jsxs("div",{className:"pdf-viewer p-2",style:{display:"flex",flexDirection:"column-reverse"},children:[_e?jsxRuntimeExports.jsx(Document$1,{file:_e,onLoadSuccess:tt,onError:st=>console.error("Error loading document",st),options,children:jsxRuntimeExports.jsx(Page$1,{pageNumber:$,width:i,loading:jsxRuntimeExports.jsx("div",{children:"Loading..."})})}):jsxRuntimeExports.jsx("div",{children:"Loading PDF..."}),jsxRuntimeExports.jsx(Pagination,{pageNumber:$,numPages:o,onPreviousPage:at,onNextPage:it})]})}function Pagination({pageNumber:a,numPages:i,onPreviousPage:o,onNextPage:s}){return jsxRuntimeExports.jsxs("div",{className:"page-controls",children:[jsxRuntimeExports.jsx("button",{type:"button",disabled:a<=1,onClick:o,children:"‹"}),jsxRuntimeExports.jsxs("span",{children:[a||(i?1:"--")," of ",i||"--"]}),jsxRuntimeExports.jsx("button",{type:"button",disabled:a>=i,onClick:s,children:"›"})]})}function PdfModal({paperId:a}){const[i,o]=useAtom(pdfModalOpen),s=()=>o(!1),[$,j]=reactExports.useState(750);return reactExports.useEffect(()=>{const _e=window.innerWidth,et=_e>1024?_e*.4:750;j(et)},[]),jsxRuntimeExports.jsx(ModalWrapper,{open:i,handleClose:s,width:$,children:jsxRuntimeExports.jsx(PdfViewer,{paperId:a,width:$-100})})}const createAuthorSearchURL=a=>{const[i,o]=a.split(" ");return!o||!i?"#":`https://arxiv.org/search/cs?searchtype=author&query=${`${i},+${o.charAt(0)}`}`},DateAuthorsPdf=({paper:a})=>{const i=useNavigate();useAtom(pdfModalOpen);const{date:o,authors:s}=a||{},$=(s==null?void 0:s.split(";").map(tt=>tt.trim()))||[],j=tt=>{if(tt.stopPropagation(),!(a!=null&&a.id)){i("/calendar");return}window.open(`http://export.arxiv.org/pdf/${a.id}`,"_blank")},_e=tt=>nt=>{i(`/date/${tt}`)},et=formatDate$1(o||"",{year:"numeric",month:"long",day:"numeric"});return jsxRuntimeExports.jsxs(Box$1,{display:"flex",justifyContent:"space-between",alignItems:"center",marginBottom:2,children:[o&&jsxRuntimeExports.jsx(Typography$1,{variant:"subtitle1",color:"textSecondary",onClick:_e(o),style:{cursor:"pointer",paddingRight:"1rem"},sx:{"&:hover":{textDecoration:"underline"}},children:et}),jsxRuntimeExports.jsx(Box$1,{sx:{maxWidth:"70%"},children:$.map((tt,nt)=>jsxRuntimeExports.jsxs(React$1.Fragment,{children:[jsxRuntimeExports.jsx(Link$1,{href:createAuthorSearchURL(tt),color:"secondary.light",underline:"hover",target:"_blank",children:tt}),nt<$.length-1?", ":""]},nt))}),jsxRuntimeExports.jsxs(Box$1,{sx:{display:"flex"},children:[jsxRuntimeExports.jsx(Favorite,{paper:a}),jsxRuntimeExports.jsx(Button$1,{sx:{ml:2},variant:"contained",color:"secondary",onClick:j,children:"View PDF"})]})]})},EntryTitleStyled=styled(Typography$1)(({theme:a})=>({fontWeight:600,background:"rgba(0, 0, 0, 0.5)",padding:a.spacing(1),borderRadius:a.shape.borderRadius,textShadow:"2px 2px 4px #00000040",maxWidth:"fit-content",position:"relative",zIndex:1})),GradientBackground=styled(Box$1)(({theme:a})=>({marginBottom:a.spacing(4),marginTop:a.spacing(2),minHeight:"5rem",background:"linear-gradient(135deg, #B4191C 50%, #222222 50%)",borderRadius:a.shape.borderRadius,padding:a.spacing(1),alignSelf:"center",display:"flex",justifyContent:"center",width:"100%",textAlign:"center"})),PaperTitle=({title:a,id:i})=>{const o=useNavigate(),s=$=>{if($.stopPropagation(),!i){o("/calendar");return}window.open(`https://arxiv.org/abs/${i}`,"_blank")};return jsxRuntimeExports.jsx(GradientBackground,{onClick:s,sx:{cursor:"pointer"},children:a&&jsxRuntimeExports.jsx(EntryTitleStyled,{variant:"h4",gutterBottom:!0,sx:{maxWidth:"none",width:"100%"},children:a})})};var ExpandMore={},_interopRequireDefault$i=interopRequireDefaultExports;Object.defineProperty(ExpandMore,"__esModule",{value:!0});var default_1$i=ExpandMore.default=void 0,_createSvgIcon$i=_interopRequireDefault$i(requireCreateSvgIcon()),_jsxRuntime$i=jsxRuntimeExports;default_1$i=ExpandMore.default=(0,_createSvgIcon$i.default)((0,_jsxRuntime$i.jsx)("path",{d:"M16.59 8.59 12 13.17 7.41 8.59 6 10l6 6 6-6z"}),"ExpandMore");const MockEntry={videoTitle:"AI Brainstorm, Process Mining Revolution, Business Superpowers Unleashed!",thumbnailLarge:"https://picsum.photos/200/300",thumbnailSmall1:"https://picsum.photos/200/300",thumbnailSmall2:"https://picsum.photos/200/300",keywords:["Keywords","Keywords","Keywords"],description:"Description",videoScript:`Imagine you're driving a car. The car represents your business, and the journey represents your business processes. Now, traditionally, to navigate the journey, you'd need to understand maps, road signs, and maybe even some complex GPS equipment. This is like the traditional process mining - it's powerful, but it requires specific knowledge and skills.

  Now, imagine if your car had an advanced GPS system where you could just tell it where you want to go in plain language, and it would understand and guide you there. Not only that, but it could also understand complex requests like "find a route with the least traffic" or "find a route that passes by a gas station and a Chinese restaurant". This is what the AI in this research is doing for process mining. It's making it as easy to use as telling your GPS where you want to go.
  
  But there's more. This GPS isn't perfect. Sometimes it might not understand your request, or it might get confused by unusual road layouts. So, the researchers have developed a system to handle these situations, to correct errors, and to learn from them. This is like the AI's ability to handle complex queries, to generate meaningful responses, and to learn from its mistakes.
  
  So, in a nutshell, this research is about turning the complex map of process mining into an easy-to-use GPS system that anyone in your business can use to navigate your business processes.`};function ContentTab(){const a=MockEntry;return jsxRuntimeExports.jsxs(Box$1,{children:[jsxRuntimeExports.jsxs(Accordion$1,{children:[jsxRuntimeExports.jsx(AccordionSummary$1,{expandIcon:jsxRuntimeExports.jsx(default_1$i,{}),children:jsxRuntimeExports.jsx(Typography$1,{children:"Metadata"})}),jsxRuntimeExports.jsx(AccordionDetails$1,{children:jsxRuntimeExports.jsxs(Box$1,{children:[jsxRuntimeExports.jsx(TextField$1,{fullWidth:!0,label:"Video Title",value:a.videoTitle,sx:{marginTop:3}}),jsxRuntimeExports.jsx(TextField$1,{fullWidth:!0,label:"Keywords",value:a.keywords,multiline:!0,sx:{marginTop:3,marginBottom:3}}),jsxRuntimeExports.jsx(TextField$1,{fullWidth:!0,label:"Description",multiline:!0,rows:4,variant:"outlined",value:a.description,sx:{marginTop:3}})]})})]}),jsxRuntimeExports.jsxs(Accordion$1,{children:[jsxRuntimeExports.jsx(AccordionSummary$1,{expandIcon:jsxRuntimeExports.jsx(default_1$i,{}),children:jsxRuntimeExports.jsx(Typography$1,{children:"Video"})}),jsxRuntimeExports.jsx(AccordionDetails$1,{children:jsxRuntimeExports.jsx(Box$1,{children:jsxRuntimeExports.jsx(TextField$1,{fullWidth:!0,label:"Script",multiline:!0,minRows:5,variant:"outlined",value:a.videoScript,sx:{marginTop:3}})})})]}),jsxRuntimeExports.jsxs(Accordion$1,{children:[jsxRuntimeExports.jsx(AccordionSummary$1,{expandIcon:jsxRuntimeExports.jsx(default_1$i,{}),children:jsxRuntimeExports.jsx(Typography$1,{children:"Thumbnail"})}),jsxRuntimeExports.jsx(AccordionDetails$1,{children:jsxRuntimeExports.jsxs(Grid$1,{container:!0,spacing:3,alignItems:"center",children:[jsxRuntimeExports.jsx(Grid$1,{item:!0,xs:6,container:!0,justifyContent:"center",children:jsxRuntimeExports.jsxs(Box$1,{maxWidth:"1280px",width:"100%",children:[jsxRuntimeExports.jsx(CardMedia$1,{component:"img",image:a.thumbnailLarge,alt:"Large Thumbnail",style:{width:"100%",height:"auto",aspectRatio:"16/9"}}),jsxRuntimeExports.jsxs(Box$1,{display:"flex",justifyContent:"space-between",marginTop:2,children:[jsxRuntimeExports.jsxs(Select$1,{defaultValue:"white",children:[jsxRuntimeExports.jsx(MenuItem$1,{value:"white",children:"White"}),jsxRuntimeExports.jsx(MenuItem$1,{value:"black",children:"Black"})]}),jsxRuntimeExports.jsx(FormControlLabel$1,{control:jsxRuntimeExports.jsx(Checkbox$1,{}),label:"Seminal?"})]})]})}),jsxRuntimeExports.jsxs(Grid$1,{item:!0,xs:6,container:!0,direction:"column",spacing:2,children:[jsxRuntimeExports.jsx(Grid$1,{item:!0,container:!0,justifyContent:"center",children:jsxRuntimeExports.jsxs(Box$1,{maxWidth:"640px",width:"100%",children:[jsxRuntimeExports.jsx(Button$1,{children:"Reroll"}),jsxRuntimeExports.jsx(CardMedia$1,{component:"img",image:a.thumbnailSmall1,alt:"Small Thumbnail 1",style:{width:"100%",height:"auto",aspectRatio:"16/9"}}),jsxRuntimeExports.jsx(TextField$1,{fullWidth:!0,label:"Description"})]})}),jsxRuntimeExports.jsx(Grid$1,{item:!0,container:!0,justifyContent:"center",children:jsxRuntimeExports.jsxs(Box$1,{maxWidth:"640px",width:"100%",children:[jsxRuntimeExports.jsx(Button$1,{children:"Reroll"}),jsxRuntimeExports.jsx(CardMedia$1,{component:"img",image:a.thumbnailSmall2,alt:"Small Thumbnail 2",style:{width:"100%",height:"auto",aspectRatio:"16/9"}}),jsxRuntimeExports.jsx(TextField$1,{fullWidth:!0,label:"Description"})]})})]})]})})]})]})}const inputRefAtom=atom(null),promptPresetsOpenAtom=atom(!1),tokenUsageAtom=atom({document:10,total:10,max:128}),inputAtom=atom(""),inputEnabledAtom=atom(!0),messagesAtom=atom([{id:2,text:"Can this research help me bake a raspberry cheesecake?",timestamp:"2023-05-31T09:01:00Z",role:"user"},{id:3,text:"No.",timestamp:"2023-05-31T09:02:00Z",role:"assistant"}]),promptOptionsAtom=atomWithStorage("promptPresets",[{id:1,text:"Just give me the recipe damnit!"},{id:2,text:"Write me a very clear explanation of the core assertions, implications, and mechanics elucidated in this paper."},{id:3,text:"Write an analogy or metaphor that will help explain this paper to a broad audience."},{id:4,text:"Explain the value of this in basic terms like you're talking to a CEO. So what? What's the bottom line here?"}]),sendMessageAtom=atom(null,async(a,i,{paperId:o,threadId:s,text:$})=>{i(inputAtom,""),i(inputEnabledAtom,!1);const j={threadId:s,id:Date.now(),text:$,timestamp:new Date().toISOString(),role:"user"};i(messagesAtom,_e=>[..._e,j]);try{const et=(await sendMessage({paperId:o,threadId:s,text:$})).data;i(messagesAtom,it=>it.map(st=>st.id===j.id?{...st,id:et}:st));const nt=(await streamResponse({paperId:o,threadId:s,text:$})).data,at={threadId:s,id:nt,text:"...",role:"assistant",streaming:1};i(messagesAtom,it=>[...it,at])}catch(_e){console.error("Failed to send message",_e)}}),handleStreamStatusAtom=atom(null,async(a,i,{key:o,status:s,data:$,final:j})=>{if(!o){console.error("Message id not provided",o);return}i(messagesAtom,_e=>_e.map(et=>et.id===o?{...et,text:$,streaming:j?0:1}:et)),j&&i(inputEnabledAtom,!0)}),selectedThreadsAtom=atomWithStorage("selectedThread",{}),threadOptionsAtom=atom([{description:"Main thread",id:0}]);atom(null,async(a,i,o,s)=>{try{i(selectedThreadsAtom,_e=>({..._e,[o]:s})),i(chatStateAtom,"loading");const j=(await getMessages(s.id)).data;i(chatStateAtom,"ready"),i(messagesAtom,j)}catch($){i(chatStateAtom,"error"),console.error(`Failed to load chat data: ${o}`,$)}});const branchThreadAtom=atom(null,async(a,i,o,s)=>{const $=a(threadOptionsAtom),j={paperId:o,parentThreadId:s.threadId,messageId:s.id,description:s.text,id:`${$.length+1}`};i(threadOptionsAtom,_e=>[..._e,j]),i(selectedThreadsAtom,_e=>({..._e,[o]:j})),i(chatStateAtom,"loading");try{const _e=await branchThread(j),{messages:et,thread:tt}=_e.data;i(messagesAtom,et),i(threadOptionsAtom,nt=>nt.map(at=>at.id===j.id?{...at,...tt}:at)),i(selectedThreadsAtom,nt=>({...nt,[o]:tt})),i(chatStateAtom,"ready"),console.log("branched thread: ",tt)}catch(_e){console.error("Failed to create new thread",_e)}});atom(null,async(a,i,o)=>{const s=a(threadOptionsAtom),$={paperId:o,description:`Thread ${s.length+1}`,id:`${s.length+1}`};i(threadOptionsAtom,j=>[...j,$]),i(selectedThreadsAtom,j=>({...j,[o]:$})),i(messagesAtom,[]),i(chatStateAtom,"loading");try{const _e=(await createThread($)).data;i(threadOptionsAtom,et=>et.map(tt=>tt.id===$.id?{...tt,id:_e.id}:tt)),i(selectedThreadsAtom,et=>({...et,[o]:_e})),i(chatStateAtom,"ready"),console.log("new thread: ",_e)}catch(j){console.error("Failed to create new thread",j)}});const modelAtom=atom("gpt-4o"),chatStateAtom=atom("ready");atom(null,async(a,i,o)=>{var s,$;try{const j=(s=a(selectedThreadsAtom)[o])==null?void 0:s.id,_e=j==null,tt=(await getThreads(o)).data,nt=_e?($=tt[0])==null?void 0:$.id:j,it=(await getMessages(nt)).data,st=tt.find(rt=>rt.id===j)===void 0;i(messagesAtom,it),i(threadOptionsAtom,tt),(_e||st)&&i(selectedThreadsAtom,{[o]:tt[0]});const ct=(await initializeChat(o)).data;i(tokenUsageAtom,rt=>({...rt,document:ct})),i(chatStateAtom,"ready"),console.log("chat data: ",{messages:it,threads:tt,thread:nt})}catch(j){i(chatStateAtom,"error"),console.error(`Failed to load chat data: ${o}`,j)}});var AltRoute={},_interopRequireDefault$h=interopRequireDefaultExports;Object.defineProperty(AltRoute,"__esModule",{value:!0});var default_1$h=AltRoute.default=void 0,_createSvgIcon$h=_interopRequireDefault$h(requireCreateSvgIcon()),_jsxRuntime$h=jsxRuntimeExports;default_1$h=AltRoute.default=(0,_createSvgIcon$h.default)((0,_jsxRuntime$h.jsx)("path",{d:"m9.78 11.16-1.42 1.42c-.68-.69-1.34-1.58-1.79-2.94l1.94-.49c.32.89.77 1.5 1.27 2.01M11 6 7 2 3 6h3.02c.02.81.08 1.54.19 2.17l1.94-.49C8.08 7.2 8.03 6.63 8.02 6zm10 0-4-4-4 4h2.99c-.1 3.68-1.28 4.75-2.54 5.88-.5.44-1.01.92-1.45 1.55-.34-.49-.73-.88-1.13-1.24L9.46 13.6c.93.85 1.54 1.54 1.54 3.4v5h2v-5c0-2.02.71-2.66 1.79-3.63 1.38-1.24 3.08-2.78 3.2-7.37z"}),"AltRoute");const truncateText=(a,i)=>i?i.slice(0,a):"";function ThreadOptions(){const a=useAtomValue(paperAtom),i=useAtomValue(threadOptionsAtom),o=useSetAtom(featureDisabledAlertAtom),s=async()=>{o()},$=j=>{const _e=i.find(et=>et.id===j.target.value);!(a!=null&&a.id)||!j.target.value||!_e||o()};return jsxRuntimeExports.jsx(Box$1,{flex:1,pl:3,sx:{display:"flex",width:"40rem",justifyContent:"space-between"},children:jsxRuntimeExports.jsxs(FormControl$1,{color:"secondary",sx:{width:"20rem"},children:[jsxRuntimeExports.jsx(InputLabel$1,{id:"thread-select-label",sx:{color:"#9e9e9e !important"},children:"Thread"}),jsxRuntimeExports.jsxs(Select$1,{disabled:!0,labelId:"thread-select-label",value:0,label:"Thread",onChange:$,startAdornment:jsxRuntimeExports.jsx(default_1$h,{sx:{mr:1}}),children:[i.map(j=>jsxRuntimeExports.jsxs(MenuItem$1,{value:j.id,children:[j.description.length>25?truncateText(25,j.description)+"...":j.description,j.duplicateNumber?` [${j.duplicateNumber}]`:""]},j.id)),jsxRuntimeExports.jsx(Button$1,{variant:"contained",color:"secondary",onClick:s,sx:{width:"100%",mb:-.9,borderTop:"1px solid rgba(255, 255, 255, 0.1)",borderBottom:"3px solid rgba(0, 0, 0, 0.3 )",borderTopRightRadius:0,borderTopLeftRadius:0},children:"Start new thread"})]})]})})}function ChatOptions(){return jsxRuntimeExports.jsxs(Box$1,{bgcolor:colors.palette.background.paper,className:"flex row w-full pt-6 pl-3 pb-3",children:[jsxRuntimeExports.jsx(ModelOptions,{}),jsxRuntimeExports.jsx(ThreadOptions,{})]})}function ModelOptions(){const[a,i]=useAtom(modelAtom);return jsxRuntimeExports.jsx(Box$1,{sx:{display:"flex",flexDirection:"column"},children:jsxRuntimeExports.jsxs(FormControl$1,{sx:{width:"10rem"},children:[jsxRuntimeExports.jsx(InputLabel$1,{id:"model-select-label",children:"Model"}),jsxRuntimeExports.jsxs(Select$1,{disabled:!0,label:jsxRuntimeExports.jsx("span",{style:{color:"#9e9e9e"},children:"After Date"}),labelId:"model-select-label",value:a,onChange:o=>i(o.target.value),children:[jsxRuntimeExports.jsx(MenuItem$1,{value:"gpt-4o",children:"GPT-4o"}),jsxRuntimeExports.jsx(MenuItem$1,{value:"claude-sonnet",children:"Claude Sonnet"})]})]})})}var DeleteForever={},_interopRequireDefault$g=interopRequireDefaultExports;Object.defineProperty(DeleteForever,"__esModule",{value:!0});var default_1$g=DeleteForever.default=void 0,_createSvgIcon$g=_interopRequireDefault$g(requireCreateSvgIcon()),_jsxRuntime$g=jsxRuntimeExports;default_1$g=DeleteForever.default=(0,_createSvgIcon$g.default)((0,_jsxRuntime$g.jsx)("path",{d:"M6 19c0 1.1.9 2 2 2h8c1.1 0 2-.9 2-2V7H6zm2.46-7.12 1.41-1.41L12 12.59l2.12-2.12 1.41 1.41L13.41 14l2.12 2.12-1.41 1.41L12 15.41l-2.12 2.12-1.41-1.41L10.59 14zM15.5 4l-1-1h-5l-1 1H5v2h14V4z"}),"DeleteForever");var VisibilityOff$1={},_interopRequireDefault$f=interopRequireDefaultExports;Object.defineProperty(VisibilityOff$1,"__esModule",{value:!0});var default_1$f=VisibilityOff$1.default=void 0,_createSvgIcon$f=_interopRequireDefault$f(requireCreateSvgIcon()),_jsxRuntime$f=jsxRuntimeExports;default_1$f=VisibilityOff$1.default=(0,_createSvgIcon$f.default)((0,_jsxRuntime$f.jsx)("path",{d:"M12 7c2.76 0 5 2.24 5 5 0 .65-.13 1.26-.36 1.83l2.92 2.92c1.51-1.26 2.7-2.89 3.43-4.75-1.73-4.39-6-7.5-11-7.5-1.4 0-2.74.25-3.98.7l2.16 2.16C10.74 7.13 11.35 7 12 7M2 4.27l2.28 2.28.46.46C3.08 8.3 1.78 10.02 1 12c1.73 4.39 6 7.5 11 7.5 1.55 0 3.03-.3 4.38-.84l.42.42L19.73 22 21 20.73 3.27 3zM7.53 9.8l1.55 1.55c-.05.21-.08.43-.08.65 0 1.66 1.34 3 3 3 .22 0 .44-.03.65-.08l1.55 1.55c-.67.33-1.41.53-2.2.53-2.76 0-5-2.24-5-5 0-.79.2-1.53.53-2.2m4.31-.78 3.15 3.15.02-.16c0-1.66-1.34-3-3-3z"}),"VisibilityOff");var Visibility$1={},_interopRequireDefault$e=interopRequireDefaultExports;Object.defineProperty(Visibility$1,"__esModule",{value:!0});var default_1$e=Visibility$1.default=void 0,_createSvgIcon$e=_interopRequireDefault$e(requireCreateSvgIcon()),_jsxRuntime$e=jsxRuntimeExports;default_1$e=Visibility$1.default=(0,_createSvgIcon$e.default)((0,_jsxRuntime$e.jsx)("path",{d:"M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5M12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5m0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"}),"Visibility");var Cached={},_interopRequireDefault$d=interopRequireDefaultExports;Object.defineProperty(Cached,"__esModule",{value:!0});var default_1$d=Cached.default=void 0,_createSvgIcon$d=_interopRequireDefault$d(requireCreateSvgIcon()),_jsxRuntime$d=jsxRuntimeExports;default_1$d=Cached.default=(0,_createSvgIcon$d.default)((0,_jsxRuntime$d.jsx)("path",{d:"m19 8-4 4h3c0 3.31-2.69 6-6 6-1.01 0-1.97-.25-2.8-.7l-1.46 1.46C8.97 19.54 10.43 20 12 20c4.42 0 8-3.58 8-8h3zM6 12c0-3.31 2.69-6 6-6 1.01 0 1.97.25 2.8.7l1.46-1.46C15.03 4.46 13.57 4 12 4c-4.42 0-8 3.58-8 8H1l4 4 4-4z"}),"Cached");const StopCircleOutlined=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8-3.58 8-8 8m4-4H8V8h8z"}),"StopCircleOutlined"),Visibility=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5M12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5m0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"}),"Visibility"),VisibilityOff=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M12 7c2.76 0 5 2.24 5 5 0 .65-.13 1.26-.36 1.83l2.92 2.92c1.51-1.26 2.7-2.89 3.43-4.75-1.73-4.39-6-7.5-11-7.5-1.4 0-2.74.25-3.98.7l2.16 2.16C10.74 7.13 11.35 7 12 7M2 4.27l2.28 2.28.46.46C3.08 8.3 1.78 10.02 1 12c1.73 4.39 6 7.5 11 7.5 1.55 0 3.03-.3 4.38-.84l.42.42L19.73 22 21 20.73 3.27 3zM7.53 9.8l1.55 1.55c-.05.21-.08.43-.08.65 0 1.66 1.34 3 3 3 .22 0 .44-.03.65-.08l1.55 1.55c-.67.33-1.41.53-2.2.53-2.76 0-5-2.24-5-5 0-.79.2-1.53.53-2.2m4.31-.78 3.15 3.15.02-.16c0-1.66-1.34-3-3-3z"}),"VisibilityOff"),capitalize=a=>a&&a[0].toUpperCase()+a.slice(1)||"",actions=[{name:"regenerate",icon:jsxRuntimeExports.jsx(default_1$d,{}),color:"#9c27b0"},{name:"delete",icon:jsxRuntimeExports.jsx(default_1$g,{}),color:"#e53935"},{name:"show",icon:jsxRuntimeExports.jsx(default_1$e,{}),color:"#43a047"},{name:"hide",icon:jsxRuntimeExports.jsx(default_1$f,{}),color:"#fdd835"},{name:"thread",icon:jsxRuntimeExports.jsx(default_1$h,{}),color:"#1e88e5"},{name:"stop",icon:jsxRuntimeExports.jsx(StopCircleOutlined,{}),color:"yellow"}];function Actions({message:a}){const i=useSetAtom(featureDisabledAlertAtom);useAtomValue(paperAtom),useAtom(messagesAtom),useSetAtom(branchThreadAtom);const o={regenerate:j=>!j.streaming&&j.role==="assistant",show:j=>!j.streaming&&j.hidden,hide:j=>!j.streaming&&!j.hidden,thread:j=>!j.streaming,stop:j=>j.streaming&&j.role==="assistant"},s=actions.filter(j=>o[j.name]?o[j.name](a):!0),$={regenerate:i,stop:i,delete:i,show:i,hide:i,thread:i};return jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:s.map(j=>jsxRuntimeExports.jsx(Tooltip$1,{title:capitalize(j.name),placement:"top",children:jsxRuntimeExports.jsx(IconButton$1,{onClick:$[j.name],sx:{padding:".3rem",scale:".8","&:hover .MuiSvgIcon-root":{color:j.color}},children:j.icon})},j.name))})}function Message({message:a}){const[i,o]=reactExports.useState(!1),s=a.role==="assistant",$=a.hidden,j="rgba(255, 235, 59, 0.1)";colors.palette.background.paper;const _e=$?.4:1;return jsxRuntimeExports.jsxs(Box$1,{mb:2,p:2,sx:{textAlign:"left",whiteSpace:"pre-wrap",backgroundColor:$?j:s?"rgba(0, 0, 0, 0.1)":""},className:"",onMouseEnter:()=>o(!0),onMouseLeave:()=>o(!1),children:[jsxRuntimeExports.jsxs("div",{className:"flex items-center",children:[jsxRuntimeExports.jsx(Tooltip$1,{title:dayjs(a.timestamp).format("MMM D, YYYY h:mm A"),placement:"top",children:jsxRuntimeExports.jsx("p",{style:{fontWeight:"600",color:s?colors.palette.secondary.light:"rgba(255, 255, 255, 0.65)",opacity:_e},className:"pr-4  pb-1",children:s?"Assistant":"You"})}),i&&!a.stream&&jsxRuntimeExports.jsx(Actions,{message:a})]}),jsxRuntimeExports.jsx(Typography$1,{sx:{opacity:_e},children:a.text})]})}var Send={},_interopRequireDefault$c=interopRequireDefaultExports;Object.defineProperty(Send,"__esModule",{value:!0});var default_1$c=Send.default=void 0,_createSvgIcon$c=_interopRequireDefault$c(requireCreateSvgIcon()),_jsxRuntime$c=jsxRuntimeExports;default_1$c=Send.default=(0,_createSvgIcon$c.default)((0,_jsxRuntime$c.jsx)("path",{d:"M2.01 21 23 12 2.01 3 2 10l15 2-15 2z"}),"Send");var MoreVert={},_interopRequireDefault$b=interopRequireDefaultExports;Object.defineProperty(MoreVert,"__esModule",{value:!0});var default_1$b=MoreVert.default=void 0,_createSvgIcon$b=_interopRequireDefault$b(requireCreateSvgIcon()),_jsxRuntime$b=jsxRuntimeExports;default_1$b=MoreVert.default=(0,_createSvgIcon$b.default)((0,_jsxRuntime$b.jsx)("path",{d:"M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2m0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2m0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2"}),"MoreVert");const ChatInput=()=>{const[a,i]=useAtom(inputAtom);useSetAtom(sendMessageAtom);const[o,s]=useAtom(promptPresetsOpenAtom),$=useAtomValue(inputEnabledAtom);useAtomValue(paperAtom);const j=useAtomValue(chatStateAtom),_e=$&&j==="ready",et=useSetAtom(inputRefAtom),tt=useSetAtom(featureDisabledAlertAtom),nt=reactExports.useRef(null);reactExports.useEffect(()=>{et(nt)},[et]);const at=async()=>{j==="ready"&&a.trim()&&tt()},it=lt=>{!lt.shiftKey&&lt.key==="Enter"&&(lt.preventDefault(),at())},st=lt=>{s(!o)};return jsxRuntimeExports.jsxs(Box$1,{display:"flex",position:"relative",flexDirection:"column",children:[jsxRuntimeExports.jsx(TextField$1,{color:"secondary",inputRef:nt,disabled:!_e,multiline:!0,value:a,onChange:lt=>i(lt.target.value),onKeyDown:it,fullWidth:!0,placeholder:"Type a message",InputProps:{sx:{borderTopLeftRadius:0,borderTopRightRadius:0},startAdornment:jsxRuntimeExports.jsx(IconButton$1,{sx:{mr:1},disabled:!_e,onClick:st,className:"menu-toggle-button",children:jsxRuntimeExports.jsx(default_1$b,{})}),endAdornment:jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:jsxRuntimeExports.jsx(IconButton$1,{disabled:!_e,onClick:at,children:jsxRuntimeExports.jsx(default_1$c,{})})})}}),jsxRuntimeExports.jsx(TokenUsage,{})]})},TokenUsage=()=>{const[a,i]=useAtom(tokenUsageAtom),o=a.total>a.max*.95,s=a.total>a.max*.7;return jsxRuntimeExports.jsxs(Typography$1,{variant:"caption",mt:1,mb:3,pl:1,sx:{opacity:".7",color:s?"orange":o?"red":""},children:["Token estimate 10k / ",a.max,"k"]})};var Add={},_interopRequireDefault$a=interopRequireDefaultExports;Object.defineProperty(Add,"__esModule",{value:!0});var default_1$a=Add.default=void 0,_createSvgIcon$a=_interopRequireDefault$a(requireCreateSvgIcon()),_jsxRuntime$a=jsxRuntimeExports;default_1$a=Add.default=(0,_createSvgIcon$a.default)((0,_jsxRuntime$a.jsx)("path",{d:"M19 13h-6v6h-2v-6H5v-2h6V5h2v6h6z"}),"Add");var Close={},_interopRequireDefault$9=interopRequireDefaultExports;Object.defineProperty(Close,"__esModule",{value:!0});var default_1$9=Close.default=void 0,_createSvgIcon$9=_interopRequireDefault$9(requireCreateSvgIcon()),_jsxRuntime$9=jsxRuntimeExports;default_1$9=Close.default=(0,_createSvgIcon$9.default)((0,_jsxRuntime$9.jsx)("path",{d:"M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"}),"Close");const PromptMenu=()=>{const[a,i]=useAtom(promptOptionsAtom),o=useSetAtom(promptPresetsOpenAtom),s=useSetAtom(inputAtom),$=useAtomValue(inputRefAtom),j=()=>{o(!1)},_e=tt=>{var nt;s(tt.text),j(),(nt=$==null?void 0:$.current)==null||nt.focus()},et=tt=>nt=>{nt.stopPropagation(),i(a.filter((at,it)=>it!==tt))};return reactExports.useEffect(()=>{const tt=nt=>{const at=it=>it.classList.contains("menu-toggle-button")||it.closest(".menu-toggle-button")!=null;!nt.target.closest("#autofill-menu")&&!at(nt.target)&&j()};return document.addEventListener("mousedown",tt),()=>{document.removeEventListener("mousedown",tt)}},[]),jsxRuntimeExports.jsxs("div",{id:"autofill-menu",className:"absolute w-full z-50 left-0 bottom-0",children:[jsxRuntimeExports.jsx(AddPrompt,{}),jsxRuntimeExports.jsx(Box$1,{width:"100%",bgcolor:"background.paper",boxShadow:3,sx:{maxHeight:"300px",overflowY:"auto",whiteSpace:"pre-wrap",borderRight:"1px solid rgba(57, 61, 64, .3)",borderLeft:"1px solid rgba(57, 61, 64, .3)",borderTop:"1px solid rgba(57, 61, 64, .3)"},children:jsxRuntimeExports.jsx(List$2,{children:a.map((tt,nt)=>jsxRuntimeExports.jsxs(ListItem$1,{onClick:()=>_e(tt),sx:{borderBottom:nt===a.length-1?"none":"1px solid rgba(57, 61, 64, .3)",cursor:"pointer",display:"flex",justifyContent:"space-between"},children:[jsxRuntimeExports.jsx(Typography$1,{children:tt.text}),jsxRuntimeExports.jsx(Tooltip$1,{title:"Remove",children:jsxRuntimeExports.jsx(IconButton$1,{onClick:et(nt),children:jsxRuntimeExports.jsx(default_1$9,{})})})]},nt))})})]})};function AddPrompt(){const[a,i]=useAtom(promptOptionsAtom),[o,s]=reactExports.useState(""),[$,j]=reactExports.useState(!1),_e=()=>{o.trim()&&(i([{text:o.trim()},...a]),s("")),j(!1)},et=tt=>{!tt.shiftKey&&tt.key==="Enter"&&(tt.preventDefault(),_e())};return jsxRuntimeExports.jsx("div",{children:$?jsxRuntimeExports.jsx(Box$1,{display:"flex",alignItems:"center",bgcolor:"background.paper",children:jsxRuntimeExports.jsx(TextField$1,{color:"secondary",autoFocus:!0,multiline:!0,value:o,onChange:tt=>s(tt.target.value),fullWidth:!0,onKeyDown:et,InputProps:{sx:{borderRadius:0,borderBottom:"none"},endAdornment:jsxRuntimeExports.jsx(IconButton$1,{onClick:_e,children:jsxRuntimeExports.jsx(default_1$a,{})})}})}):jsxRuntimeExports.jsx(Button$1,{variant:"contained",color:"secondary",onClick:()=>j(!$),startIcon:jsxRuntimeExports.jsx(default_1$a,{}),sx:{borderBottomLeftRadius:0,borderBottomRightRadius:0,borderTopLeftRadius:0},children:"Add New Prompt"})})}function MessageList(){const a=useAtomValue(promptPresetsOpenAtom),i=useAtomValue(messagesAtom),[o]=useAtom(scrollableContainerRefAtom),s=useAtomValue(chatStateAtom),$=s==="loading",j=s==="error";return reactExports.useEffect(()=>{const _e=o==null?void 0:o.current;_e&&setTimeout(()=>{_e.scrollTo({top:_e.scrollHeight,behavior:"smooth"})},250)},[o]),jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsxs("div",{className:"relative",children:[(a||$)&&jsxRuntimeExports.jsx(Box$1,{sx:{position:"absolute",top:0,left:0,right:0,bottom:0,backgroundColor:"rgba(0, 0, 0, 0.6)",zIndex:1}}),j&&jsxRuntimeExports.jsx(Box$1,{sx:{position:"absolute",top:0,left:0,right:0,bottom:0,backgroundColor:"rgba(255, 0, 0, 0.1)",zIndex:1}}),jsxRuntimeExports.jsx(Box$1,{sx:{display:"flex",flexDirection:"column-reverse",width:"100%",height:"420px",maxHeight:"500px",overflowY:"auto",borderRight:"1px solid rgba(57, 61, 64, .3)",borderLeft:"1px solid rgba(57, 61, 64, .3)"},children:$?jsxRuntimeExports.jsx(Loader,{}):jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:i.slice().reverse().map(_e=>jsxRuntimeExports.jsx(Message,{message:_e},_e.id))})}),a&&jsxRuntimeExports.jsx(PromptMenu,{})]}),jsxRuntimeExports.jsx(ChatInput,{})]})}function Loader(){return jsxRuntimeExports.jsx(Box$1,{sx:{display:"flex",justifyContent:"center",alignItems:"center",height:"100%",width:"100%"},children:jsxRuntimeExports.jsx(CircularProgress$1,{})})}function ChatTab({paperId:a}){return jsxRuntimeExports.jsxs(Box$1,{sx:{marginTop:2,mb:1},children:[jsxRuntimeExports.jsx(ChatOptions,{}),jsxRuntimeExports.jsx(MessageList,{})]})}const orEmpty=a=>a||"",PaperEntryPage=()=>{let{paperId:a}=useParams();a=orEmpty(a);const[,i]=useAtom(scrollableContainerRefAtom),[,o]=useAtom(fetchPaperAtom),[s]=useAtom(paperAtom),[$,j]=useAtom(pageStateAtom),_e=useSetAtom(updatePaperAtom),et=useSetAtom(handleStreamStatusAtom),tt=reactExports.useRef(null);return reactExports.useEffect(()=>{i(tt)},[i]),reactExports.useEffect(()=>{const nt=at=>{const{id:it,changes:st}=at.detail,{field:lt,value:ct}=st;_e({paperAtom,id:it,field:lt,newValue:ct})};return window.addEventListener("paperUpdate",nt),()=>{j("loading"),window.removeEventListener("paperUpdate",nt)}},[]),reactExports.useEffect(()=>{o(a)},[o]),jsxRuntimeExports.jsxs(PageLayout,{ref:tt,padding:3,children:[$==="error"?jsxRuntimeExports.jsx(PaperTitle,{title:`Error Loading Paper ${a}`,id:null}):jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsxs(Box$1,{display:"flex",justifyContent:"center",flexDirection:"column",marginBottom:1,children:[jsxRuntimeExports.jsx(DateAuthorsPdf,{paper:s}),jsxRuntimeExports.jsx(PaperTitle,{title:s==null?void 0:s.title,id:s==null?void 0:s.id}),jsxRuntimeExports.jsx(Typography$1,{variant:"body1",paragraph:!0,children:orEmpty(s==null?void 0:s.abstract)})]}),jsxRuntimeExports.jsx(TabSection,{paperId:a}),jsxRuntimeExports.jsx(PdfModal,{paperId:s==null?void 0:s.id})]}),jsxRuntimeExports.jsx(SocketListener,{eventName:"chat_status",handleEvent:et})]})},TabSection=({paperId:a})=>{const[i,o]=reactExports.useState("chat");return jsxRuntimeExports.jsxs(Box$1,{children:[i==="chat"&&jsxRuntimeExports.jsx(ChatTab,{paperId:a}),"  ",i==="content"&&jsxRuntimeExports.jsx(ContentTab,{})]})},results=[{id:"2405.18208",date:"2024-05-28",title:`A Human-Like Reasoning Framework for Multi-Phases Planning Task with
  Large Language Models`,abstract:`  Recent studies have highlighted their proficiency in some simple tasks like
writing and coding through various reasoning strategies. However, LLM agents
still struggle with tasks that require comprehensive planning, a process that
challenges current models and remains a critical research issue. In this study,
we concentrate on travel planning, a Multi-Phases planning problem, that
involves multiple interconnected stages, such as outlining, information
gathering, and planning, often characterized by the need to manage various
constraints and uncertainties. Existing reasoning approaches have struggled to
effectively address this complex task. Our research aims to address this
challenge by developing a human-like planning framework for LLM agents, i.e.,
guiding the LLM agent to simulate various steps that humans take when solving
Multi-Phases problems. Specifically, we implement several strategies to enable
LLM agents to generate a coherent outline for each travel query, mirroring
human planning patterns. Additionally, we integrate Strategy Block and
Knowledge Block into our framework: Strategy Block facilitates information
collection, while Knowledge Block provides essential information for detailed
planning. Through our extensive experiments, we demonstrate that our framework
significantly improves the planning capabilities of LLM agents, enabling them
to tackle the travel planning task with improved efficiency and effectiveness.
Our experimental results showcase the exceptional performance of the proposed
framework; when combined with GPT-4-Turbo, it attains $10\\times$ the
performance gains in comparison to the baseline framework deployed on
GPT-4-Turbo.
`,authors:"Chengxing Xie; Difan Zou",status:0,relevancy:.6687598875397698,isStarred:1,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:52:20.960 +00:00"},{id:"2405.20309",date:"2024-05-30",title:"Large Language Models Can Self-Improve At Web Agent Tasks",abstract:`  Training models to act as agents that can effectively navigate and perform
actions in a complex environment, such as a web browser, has typically been
challenging due to lack of training data. Large language models (LLMs) have
recently demonstrated some capability to navigate novel environments as agents
in a zero-shot or few-shot fashion, purely guided by natural language
instructions as prompts. Recent research has also demonstrated LLMs have the
capability to exceed their base performance through self-improvement, i.e.
fine-tuning on data generated by the model itself. In this work, we explore the
extent to which LLMs can self-improve their performance as agents in
long-horizon tasks in a complex environment using the WebArena benchmark. In
WebArena, an agent must autonomously navigate and perform actions on web pages
to achieve a specified objective. We explore fine-tuning on three distinct
synthetic training data mixtures and achieve a 31\\% improvement in task
completion rate over the base model on the WebArena benchmark through a
self-improvement procedure. We additionally contribute novel evaluation metrics
for assessing the performance, robustness, capabilities, and quality of
trajectories of our fine-tuned agent models to a greater degree than simple,
aggregate-level benchmark scores currently used to measure self-improvement.
`,authors:"Ajay Patel; Markus Hofmarcher; Claudiu Leoveanu-Condrei; Marius-Constantin Dinu; Chris Callison-Burch; Sepp Hochreiter",status:0,relevancy:.6260053599812707,isStarred:1,keywords:null,createdAt:"2024-05-31 04:58:28.951 +00:00",updatedAt:"2024-05-31 07:09:19.690 +00:00"},{id:"2405.17974",date:"2024-05-28",title:`Recent Trends in Personalized Dialogue Generation: A Review of Datasets,
  Methodologies, and Evaluations`,abstract:`  Enhancing user engagement through personalization in conversational agents
has gained significance, especially with the advent of large language models
that generate fluent responses. Personalized dialogue generation, however, is
multifaceted and varies in its definition -- ranging from instilling a persona
in the agent to capturing users' explicit and implicit cues. This paper seeks
to systemically survey the recent landscape of personalized dialogue
generation, including the datasets employed, methodologies developed, and
evaluation metrics applied. Covering 22 datasets, we highlight benchmark
datasets and newer ones enriched with additional features. We further analyze
17 seminal works from top conferences between 2021-2023 and identify five
distinct types of problems. We also shed light on recent progress by LLMs in
personalized dialogue generation. Our evaluation section offers a comprehensive
summary of assessment facets and metrics utilized in these works. In
conclusion, we discuss prevailing challenges and envision prospect directions
for future research in personalized dialogue generation.
`,authors:"Yi-Pei Chen; Noriki Nishida; Hideki Nakayama; Yuji Matsumoto",status:0,relevancy:.658808673281338,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:52:09.160 +00:00"},{id:"2405.18110",date:"2024-05-28",title:`Individual Contributions as Intrinsic Exploration Scaffolds for
  Multi-agent Reinforcement Learning`,abstract:`  In multi-agent reinforcement learning (MARL), effective exploration is
critical, especially in sparse reward environments. Although introducing global
intrinsic rewards can foster exploration in such settings, it often complicates
credit assignment among agents. To address this difficulty, we propose
Individual Contributions as intrinsic Exploration Scaffolds (ICES), a novel
approach to motivate exploration by assessing each agent's contribution from a
global view. In particular, ICES constructs exploration scaffolds with Bayesian
surprise, leveraging global transition information during centralized training.
These scaffolds, used only in training, help to guide individual agents towards
actions that significantly impact the global latent state transitions.
Additionally, ICES separates exploration policies from exploitation policies,
enabling the former to utilize privileged global information during training.
Extensive experiments on cooperative benchmark tasks with sparse rewards,
including Google Research Football (GRF) and StarCraft Multi-agent Challenge
(SMAC), demonstrate that ICES exhibits superior exploration capabilities
compared with baselines. The code is publicly available at
https://github.com/LXXXXR/ICES.
`,authors:"Xinran Li; Zifan Liu; Shibo Chen; Jun Zhang",status:0,relevancy:.610816543969512,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.18118",date:"2024-05-28",title:`An approach to improve agent learning via guaranteeing goal reaching in
  all episodes`,abstract:`  Reinforcement learning is commonly concerned with problems of maximizing
accumulated rewards in Markov decision processes. Oftentimes, a certain goal
state or a subset of the state space attain maximal reward. In such a case, the
environment may be considered solved when the goal is reached. Whereas numerous
techniques, learning or non-learning based, exist for solving environments,
doing so optimally is the biggest challenge. Say, one may choose a reward rate
which penalizes the action effort. Reinforcement learning is currently among
the most actively developed frameworks for solving environments optimally by
virtue of maximizing accumulated reward, in other words, returns. Yet, tuning
agents is a notoriously hard task as reported in a series of works. Our aim
here is to help the agent learn a near-optimal policy efficiently while
ensuring a goal reaching property of some basis policy that merely solves the
environment. We suggest an algorithm, which is fairly flexible, and can be used
to augment practically any agent as long as it comprises of a critic. A formal
proof of a goal reaching property is provided. Simulation experiments on six
problems under five agents, including the benchmarked one, provided an
empirical evidence that the learning can indeed be boosted while ensuring goal
reaching property.
`,authors:"Pavel Osinenko; Grigory Yaremenko; Georgiy Malaniya; Anton Bolychev",status:0,relevancy:.599505902476585,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.17243",date:"2024-05-27",title:`Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement
  Learning`,abstract:`  Both entropy-minimizing and entropy-maximizing (curiosity) objectives for
unsupervised reinforcement learning (RL) have been shown to be effective in
different environments, depending on the environment's level of natural
entropy. However, neither method alone results in an agent that will
consistently learn intelligent behavior across environments. In an effort to
find a single entropy-based method that will encourage emergent behaviors in
any environment, we propose an agent that can adapt its objective online,
depending on the entropy conditions by framing the choice as a multi-armed
bandit problem. We devise a novel intrinsic feedback signal for the bandit,
which captures the agent's ability to control the entropy in its environment.
We demonstrate that such agents can learn to control entropy and exhibit
emergent behaviors in both high- and low-entropy regimes and can learn skillful
behaviors in benchmark tasks. Videos of the trained agents and summarized
findings can be found on our project page
https://sites.google.com/view/surprise-adaptive-agents
`,authors:"Adriana Hugessen; Roger Creus Castanyer; Faisal Mohamed; Glen Berseth",status:0,relevancy:.5958386501330759,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.16751",date:"2024-05-27",title:`LLM-Based Cooperative Agents using Information Relevance and Plan
  Validation`,abstract:`  We address the challenge of multi-agent cooperation, where agents achieve a
common goal by interacting with a 3D scene and cooperating with decentralized
agents under complex partial observations. This involves managing communication
costs and optimizing interaction trajectories in dynamic environments. Our
research focuses on three primary limitations of existing cooperative agent
systems. Firstly, current systems demonstrate inefficiency in managing acquired
information through observation, resulting in declining planning performance as
the environment becomes more complex with additional objects or goals.
Secondly, the neglect of false plans in partially observable settings leads to
suboptimal cooperative performance, as agents struggle to adapt to
environmental changes influenced by the unseen actions of other agents. Lastly,
the failure to incorporate spatial data into decision-making processes
restricts the agent's ability to construct optimized trajectories. To overcome
these limitations, we propose the RElevance and Validation-Enhanced Cooperative
Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.
REVECA leverages relevance assessment, plan validation, and spatial information
to enhance the efficiency and robustness of agent cooperation in dynamic and
partially observable environments while minimizing continuous communication
costs and effectively managing irrelevant dummy objects. Our extensive
experiments demonstrate the superiority of REVECA over previous approaches,
including those driven by GPT-4.0. Additionally, a user study highlights
REVECA's potential for achieving trustworthy human-AI cooperation. We expect
that REVECA will have significant applications in gaming, XR applications,
educational tools, and humanoid robots, contributing to substantial economic,
commercial, and academic advancements.
`,authors:"SeungWon Seo; Junhyeok Lee; SeongRae Noh; HyeongYeop Kang",status:0,relevancy:.5720936333177186,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.17009",date:"2024-05-27",title:"Position: Foundation Agents as the Paradigm Shift for Decision Making",abstract:`  Decision making demands intricate interplay between perception, memory, and
reasoning to discern optimal policies. Conventional approaches to decision
making face challenges related to low sample efficiency and poor
generalization. In contrast, foundation models in language and vision have
showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the
construction of foundation agents as a transformative shift in the learning
paradigm of agents. This proposal is underpinned by the formulation of
foundation agents with their fundamental characteristics and challenges
motivated by the success of large language models (LLMs). Moreover, we specify
the roadmap of foundation agents from large interactive data collection or
generation, to self-supervised pretraining and adaptation, and knowledge and
value alignment with LLMs. Lastly, we pinpoint critical research questions
derived from the formulation and delineate trends for foundation agents
supported by real-world use cases, addressing both technical and theoretical
aspects to propel the field towards a more comprehensive and impactful future.
`,authors:"Xiaoqian Liu; Xingzhou Lou; Jianbin Jiao; Junge Zhang",status:0,relevancy:.5700282258642639,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.20318",date:"2024-05-30",title:"CausalQuest: Collecting Natural Causal Questions for AI Agents",abstract:`  Humans have an innate drive to seek out causality. Whether fuelled by
curiosity or specific goals, we constantly question why things happen, how they
are interconnected, and many other related phenomena. To develop AI agents
capable of addressing this natural human quest for causality, we urgently need
a comprehensive dataset of natural causal questions. Unfortunately, existing
datasets either contain only artificially-crafted questions that do not reflect
real AI usage scenarios or have limited coverage of questions from specific
sources. To address this gap, we present CausalQuest, a dataset of 13,500
naturally occurring questions sourced from social networks, search engines, and
AI assistants. We formalize the definition of causal questions and establish a
taxonomy for finer-grained classification. Through a combined effort of human
annotators and large language models (LLMs), we carefully label the dataset. We
find that 42% of the questions humans ask are indeed causal, with the majority
seeking to understand the causes behind given effects. Using this dataset, we
train efficient classifiers (up to 2.85B parameters) for the binary task of
identifying causal questions, achieving high performance with F1 scores of up
to 0.877. We conclude with a rich set of future research directions that can
build upon our data and models.
`,authors:"Roberto Ceraolo; Dmitrii Kharlapenko; Amélie Reymond; Rada Mihalcea; Mrinmaya Sachan; Bernhard Schölkopf; Zhijing Jin",status:0,relevancy:.5681685308949704,isStarred:0,keywords:null,createdAt:"2024-05-31 04:58:28.951 +00:00",updatedAt:"2024-05-31 04:58:28.951 +00:00"},{id:"2405.20189",date:"2024-05-30",title:`Nadine: An LLM-driven Intelligent Social Robot with Affective
  Capabilities and Human-like Memory`,abstract:`  In this work, we describe our approach to developing an intelligent and
robust social robotic system for the Nadine social robot platform. We achieve
this by integrating Large Language Models (LLMs) and skilfully leveraging the
powerful reasoning and instruction-following capabilities of these types of
models to achieve advanced human-like affective and cognitive capabilities.
This approach is novel compared to the current state-of-the-art LLM-based
agents which do not implement human-like long-term memory or sophisticated
emotional appraisal. The naturalness of social robots, consisting of multiple
modules, highly depends on the performance and capabilities of each component
of the system and the seamless integration of the components. We built a social
robot system that enables generating appropriate behaviours through multimodal
input processing, bringing episodic memories accordingly to the recognised
user, and simulating the emotional states of the robot induced by the
interaction with the human partner. In particular, we introduce an LLM-agent
frame for social robots, SoR-ReAct, serving as a core component for the
interaction module in our system. This design has brought forth the advancement
of social robots and aims to increase the quality of human-robot interaction.
`,authors:"Hangyeol Kang; Maher Ben Moussa; Nadia Magnenat-Thalmann",status:0,relevancy:.5658114955122293,isStarred:0,keywords:null,createdAt:"2024-05-31 04:58:28.951 +00:00",updatedAt:"2024-05-31 04:58:28.951 +00:00"},{id:"2405.18123",date:"2024-05-28",title:"PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning",abstract:`  Modern Tabletop Games present various interesting challenges for Multi-agent
Reinforcement Learning. In this paper, we introduce PyTAG, a new framework that
supports interacting with a large collection of games implemented in the
Tabletop Games framework. In this work we highlight the challenges tabletop
games provide, from a game-playing agent perspective, along with the
opportunities they provide for future research. Additionally, we highlight the
technical challenges that involve training Reinforcement Learning agents on
these games. To explore the Multi-agent setting provided by PyTAG we train the
popular Proximal Policy Optimisation Reinforcement Learning algorithm using
self-play on a subset of games and evaluate the trained policies against some
simple agents and Monte-Carlo Tree Search implemented in the Tabletop Games
framework.
`,authors:"Martin Balla; George E. M. Long; James Goodman; Raluca D. Gaina; Diego Perez-Liebana",status:0,relevancy:.5615817257458973,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.17287",date:"2024-05-27",title:"Opinion-Guided Reinforcement Learning",abstract:`  Human guidance is often desired in reinforcement learning to improve the
performance of the learning agent. However, human insights are often mere
opinions and educated guesses rather than well-formulated arguments. While
opinions are subject to uncertainty, e.g., due to partial informedness or
ignorance about a problem, they also emerge earlier than hard evidence could be
produced. Thus, guiding reinforcement learning agents through opinions offers
the potential for more performant learning processes, but comes with the
challenge of modeling and managing opinions in a formal way. In this article,
we present a method to guide reinforcement learning agents through opinions. To
this end, we provide an end-to-end method to model and manage advisors'
opinions. To assess the utility of the approach, we evaluate it with synthetic
and human advisors, at different levels of uncertainty, and under multiple
advise strategies. Our results indicate that opinions, even if uncertain,
improve the performance of reinforcement learning agents, resulting in higher
rewards, more efficient exploration, and a better reinforced policy. Although
we demonstrate our approach in a simplified topological running example, our
approach is applicable to complex problems with higher dimensions as well.
`,authors:"Kyanna Dagenais; Istvan David",status:0,relevancy:.5491430684706172,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.18650",date:"2024-05-28",title:"Approximating Human Models During Argumentation-based Dialogues",abstract:`  Explainable AI Planning (XAIP) aims to develop AI agents that can effectively
explain their decisions and actions to human users, fostering trust and
facilitating human-AI collaboration. A key challenge in XAIP is model
reconciliation, which seeks to align the mental models of AI agents and humans.
While existing approaches often assume a known and deterministic human model,
this simplification may not capture the complexities and uncertainties of
real-world interactions. In this paper, we propose a novel framework that
enables AI agents to learn and update a probabilistic human model through
argumentation-based dialogues. Our approach incorporates trust-based and
certainty-based update mechanisms, allowing the agent to refine its
understanding of the human's mental state based on the human's expressed trust
in the agent's arguments and certainty in their own arguments. We employ a
probability weighting function inspired by prospect theory to capture the
relationship between trust and perceived probability, and use a Bayesian
approach to update the agent's probability distribution over possible human
models. We conduct a human-subject study to empirically evaluate the
effectiveness of our approach in an argumentation scenario, demonstrating its
ability to capture the dynamics of human belief formation and adaptation.
`,authors:"Yinxu Tang; Stylianos Loukas Vasileiou; William Yeoh",status:0,relevancy:.5392468235525845,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.18688",date:"2024-05-29",title:`Efficient Preference-based Reinforcement Learning via Aligned Experience
  Estimation`,abstract:`  Preference-based reinforcement learning (PbRL) has shown impressive
capabilities in training agents without reward engineering. However, a notable
limitation of PbRL is its dependency on substantial human feedback. This
dependency stems from the learning loop, which entails accurate reward learning
compounded with value/policy learning, necessitating a considerable number of
samples. To boost the learning loop, we propose SEER, an efficient PbRL method
that integrates label smoothing and policy regularization techniques. Label
smoothing reduces overfitting of the reward model by smoothing human preference
labels. Additionally, we bootstrap a conservative estimate $\\widehat{Q}$ using
well-supported state-action pairs from the current replay memory to mitigate
overestimation bias and utilize it for policy learning regularization. Our
experimental results across a variety of complex tasks, both in online and
offline settings, demonstrate that our approach improves feedback efficiency,
outperforming state-of-the-art methods by a large margin. Ablation studies
further reveal that SEER achieves a more accurate Q-function compared to prior
work.
`,authors:"Fengshuo Bai; Rui Zhao; Hongming Zhang; Sijia Cui; Ying Wen; Yaodong Yang; Bo Xu; Lei Han",status:0,relevancy:.5366568000368498,isStarred:0,keywords:null,createdAt:"2024-05-31 05:21:04.773 +00:00",updatedAt:"2024-05-31 05:21:04.773 +00:00"},{id:"2405.18092",date:"2024-05-28",title:`LLM experiments with simulation: Large Language Model Multi-Agent System
  for Process Simulation Parametrization in Digital Twins`,abstract:`  This paper presents a novel design of a multi-agent system framework that
applies a large language model (LLM) to automate the parametrization of process
simulations in digital twins. We propose a multi-agent framework that includes
four types of agents: observation, reasoning, decision and summarization. By
enabling dynamic interaction between LLM agents and simulation model, the
developed system can automatically explore the parametrization of the
simulation and use heuristic reasoning to determine a set of parameters to
control the simulation to achieve an objective. The proposed approach enhances
the simulation model by infusing it with heuristics from LLM and enables
autonomous search for feasible parametrization to solve a user task.
Furthermore, the system has the potential to increase user-friendliness and
reduce the cognitive load on human users by assisting in complex
decision-making processes. The effectiveness and functionality of the system
are demonstrated through a case study, and the visualized demos are available
at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation
`,authors:"Yuchen Xia; Daniel Dittler; Nasser Jazdi; Haonan Chen; Michael Weyrich",status:0,relevancy:.5365061176244996,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.18721",date:"2024-05-29",title:`Correctable Landmark Discovery via Large Models for Vision-Language
  Navigation`,abstract:`  Vision-Language Navigation (VLN) requires the agent to follow language
instructions to reach a target position. A key factor for successful navigation
is to align the landmarks implied in the instruction with diverse visual
observations. However, previous VLN agents fail to perform accurate modality
alignment especially in unexplored scenes, since they learn from limited
navigation data and lack sufficient open-world alignment knowledge. In this
work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via
Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential
landmark discovery problem, by introducing a novel correctable landmark
discovery scheme based on two large models ChatGPT and CLIP. Specifically, we
use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and
conduct CLIP-driven landmark discovery based on these commonsense priors. To
mitigate the noise in the priors due to the lack of visual constraints, we
introduce a learnable cooccurrence scoring module, which corrects the
importance of each cooccurrence according to actual observations for accurate
landmark discovery. We further design an observation enhancement strategy for
an elegant combination of our framework with different VLN agents, where we
utilize the corrected landmark features to obtain enhanced observation features
for action decision. Extensive experimental results on multiple popular VLN
benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE
over strong baselines. Especially, our CONSOLE establishes the new
state-of-the-art results on R2R and R4R in unseen scenarios. Code is available
at https://github.com/expectorlin/CONSOLE.
`,authors:"Bingqian Lin; Yunshuang Nie; Ziming Wei; Yi Zhu; Hang Xu; Shikui Ma; Jianzhuang Liu; Xiaodan Liang",status:0,relevancy:.5362481468861078,isStarred:0,keywords:null,createdAt:"2024-05-31 05:21:04.773 +00:00",updatedAt:"2024-05-31 05:21:04.773 +00:00"},{id:"2405.19946",date:"2024-05-30",title:`Learning to Discuss Strategically: A Case Study on One Night Ultimate
  Werewolf`,abstract:`  Communication is a fundamental aspect of human society, facilitating the
exchange of information and beliefs among people. Despite the advancements in
large language models (LLMs), recent agents built with these often neglect the
control over discussion tactics, which are essential in communication scenarios
and games. As a variant of the famous communication game Werewolf, One Night
Ultimate Werewolf (ONUW) requires players to develop strategic discussion
policies due to the potential role changes that increase the uncertainty and
complexity of the game. In this work, we first present the existence of the
Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with
discussion and one without. The results showcase that the discussion greatly
changes players' utilities by affecting their beliefs, emphasizing the
significance of discussion tactics. Based on the insights obtained from the
analyses, we propose an RL-instructed language agent framework, where a
discussion policy trained by reinforcement learning (RL) is employed to
determine appropriate discussion tactics to adopt. Our experimental results on
several ONUW game settings demonstrate the effectiveness and generalizability
of our proposed framework.
`,authors:"Xuanfa Jin; Ziyan Wang; Yali Du; Meng Fang; Haifeng Zhang; Jun Wang",status:0,relevancy:.5235238922992325,isStarred:0,keywords:null,createdAt:"2024-05-31 04:58:28.951 +00:00",updatedAt:"2024-05-31 04:58:28.951 +00:00"},{id:"2405.16766",date:"2024-05-27",title:"Reframing the Relationship in Out-of-Distribution Detection",abstract:`  The remarkable achievements of Large Language Models (LLMs) have captivated
the attention of both academia and industry, transcending their initial role in
dialogue generation. The utilization of LLMs as intermediary agents in various
tasks has yielded promising results, sparking a wave of innovation in
artificial intelligence. Building on these breakthroughs, we introduce a novel
approach that integrates the agent paradigm into the Out-of-distribution (OOD)
detection task, aiming to enhance its robustness and adaptability. Our proposed
method, Concept Matching with Agent (CMA), employs neutral prompts as agents to
augment the CLIP-based OOD detection process. These agents function as dynamic
observers and communication hubs, interacting with both In-distribution (ID)
labels and data inputs to form vector triangle relationships. This triangular
framework offers a more nuanced approach than the traditional binary
relationship, allowing for better separation and identification of ID and OOD
inputs. Our extensive experimental results showcase the superior performance of
CMA over both zero-shot and training-required methods in a diverse array of
real-world scenarios.
`,authors:"YuXiao Lee; Xiaofeng Cao",status:0,relevancy:.5193883135881757,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.16994",date:"2024-05-27",title:"Vision-and-Language Navigation Generative Pretrained Transformer",abstract:`  In the Vision-and-Language Navigation (VLN) field, agents are tasked with
navigating real-world scenes guided by linguistic instructions. Enabling the
agent to adhere to instructions throughout the process of navigation represents
a significant challenge within the domain of VLN. To address this challenge,
common approaches often rely on encoders to explicitly record past locations
and actions, increasing model complexity and resource consumption. Our
proposal, the Vision-and-Language Navigation Generative Pretrained Transformer
(VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory
sequence dependencies, bypassing the need for historical encoding modules. This
method allows for direct historical information access through trajectory
sequence, enhancing efficiency. Furthermore, our model separates the training
process into offline pre-training with imitation learning and online
fine-tuning with reinforcement learning. This distinction allows for more
focused training objectives and improved performance. Performance assessments
on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art
encoder-based models.
`,authors:"Wen Hanlin",status:0,relevancy:.5154482457814616,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.19334",date:"2024-05-29",title:"LLMs Meet Multimodal Generation and Editing: A Survey",abstract:`  With the recent advancement in large language models (LLMs), there is a
growing interest in combining LLMs with multimodal learning. Previous surveys
of multimodal large language models (MLLMs) mainly focus on understanding. This
survey elaborates on multimodal generation across different domains, including
image, video, 3D, and audio, where we highlight the notable advancements with
milestone works in these fields. Specifically, we exhaustively investigate the
key technical components behind methods and multimodal datasets utilized in
these studies. Moreover, we dig into tool-augmented multimodal agents that can
use existing generative models for human-computer interaction. Lastly, we also
comprehensively discuss the advancement in AI safety and investigate emerging
applications as well as future prospects. Our work provides a systematic and
insightful overview of multimodal generation, which is expected to advance the
development of Artificial Intelligence for Generative Content (AIGC) and world
models. A curated list of all related papers can be found at
https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
`,authors:"Yingqing He; Zhaoyang Liu; Jingye Chen; Zeyue Tian; Hongyu Liu; Xiaowei Chi; Runtao Liu; Ruibin Yuan; Yazhou Xing; Wenhai Wang; Jifeng Dai; Yong Zhang; Wei Xue; Qifeng Liu; Yike Guo; Qifeng Chen",status:0,relevancy:.5151688872910891,isStarred:0,keywords:null,createdAt:"2024-05-31 05:21:04.773 +00:00",updatedAt:"2024-05-31 05:21:04.773 +00:00"},{id:"2405.17631",date:"2024-05-27",title:`BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation
  Experiments`,abstract:`  Agents based on large language models have shown great potential in
accelerating scientific discovery by leveraging their rich background knowledge
and reasoning capabilities. Here, we develop BioDiscoveryAgent, an agent that
designs new experiments, reasons about their outcomes, and efficiently
navigates the hypothesis space to reach desired solutions. We demonstrate our
agent on the problem of designing genetic perturbation experiments, where the
aim is to find a small subset out of many possible genes that, when perturbed,
result in a specific phenotype (e.g., cell growth). Utilizing its biological
knowledge, BioDiscoveryAgent can uniquely design new experiments without the
need to train a machine learning model or explicitly design an acquisition
function. Moreover, BioDiscoveryAgent achieves an average of 18% improvement in
detecting desired phenotypes across five datasets, compared to existing
Bayesian optimization baselines specifically trained for this task. Our
evaluation includes one dataset that is unpublished, ensuring it is not part of
the language model's training data. Additionally, BioDiscoveryAgent predicts
gene combinations to perturb twice as accurately as a random baseline, a task
so far not explored in the context of closed-loop experiment design. The agent
also has access to tools for searching the biomedical literature, executing
code to analyze biological datasets, and prompting another agent to critically
evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every
stage, representing an accessible new paradigm in the computational design of
biological experiments with the potential to augment scientists' capabilities.
`,authors:"Yusuf Roohani; Jian Vora; Qian Huang; Zachary Steinhart; Alexander Marson; Percy Liang; Jure Leskovec",status:0,relevancy:.510634061615632,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.17691",date:"2024-05-27",title:`Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and
  Partially Observable Environments`,abstract:`  Agents, whether software or hardware, perceive their environment through
sensors and act using actuators, often operating in dynamic, partially
observable settings. They face challenges like incomplete and noisy data,
unforeseen situations, and the need to adapt goals in real-time. Traditional
reasoning and ML methods, including Reinforcement Learning (RL), help but are
limited by data needs, predefined goals, and extensive exploration periods.
Ontologies offer a solution by integrating diverse information sources,
enhancing decision-making in complex environments. This thesis introduces an
ontology-enhanced decision-making model (OntoDeM) for autonomous agents.
OntoDeM enriches agents' domain knowledge, allowing them to interpret
unforeseen events, generate or adapt goals, and make better decisions. Key
contributions include: 1. An ontology-based method to improve agents' real-time
observations using prior knowledge. 2. The OntoDeM model for handling dynamic,
unforeseen situations by evolving or generating new goals. 3. Implementation
and evaluation in four real-world applications, demonstrating its
effectiveness. Compared to traditional and advanced learning algorithms,
OntoDeM shows superior performance in improving agents' observations and
decision-making in dynamic, partially observable environments.
`,authors:"Saeedeh Ghanadbashi; Fatemeh Golpayegani",status:0,relevancy:.5103616698161793,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.18917",date:"2024-05-29",title:"Causal Action Influence Aware Counterfactual Data Augmentation",abstract:`  Offline data are both valuable and practical resources for teaching robots
complex behaviors. Ideally, learning agents should not be constrained by the
scarcity of available demonstrations, but rather generalize beyond the training
distribution. However, the complexity of real-world scenarios typically
requires huge amounts of data to prevent neural network policies from picking
up on spurious correlations and learning non-causal relationships. We propose
CAIAC, a data augmentation method that can create feasible synthetic
transitions from a fixed dataset without having access to online environment
interactions. By utilizing principled methods for quantifying causal influence,
we are able to perform counterfactual reasoning by swapping
$\\it{action}$-unaffected parts of the state-space between independent
trajectories in the dataset. We empirically show that this leads to a
substantial increase in robustness of offline learning algorithms against
distributional shift.
`,authors:"Núria Armengol Urpí; Marco Bagatella; Marin Vlastelica; Georg Martius",status:0,relevancy:.5087991529676509,isStarred:0,keywords:null,createdAt:"2024-05-31 05:21:04.773 +00:00",updatedAt:"2024-05-31 05:21:04.773 +00:00"},{id:"2405.18180",date:"2024-05-28",title:`Safe Reinforcement Learning in Black-Box Environments via Adaptive
  Shielding`,abstract:`  Empowering safe exploration of reinforcement learning (RL) agents during
training is a critical impediment towards deploying RL agents in many
real-world scenarios. Training RL agents in unknown, black-box environments
poses an even greater safety risk when prior knowledge of the domain/task is
unavailable. We introduce ADVICE (Adaptive Shielding with a Contrastive
Autoencoder), a novel post-shielding technique that distinguishes safe and
unsafe features of state-action pairs during training, thus protecting the RL
agent from executing actions that yield potentially hazardous outcomes. Our
comprehensive experimental evaluation against state-of-the-art safe RL
exploration techniques demonstrates how ADVICE can significantly reduce safety
violations during training while maintaining a competitive outcome reward.
`,authors:"Daniel Bethell; Simos Gerasimou; Radu Calinescu; Calum Imrie",status:0,relevancy:.5066017536340098,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.16899",date:"2024-05-27",title:`Partial Models for Building Adaptive Model-Based Reinforcement Learning
  Agents`,abstract:`  In neuroscience, one of the key behavioral tests for determining whether a
subject of study exhibits model-based behavior is to study its adaptiveness to
local changes in the environment. In reinforcement learning, however, recent
studies have shown that modern model-based agents display poor adaptivity to
such changes. The main reason for this is that modern agents are typically
designed to improve sample efficiency in single task settings and thus do not
take into account the challenges that can arise in other settings. In local
adaptation settings, one particularly important challenge is in quickly
building and maintaining a sufficiently accurate model after a local change.
This is challenging for deep model-based agents as their models and replay
buffers are monolithic structures lacking distribution shift handling
capabilities. In this study, we show that the conceptually simple idea of
partial models can allow deep model-based agents to overcome this challenge and
thus allow for building locally adaptive model-based agents. By modeling the
different parts of the state space through different models, the agent can not
only maintain a model that is accurate across the state space, but it can also
quickly adapt it in the presence of a local change in the environment. We
demonstrate this by showing that the use of partial models in agents such as
deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the
local changes in their environments.
`,authors:"Safa Alver; Ali Rahimi-Kalahroudi; Doina Precup",status:0,relevancy:.49376953607242435,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.19047",date:"2024-05-29",title:"Statistical Context Detection for Deep Lifelong Reinforcement Learning",abstract:`  Context detection involves labeling segments of an online stream of data as
belonging to different tasks. Task labels are used in lifelong learning
algorithms to perform consolidation or other procedures that prevent
catastrophic forgetting. Inferring task labels from online experiences remains
a challenging problem. Most approaches assume finite and low-dimension
observation spaces or a preliminary training phase during which task labels are
learned. Moreover, changes in the transition or reward functions can be
detected only in combination with a policy, and therefore are more difficult to
detect than changes in the input distribution. This paper presents an approach
to learning both policies and labels in an online deep reinforcement learning
setting. The key idea is to use distance metrics, obtained via optimal
transport methods, i.e., Wasserstein distance, on suitable latent action-reward
spaces to measure distances between sets of data points from past and current
streams. Such distances can then be used for statistical tests based on an
adapted Kolmogorov-Smirnov calculation to assign labels to sequences of
experiences. A rollback procedure is introduced to learn multiple policies by
ensuring that only the appropriate data is used to train the corresponding
policy. The combination of task detection and policy deployment allows for the
optimization of lifelong reinforcement learning agents without an oracle that
provides task labels. The approach is tested using two benchmarks and the
results show promising performance when compared with related context detection
algorithms. The results suggest that optimal transport statistical methods
provide an explainable and justifiable procedure for online context detection
and reward optimization in lifelong reinforcement learning.
`,authors:"Jeffery Dick; Saptarshi Nath; Christos Peridis; Eseoghene Benjamin; Soheil Kolouri; Andrea Soltoggio",status:0,relevancy:.48755249325037087,isStarred:0,keywords:null,createdAt:"2024-05-31 05:21:04.773 +00:00",updatedAt:"2024-05-31 05:21:04.773 +00:00"},{id:"2405.16946",date:"2024-05-27",title:`Biological Neurons Compete with Deep Reinforcement Learning in Sample
  Efficiency in a Simulated Gameworld`,abstract:`  How do biological systems and machine learning algorithms compare in the
number of samples required to show significant improvements in completing a
task? We compared the learning efficiency of in vitro biological neural
networks to the state-of-the-art deep reinforcement learning (RL) algorithms in
a simplified simulation of the game \`Pong'. Using DishBrain, a system that
embodies in vitro neural networks with in silico computation using a
high-density multi-electrode array, we contrasted the learning rate and the
performance of these biological systems against time-matched learning from
three state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same
game environment. This allowed a meaningful comparison between biological
neural systems and deep RL. We find that when samples are limited to a
real-world time course, even these very simple biological cultures outperformed
deep RL algorithms across various game performance characteristics, implying a
higher sample efficiency. Ultimately, even when tested across multiple types of
information input to assess the impact of higher dimensional data input,
biological neurons showcased faster learning than all deep reinforcement
learning agents.
`,authors:"Moein Khajehnejad; Forough Habibollahi; Aswin Paul; Adeel Razi; Brett J. Kagan",status:0,relevancy:.4771512731192644,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.17372",date:"2024-05-27",title:`BehaviorGPT: Smart Agent Simulation for Autonomous Driving with
  Next-Patch Prediction`,abstract:`  Simulating realistic interactions among traffic agents is crucial for
efficiently validating the safety of autonomous driving systems. Existing
leading simulators primarily use an encoder-decoder structure to encode the
historical trajectories for future simulation. However, such a paradigm
complicates the model architecture, and the manual separation of history and
future trajectories leads to low data utilization. To address these challenges,
we propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a
decoder-only, autoregressive architecture designed to simulate the sequential
motion of multiple agents. Crucially, our approach discards the traditional
separation between "history" and "future," treating each time step as the
"current" one, resulting in a simpler, more parameter- and data-efficient
design that scales seamlessly with data and computation. Additionally, we
introduce the Next-Patch Prediction Paradigm (NP3), which enables models to
reason at the patch level of trajectories and capture long-range
spatial-temporal interactions. BehaviorGPT ranks first across several metrics
on the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in
multi-agent and agent-map interactions. We outperformed state-of-the-art models
with a realism score of 0.741 and improved the minADE metric to 1.540, with an
approximately 91.6% reduction in model parameters.
`,authors:"Zikang Zhou; Haibo Hu; Xinhong Chen; Jianping Wang; Nan Guan; Kui Wu; Yung-Hui Li; Yu-Kai Huang; Chun Jason Xue",status:0,relevancy:.46652290848841327,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.18044",date:"2024-05-28",title:`Cognitive Insights and Stable Coalition Matching for Fostering
  Multi-Agent Cooperation`,abstract:`  Cognitive abilities, such as Theory of Mind (ToM), play a vital role in
facilitating cooperation in human social interactions. However, our study
reveals that agents with higher ToM abilities may not necessarily exhibit
better cooperative behavior compared to those with lower ToM abilities. To
address this challenge, we propose a novel matching coalition mechanism that
leverages the strengths of agents with different ToM levels by explicitly
considering belief alignment and specialized abilities when forming coalitions.
Our proposed matching algorithm seeks to find stable coalitions that maximize
the potential for cooperative behavior and ensure long-term viability. By
incorporating cognitive insights into the design of multi-agent systems, our
work demonstrates the potential of leveraging ToM to create more sophisticated
and human-like coordination strategies that foster cooperation and improve
overall system performance.
`,authors:"Jiaqi Shao; Tianjun Yuan; Tao Lin; Xuanyu Cao; Bing Luo",status:0,relevancy:.46599837101190855,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.16830",date:"2024-05-27",title:`Structured Graph Network for Constrained Robot Crowd Navigation with Low
  Fidelity Simulation`,abstract:`  We investigate the feasibility of deploying reinforcement learning (RL)
policies for constrained crowd navigation using a low-fidelity simulator. We
introduce a representation of the dynamic environment, separating human and
obstacle representations. Humans are represented through detected states, while
obstacles are represented as computed point clouds based on maps and robot
localization. This representation enables RL policies trained in a low-fidelity
simulator to deploy in real world with a reduced sim2real gap. Additionally, we
propose a spatio-temporal graph to model the interactions between agents and
obstacles. Based on the graph, we use attention mechanisms to capture the
robot-human, human-human, and human-obstacle interactions. Our method
significantly improves navigation performance in both simulated and real-world
environments. Video demonstrations can be found at
https://sites.google.com/view/constrained-crowdnav/home.
`,authors:"Shuijing Liu; Kaiwen Hong; Neeloy Chakraborty; Katherine Driggs-Campbell",status:0,relevancy:.45884806849533766,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.19815",date:"2024-05-30",title:`Efficient Stimuli Generation using Reinforcement Learning in Design
  Verification`,abstract:`  The increasing design complexity of System-on-Chips (SoCs) has led to
significant verification challenges, particularly in meeting coverage targets
within a timely manner. At present, coverage closure is heavily dependent on
constrained random and coverage driven verification methodologies where the
randomized stimuli are bounded to verify certain scenarios and to reach
coverage goals. This process is said to be exhaustive and to consume a lot of
project time. In this paper, a novel methodology is proposed to generate
efficient stimuli with the help of Reinforcement Learning (RL) to reach the
maximum code coverage of the Design Under Verification (DUV). Additionally, an
automated framework is created using metamodeling to generate a SystemVerilog
testbench and an RL environment for any given design. The proposed approach is
applied to various designs and the produced results proves that the RL agent
provides effective stimuli to achieve code coverage faster in comparison with
baseline random simulations. Furthermore, various RL agents and reward schemes
are analyzed in our work.
`,authors:"Deepak Narayan Gadde; Thomas Nalapat; Aman Kumar; Djones Lettnin; Wolfgang Kunz; Sebastian Simon",status:0,relevancy:.44928077451128423,isStarred:0,keywords:null,createdAt:"2024-05-31 04:58:28.951 +00:00",updatedAt:"2024-05-31 04:58:28.951 +00:00"},{id:"2405.19238",date:"2024-05-29",title:`Explanation-based Belief Revision: Moving Beyond Minimalism to
  Explanatory Understanding`,abstract:`  In belief revision, agents typically modify their beliefs when they receive
some new piece of information that is in conflict with them. The guiding
principle behind most belief revision frameworks is that of minimalism, which
advocates minimal changes to existing beliefs. However, minimalism may not
necessarily capture the nuanced ways in which human agents reevaluate and
modify their beliefs. In contrast, the explanatory hypothesis indicates that
people are inherently driven to seek explanations for inconsistencies, thereby
striving for explanatory coherence rather than minimal changes when revising
beliefs. Our contribution in this paper is two-fold. Motivated by the
explanatory hypothesis, we first present a novel, yet simple belief revision
operator that, given a belief base and an explanation for an explanandum, it
revises the belief bases in a manner that preserves the explanandum and is not
necessarily minimal. We call this operator explanation-based belief revision.
Second, we conduct two human-subject studies to empirically validate our
approach and investigate belief revision behavior in real-world scenarios. Our
findings support the explanatory hypothesis and provide insights into the
strategies people employ when resolving inconsistencies.
`,authors:"Stylianos Loukas Vasileiou; William Yeoh",status:0,relevancy:.4043755067873307,isStarred:0,keywords:null,createdAt:"2024-05-31 05:21:04.773 +00:00",updatedAt:"2024-05-31 05:21:04.773 +00:00"},{id:"2405.16887",date:"2024-05-27",title:`A Large Language Model-based multi-agent manufacturing system for
  intelligent shopfloor`,abstract:`  As productivity advances, the demand of customers for multi-variety and
small-batch production is increasing, thereby putting forward higher
requirements for manufacturing systems. When production tasks frequent changes
due to this demand, traditional manufacturing systems often cannot response
promptly. The multi-agent manufacturing system is proposed to address this
problem. However, because of technical limitations, the negotiation among
agents in this kind of system is realized through predefined heuristic rules,
which is not intelligent enough to deal with the multi-variety and small batch
production. To this end, a Large Language Model-based (LLM-based) multi-agent
manufacturing system for intelligent shopfloor is proposed in the present
study. This system delineates the diverse agents and defines their
collaborative methods. The roles of the agents encompass Machine Server Agent
(MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and
Decision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability
of analyzing the shopfloor condition and choosing the most suitable machine, as
opposed to executing a predefined program artificially. The negotiation between
BAs and BIA is the most crucial step in connecting manufacturing resources.
With the support of TA and DA, BIA will finalize the distribution of orders,
relying on the information of each machine returned by BA. MSAs bears the
responsibility for connecting the agents with the physical shopfloor. This
system aims to distribute and transmit workpieces through the collaboration of
the agents with these distinct roles, distinguishing it from other scheduling
approaches. Comparative experiments were also conducted to validate the
performance of this system.
`,authors:"Zhen Zhao; Dunbing Tang; Haihua Zhu; Zequn Zhang; Kai Chen; Changchun Liu; Yuchen Ji",status:0,relevancy:.39624565173825943,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:34.029 +00:00",updatedAt:"2024-05-31 04:51:34.029 +00:00"},{id:"2405.17746",date:"2024-05-28",title:"Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective",abstract:`  Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks,
posing concerning threats to their reliable deployment. Recent research reveals
that backdoors can be erased from infected DNNs by pruning a specific group of
neurons, while how to effectively identify and remove these backdoor-associated
neurons remains an open challenge. Most of the existing defense methods rely on
defined rules and focus on neuron's local properties, ignoring the exploration
and optimization of pruning policies. To address this gap, we propose an
Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN)
and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP
first models the target DNN as graphs based on neuron connectivity, and then
uses GNN-based RL agents to learn graph embeddings and find a suitable pruning
policy. To the best of our knowledge, this is the first attempt to employ GNN
and RL for optimizing pruning policies in the field of backdoor defense.
Experiments show, with a small amount of clean data, ONP can effectively prune
the backdoor neurons implanted by a set of backdoor attacks at the cost of
negligible performance degradation, achieving a new state-of-the-art
performance for backdoor mitigation.
`,authors:"Nan Li; Haiyang Yu; Ping Yi",status:0,relevancy:.3860347237446391,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.18300",date:"2024-05-28",title:"CompetEvo: Towards Morphological Evolution from Competition",abstract:`  Training an agent to adapt to specific tasks through co-optimization of
morphology and control has widely attracted attention. However, whether there
exists an optimal configuration and tactics for agents in a multiagent
competition scenario is still an issue that is challenging to definitively
conclude. In this context, we propose competitive evolution (CompetEvo), which
co-evolves agents' designs and tactics in confrontation. We build arenas
consisting of three animals and their evolved derivatives, placing agents with
different morphologies in direct competition with each other. The results
reveal that our method enables agents to evolve a more suitable design and
strategy for fighting compared to fixed-morph agents, allowing them to obtain
advantages in combat scenarios. Moreover, we demonstrate the amazing and
impressive behaviors that emerge when confrontations are conducted under
asymmetrical morphs.
`,authors:"Kangyao Huang; Di Guo; Xinyu Zhang; Xiangyang Ji; Huaping Liu",status:0,relevancy:.3731645532957174,isStarred:0,keywords:null,createdAt:"2024-05-31 04:51:45.387 +00:00",updatedAt:"2024-05-31 04:51:45.387 +00:00"},{id:"2405.19743",date:"2024-05-30",title:"May the Dance be with You: Dance Generation Framework for Non-Humanoids",abstract:`  We hypothesize dance as a motion that forms a visual rhythm from music, where
the visual rhythm can be perceived from an optical flow. If an agent can
recognize the relationship between visual rhythm and music, it will be able to
dance by generating a motion to create a visual rhythm that matches the music.
Based on this, we propose a framework for any kind of non-humanoid agents to
learn how to dance from human videos. Our framework works in two processes: (1)
training a reward model which perceives the relationship between optical flow
(visual rhythm) and music from human dance videos, (2) training the
non-humanoid dancer based on that reward model, and reinforcement learning. Our
reward model consists of two feature encoders for optical flow and music. They
are trained based on contrastive learning which makes the higher similarity
between concurrent optical flow and music features. With this reward model, the
agent learns dancing by getting a higher reward when its action creates an
optical flow whose feature has a higher similarity with the given music
feature. Experiment results show that generated dance motion can align with the
music beat properly, and user study result indicates that our framework is more
preferred by humans compared to the baselines. To the best of our knowledge,
our work of non-humanoid agents which learn dance from human videos is
unprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.
`,authors:"Hyemin Ahn",status:0,relevancy:.35629470656024,isStarred:0,keywords:null,createdAt:"2024-05-31 04:58:28.951 +00:00",updatedAt:"2024-05-31 04:58:28.951 +00:00"}],searchStateAtom=atom("complete"),tabValueAtom=atom("table"),resultListAtom=atom(results),queryAtom=atom("agent"),queryFieldAtom=atom("all"),favoriteAtom=atom(!1),viewedAtom=atom(!1),relevancyAtom=atom(""),comparisonOperatorAtom=atom("≥"),dateStartAtom$1=atom(dayjs("2024-05-27")),dateEndAtom$1=atom(null),initialStateAtom=atom(!1),approvedStateAtom=atom(!1),generatedStateAtom=atom(!1),publishedStateAtom=atom(!1);atom(null,async(a,i,o)=>{i(searchStateAtom,"loading");const s=a(dateStartAtom$1),$=a(dateEndAtom$1),j=o||{query:a(queryAtom),queryField:a(queryFieldAtom),relevancy:{operator:a(comparisonOperatorAtom),value:a(relevancyAtom)},dateStart:s?s.format("YYYY-MM-DD"):null,dateEnd:$?$.format("YYYY-MM-DD"):null,viewed:a(viewedAtom),favorite:a(favoriteAtom),initialState:a(initialStateAtom),approvedState:a(approvedStateAtom),generatedState:a(generatedStateAtom),publishedState:a(publishedStateAtom)};try{const _e=await searchPapers(j),et=_e.data;console.log("search results: ",_e.data),et.length===1e3&&i(addAlertAtom,{message:"Results limited to 1000 papers. Please refine your search criteria.",autoClose:!0}),et.length===0?i(searchStateAtom,"empty"):i(searchStateAtom,"complete"),i(resultListAtom,et)}catch(_e){console.error("Failed to search papers:",_e),i(searchStateAtom,"error")}});const resetFieldsAtom=atom(null,(a,i)=>{i(queryAtom,""),i(queryFieldAtom,"all"),i(favoriteAtom,!1),i(viewedAtom,!1),i(relevancyAtom,""),i(comparisonOperatorAtom,"≥"),i(dateStartAtom$1,null),i(dateEndAtom$1,null),i(initialStateAtom,!1),i(approvedStateAtom,!1),i(generatedStateAtom,!1),i(publishedStateAtom,!1)}),PageMessage=({message:a})=>jsxRuntimeExports.jsx(Paper$1,{className:"px-6 py-8 max-w-md mx-auto my-8",children:jsxRuntimeExports.jsx("div",{className:"font-medium text-lg",children:a})}),QueryControl=()=>{const[a,i]=useAtom(queryAtom),[o,s]=useAtom(queryFieldAtom),j=new URLSearchParams(location.search).get("query"),_e=useSetAtom(resetFieldsAtom),et=useSetAtom(featureDisabledAlertAtom),tt=nt=>{!nt.shiftKey&&nt.key==="Enter"&&(nt.preventDefault(),et())};return reactExports.useEffect(()=>{j?et():j!==null&&i(j)},[j]),jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsxs(FormControl$1,{variant:"outlined",sx:{minWidth:120},size:"small",children:[jsxRuntimeExports.jsx(InputLabel$1,{id:"search-field-label",sx:{color:"#9e9e9e !important"},children:"Field"}),jsxRuntimeExports.jsxs(Select$1,{labelId:"search-field-label",id:"search-field-select",value:o,onChange:nt=>s(nt.target.value),label:"Field",children:[jsxRuntimeExports.jsx(MenuItem$1,{value:"all",children:"All"}),jsxRuntimeExports.jsx(MenuItem$1,{value:"title",children:"Title"}),jsxRuntimeExports.jsx(MenuItem$1,{value:"abstract",children:"Abstract"}),jsxRuntimeExports.jsx(MenuItem$1,{value:"authors",children:"Authors"})]})]}),jsxRuntimeExports.jsx(Box$1,{sx:{width:"100%"},children:jsxRuntimeExports.jsx(TextField$1,{InputLabelProps:{style:{color:"#9e9e9e"}},id:"query-input",label:"Search Query",variant:"outlined",size:"small",sx:{marginRight:2,minWidth:"20rem"},value:a,onKeyDown:tt,onChange:nt=>i(nt.target.value),onFocus:nt=>{nt.target.select()},fullWidth:!0})}),jsxRuntimeExports.jsxs(Box$1,{sx:{display:"flex",justifyContent:"space-between",placeSelf:"center"},children:[jsxRuntimeExports.jsx(Button$1,{variant:"contained",color:"primary",onClick:()=>et(),children:"Search"}),jsxRuntimeExports.jsx(Button$1,{variant:"contained",color:"secondary",onClick:_e,sx:{ml:1},children:"Reset"})]})]})},RelevancyCriteria=()=>{const[a,i]=useAtom(relevancyAtom),[o,s]=useAtom(comparisonOperatorAtom),$=j=>{let _e=parseInt(j.target.value,10);isNaN(_e)?i(""):(_e=Math.max(0,Math.min(100,_e)),i(_e.toString()))};return jsxRuntimeExports.jsx(FormControl$1,{variant:"outlined",children:jsxRuntimeExports.jsx(TextField$1,{id:"relevancy-score-input",label:jsxRuntimeExports.jsx("span",{style:{color:"#9e9e9e"},children:"Relevancy"}),variant:"outlined",type:"number",placeholder:"0",InputProps:{inputProps:{min:0,max:100},sx:{textAlign:"right"},startAdornment:jsxRuntimeExports.jsx(IconButton$1,{sx:{color:"white",boxShadow:"none",padding:2.2,height:"1em",maxWidth:"1em",minWidth:"1em",mr:2,fontSize:"1.2rem"},onClick:j=>s(o==="≥"?"≤":"≥"),children:o}),endAdornment:jsxRuntimeExports.jsx(InputAdornment$1,{position:"end",sx:{ml:1},children:"%"})},value:a,onChange:$,sx:{minWidth:170}})})},BasicCriteriaControl=()=>{const[a,i]=useAtom(favoriteAtom);return jsxRuntimeExports.jsx(jsxRuntimeExports.Fragment,{children:jsxRuntimeExports.jsxs("div",{className:"flex justify-center items-center mr-10",children:[jsxRuntimeExports.jsx("div",{className:"flex justify-center items-center mr-20",children:jsxRuntimeExports.jsx(RelevancyCriteria,{})}),jsxRuntimeExports.jsx(FormControlLabel$1,{control:jsxRuntimeExports.jsx(Checkbox$1,{sx:{color:"#9e9e9e !important"},checked:a,onChange:o=>i(o.target.checked),name:"favorite"}),label:jsxRuntimeExports.jsxs("span",{children:["Starred",jsxRuntimeExports.jsx(default_1$o,{color:"warning",style:{marginLeft:"8px",marginTop:"-4px"}})]})})]})})},_excluded$n=["localeText"],MuiPickersAdapterContext=reactExports.createContext(null),LocalizationProvider=function a(i){var o;const{localeText:s}=i,$=_objectWithoutPropertiesLoose(i,_excluded$n),{utils:j,localeText:_e}=(o=reactExports.useContext(MuiPickersAdapterContext))!=null?o:{utils:void 0,localeText:void 0},et=useThemeProps$6({props:$,name:"MuiLocalizationProvider"}),{children:tt,dateAdapter:nt,dateFormats:at,dateLibInstance:it,adapterLocale:st,localeText:lt}=et,ct=reactExports.useMemo(()=>_extends({},lt,_e,s),[lt,_e,s]),rt=reactExports.useMemo(()=>{if(!nt)return j||null;const dt=new nt({locale:st,formats:at,instance:it});if(!dt.isMUIAdapter)throw new Error(["MUI: The date adapter should be imported from `@mui/x-date-pickers` or `@mui/x-date-pickers-pro`, not from `@date-io`","For example, `import { AdapterDayjs } from '@mui/x-date-pickers/AdapterDayjs'` instead of `import AdapterDayjs from '@date-io/dayjs'`","More information on the installation documentation: https://mui.com/x/react-date-pickers/getting-started/#installation"].join(`
`));return dt},[nt,st,at,it,j]),ut=reactExports.useMemo(()=>rt?{minDate:rt.date("1900-01-01T00:00:00.000"),maxDate:rt.date("2099-12-31T00:00:00.000")}:null,[rt]),ot=reactExports.useMemo(()=>({utils:rt,defaultDates:ut,localeText:ct}),[ut,rt,ct]);return jsxRuntimeExports.jsx(MuiPickersAdapterContext.Provider,{value:ot,children:tt})},getPickersLocalization=a=>({components:{MuiLocalizationProvider:{defaultProps:{localeText:_extends({},a)}}}}),enUSPickers={previousMonth:"Previous month",nextMonth:"Next month",openPreviousView:"open previous view",openNextView:"open next view",calendarViewSwitchingButtonAriaLabel:a=>a==="year"?"year view is open, switch to calendar view":"calendar view is open, switch to year view",start:"Start",end:"End",cancelButtonLabel:"Cancel",clearButtonLabel:"Clear",okButtonLabel:"OK",todayButtonLabel:"Today",datePickerToolbarTitle:"Select date",dateTimePickerToolbarTitle:"Select date & time",timePickerToolbarTitle:"Select time",dateRangePickerToolbarTitle:"Select date range",clockLabelText:(a,i,o)=>`Select ${a}. ${i===null?"No time selected":`Selected time is ${o.format(i,"fullTime")}`}`,hoursClockNumberText:a=>`${a} hours`,minutesClockNumberText:a=>`${a} minutes`,secondsClockNumberText:a=>`${a} seconds`,selectViewText:a=>`Select ${a}`,calendarWeekNumberHeaderLabel:"Week number",calendarWeekNumberHeaderText:"#",calendarWeekNumberAriaLabelText:a=>`Week ${a}`,calendarWeekNumberText:a=>`${a}`,openDatePickerDialogue:(a,i)=>a!==null&&i.isValid(a)?`Choose date, selected date is ${i.format(a,"fullDate")}`:"Choose date",openTimePickerDialogue:(a,i)=>a!==null&&i.isValid(a)?`Choose time, selected time is ${i.format(a,"fullTime")}`:"Choose time",fieldClearLabel:"Clear value",timeTableLabel:"pick time",dateTableLabel:"pick date",fieldYearPlaceholder:a=>"Y".repeat(a.digitAmount),fieldMonthPlaceholder:a=>a.contentType==="letter"?"MMMM":"MM",fieldDayPlaceholder:()=>"DD",fieldWeekDayPlaceholder:a=>a.contentType==="letter"?"EEEE":"EE",fieldHoursPlaceholder:()=>"hh",fieldMinutesPlaceholder:()=>"mm",fieldSecondsPlaceholder:()=>"ss",fieldMeridiemPlaceholder:()=>"aa"},DEFAULT_LOCALE=enUSPickers;getPickersLocalization(enUSPickers);const useLocalizationContext=()=>{const a=reactExports.useContext(MuiPickersAdapterContext);if(a===null)throw new Error(["MUI: Can not find the date and time pickers localization context.","It looks like you forgot to wrap your component in LocalizationProvider.","This can also happen if you are bundling multiple versions of the `@mui/x-date-pickers` package"].join(`
`));if(a.utils===null)throw new Error(["MUI: Can not find the date and time pickers adapter from its localization context.","It looks like you forgot to pass a `dateAdapter` to your LocalizationProvider."].join(`
`));const i=reactExports.useMemo(()=>_extends({},DEFAULT_LOCALE,a.localeText),[a.localeText]);return reactExports.useMemo(()=>_extends({},a,{localeText:i}),[a,i])},useUtils=()=>useLocalizationContext().utils,useDefaultDates=()=>useLocalizationContext().defaultDates,useLocaleText=()=>useLocalizationContext().localeText,useNow=a=>{const i=useUtils(),o=reactExports.useRef();return o.current===void 0&&(o.current=i.dateWithTimezone(void 0,a)),o.current},ArrowDropDownIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M7 10l5 5 5-5z"}),"ArrowDropDown"),ArrowLeftIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M15.41 16.59L10.83 12l4.58-4.59L14 6l-6 6 6 6 1.41-1.41z"}),"ArrowLeft"),ArrowRightIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M8.59 16.59L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.41z"}),"ArrowRight"),CalendarIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M17 12h-5v5h5v-5zM16 1v2H8V1H6v2H5c-1.11 0-1.99.9-1.99 2L3 19c0 1.1.89 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2h-1V1h-2zm3 18H5V8h14v11z"}),"Calendar");createSvgIcon$1(jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx("path",{d:"M11.99 2C6.47 2 2 6.48 2 12s4.47 10 9.99 10C17.52 22 22 17.52 22 12S17.52 2 11.99 2zM12 20c-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8-3.58 8-8 8z"}),jsxRuntimeExports.jsx("path",{d:"M12.5 7H11v6l5.25 3.15.75-1.23-4.5-2.67z"})]}),"Clock");createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M9 11H7v2h2v-2zm4 0h-2v2h2v-2zm4 0h-2v2h2v-2zm2-7h-1V2h-2v2H8V2H6v2H5c-1.11 0-1.99.9-1.99 2L3 20c0 1.1.89 2 2 2h14c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 16H5V9h14v11z"}),"DateRange");createSvgIcon$1(jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[jsxRuntimeExports.jsx("path",{d:"M11.99 2C6.47 2 2 6.48 2 12s4.47 10 9.99 10C17.52 22 22 17.52 22 12S17.52 2 11.99 2zM12 20c-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8-3.58 8-8 8z"}),jsxRuntimeExports.jsx("path",{d:"M12.5 7H11v6l5.25 3.15.75-1.23-4.5-2.67z"})]}),"Time");const ClearIcon=createSvgIcon$1(jsxRuntimeExports.jsx("path",{d:"M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"}),"Clear");function getPickersArrowSwitcherUtilityClass(a){return generateUtilityClass$1("MuiPickersArrowSwitcher",a)}generateUtilityClasses$1("MuiPickersArrowSwitcher",["root","spacer","button"]);const _excluded$m=["children","className","slots","slotProps","isNextDisabled","isNextHidden","onGoToNext","nextLabel","isPreviousDisabled","isPreviousHidden","onGoToPrevious","previousLabel"],_excluded2$5=["ownerState"],_excluded3$2=["ownerState"],PickersArrowSwitcherRoot=styled("div",{name:"MuiPickersArrowSwitcher",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"flex"}),PickersArrowSwitcherSpacer=styled("div",{name:"MuiPickersArrowSwitcher",slot:"Spacer",overridesResolver:(a,i)=>i.spacer})(({theme:a})=>({width:a.spacing(3)})),PickersArrowSwitcherButton=styled(IconButton$1,{name:"MuiPickersArrowSwitcher",slot:"Button",overridesResolver:(a,i)=>i.button})(({ownerState:a})=>_extends({},a.hidden&&{visibility:"hidden"})),useUtilityClasses$g=a=>{const{classes:i}=a;return composeClasses({root:["root"],spacer:["spacer"],button:["button"]},getPickersArrowSwitcherUtilityClass,i)},PickersArrowSwitcher=reactExports.forwardRef(function a(i,o){var s,$,j,_e;const tt=useTheme$1().direction==="rtl",nt=useThemeProps$6({props:i,name:"MuiPickersArrowSwitcher"}),{children:at,className:it,slots:st,slotProps:lt,isNextDisabled:ct,isNextHidden:rt,onGoToNext:ut,nextLabel:ot,isPreviousDisabled:dt,isPreviousHidden:pt,onGoToPrevious:mt,previousLabel:ft}=nt,ht=_objectWithoutPropertiesLoose(nt,_excluded$m),yt=nt,bt=useUtilityClasses$g(yt),gt={isDisabled:ct,isHidden:rt,goTo:ut,label:ot},xt={isDisabled:dt,isHidden:pt,goTo:mt,label:ft},vt=(s=st==null?void 0:st.previousIconButton)!=null?s:PickersArrowSwitcherButton,Lt=useSlotProps({elementType:vt,externalSlotProps:lt==null?void 0:lt.previousIconButton,additionalProps:{size:"medium",title:xt.label,"aria-label":xt.label,disabled:xt.isDisabled,edge:"end",onClick:xt.goTo},ownerState:_extends({},yt,{hidden:xt.isHidden}),className:bt.button}),$t=($=st==null?void 0:st.nextIconButton)!=null?$:PickersArrowSwitcherButton,Tt=useSlotProps({elementType:$t,externalSlotProps:lt==null?void 0:lt.nextIconButton,additionalProps:{size:"medium",title:gt.label,"aria-label":gt.label,disabled:gt.isDisabled,edge:"start",onClick:gt.goTo},ownerState:_extends({},yt,{hidden:gt.isHidden}),className:bt.button}),Et=(j=st==null?void 0:st.leftArrowIcon)!=null?j:ArrowLeftIcon,Dt=useSlotProps({elementType:Et,externalSlotProps:lt==null?void 0:lt.leftArrowIcon,additionalProps:{fontSize:"inherit"},ownerState:void 0}),It=_objectWithoutPropertiesLoose(Dt,_excluded2$5),Ct=(_e=st==null?void 0:st.rightArrowIcon)!=null?_e:ArrowRightIcon,jt=useSlotProps({elementType:Ct,externalSlotProps:lt==null?void 0:lt.rightArrowIcon,additionalProps:{fontSize:"inherit"},ownerState:void 0}),Zt=_objectWithoutPropertiesLoose(jt,_excluded3$2);return jsxRuntimeExports.jsxs(PickersArrowSwitcherRoot,_extends({ref:o,className:clsx(bt.root,it),ownerState:yt},ht,{children:[jsxRuntimeExports.jsx(vt,_extends({},Lt,{children:tt?jsxRuntimeExports.jsx(Ct,_extends({},Zt)):jsxRuntimeExports.jsx(Et,_extends({},It))})),at?jsxRuntimeExports.jsx(Typography$1,{variant:"subtitle1",component:"span",children:at}):jsxRuntimeExports.jsx(PickersArrowSwitcherSpacer,{className:bt.spacer,ownerState:yt}),jsxRuntimeExports.jsx($t,_extends({},Tt,{children:tt?jsxRuntimeExports.jsx(Et,_extends({},It)):jsxRuntimeExports.jsx(Ct,_extends({},Zt))}))]}))}),areViewsEqual=(a,i)=>a.length!==i.length?!1:i.every(o=>a.includes(o)),applyDefaultViewProps=({openTo:a,defaultOpenTo:i,views:o,defaultViews:s})=>{const $=o??s;let j;if(a!=null)j=a;else if($.includes(i))j=i;else if($.length>0)j=$[0];else throw new Error("MUI: The `views` prop must contain at least one view");return{views:$,openTo:j}},timeViews=["hours","minutes","seconds"],isTimeView=a=>timeViews.includes(a),getSecondsInDay=(a,i)=>i.getHours(a)*3600+i.getMinutes(a)*60+i.getSeconds(a),createIsAfterIgnoreDatePart=(a,i)=>(o,s)=>a?i.isAfter(o,s):getSecondsInDay(o,i)>getSecondsInDay(s,i);function useViews({onChange:a,onViewChange:i,openTo:o,view:s,views:$,autoFocus:j,focusedView:_e,onFocusedViewChange:et}){var tt,nt;const at=reactExports.useRef(o),it=reactExports.useRef($),st=reactExports.useRef($.includes(o)?o:$[0]),[lt,ct]=useControlled({name:"useViews",state:"view",controlled:s,default:st.current}),rt=reactExports.useRef(j?lt:null),[ut,ot]=useControlled({name:"useViews",state:"focusedView",controlled:_e,default:rt.current});reactExports.useEffect(()=>{(at.current&&at.current!==o||it.current&&it.current.some(gt=>!$.includes(gt)))&&(ct($.includes(o)?o:$[0]),it.current=$,at.current=o)},[o,ct,lt,$]);const dt=$.indexOf(lt),pt=(tt=$[dt-1])!=null?tt:null,mt=(nt=$[dt+1])!=null?nt:null,ft=useEventCallback((gt,xt)=>{ot(xt?gt:vt=>gt===vt?null:vt),et==null||et(gt,xt)}),ht=useEventCallback(gt=>{ft(gt,!0),gt!==lt&&(ct(gt),i&&i(gt))}),yt=useEventCallback(()=>{mt&&ht(mt)}),bt=useEventCallback((gt,xt,vt)=>{const Lt=xt==="finish",$t=vt?$.indexOf(vt)<$.length-1:!!mt;if(a(gt,Lt&&$t?"partial":xt,vt),vt&&vt!==lt){const Et=$[$.indexOf(vt)+1];Et&&ht(Et)}else Lt&&yt()});return{view:lt,setView:ht,focusedView:ut,setFocusedView:ft,nextView:mt,previousView:pt,defaultView:$.includes(o)?o:$[0],goToNextView:yt,setValueAndGoToNextView:bt}}function useNextMonthDisabled(a,{disableFuture:i,maxDate:o,timezone:s}){const $=useUtils();return reactExports.useMemo(()=>{const j=$.dateWithTimezone(void 0,s),_e=$.startOfMonth(i&&$.isBefore(j,o)?j:o);return!$.isAfter(_e,a)},[i,o,a,$,s])}function usePreviousMonthDisabled(a,{disablePast:i,minDate:o,timezone:s}){const $=useUtils();return reactExports.useMemo(()=>{const j=$.dateWithTimezone(void 0,s),_e=$.startOfMonth(i&&$.isAfter(j,o)?j:o);return!$.isBefore(_e,a)},[i,o,a,$,s])}const DAY_SIZE=36,DAY_MARGIN=2,DIALOG_WIDTH=320,MAX_CALENDAR_HEIGHT=280,VIEW_HEIGHT=334,PickerViewRoot=styled("div")({overflow:"hidden",width:DIALOG_WIDTH,maxHeight:VIEW_HEIGHT,display:"flex",flexDirection:"column",margin:"0 auto"}),mergeDateAndTime=(a,i,o)=>{let s=i;return s=a.setHours(s,a.getHours(o)),s=a.setMinutes(s,a.getMinutes(o)),s=a.setSeconds(s,a.getSeconds(o)),s},findClosestEnabledDate=({date:a,disableFuture:i,disablePast:o,maxDate:s,minDate:$,isDateDisabled:j,utils:_e,timezone:et})=>{const tt=mergeDateAndTime(_e,_e.dateWithTimezone(void 0,et),a);o&&_e.isBefore($,tt)&&($=tt),i&&_e.isAfter(s,tt)&&(s=tt);let nt=a,at=a;for(_e.isBefore(a,$)&&(nt=$,at=null),_e.isAfter(a,s)&&(at&&(at=s),nt=null);nt||at;){if(nt&&_e.isAfter(nt,s)&&(nt=null),at&&_e.isBefore(at,$)&&(at=null),nt){if(!j(nt))return nt;nt=_e.addDays(nt,1)}if(at){if(!j(at))return at;at=_e.addDays(at,-1)}}return null},replaceInvalidDateByNull=(a,i)=>i==null||!a.isValid(i)?null:i,applyDefaultDate=(a,i,o)=>i==null||!a.isValid(i)?o:i,areDatesEqual=(a,i,o)=>!a.isValid(i)&&i!=null&&!a.isValid(o)&&o!=null?!0:a.isEqual(i,o),getMonthsInYear=(a,i)=>{const s=[a.startOfYear(i)];for(;s.length<12;){const $=s[s.length-1];s.push(a.addMonths($,1))}return s},getTodayDate=(a,i,o)=>o==="date"?a.startOfDay(a.dateWithTimezone(void 0,i)):a.dateWithTimezone(void 0,i),dateViews=["year","month","day"],isDatePickerView=a=>dateViews.includes(a),resolveDateFormat=(a,{format:i,views:o},s)=>{if(i!=null)return i;const $=a.formats;return areViewsEqual(o,["year"])?$.year:areViewsEqual(o,["month"])?$.month:areViewsEqual(o,["day"])?$.dayOfMonth:areViewsEqual(o,["month","year"])?`${$.month} ${$.year}`:areViewsEqual(o,["day","month"])?`${$.month} ${$.dayOfMonth}`:s?/en/.test(a.getCurrentLocaleCode())?$.normalDateWithWeekday:$.normalDate:$.keyboardDate},getWeekdays=(a,i)=>{const o=a.startOfWeek(i);return[0,1,2,3,4,5,6].map(s=>a.addDays(o,s))},useValueWithTimezone=({timezone:a,value:i,defaultValue:o,onChange:s,valueManager:$})=>{var j,_e;const et=useUtils(),tt=reactExports.useRef(o),nt=(j=i??tt.current)!=null?j:$.emptyValue,at=reactExports.useMemo(()=>$.getTimezone(et,nt),[et,$,nt]),it=useEventCallback(rt=>at==null?rt:$.setTimezone(et,at,rt)),st=(_e=a??at)!=null?_e:"default",lt=reactExports.useMemo(()=>$.setTimezone(et,st,nt),[$,et,st,nt]),ct=useEventCallback((rt,...ut)=>{const ot=it(rt);s==null||s(ot,...ut)});return{value:lt,handleValueChange:ct,timezone:st}},useControlledValueWithTimezone=({name:a,timezone:i,value:o,defaultValue:s,onChange:$,valueManager:j})=>{const[_e,et]=useControlled({name:a,state:"value",controlled:o,default:s??j.emptyValue}),tt=useEventCallback((nt,...at)=>{et(nt),$==null||$(nt,...at)});return useValueWithTimezone({timezone:i,value:_e,defaultValue:void 0,onChange:tt,valueManager:j})},SECTION_TYPE_GRANULARITY={year:1,month:2,day:3,hours:4,minutes:5,seconds:6,milliseconds:7},getSectionTypeGranularity=a=>Math.max(...a.map(i=>{var o;return(o=SECTION_TYPE_GRANULARITY[i.type])!=null?o:1})),roundDate=(a,i,o)=>{if(i===SECTION_TYPE_GRANULARITY.year)return a.startOfYear(o);if(i===SECTION_TYPE_GRANULARITY.month)return a.startOfMonth(o);if(i===SECTION_TYPE_GRANULARITY.day)return a.startOfDay(o);let s=o;return i<SECTION_TYPE_GRANULARITY.minutes&&(s=a.setMinutes(s,0)),i<SECTION_TYPE_GRANULARITY.seconds&&(s=a.setSeconds(s,0)),i<SECTION_TYPE_GRANULARITY.milliseconds&&(s=a.setMilliseconds(s,0)),s},getDefaultReferenceDate=({props:a,utils:i,granularity:o,timezone:s,getTodayDate:$})=>{var j;let _e=$?$():roundDate(i,o,getTodayDate(i,s));a.minDate!=null&&i.isAfterDay(a.minDate,_e)&&(_e=roundDate(i,o,a.minDate)),a.maxDate!=null&&i.isBeforeDay(a.maxDate,_e)&&(_e=roundDate(i,o,a.maxDate));const et=createIsAfterIgnoreDatePart((j=a.disableIgnoringDatePartForTimeValidation)!=null?j:!1,i);return a.minTime!=null&&et(a.minTime,_e)&&(_e=roundDate(i,o,a.disableIgnoringDatePartForTimeValidation?a.minTime:mergeDateAndTime(i,_e,a.minTime))),a.maxTime!=null&&et(_e,a.maxTime)&&(_e=roundDate(i,o,a.disableIgnoringDatePartForTimeValidation?a.maxTime:mergeDateAndTime(i,_e,a.maxTime))),_e},getDateSectionConfigFromFormatToken=(a,i)=>{const o=a.formatTokenMap[i];if(o==null)throw new Error([`MUI: The token "${i}" is not supported by the Date and Time Pickers.`,"Please try using another token or open an issue on https://github.com/mui/mui-x/issues/new/choose if you think it should be supported."].join(`
`));return typeof o=="string"?{type:o,contentType:o==="meridiem"?"letter":"digit",maxLength:void 0}:{type:o.sectionType,contentType:o.contentType,maxLength:o.maxLength}},getDeltaFromKeyCode=a=>{switch(a){case"ArrowUp":return 1;case"ArrowDown":return-1;case"PageUp":return 5;case"PageDown":return-5;default:return 0}},getDaysInWeekStr=(a,i,o)=>{const s=[],$=a.dateWithTimezone(void 0,i),j=a.startOfWeek($),_e=a.endOfWeek($);let et=j;for(;a.isBefore(et,_e);)s.push(et),et=a.addDays(et,1);return s.map(tt=>a.formatByString(tt,o))},getLetterEditingOptions=(a,i,o,s)=>{switch(o){case"month":return getMonthsInYear(a,a.dateWithTimezone(void 0,i)).map($=>a.formatByString($,s));case"weekDay":return getDaysInWeekStr(a,i,s);case"meridiem":{const $=a.dateWithTimezone(void 0,i);return[a.startOfDay($),a.endOfDay($)].map(j=>a.formatByString(j,s))}default:return[]}},cleanLeadingZeros=(a,i,o)=>{let s=i;for(s=Number(s).toString();s.length<o;)s=`0${s}`;return s},cleanDigitSectionValue=(a,i,o,s,$)=>{if($.type==="day"&&$.contentType==="digit-with-letter"){const _e=a.setDate(s.longestMonth,o);return a.formatByString(_e,$.format)}const j=o.toString();return $.hasLeadingZerosInInput?cleanLeadingZeros(a,j,$.maxLength):j},adjustSectionValue=(a,i,o,s,$,j,_e)=>{const et=getDeltaFromKeyCode(s),tt=s==="Home",nt=s==="End",at=o.value===""||tt||nt,it=()=>{const lt=$[o.type]({currentDate:j,format:o.format,contentType:o.contentType}),ct=dt=>cleanDigitSectionValue(a,i,dt,lt,o),rt=o.type==="minutes"&&_e!=null&&_e.minutesStep?_e.minutesStep:1;let ot=parseInt(o.value,10)+et*rt;if(at){if(o.type==="year"&&!nt&&!tt)return a.formatByString(a.dateWithTimezone(void 0,i),o.format);et>0||tt?ot=lt.minimum:ot=lt.maximum}return ot%rt!==0&&((et<0||tt)&&(ot+=rt-(rt+ot)%rt),(et>0||nt)&&(ot-=ot%rt)),ot>lt.maximum?ct(lt.minimum+(ot-lt.maximum-1)%(lt.maximum-lt.minimum+1)):ot<lt.minimum?ct(lt.maximum-(lt.minimum-ot-1)%(lt.maximum-lt.minimum+1)):ct(ot)},st=()=>{const lt=getLetterEditingOptions(a,i,o.type,o.format);if(lt.length===0)return o.value;if(at)return et>0||tt?lt[0]:lt[lt.length-1];const rt=(lt.indexOf(o.value)+lt.length+et)%lt.length;return lt[rt]};return o.contentType==="digit"||o.contentType==="digit-with-letter"?it():st()},getSectionVisibleValue=(a,i)=>{let o=a.value||a.placeholder;const s=i==="non-input"?a.hasLeadingZerosInFormat:a.hasLeadingZerosInInput;return i==="non-input"&&a.hasLeadingZerosInInput&&!a.hasLeadingZerosInFormat&&(o=Number(o).toString()),["input-rtl","input-ltr"].includes(i)&&a.contentType==="digit"&&!s&&o.length===1&&(o=`${o}‎`),i==="input-rtl"&&(o=`⁨${o}⁩`),o},cleanString=a=>a.replace(/[\u2066\u2067\u2068\u2069]/g,""),addPositionPropertiesToSections=(a,i)=>{let o=0,s=i?1:0;const $=[];for(let j=0;j<a.length;j+=1){const _e=a[j],et=getSectionVisibleValue(_e,i?"input-rtl":"input-ltr"),tt=`${_e.startSeparator}${et}${_e.endSeparator}`,nt=cleanString(tt).length,at=tt.length,it=cleanString(et),st=s+et.indexOf(it[0])+_e.startSeparator.length,lt=st+it.length;$.push(_extends({},_e,{start:o,end:o+nt,startInInput:st,endInInput:lt})),o+=nt,s+=at}return $},getSectionPlaceholder=(a,i,o,s,$)=>{switch(s.type){case"year":return o.fieldYearPlaceholder({digitAmount:a.formatByString(a.dateWithTimezone(void 0,i),$).length,format:$});case"month":return o.fieldMonthPlaceholder({contentType:s.contentType,format:$});case"day":return o.fieldDayPlaceholder({format:$});case"weekDay":return o.fieldWeekDayPlaceholder({contentType:s.contentType,format:$});case"hours":return o.fieldHoursPlaceholder({format:$});case"minutes":return o.fieldMinutesPlaceholder({format:$});case"seconds":return o.fieldSecondsPlaceholder({format:$});case"meridiem":return o.fieldMeridiemPlaceholder({format:$});default:return $}},changeSectionValueFormat=(a,i,o,s)=>a.formatByString(a.parse(i,o),s),isFourDigitYearFormat=(a,i,o)=>a.formatByString(a.dateWithTimezone(void 0,i),o).length===4,doesSectionFormatHaveLeadingZeros=(a,i,o,s,$)=>{if(o!=="digit")return!1;const j=a.dateWithTimezone(void 0,i);switch(s){case"year":return isFourDigitYearFormat(a,i,$)?a.formatByString(a.setYear(j,1),$)==="0001":a.formatByString(a.setYear(j,2001),$)==="01";case"month":return a.formatByString(a.startOfYear(j),$).length>1;case"day":return a.formatByString(a.startOfMonth(j),$).length>1;case"weekDay":return a.formatByString(a.startOfWeek(j),$).length>1;case"hours":return a.formatByString(a.setHours(j,1),$).length>1;case"minutes":return a.formatByString(a.setMinutes(j,1),$).length>1;case"seconds":return a.formatByString(a.setSeconds(j,1),$).length>1;default:throw new Error("Invalid section type")}},getEscapedPartsFromFormat=(a,i)=>{const o=[],{start:s,end:$}=a.escapedCharacters,j=new RegExp(`(\\${s}[^\\${$}]*\\${$})+`,"g");let _e=null;for(;_e=j.exec(i);)o.push({start:_e.index,end:j.lastIndex-1});return o},splitFormatIntoSections=(a,i,o,s,$,j,_e,et)=>{let tt="";const nt=[],at=a.date(),it=pt=>{if(pt==="")return null;const mt=getDateSectionConfigFromFormatToken(a,pt),ft=doesSectionFormatHaveLeadingZeros(a,i,mt.contentType,mt.type,pt),ht=_e?ft:mt.contentType==="digit",yt=$!=null&&a.isValid($);let bt=yt?a.formatByString($,pt):"",gt=null;if(ht)if(ft)gt=bt===""?a.formatByString(at,pt).length:bt.length;else{if(mt.maxLength==null)throw new Error(`MUI: The token ${pt} should have a 'maxDigitNumber' property on it's adapter`);gt=mt.maxLength,yt&&(bt=cleanLeadingZeros(a,bt,gt))}return nt.push(_extends({},mt,{format:pt,maxLength:gt,value:bt,placeholder:getSectionPlaceholder(a,i,o,mt,pt),hasLeadingZeros:ft,hasLeadingZerosInFormat:ft,hasLeadingZerosInInput:ht,startSeparator:nt.length===0?tt:"",endSeparator:"",modified:!1})),null};let st=10,lt=s,ct=a.expandFormat(s);for(;ct!==lt;)if(lt=ct,ct=a.expandFormat(lt),st-=1,st<0)throw new Error("MUI: The format expansion seems to be  enter in an infinite loop. Please open an issue with the format passed to the picker component");const rt=ct,ut=getEscapedPartsFromFormat(a,rt),ot=new RegExp(`^(${Object.keys(a.formatTokenMap).sort((pt,mt)=>mt.length-pt.length).join("|")})`,"g");let dt="";for(let pt=0;pt<rt.length;pt+=1){const mt=ut.find(gt=>gt.start<=pt&&gt.end>=pt),ft=rt[pt],ht=mt!=null,yt=`${dt}${rt.slice(pt)}`,bt=ot.test(yt);!ht&&ft.match(/([A-Za-z]+)/)&&bt?(dt=yt.slice(0,ot.lastIndex),pt+=ot.lastIndex-1):ht&&(mt==null?void 0:mt.start)===pt||(mt==null?void 0:mt.end)===pt||(it(dt),dt="",nt.length===0?tt+=ft:nt[nt.length-1].endSeparator+=ft)}return it(dt),nt.map(pt=>{const mt=ft=>{let ht=ft;return et&&ht!==null&&ht.includes(" ")&&(ht=`⁩${ht}⁦`),j==="spacious"&&["/",".","-"].includes(ht)&&(ht=` ${ht} `),ht};return pt.startSeparator=mt(pt.startSeparator),pt.endSeparator=mt(pt.endSeparator),pt})},getDateFromDateSections=(a,i)=>{const o=i.some(et=>et.type==="day"),s=[],$=[];for(let et=0;et<i.length;et+=1){const tt=i[et];o&&tt.type==="weekDay"||(s.push(tt.format),$.push(getSectionVisibleValue(tt,"non-input")))}const j=s.join(" "),_e=$.join(" ");return a.parse(_e,j)},createDateStrForInputFromSections=(a,i)=>{const s=a.map($=>{const j=getSectionVisibleValue($,i?"input-rtl":"input-ltr");return`${$.startSeparator}${j}${$.endSeparator}`}).join("");return i?`⁦${s}⁩`:s},getSectionsBoundaries=(a,i)=>{const o=a.dateWithTimezone(void 0,i),s=a.endOfYear(o),$=a.endOfDay(o),{maxDaysInMonth:j,longestMonth:_e}=getMonthsInYear(a,o).reduce((et,tt)=>{const nt=a.getDaysInMonth(tt);return nt>et.maxDaysInMonth?{maxDaysInMonth:nt,longestMonth:tt}:et},{maxDaysInMonth:0,longestMonth:null});return{year:({format:et})=>({minimum:0,maximum:isFourDigitYearFormat(a,i,et)?9999:99}),month:()=>({minimum:1,maximum:a.getMonth(s)+1}),day:({currentDate:et})=>({minimum:1,maximum:et!=null&&a.isValid(et)?a.getDaysInMonth(et):j,longestMonth:_e}),weekDay:({format:et,contentType:tt})=>{if(tt==="digit"){const nt=getDaysInWeekStr(a,i,et).map(Number);return{minimum:Math.min(...nt),maximum:Math.max(...nt)}}return{minimum:1,maximum:7}},hours:({format:et})=>{const tt=a.getHours($);return a.formatByString(a.endOfDay(o),et)!==tt.toString()?{minimum:1,maximum:Number(a.formatByString(a.startOfDay(o),et))}:{minimum:0,maximum:tt}},minutes:()=>({minimum:0,maximum:a.getMinutes($)}),seconds:()=>({minimum:0,maximum:a.getSeconds($)}),meridiem:()=>({minimum:0,maximum:0})}},transferDateSectionValue=(a,i,o,s,$)=>{switch(o.type){case"year":return a.setYear($,a.getYear(s));case"month":return a.setMonth($,a.getMonth(s));case"weekDay":{const j=getDaysInWeekStr(a,i,o.format),_e=a.formatByString(s,o.format),et=j.indexOf(_e),nt=j.indexOf(o.value)-et;return a.addDays(s,nt)}case"day":return a.setDate($,a.getDate(s));case"meridiem":{const j=a.getHours(s)<12,_e=a.getHours($);return j&&_e>=12?a.addHours($,-12):!j&&_e<12?a.addHours($,12):$}case"hours":return a.setHours($,a.getHours(s));case"minutes":return a.setMinutes($,a.getMinutes(s));case"seconds":return a.setSeconds($,a.getSeconds(s));default:return $}},reliableSectionModificationOrder={year:1,month:2,day:3,weekDay:4,hours:5,minutes:6,seconds:7,meridiem:8},mergeDateIntoReferenceDate=(a,i,o,s,$,j)=>[...s].sort((_e,et)=>reliableSectionModificationOrder[_e.type]-reliableSectionModificationOrder[et.type]).reduce((_e,et)=>!j||et.modified?transferDateSectionValue(a,i,et,o,_e):_e,$),isAndroid=()=>navigator.userAgent.toLowerCase().indexOf("android")>-1,getSectionOrder=(a,i)=>{const o={};if(!i)return a.forEach((tt,nt)=>{const at=nt===0?null:nt-1,it=nt===a.length-1?null:nt+1;o[nt]={leftIndex:at,rightIndex:it}}),{neighbors:o,startIndex:0,endIndex:a.length-1};const s={},$={};let j=0,_e=0,et=a.length-1;for(;et>=0;){_e=a.findIndex((tt,nt)=>{var at;return nt>=j&&((at=tt.endSeparator)==null?void 0:at.includes(" "))&&tt.endSeparator!==" / "}),_e===-1&&(_e=a.length-1);for(let tt=_e;tt>=j;tt-=1)$[tt]=et,s[et]=tt,et-=1;j=_e+1}return a.forEach((tt,nt)=>{const at=$[nt],it=at===0?null:s[at-1],st=at===a.length-1?null:s[at+1];o[nt]={leftIndex:it,rightIndex:st}}),{neighbors:o,startIndex:s[0],endIndex:s[a.length-1]}},_excluded$l=["value","referenceDate"],singleItemValueManager={emptyValue:null,getTodayValue:getTodayDate,getInitialReferenceValue:a=>{let{value:i,referenceDate:o}=a,s=_objectWithoutPropertiesLoose(a,_excluded$l);return i!=null&&s.utils.isValid(i)?i:o??getDefaultReferenceDate(s)},cleanValue:replaceInvalidDateByNull,areValuesEqual:areDatesEqual,isSameError:(a,i)=>a===i,hasError:a=>a!=null,defaultErrorState:null,getTimezone:(a,i)=>i==null||!a.isValid(i)?null:a.getTimezone(i),setTimezone:(a,i,o)=>o==null?null:a.setTimezone(o,i)},singleItemFieldValueManager={updateReferenceValue:(a,i,o)=>i==null||!a.isValid(i)?o:i,getSectionsFromValue:(a,i,o,s,$)=>!a.isValid(i)&&!!o?o:addPositionPropertiesToSections($(i),s),getValueStrFromSections:createDateStrForInputFromSections,getActiveDateManager:(a,i)=>({date:i.value,referenceDate:i.referenceValue,getSections:o=>o,getNewValuesFromNewActiveDate:o=>({value:o,referenceValue:o==null||!a.isValid(o)?i.referenceValue:o})}),parseValueStr:(a,i,o)=>o(a.trim(),i)},uncapitalizeObjectKeys=a=>{if(a!==void 0)return Object.keys(a).reduce((i,o)=>_extends({},i,{[`${o.slice(0,1).toLowerCase()}${o.slice(1)}`]:a[o]}),{})};function getPickersDayUtilityClass(a){return generateUtilityClass$1("MuiPickersDay",a)}const pickersDayClasses=generateUtilityClasses$1("MuiPickersDay",["root","dayWithMargin","dayOutsideMonth","hiddenDaySpacingFiller","today","selected","disabled"]),_excluded$k=["autoFocus","className","day","disabled","disableHighlightToday","disableMargin","hidden","isAnimating","onClick","onDaySelect","onFocus","onBlur","onKeyDown","onMouseDown","onMouseEnter","outsideCurrentMonth","selected","showDaysOutsideCurrentMonth","children","today","isFirstVisibleCell","isLastVisibleCell"],useUtilityClasses$f=a=>{const{selected:i,disableMargin:o,disableHighlightToday:s,today:$,disabled:j,outsideCurrentMonth:_e,showDaysOutsideCurrentMonth:et,classes:tt}=a,nt=_e&&!et;return composeClasses({root:["root",i&&!nt&&"selected",j&&"disabled",!o&&"dayWithMargin",!s&&$&&"today",_e&&et&&"dayOutsideMonth",nt&&"hiddenDaySpacingFiller"],hiddenDaySpacingFiller:["hiddenDaySpacingFiller"]},getPickersDayUtilityClass,tt)},styleArg=({theme:a,ownerState:i})=>_extends({},a.typography.caption,{width:DAY_SIZE,height:DAY_SIZE,borderRadius:"50%",padding:0,backgroundColor:"transparent",transition:a.transitions.create("background-color",{duration:a.transitions.duration.short}),color:(a.vars||a).palette.text.primary,"@media (pointer: fine)":{"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha$1(a.palette.primary.main,a.palette.action.hoverOpacity)}},"&:focus":{backgroundColor:a.vars?`rgba(${a.vars.palette.primary.mainChannel} / ${a.vars.palette.action.focusOpacity})`:alpha$1(a.palette.primary.main,a.palette.action.focusOpacity),[`&.${pickersDayClasses.selected}`]:{willChange:"background-color",backgroundColor:(a.vars||a).palette.primary.dark}},[`&.${pickersDayClasses.selected}`]:{color:(a.vars||a).palette.primary.contrastText,backgroundColor:(a.vars||a).palette.primary.main,fontWeight:a.typography.fontWeightMedium,"&:hover":{willChange:"background-color",backgroundColor:(a.vars||a).palette.primary.dark}},[`&.${pickersDayClasses.disabled}:not(.${pickersDayClasses.selected})`]:{color:(a.vars||a).palette.text.disabled},[`&.${pickersDayClasses.disabled}&.${pickersDayClasses.selected}`]:{opacity:.6}},!i.disableMargin&&{margin:`0 ${DAY_MARGIN}px`},i.outsideCurrentMonth&&i.showDaysOutsideCurrentMonth&&{color:(a.vars||a).palette.text.secondary},!i.disableHighlightToday&&i.today&&{[`&:not(.${pickersDayClasses.selected})`]:{border:`1px solid ${(a.vars||a).palette.text.secondary}`}}),overridesResolver=(a,i)=>{const{ownerState:o}=a;return[i.root,!o.disableMargin&&i.dayWithMargin,!o.disableHighlightToday&&o.today&&i.today,!o.outsideCurrentMonth&&o.showDaysOutsideCurrentMonth&&i.dayOutsideMonth,o.outsideCurrentMonth&&!o.showDaysOutsideCurrentMonth&&i.hiddenDaySpacingFiller]},PickersDayRoot=styled(ButtonBase$1,{name:"MuiPickersDay",slot:"Root",overridesResolver})(styleArg),PickersDayFiller=styled("div",{name:"MuiPickersDay",slot:"Root",overridesResolver})(({theme:a,ownerState:i})=>_extends({},styleArg({theme:a,ownerState:i}),{opacity:0,pointerEvents:"none"})),noop=()=>{},PickersDayRaw=reactExports.forwardRef(function a(i,o){const s=useThemeProps$6({props:i,name:"MuiPickersDay"}),{autoFocus:$=!1,className:j,day:_e,disabled:et=!1,disableHighlightToday:tt=!1,disableMargin:nt=!1,isAnimating:at,onClick:it,onDaySelect:st,onFocus:lt=noop,onBlur:ct=noop,onKeyDown:rt=noop,onMouseDown:ut=noop,onMouseEnter:ot=noop,outsideCurrentMonth:dt,selected:pt=!1,showDaysOutsideCurrentMonth:mt=!1,children:ft,today:ht=!1}=s,yt=_objectWithoutPropertiesLoose(s,_excluded$k),bt=_extends({},s,{autoFocus:$,disabled:et,disableHighlightToday:tt,disableMargin:nt,selected:pt,showDaysOutsideCurrentMonth:mt,today:ht}),gt=useUtilityClasses$f(bt),xt=useUtils(),vt=reactExports.useRef(null),Lt=useForkRef(vt,o);useEnhancedEffect(()=>{$&&!et&&!at&&!dt&&vt.current.focus()},[$,et,at,dt]);const $t=Et=>{ut(Et),dt&&Et.preventDefault()},Tt=Et=>{et||st(_e),dt&&Et.currentTarget.focus(),it&&it(Et)};return dt&&!mt?jsxRuntimeExports.jsx(PickersDayFiller,{className:clsx(gt.root,gt.hiddenDaySpacingFiller,j),ownerState:bt,role:yt.role}):jsxRuntimeExports.jsx(PickersDayRoot,_extends({className:clsx(gt.root,j),ref:Lt,centerRipple:!0,disabled:et,tabIndex:pt?0:-1,onKeyDown:Et=>rt(Et,_e),onFocus:Et=>lt(Et,_e),onBlur:Et=>ct(Et,_e),onMouseEnter:Et=>ot(Et,_e),onClick:Tt,onMouseDown:$t},yt,{ownerState:bt,children:ft||xt.format(_e,"dayOfMonth")}))}),PickersDay=reactExports.memo(PickersDayRaw);function useValidation(a,i,o,s){const{value:$,onError:j}=a,_e=useLocalizationContext(),et=reactExports.useRef(s),tt=i({adapter:_e,value:$,props:a});return reactExports.useEffect(()=>{j&&!o(tt,et.current)&&j(tt,$),et.current=tt},[o,j,et,tt,$]),tt}const useFieldState=a=>{const i=useUtils(),o=useLocaleText(),s=useLocalizationContext(),j=useTheme$1().direction==="rtl",{valueManager:_e,fieldValueManager:et,valueType:tt,validator:nt,internalProps:at,internalProps:{value:it,defaultValue:st,referenceDate:lt,onChange:ct,format:rt,formatDensity:ut="dense",selectedSections:ot,onSelectedSectionsChange:dt,shouldRespectLeadingZeros:pt=!1,timezone:mt}}=a,{timezone:ft,value:ht,handleValueChange:yt}=useValueWithTimezone({timezone:mt,value:it,defaultValue:st,onChange:ct,valueManager:_e}),bt=reactExports.useMemo(()=>getSectionsBoundaries(i,ft),[i,ft]),gt=reactExports.useCallback((wt,kt=null)=>et.getSectionsFromValue(i,wt,kt,j,At=>splitFormatIntoSections(i,ft,o,rt,At,ut,pt,j)),[et,rt,o,j,pt,i,ut,ft]),xt=reactExports.useMemo(()=>et.getValueStrFromSections(gt(_e.emptyValue),j),[et,gt,_e.emptyValue,j]),[vt,Lt]=reactExports.useState(()=>{const wt=gt(ht),kt={sections:wt,value:ht,referenceValue:_e.emptyValue,tempValueStrAndroid:null},At=getSectionTypeGranularity(wt),Pt=_e.getInitialReferenceValue({referenceDate:lt,value:ht,utils:i,props:at,granularity:At,timezone:ft});return _extends({},kt,{referenceValue:Pt})}),[$t,Tt]=useControlled({controlled:ot,default:null,name:"useField",state:"selectedSectionIndexes"}),Et=wt=>{Tt(wt),dt==null||dt(wt),Lt(kt=>_extends({},kt,{selectedSectionQuery:null}))},Dt=reactExports.useMemo(()=>{if($t==null)return null;if($t==="all")return{startIndex:0,endIndex:vt.sections.length-1,shouldSelectBoundarySelectors:!0};if(typeof $t=="number")return{startIndex:$t,endIndex:$t};if(typeof $t=="string"){const wt=vt.sections.findIndex(kt=>kt.type===$t);return{startIndex:wt,endIndex:wt}}return $t},[$t,vt.sections]),It=({value:wt,referenceValue:kt,sections:At})=>{if(Lt(Mt=>_extends({},Mt,{sections:At,value:wt,referenceValue:kt,tempValueStrAndroid:null})),_e.areValuesEqual(i,vt.value,wt))return;const Pt={validationError:nt({adapter:s,value:wt,props:_extends({},at,{value:wt,timezone:ft})})};yt(wt,Pt)},Ct=(wt,kt)=>{const At=[...vt.sections];return At[wt]=_extends({},At[wt],{value:kt,modified:!0}),addPositionPropertiesToSections(At,j)},jt=()=>{It({value:_e.emptyValue,referenceValue:vt.referenceValue,sections:gt(_e.emptyValue)})},Zt=()=>{if(Dt==null)return;const wt=vt.sections[Dt.startIndex],kt=et.getActiveDateManager(i,vt,wt),Pt=kt.getSections(vt.sections).filter(zt=>zt.value!=="").length===(wt.value===""?0:1),Mt=Ct(Dt.startIndex,""),Ot=Pt?null:i.date(new Date("")),Bt=kt.getNewValuesFromNewActiveDate(Ot);(Ot!=null&&!i.isValid(Ot))!=(kt.date!=null&&!i.isValid(kt.date))?It(_extends({},Bt,{sections:Mt})):Lt(zt=>_extends({},zt,Bt,{sections:Mt,tempValueStrAndroid:null}))},Xt=wt=>{const kt=(Mt,Ot)=>{const Bt=i.parse(Mt,rt);if(Bt==null||!i.isValid(Bt))return null;const zt=splitFormatIntoSections(i,ft,o,rt,Bt,ut,pt,j);return mergeDateIntoReferenceDate(i,ft,Bt,zt,Ot,!1)},At=et.parseValueStr(wt,vt.referenceValue,kt),Pt=et.updateReferenceValue(i,At,vt.referenceValue);It({value:At,referenceValue:Pt,sections:gt(At,vt.sections)})},sn=({activeSection:wt,newSectionValue:kt,shouldGoToNextSection:At})=>{At&&Dt&&Dt.startIndex<vt.sections.length-1?Et(Dt.startIndex+1):Dt&&Dt.startIndex!==Dt.endIndex&&Et(Dt.startIndex);const Pt=et.getActiveDateManager(i,vt,wt),Mt=Ct(Dt.startIndex,kt),Ot=Pt.getSections(Mt),Bt=getDateFromDateSections(i,Ot);let zt,Gt;if(Bt!=null&&i.isValid(Bt)){const Wt=mergeDateIntoReferenceDate(i,ft,Bt,Ot,Pt.referenceDate,!0);zt=Pt.getNewValuesFromNewActiveDate(Wt),Gt=!0}else zt=Pt.getNewValuesFromNewActiveDate(Bt),Gt=(Bt!=null&&!i.isValid(Bt))!=(Pt.date!=null&&!i.isValid(Pt.date));return Gt?It(_extends({},zt,{sections:Mt})):Lt(Wt=>_extends({},Wt,zt,{sections:Mt,tempValueStrAndroid:null}))},Ft=wt=>Lt(kt=>_extends({},kt,{tempValueStrAndroid:wt}));return reactExports.useEffect(()=>{const wt=gt(vt.value);Lt(kt=>_extends({},kt,{sections:wt}))},[rt,i.locale]),reactExports.useEffect(()=>{let wt=!1;_e.areValuesEqual(i,vt.value,ht)?wt=_e.getTimezone(i,vt.value)!==_e.getTimezone(i,ht):wt=!0,wt&&Lt(kt=>_extends({},kt,{value:ht,referenceValue:et.updateReferenceValue(i,ht,kt.referenceValue),sections:gt(ht)}))},[ht]),{state:vt,selectedSectionIndexes:Dt,setSelectedSections:Et,clearValue:jt,clearActiveSection:Zt,updateSectionValue:sn,updateValueFromValueStr:Xt,setTempAndroidValueStr:Ft,sectionsValueBoundaries:bt,placeholder:xt,timezone:ft}},QUERY_LIFE_DURATION_MS=5e3,isQueryResponseWithoutValue=a=>a.saveQuery!=null,useFieldCharacterEditing=({sections:a,updateSectionValue:i,sectionsValueBoundaries:o,setTempAndroidValueStr:s,timezone:$})=>{const j=useUtils(),[_e,et]=reactExports.useState(null),tt=useEventCallback(()=>et(null));reactExports.useEffect(()=>{var lt;_e!=null&&((lt=a[_e.sectionIndex])==null?void 0:lt.type)!==_e.sectionType&&tt()},[a,_e,tt]),reactExports.useEffect(()=>{if(_e!=null){const lt=setTimeout(()=>tt(),QUERY_LIFE_DURATION_MS);return()=>{window.clearTimeout(lt)}}return()=>{}},[_e,tt]);const nt=({keyPressed:lt,sectionIndex:ct},rt,ut)=>{const ot=lt.toLowerCase(),dt=a[ct];if(_e!=null&&(!ut||ut(_e.value))&&_e.sectionIndex===ct){const mt=`${_e.value}${ot}`,ft=rt(mt,dt);if(!isQueryResponseWithoutValue(ft))return et({sectionIndex:ct,value:mt,sectionType:dt.type}),ft}const pt=rt(ot,dt);return isQueryResponseWithoutValue(pt)&&!pt.saveQuery?(tt(),null):(et({sectionIndex:ct,value:ot,sectionType:dt.type}),isQueryResponseWithoutValue(pt)?null:pt)},at=lt=>{const ct=(ot,dt,pt)=>{const mt=dt.filter(ft=>ft.toLowerCase().startsWith(pt));return mt.length===0?{saveQuery:!1}:{sectionValue:mt[0],shouldGoToNextSection:mt.length===1}},rt=(ot,dt,pt,mt)=>{const ft=ht=>getLetterEditingOptions(j,$,dt.type,ht);if(dt.contentType==="letter")return ct(dt.format,ft(dt.format),ot);if(pt&&mt!=null&&getDateSectionConfigFromFormatToken(j,pt).contentType==="letter"){const ht=ft(pt),yt=ct(pt,ht,ot);return isQueryResponseWithoutValue(yt)?{saveQuery:!1}:_extends({},yt,{sectionValue:mt(yt.sectionValue,ht)})}return{saveQuery:!1}};return nt(lt,(ot,dt)=>{switch(dt.type){case"month":{const pt=mt=>changeSectionValueFormat(j,mt,j.formats.month,dt.format);return rt(ot,dt,j.formats.month,pt)}case"weekDay":{const pt=(mt,ft)=>ft.indexOf(mt).toString();return rt(ot,dt,j.formats.weekday,pt)}case"meridiem":return rt(ot,dt);default:return{saveQuery:!1}}})},it=lt=>{const ct=(ut,ot)=>{const dt=+`${ut}`,pt=o[ot.type]({currentDate:null,format:ot.format,contentType:ot.contentType});if(dt>pt.maximum)return{saveQuery:!1};if(dt<pt.minimum)return{saveQuery:!0};const mt=+`${ut}0`>pt.maximum||ut.length===pt.maximum.toString().length;return{sectionValue:cleanDigitSectionValue(j,$,dt,pt,ot),shouldGoToNextSection:mt}};return nt(lt,(ut,ot)=>{if(ot.contentType==="digit"||ot.contentType==="digit-with-letter")return ct(ut,ot);if(ot.type==="month"){const dt=doesSectionFormatHaveLeadingZeros(j,$,"digit","month","MM"),pt=ct(ut,{type:ot.type,format:"MM",hasLeadingZerosInFormat:dt,hasLeadingZerosInInput:!0,contentType:"digit",maxLength:2});if(isQueryResponseWithoutValue(pt))return pt;const mt=changeSectionValueFormat(j,pt.sectionValue,"MM",ot.format);return _extends({},pt,{sectionValue:mt})}if(ot.type==="weekDay"){const dt=ct(ut,ot);if(isQueryResponseWithoutValue(dt))return dt;const pt=getDaysInWeekStr(j,$,ot.format)[Number(dt.sectionValue)-1];return _extends({},dt,{sectionValue:pt})}return{saveQuery:!1}},ut=>!Number.isNaN(Number(ut)))};return{applyCharacterEditing:useEventCallback(lt=>{const ct=a[lt.sectionIndex],ut=!Number.isNaN(Number(lt.keyPressed))?it(lt):at(lt);ut==null?s(null):i({activeSection:ct,newSectionValue:ut.sectionValue,shouldGoToNextSection:ut.shouldGoToNextSection})}),resetCharacterQuery:tt}};function arrayIncludes(a,i){return Array.isArray(i)?i.every(o=>a.indexOf(o)!==-1):a.indexOf(i)!==-1}const onSpaceOrEnter=(a,i)=>o=>{(o.key==="Enter"||o.key===" ")&&(a(o),o.preventDefault(),o.stopPropagation()),i&&i(o)},getActiveElement=(a=document)=>{const i=a.activeElement;return i?i.shadowRoot?getActiveElement(i.shadowRoot):i:null},DEFAULT_DESKTOP_MODE_MEDIA_QUERY="@media (pointer: fine)",_excluded$j=["onClick","onKeyDown","onFocus","onBlur","onMouseUp","onPaste","error","clearable","onClear","disabled"],useField=a=>{const i=useUtils(),{state:o,selectedSectionIndexes:s,setSelectedSections:$,clearValue:j,clearActiveSection:_e,updateSectionValue:et,updateValueFromValueStr:tt,setTempAndroidValueStr:nt,sectionsValueBoundaries:at,placeholder:it,timezone:st}=useFieldState(a),{inputRef:lt,internalProps:ct,internalProps:{readOnly:rt=!1,unstableFieldRef:ut,minutesStep:ot},forwardedProps:{onClick:dt,onKeyDown:pt,onFocus:mt,onBlur:ft,onMouseUp:ht,onPaste:yt,error:bt,clearable:gt,onClear:xt,disabled:vt},fieldValueManager:Lt,valueManager:$t,validator:Tt}=a,Et=_objectWithoutPropertiesLoose(a.forwardedProps,_excluded$j),{applyCharacterEditing:Dt,resetCharacterQuery:It}=useFieldCharacterEditing({sections:o.sections,updateSectionValue:et,sectionsValueBoundaries:at,setTempAndroidValueStr:nt,timezone:st}),Ct=reactExports.useRef(null),jt=useForkRef(lt,Ct),Zt=reactExports.useRef(void 0),sn=useTheme$1().direction==="rtl",Ft=reactExports.useMemo(()=>getSectionOrder(o.sections,sn),[o.sections,sn]),wt=()=>{var cn;if(rt){$(null);return}const xn=(cn=Ct.current.selectionStart)!=null?cn:0;let hn;xn<=o.sections[0].startInInput||xn>=o.sections[o.sections.length-1].endInInput?hn=1:hn=o.sections.findIndex(Jt=>Jt.startInInput-Jt.startSeparator.length>xn);const en=hn===-1?o.sections.length-1:hn-1;$(en)},kt=useEventCallback((cn,...xn)=>{cn.isDefaultPrevented()||(dt==null||dt(cn,...xn),wt())}),At=useEventCallback(cn=>{ht==null||ht(cn),cn.preventDefault()}),Pt=useEventCallback((...cn)=>{mt==null||mt(...cn);const xn=Ct.current;window.clearTimeout(Zt.current),Zt.current=setTimeout(()=>{!xn||xn!==Ct.current||s!=null||rt||(xn.value.length&&Number(xn.selectionEnd)-Number(xn.selectionStart)===xn.value.length?$("all"):wt())})}),Mt=useEventCallback((...cn)=>{ft==null||ft(...cn),$(null)}),Ot=useEventCallback(cn=>{if(yt==null||yt(cn),rt){cn.preventDefault();return}const xn=cn.clipboardData.getData("text");if(s&&s.startIndex===s.endIndex){const hn=o.sections[s.startIndex],en=/^[a-zA-Z]+$/.test(xn),Jt=/^[0-9]+$/.test(xn),vn=/^(([a-zA-Z]+)|)([0-9]+)(([a-zA-Z]+)|)$/.test(xn);if(hn.contentType==="letter"&&en||hn.contentType==="digit"&&Jt||hn.contentType==="digit-with-letter"&&vn){It(),et({activeSection:hn,newSectionValue:xn,shouldGoToNextSection:!0}),cn.preventDefault();return}if(en||Jt){cn.preventDefault();return}}cn.preventDefault(),It(),tt(xn)}),Bt=useEventCallback(cn=>{if(rt)return;const xn=cn.target.value;if(xn===""){It(),j();return}const hn=cn.nativeEvent.data,en=hn&&hn.length>1,Jt=en?hn:xn,vn=cleanString(Jt);if(s==null||en){tt(en?hn:vn);return}let $n;if(s.startIndex===0&&s.endIndex===o.sections.length-1&&vn.length===1)$n=vn;else{const Mn=cleanString(Lt.getValueStrFromSections(o.sections,sn));let On=-1,En=-1;for(let _n=0;_n<Mn.length;_n+=1)On===-1&&Mn[_n]!==vn[_n]&&(On=_n),En===-1&&Mn[Mn.length-_n-1]!==vn[vn.length-_n-1]&&(En=_n);const Bn=o.sections[s.startIndex];if(On<Bn.start||Mn.length-En-1>Bn.end)return;const Wn=vn.length-Mn.length+Bn.end-cleanString(Bn.endSeparator||"").length;$n=vn.slice(Bn.start+cleanString(Bn.startSeparator||"").length,Wn)}if($n.length===0){isAndroid()?nt(Jt):(It(),_e());return}Dt({keyPressed:$n,sectionIndex:s.startIndex})}),zt=useEventCallback(cn=>{switch(pt==null||pt(cn),!0){case(cn.key==="a"&&(cn.ctrlKey||cn.metaKey)):{cn.preventDefault(),$("all");break}case cn.key==="ArrowRight":{if(cn.preventDefault(),s==null)$(Ft.startIndex);else if(s.startIndex!==s.endIndex)$(s.endIndex);else{const xn=Ft.neighbors[s.startIndex].rightIndex;xn!==null&&$(xn)}break}case cn.key==="ArrowLeft":{if(cn.preventDefault(),s==null)$(Ft.endIndex);else if(s.startIndex!==s.endIndex)$(s.startIndex);else{const xn=Ft.neighbors[s.startIndex].leftIndex;xn!==null&&$(xn)}break}case cn.key==="Delete":{if(cn.preventDefault(),rt)break;s==null||s.startIndex===0&&s.endIndex===o.sections.length-1?j():_e(),It();break}case["ArrowUp","ArrowDown","Home","End","PageUp","PageDown"].includes(cn.key):{if(cn.preventDefault(),rt||s==null)break;const xn=o.sections[s.startIndex],hn=Lt.getActiveDateManager(i,o,xn),en=adjustSectionValue(i,st,xn,cn.key,at,hn.date,{minutesStep:ot});et({activeSection:xn,newSectionValue:en,shouldGoToNextSection:!1});break}}});useEnhancedEffect(()=>{if(!Ct.current)return;if(s==null){Ct.current.scrollLeft&&(Ct.current.scrollLeft=0);return}const cn=o.sections[s.startIndex],xn=o.sections[s.endIndex];let hn=cn.startInInput,en=xn.endInInput;if(s.shouldSelectBoundarySelectors&&(hn-=cn.startSeparator.length,en+=xn.endSeparator.length),hn!==Ct.current.selectionStart||en!==Ct.current.selectionEnd){const Jt=Ct.current.scrollTop;Ct.current===getActiveElement(document)&&Ct.current.setSelectionRange(hn,en),Ct.current.scrollTop=Jt}});const Gt=useValidation(_extends({},ct,{value:o.value,timezone:st}),Tt,$t.isSameError,$t.defaultErrorState),Wt=reactExports.useMemo(()=>bt!==void 0?bt:$t.hasError(Gt),[$t,Gt,bt]);reactExports.useEffect(()=>{!Wt&&!s&&It()},[o.referenceValue,s,Wt]),reactExports.useEffect(()=>(Ct.current&&Ct.current===document.activeElement&&$("all"),()=>window.clearTimeout(Zt.current)),[]),reactExports.useEffect(()=>{o.tempValueStrAndroid!=null&&s!=null&&(It(),_e())},[o.tempValueStrAndroid]);const qt=reactExports.useMemo(()=>{var cn;return(cn=o.tempValueStrAndroid)!=null?cn:Lt.getValueStrFromSections(o.sections,sn)},[o.sections,Lt,o.tempValueStrAndroid,sn]),tn=reactExports.useMemo(()=>s==null||o.sections[s.startIndex].contentType==="letter"?"text":"numeric",[s,o.sections]),ln=Ct.current&&Ct.current===getActiveElement(document),gn=$t.areValuesEqual(i,o.value,$t.emptyValue),yn=!ln&&gn;reactExports.useImperativeHandle(ut,()=>({getSections:()=>o.sections,getActiveSectionIndex:()=>{var cn,xn;const hn=(cn=Ct.current.selectionStart)!=null?cn:0,en=(xn=Ct.current.selectionEnd)!=null?xn:0;if(hn===0&&en===0)return null;const Jt=hn<=o.sections[0].startInInput?1:o.sections.findIndex(vn=>vn.startInInput-vn.startSeparator.length>hn);return Jt===-1?o.sections.length-1:Jt-1},setSelectedSections:cn=>$(cn)}));const Pn=useEventCallback((cn,...xn)=>{var hn;cn.preventDefault(),xt==null||xt(cn,...xn),j(),Ct==null||(hn=Ct.current)==null||hn.focus(),$(0)});return _extends({placeholder:it,autoComplete:"off",disabled:!!vt},Et,{value:yn?"":qt,inputMode:tn,readOnly:rt,onClick:kt,onFocus:Pt,onBlur:Mt,onPaste:Ot,onChange:Bt,onKeyDown:zt,onMouseUp:At,onClear:Pn,error:Wt,ref:jt,clearable:!!(gt&&!gn&&!rt&&!vt)})},validateDate=({props:a,value:i,adapter:o})=>{if(i===null)return null;const{shouldDisableDate:s,shouldDisableMonth:$,shouldDisableYear:j,disablePast:_e,disableFuture:et,timezone:tt}=a,nt=o.utils.dateWithTimezone(void 0,tt),at=applyDefaultDate(o.utils,a.minDate,o.defaultDates.minDate),it=applyDefaultDate(o.utils,a.maxDate,o.defaultDates.maxDate);switch(!0){case!o.utils.isValid(i):return"invalidDate";case!!(s&&s(i)):return"shouldDisableDate";case!!($&&$(i)):return"shouldDisableMonth";case!!(j&&j(i)):return"shouldDisableYear";case!!(et&&o.utils.isAfterDay(i,nt)):return"disableFuture";case!!(_e&&o.utils.isBeforeDay(i,nt)):return"disablePast";case!!(at&&o.utils.isBeforeDay(i,at)):return"minDate";case!!(it&&o.utils.isAfterDay(i,it)):return"maxDate";default:return null}},DATE_VALIDATION_PROP_NAMES=["disablePast","disableFuture","minDate","maxDate","shouldDisableDate","shouldDisableMonth","shouldDisableYear"],TIME_VALIDATION_PROP_NAMES=["disablePast","disableFuture","minTime","maxTime","shouldDisableClock","shouldDisableTime","minutesStep","ampm","disableIgnoringDatePartForTimeValidation"],DATE_TIME_VALIDATION_PROP_NAMES=["minDateTime","maxDateTime"],VALIDATION_PROP_NAMES=[...DATE_VALIDATION_PROP_NAMES,...TIME_VALIDATION_PROP_NAMES,...DATE_TIME_VALIDATION_PROP_NAMES],extractValidationProps=a=>VALIDATION_PROP_NAMES.reduce((i,o)=>(a.hasOwnProperty(o)&&(i[o]=a[o]),i),{}),SHARED_FIELD_INTERNAL_PROP_NAMES=["value","defaultValue","referenceDate","format","formatDensity","onChange","timezone","readOnly","onError","shouldRespectLeadingZeros","selectedSections","onSelectedSectionsChange","unstableFieldRef"],splitFieldInternalAndForwardedProps=(a,i)=>{const o=_extends({},a),s={},$=j=>{o.hasOwnProperty(j)&&(s[j]=o[j],delete o[j])};return SHARED_FIELD_INTERNAL_PROP_NAMES.forEach($),i==="date"?DATE_VALIDATION_PROP_NAMES.forEach($):i==="time"?TIME_VALIDATION_PROP_NAMES.forEach($):i==="date-time"&&(DATE_VALIDATION_PROP_NAMES.forEach($),TIME_VALIDATION_PROP_NAMES.forEach($),DATE_TIME_VALIDATION_PROP_NAMES.forEach($)),{forwardedProps:o,internalProps:s}},useDefaultizedDateField=a=>{var i,o,s;const $=useUtils(),j=useDefaultDates();return _extends({},a,{disablePast:(i=a.disablePast)!=null?i:!1,disableFuture:(o=a.disableFuture)!=null?o:!1,format:(s=a.format)!=null?s:$.formats.keyboardDate,minDate:applyDefaultDate($,a.minDate,j.minDate),maxDate:applyDefaultDate($,a.maxDate,j.maxDate)})},useDateField=({props:a,inputRef:i})=>{const o=useDefaultizedDateField(a),{forwardedProps:s,internalProps:$}=splitFieldInternalAndForwardedProps(o,"date");return useField({inputRef:i,forwardedProps:s,internalProps:$,valueManager:singleItemValueManager,fieldValueManager:singleItemFieldValueManager,validator:validateDate,valueType:"date"})},PickersModalDialogRoot=styled(MuiDialog)({[`& .${dialogClasses.container}`]:{outline:0},[`& .${dialogClasses.paper}`]:{outline:0,minWidth:DIALOG_WIDTH}}),PickersModalDialogContent=styled(DialogContent$1)({"&:first-of-type":{padding:0}});function PickersModalDialog(a){var i,o;const{children:s,onDismiss:$,open:j,slots:_e,slotProps:et}=a,tt=(i=_e==null?void 0:_e.dialog)!=null?i:PickersModalDialogRoot,nt=(o=_e==null?void 0:_e.mobileTransition)!=null?o:Fade$1;return jsxRuntimeExports.jsx(tt,_extends({open:j,onClose:$},et==null?void 0:et.dialog,{TransitionComponent:nt,TransitionProps:et==null?void 0:et.mobileTransition,PaperComponent:_e==null?void 0:_e.mobilePaper,PaperProps:et==null?void 0:et.mobilePaper,children:jsxRuntimeExports.jsx(PickersModalDialogContent,{children:s})}))}function getPickersPopperUtilityClass(a){return generateUtilityClass$1("MuiPickersPopper",a)}generateUtilityClasses$1("MuiPickersPopper",["root","paper"]);const PREFERS_REDUCED_MOTION="@media (prefers-reduced-motion: reduce)",mobileVersionMatches=typeof navigator<"u"&&navigator.userAgent.match(/android\s(\d+)|OS\s(\d+)/i),androidVersion=mobileVersionMatches&&mobileVersionMatches[1]?parseInt(mobileVersionMatches[1],10):null,iOSVersion=mobileVersionMatches&&mobileVersionMatches[2]?parseInt(mobileVersionMatches[2],10):null,slowAnimationDevices=androidVersion&&androidVersion<10||iOSVersion&&iOSVersion<13||!1,useDefaultReduceAnimations=()=>useMediaQuery(PREFERS_REDUCED_MOTION,{defaultMatches:!1})||slowAnimationDevices,_excluded$i=["PaperComponent","popperPlacement","ownerState","children","paperSlotProps","paperClasses","onPaperClick","onPaperTouchStart"],useUtilityClasses$e=a=>{const{classes:i}=a;return composeClasses({root:["root"],paper:["paper"]},getPickersPopperUtilityClass,i)},PickersPopperRoot=styled(MuiPopper,{name:"MuiPickersPopper",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a})=>({zIndex:a.zIndex.modal})),PickersPopperPaper=styled(Paper$1,{name:"MuiPickersPopper",slot:"Paper",overridesResolver:(a,i)=>i.paper})(({ownerState:a})=>_extends({outline:0,transformOrigin:"top center"},a.placement.includes("top")&&{transformOrigin:"bottom center"}));function clickedRootScrollbar(a,i){return i.documentElement.clientWidth<a.clientX||i.documentElement.clientHeight<a.clientY}function useClickAwayListener(a,i){const o=reactExports.useRef(!1),s=reactExports.useRef(!1),$=reactExports.useRef(null),j=reactExports.useRef(!1);reactExports.useEffect(()=>{if(!a)return;function tt(){j.current=!0}return document.addEventListener("mousedown",tt,!0),document.addEventListener("touchstart",tt,!0),()=>{document.removeEventListener("mousedown",tt,!0),document.removeEventListener("touchstart",tt,!0),j.current=!1}},[a]);const _e=useEventCallback(tt=>{if(!j.current)return;const nt=s.current;s.current=!1;const at=ownerDocument($.current);if(!$.current||"clientX"in tt&&clickedRootScrollbar(tt,at))return;if(o.current){o.current=!1;return}let it;tt.composedPath?it=tt.composedPath().indexOf($.current)>-1:it=!at.documentElement.contains(tt.target)||$.current.contains(tt.target),!it&&!nt&&i(tt)}),et=()=>{s.current=!0};return reactExports.useEffect(()=>{if(a){const tt=ownerDocument($.current),nt=()=>{o.current=!0};return tt.addEventListener("touchstart",_e),tt.addEventListener("touchmove",nt),()=>{tt.removeEventListener("touchstart",_e),tt.removeEventListener("touchmove",nt)}}},[a,_e]),reactExports.useEffect(()=>{if(a){const tt=ownerDocument($.current);return tt.addEventListener("click",_e),()=>{tt.removeEventListener("click",_e),s.current=!1}}},[a,_e]),[$,et,et]}const PickersPopperPaperWrapper=reactExports.forwardRef((a,i)=>{const{PaperComponent:o,popperPlacement:s,ownerState:$,children:j,paperSlotProps:_e,paperClasses:et,onPaperClick:tt,onPaperTouchStart:nt}=a,at=_objectWithoutPropertiesLoose(a,_excluded$i),it=_extends({},$,{placement:s}),st=useSlotProps({elementType:o,externalSlotProps:_e,additionalProps:{tabIndex:-1,elevation:8,ref:i},className:et,ownerState:it});return jsxRuntimeExports.jsx(o,_extends({},at,st,{onClick:lt=>{var ct;tt(lt),(ct=st.onClick)==null||ct.call(st,lt)},onTouchStart:lt=>{var ct;nt(lt),(ct=st.onTouchStart)==null||ct.call(st,lt)},ownerState:it,children:j}))});function PickersPopper(a){var i,o,s,$;const j=useThemeProps$6({props:a,name:"MuiPickersPopper"}),{anchorEl:_e,children:et,containerRef:tt=null,shouldRestoreFocus:nt,onBlur:at,onDismiss:it,open:st,role:lt,placement:ct,slots:rt,slotProps:ut,reduceAnimations:ot}=j;reactExports.useEffect(()=>{function jt(Zt){st&&(Zt.key==="Escape"||Zt.key==="Esc")&&it()}return document.addEventListener("keydown",jt),()=>{document.removeEventListener("keydown",jt)}},[it,st]);const dt=reactExports.useRef(null);reactExports.useEffect(()=>{lt==="tooltip"||nt&&!nt()||(st?dt.current=getActiveElement(document):dt.current&&dt.current instanceof HTMLElement&&setTimeout(()=>{dt.current instanceof HTMLElement&&dt.current.focus()}))},[st,lt,nt]);const[pt,mt,ft]=useClickAwayListener(st,at??it),ht=reactExports.useRef(null),yt=useForkRef(ht,tt),bt=useForkRef(yt,pt),gt=j,xt=useUtilityClasses$e(gt),vt=useDefaultReduceAnimations(),Lt=ot??vt,$t=jt=>{jt.key==="Escape"&&(jt.stopPropagation(),it())},Tt=((i=rt==null?void 0:rt.desktopTransition)!=null?i:Lt)?Fade$1:Grow$1,Et=(o=rt==null?void 0:rt.desktopTrapFocus)!=null?o:FocusTrap,Dt=(s=rt==null?void 0:rt.desktopPaper)!=null?s:PickersPopperPaper,It=($=rt==null?void 0:rt.popper)!=null?$:PickersPopperRoot,Ct=useSlotProps({elementType:It,externalSlotProps:ut==null?void 0:ut.popper,additionalProps:{transition:!0,role:lt,open:st,anchorEl:_e,placement:ct,onKeyDown:$t},className:xt.root,ownerState:j});return jsxRuntimeExports.jsx(It,_extends({},Ct,{children:({TransitionProps:jt,placement:Zt})=>jsxRuntimeExports.jsx(Et,_extends({open:st,disableAutoFocus:!0,disableRestoreFocus:!0,disableEnforceFocus:lt==="tooltip",isEnabled:()=>!0},ut==null?void 0:ut.desktopTrapFocus,{children:jsxRuntimeExports.jsx(Tt,_extends({},jt,ut==null?void 0:ut.desktopTransition,{children:jsxRuntimeExports.jsx(PickersPopperPaperWrapper,{PaperComponent:Dt,ownerState:gt,popperPlacement:Zt,ref:bt,onPaperClick:mt,onPaperTouchStart:ft,paperClasses:xt.paper,paperSlotProps:ut==null?void 0:ut.desktopPaper,children:et})}))}))}))}function getPickersToolbarUtilityClass(a){return generateUtilityClass$1("MuiPickersToolbar",a)}generateUtilityClasses$1("MuiPickersToolbar",["root","content"]);const useUtilityClasses$d=a=>{const{classes:i,isLandscape:o}=a;return composeClasses({root:["root"],content:["content"],penIconButton:["penIconButton",o&&"penIconButtonLandscape"]},getPickersToolbarUtilityClass,i)},PickersToolbarRoot=styled("div",{name:"MuiPickersToolbar",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a,ownerState:i})=>_extends({display:"flex",flexDirection:"column",alignItems:"flex-start",justifyContent:"space-between",padding:a.spacing(2,3)},i.isLandscape&&{height:"auto",maxWidth:160,padding:16,justifyContent:"flex-start",flexWrap:"wrap"})),PickersToolbarContent=styled("div",{name:"MuiPickersToolbar",slot:"Content",overridesResolver:(a,i)=>i.content})(({ownerState:a})=>{var i;return{display:"flex",flexWrap:"wrap",width:"100%",justifyContent:a.isLandscape?"flex-start":"space-between",flexDirection:a.isLandscape?(i=a.landscapeDirection)!=null?i:"column":"row",flex:1,alignItems:a.isLandscape?"flex-start":"center"}}),PickersToolbar=reactExports.forwardRef(function a(i,o){const s=useThemeProps$6({props:i,name:"MuiPickersToolbar"}),{children:$,className:j,toolbarTitle:_e,hidden:et,titleId:tt}=s,nt=s,at=useUtilityClasses$d(nt);return et?null:jsxRuntimeExports.jsxs(PickersToolbarRoot,{ref:o,className:clsx(at.root,j),ownerState:nt,children:[jsxRuntimeExports.jsx(Typography$1,{color:"text.secondary",variant:"overline",id:tt,children:_e}),jsxRuntimeExports.jsx(PickersToolbarContent,{className:at.content,ownerState:nt,children:$})]})}),useOpenState=({open:a,onOpen:i,onClose:o})=>{const s=reactExports.useRef(typeof a=="boolean").current,[$,j]=reactExports.useState(!1);reactExports.useEffect(()=>{if(s){if(typeof a!="boolean")throw new Error("You must not mix controlling and uncontrolled mode for `open` prop");j(a)}},[s,a]);const _e=reactExports.useCallback(et=>{s||j(et),et&&i&&i(),!et&&o&&o()},[s,i,o]);return{isOpen:$,setIsOpen:_e}},shouldPublishValue=a=>{const{action:i,hasChanged:o,dateState:s,isControlled:$}=a,j=!$&&!s.hasBeenModifiedSinceMount;return i.name==="setValueFromField"?!0:i.name==="setValueFromAction"?j&&["accept","today","clear"].includes(i.pickerAction)?!0:o(s.lastPublishedValue):i.name==="setValueFromView"&&i.selectionState!=="shallow"||i.name==="setValueFromShortcut"?j?!0:o(s.lastPublishedValue):!1},shouldCommitValue=a=>{const{action:i,hasChanged:o,dateState:s,isControlled:$,closeOnSelect:j}=a,_e=!$&&!s.hasBeenModifiedSinceMount;return i.name==="setValueFromAction"?_e&&["accept","today","clear"].includes(i.pickerAction)?!0:o(s.lastCommittedValue):i.name==="setValueFromView"&&i.selectionState==="finish"&&j?_e?!0:o(s.lastCommittedValue):i.name==="setValueFromShortcut"?i.changeImportance==="accept"&&o(s.lastCommittedValue):!1},shouldClosePicker=a=>{const{action:i,closeOnSelect:o}=a;return i.name==="setValueFromAction"?!0:i.name==="setValueFromView"?i.selectionState==="finish"&&o:i.name==="setValueFromShortcut"?i.changeImportance==="accept":!1},usePickerValue=({props:a,valueManager:i,valueType:o,wrapperVariant:s,validator:$})=>{const{onAccept:j,onChange:_e,value:et,defaultValue:tt,closeOnSelect:nt=s==="desktop",selectedSections:at,onSelectedSectionsChange:it,timezone:st}=a,{current:lt}=reactExports.useRef(tt),{current:ct}=reactExports.useRef(et!==void 0),rt=useUtils(),ut=useLocalizationContext(),[ot,dt]=useControlled({controlled:at,default:null,name:"usePickerValue",state:"selectedSections"}),{isOpen:pt,setIsOpen:mt}=useOpenState(a),[ft,ht]=reactExports.useState(()=>{let Pt;return et!==void 0?Pt=et:lt!==void 0?Pt=lt:Pt=i.emptyValue,{draft:Pt,lastPublishedValue:Pt,lastCommittedValue:Pt,lastControlledValue:et,hasBeenModifiedSinceMount:!1}}),{timezone:yt,handleValueChange:bt}=useValueWithTimezone({timezone:st,value:et,defaultValue:lt,onChange:_e,valueManager:i});useValidation(_extends({},a,{value:ft.draft,timezone:yt}),$,i.isSameError,i.defaultErrorState);const gt=useEventCallback(Pt=>{const Mt={action:Pt,dateState:ft,hasChanged:Gt=>!i.areValuesEqual(rt,Pt.value,Gt),isControlled:ct,closeOnSelect:nt},Ot=shouldPublishValue(Mt),Bt=shouldCommitValue(Mt),zt=shouldClosePicker(Mt);if(ht(Gt=>_extends({},Gt,{draft:Pt.value,lastPublishedValue:Ot?Pt.value:Gt.lastPublishedValue,lastCommittedValue:Bt?Pt.value:Gt.lastCommittedValue,hasBeenModifiedSinceMount:!0})),Ot){const Wt={validationError:Pt.name==="setValueFromField"?Pt.context.validationError:$({adapter:ut,value:Pt.value,props:_extends({},a,{value:Pt.value,timezone:yt})})};Pt.name==="setValueFromShortcut"&&Pt.shortcut!=null&&(Wt.shortcut=Pt.shortcut),bt(Pt.value,Wt)}Bt&&j&&j(Pt.value),zt&&mt(!1)});if(et!==void 0&&(ft.lastControlledValue===void 0||!i.areValuesEqual(rt,ft.lastControlledValue,et))){const Pt=i.areValuesEqual(rt,ft.draft,et);ht(Mt=>_extends({},Mt,{lastControlledValue:et},Pt?{}:{lastCommittedValue:et,lastPublishedValue:et,draft:et,hasBeenModifiedSinceMount:!0}))}const xt=useEventCallback(()=>{gt({value:i.emptyValue,name:"setValueFromAction",pickerAction:"clear"})}),vt=useEventCallback(()=>{gt({value:ft.lastPublishedValue,name:"setValueFromAction",pickerAction:"accept"})}),Lt=useEventCallback(()=>{gt({value:ft.lastPublishedValue,name:"setValueFromAction",pickerAction:"dismiss"})}),$t=useEventCallback(()=>{gt({value:ft.lastCommittedValue,name:"setValueFromAction",pickerAction:"cancel"})}),Tt=useEventCallback(()=>{gt({value:i.getTodayValue(rt,yt,o),name:"setValueFromAction",pickerAction:"today"})}),Et=useEventCallback(()=>mt(!0)),Dt=useEventCallback(()=>mt(!1)),It=useEventCallback((Pt,Mt="partial")=>gt({name:"setValueFromView",value:Pt,selectionState:Mt})),Ct=useEventCallback((Pt,Mt,Ot)=>gt({name:"setValueFromShortcut",value:Pt,changeImportance:Mt??"accept",shortcut:Ot})),jt=useEventCallback((Pt,Mt)=>gt({name:"setValueFromField",value:Pt,context:Mt})),Zt=useEventCallback(Pt=>{dt(Pt),it==null||it(Pt)}),Xt={onClear:xt,onAccept:vt,onDismiss:Lt,onCancel:$t,onSetToday:Tt,onOpen:Et,onClose:Dt},sn={value:ft.draft,onChange:jt,selectedSections:ot,onSelectedSectionsChange:Zt},Ft=reactExports.useMemo(()=>i.cleanValue(rt,ft.draft),[rt,i,ft.draft]),wt={value:Ft,onChange:It,onClose:Dt,open:pt,onSelectedSectionsChange:Zt},At=_extends({},Xt,{value:Ft,onChange:It,onSelectShortcut:Ct,isValid:Pt=>{const Mt=$({adapter:ut,value:Pt,props:_extends({},a,{value:Pt,timezone:yt})});return!i.hasError(Mt)}});return{open:pt,fieldProps:sn,viewProps:wt,layoutProps:At,actions:Xt}},_excluded$h=["className","sx"],usePickerViews=({props:a,propsFromPickerValue:i,additionalViewProps:o,inputRef:s,autoFocusView:$})=>{const{onChange:j,open:_e,onSelectedSectionsChange:et,onClose:tt}=i,{views:nt,openTo:at,onViewChange:it,disableOpenPicker:st,viewRenderers:lt,timezone:ct}=a,rt=_objectWithoutPropertiesLoose(a,_excluded$h),{view:ut,setView:ot,defaultView:dt,focusedView:pt,setFocusedView:mt,setValueAndGoToNextView:ft}=useViews({view:void 0,views:nt,openTo:at,onChange:j,onViewChange:it,autoFocus:$}),{hasUIView:ht,viewModeLookup:yt}=reactExports.useMemo(()=>nt.reduce((Tt,Et)=>{let Dt;return st?Dt="field":lt[Et]!=null?Dt="UI":Dt="field",Tt.viewModeLookup[Et]=Dt,Dt==="UI"&&(Tt.hasUIView=!0),Tt},{hasUIView:!1,viewModeLookup:{}}),[st,lt,nt]),bt=reactExports.useMemo(()=>nt.reduce((Tt,Et)=>lt[Et]!=null&&isTimeView(Et)?Tt+1:Tt,0),[lt,nt]),gt=yt[ut],xt=useEventCallback(()=>gt==="UI"),[vt,Lt]=reactExports.useState(gt==="UI"?ut:null);return vt!==ut&&yt[ut]==="UI"&&Lt(ut),useEnhancedEffect(()=>{gt==="field"&&_e&&(tt(),setTimeout(()=>{s==null||s.current.focus(),et(ut)}))},[ut]),useEnhancedEffect(()=>{if(!_e)return;let Tt=ut;gt==="field"&&vt!=null&&(Tt=vt),Tt!==dt&&yt[Tt]==="UI"&&yt[dt]==="UI"&&(Tt=dt),Tt!==ut&&ot(Tt),mt(Tt,!0)},[_e]),{hasUIView:ht,shouldRestoreFocus:xt,layoutProps:{views:nt,view:vt,onViewChange:ot},renderCurrentView:()=>{if(vt==null)return null;const Tt=lt[vt];return Tt==null?null:Tt(_extends({},rt,o,i,{views:nt,timezone:ct,onChange:ft,view:vt,onViewChange:ot,focusedView:pt,onFocusedViewChange:mt,showViewSwitcher:bt>1,timeViewsCount:bt}))}}};function getOrientation(){return typeof window>"u"?"portrait":window.screen&&window.screen.orientation&&window.screen.orientation.angle?Math.abs(window.screen.orientation.angle)===90?"landscape":"portrait":window.orientation&&Math.abs(Number(window.orientation))===90?"landscape":"portrait"}const useIsLandscape=(a,i)=>{const[o,s]=reactExports.useState(getOrientation);return useEnhancedEffect(()=>{const j=()=>{s(getOrientation())};return window.addEventListener("orientationchange",j),()=>{window.removeEventListener("orientationchange",j)}},[]),arrayIncludes(a,["hours","minutes","seconds"])?!1:(i||o)==="landscape"},usePickerLayoutProps=({props:a,propsFromPickerValue:i,propsFromPickerViews:o,wrapperVariant:s})=>{const{orientation:$}=a,j=useIsLandscape(o.views,$);return{layoutProps:_extends({},o,i,{isLandscape:j,wrapperVariant:s,disabled:a.disabled,readOnly:a.readOnly})}},buildWarning=(a,i="warning")=>{let o=!1;const s=Array.isArray(a)?a.join(`
`):a;return()=>{o||(o=!0,i==="error"?console.error(s):console.warn(s))}};buildWarning(["The `renderInput` prop has been removed in version 6.0 of the Date and Time Pickers.","You can replace it with the `textField` component slot in most cases.","For more information, please have a look at the migration guide (https://mui.com/x/migration/migration-pickers-v5/#input-renderer-required-in-v5)."]);const usePicker=({props:a,valueManager:i,valueType:o,wrapperVariant:s,inputRef:$,additionalViewProps:j,validator:_e,autoFocusView:et})=>{const tt=usePickerValue({props:a,valueManager:i,valueType:o,wrapperVariant:s,validator:_e}),nt=usePickerViews({props:a,inputRef:$,additionalViewProps:j,autoFocusView:et,propsFromPickerValue:tt.viewProps}),at=usePickerLayoutProps({props:a,wrapperVariant:s,propsFromPickerValue:tt.layoutProps,propsFromPickerViews:nt.layoutProps});return{open:tt.open,actions:tt.actions,fieldProps:tt.fieldProps,renderCurrentView:nt.renderCurrentView,hasUIView:nt.hasUIView,shouldRestoreFocus:nt.shouldRestoreFocus,layoutProps:at.layoutProps}};function getPickersLayoutUtilityClass(a){return generateUtilityClass$1("MuiPickersLayout",a)}const pickersLayoutClasses=generateUtilityClasses$1("MuiPickersLayout",["root","landscape","contentWrapper","toolbar","actionBar","tabs","shortcuts"]),_excluded$g=["onAccept","onClear","onCancel","onSetToday","actions"];function PickersActionBar(a){const{onAccept:i,onClear:o,onCancel:s,onSetToday:$,actions:j}=a,_e=_objectWithoutPropertiesLoose(a,_excluded$g),et=useLocaleText();if(j==null||j.length===0)return null;const tt=j==null?void 0:j.map(nt=>{switch(nt){case"clear":return jsxRuntimeExports.jsx(Button$1,{onClick:o,children:et.clearButtonLabel},nt);case"cancel":return jsxRuntimeExports.jsx(Button$1,{onClick:s,children:et.cancelButtonLabel},nt);case"accept":return jsxRuntimeExports.jsx(Button$1,{onClick:i,children:et.okButtonLabel},nt);case"today":return jsxRuntimeExports.jsx(Button$1,{onClick:$,children:et.todayButtonLabel},nt);default:return null}});return jsxRuntimeExports.jsx(DialogActions$1,_extends({},_e,{children:tt}))}const _excluded$f=["items","changeImportance","isLandscape","onChange","isValid"],_excluded2$4=["getValue"];function PickersShortcuts(a){const{items:i,changeImportance:o,onChange:s,isValid:$}=a,j=_objectWithoutPropertiesLoose(a,_excluded$f);if(i==null||i.length===0)return null;const _e=i.map(et=>{let{getValue:tt}=et,nt=_objectWithoutPropertiesLoose(et,_excluded2$4);const at=tt({isValid:$});return{label:nt.label,onClick:()=>{s(at,o,nt)},disabled:!$(at)}});return jsxRuntimeExports.jsx(List$2,_extends({dense:!0,sx:[{maxHeight:VIEW_HEIGHT,maxWidth:200,overflow:"auto"},...Array.isArray(j.sx)?j.sx:[j.sx]]},j,{children:_e.map(et=>jsxRuntimeExports.jsx(ListItem$1,{children:jsxRuntimeExports.jsx(Chip$1,_extends({},et))},et.label))}))}function toolbarHasView(a){return a.view!==null}const useUtilityClasses$c=a=>{const{classes:i,isLandscape:o}=a;return composeClasses({root:["root",o&&"landscape"],contentWrapper:["contentWrapper"],toolbar:["toolbar"],actionBar:["actionBar"],tabs:["tabs"],landscape:["landscape"],shortcuts:["shortcuts"]},getPickersLayoutUtilityClass,i)},usePickerLayout=a=>{var i,o;const{wrapperVariant:s,onAccept:$,onClear:j,onCancel:_e,onSetToday:et,view:tt,views:nt,onViewChange:at,value:it,onChange:st,onSelectShortcut:lt,isValid:ct,isLandscape:rt,disabled:ut,readOnly:ot,children:dt,components:pt,componentsProps:mt,slots:ft,slotProps:ht}=a,yt=ft??uncapitalizeObjectKeys(pt),bt=ht??mt,gt=useUtilityClasses$c(a),xt=(i=yt==null?void 0:yt.actionBar)!=null?i:PickersActionBar,vt=useSlotProps({elementType:xt,externalSlotProps:bt==null?void 0:bt.actionBar,additionalProps:{onAccept:$,onClear:j,onCancel:_e,onSetToday:et,actions:s==="desktop"?[]:["cancel","accept"],className:gt.actionBar},ownerState:_extends({},a,{wrapperVariant:s})}),Lt=jsxRuntimeExports.jsx(xt,_extends({},vt)),$t=yt==null?void 0:yt.toolbar,Tt=useSlotProps({elementType:$t,externalSlotProps:bt==null?void 0:bt.toolbar,additionalProps:{isLandscape:rt,onChange:st,value:it,view:tt,onViewChange:at,views:nt,disabled:ut,readOnly:ot,className:gt.toolbar},ownerState:_extends({},a,{wrapperVariant:s})}),Et=toolbarHasView(Tt)&&$t?jsxRuntimeExports.jsx($t,_extends({},Tt)):null,Dt=dt,It=yt==null?void 0:yt.tabs,Ct=tt&&It?jsxRuntimeExports.jsx(It,_extends({view:tt,onViewChange:at,className:gt.tabs},bt==null?void 0:bt.tabs)):null,jt=(o=yt==null?void 0:yt.shortcuts)!=null?o:PickersShortcuts,Zt=useSlotProps({elementType:jt,externalSlotProps:bt==null?void 0:bt.shortcuts,additionalProps:{isValid:ct,isLandscape:rt,onChange:lt,className:gt.shortcuts},ownerState:{isValid:ct,isLandscape:rt,onChange:lt,className:gt.shortcuts,wrapperVariant:s}}),Xt=tt&&jt?jsxRuntimeExports.jsx(jt,_extends({},Zt)):null;return{toolbar:Et,content:Dt,tabs:Ct,actionBar:Lt,shortcuts:Xt}},useUtilityClasses$b=a=>{const{isLandscape:i,classes:o}=a;return composeClasses({root:["root",i&&"landscape"],contentWrapper:["contentWrapper"]},getPickersLayoutUtilityClass,o)},PickersLayoutRoot=styled("div",{name:"MuiPickersLayout",slot:"Root",overridesResolver:(a,i)=>i.root})(({theme:a,ownerState:i})=>({display:"grid",gridAutoColumns:"max-content auto max-content",gridAutoRows:"max-content auto max-content",[`& .${pickersLayoutClasses.toolbar}`]:i.isLandscape?{gridColumn:a.direction==="rtl"?3:1,gridRow:"2 / 3"}:{gridColumn:"2 / 4",gridRow:1},[`.${pickersLayoutClasses.shortcuts}`]:i.isLandscape?{gridColumn:"2 / 4",gridRow:1}:{gridColumn:a.direction==="rtl"?3:1,gridRow:"2 / 3"},[`& .${pickersLayoutClasses.actionBar}`]:{gridColumn:"1 / 4",gridRow:3}}));PickersLayoutRoot.propTypes={as:PropTypes.elementType,ownerState:PropTypes.shape({isLandscape:PropTypes.bool.isRequired}).isRequired,sx:PropTypes.oneOfType([PropTypes.arrayOf(PropTypes.oneOfType([PropTypes.func,PropTypes.object,PropTypes.bool])),PropTypes.func,PropTypes.object])};const PickersLayoutContentWrapper=styled("div",{name:"MuiPickersLayout",slot:"ContentWrapper",overridesResolver:(a,i)=>i.contentWrapper})({gridColumn:2,gridRow:2,display:"flex",flexDirection:"column"}),PickersLayout=function a(i){const o=useThemeProps$6({props:i,name:"MuiPickersLayout"}),{toolbar:s,content:$,tabs:j,actionBar:_e,shortcuts:et}=usePickerLayout(o),{sx:tt,className:nt,isLandscape:at,ref:it,wrapperVariant:st}=o,lt=o,ct=useUtilityClasses$b(lt);return jsxRuntimeExports.jsxs(PickersLayoutRoot,{ref:it,sx:tt,className:clsx(nt,ct.root),ownerState:lt,children:[at?et:s,at?s:et,jsxRuntimeExports.jsx(PickersLayoutContentWrapper,{className:ct.contentWrapper,children:st==="desktop"?jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[$,j]}):jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[j,$]})}),_e]})},getPickersSlideTransitionUtilityClass=a=>generateUtilityClass$1("MuiPickersSlideTransition",a),pickersSlideTransitionClasses=generateUtilityClasses$1("MuiPickersSlideTransition",["root","slideEnter-left","slideEnter-right","slideEnterActive","slideExit","slideExitActiveLeft-left","slideExitActiveLeft-right"]),_excluded$e=["children","className","reduceAnimations","slideDirection","transKey","classes"],useUtilityClasses$a=a=>{const{classes:i,slideDirection:o}=a,s={root:["root"],exit:["slideExit"],enterActive:["slideEnterActive"],enter:[`slideEnter-${o}`],exitActive:[`slideExitActiveLeft-${o}`]};return composeClasses(s,getPickersSlideTransitionUtilityClass,i)},PickersSlideTransitionRoot=styled(TransitionGroup$1,{name:"MuiPickersSlideTransition",slot:"Root",overridesResolver:(a,i)=>[i.root,{[`.${pickersSlideTransitionClasses["slideEnter-left"]}`]:i["slideEnter-left"]},{[`.${pickersSlideTransitionClasses["slideEnter-right"]}`]:i["slideEnter-right"]},{[`.${pickersSlideTransitionClasses.slideEnterActive}`]:i.slideEnterActive},{[`.${pickersSlideTransitionClasses.slideExit}`]:i.slideExit},{[`.${pickersSlideTransitionClasses["slideExitActiveLeft-left"]}`]:i["slideExitActiveLeft-left"]},{[`.${pickersSlideTransitionClasses["slideExitActiveLeft-right"]}`]:i["slideExitActiveLeft-right"]}]})(({theme:a})=>{const i=a.transitions.create("transform",{duration:a.transitions.duration.complex,easing:"cubic-bezier(0.35, 0.8, 0.4, 1)"});return{display:"block",position:"relative",overflowX:"hidden","& > *":{position:"absolute",top:0,right:0,left:0},[`& .${pickersSlideTransitionClasses["slideEnter-left"]}`]:{willChange:"transform",transform:"translate(100%)",zIndex:1},[`& .${pickersSlideTransitionClasses["slideEnter-right"]}`]:{willChange:"transform",transform:"translate(-100%)",zIndex:1},[`& .${pickersSlideTransitionClasses.slideEnterActive}`]:{transform:"translate(0%)",transition:i},[`& .${pickersSlideTransitionClasses.slideExit}`]:{transform:"translate(0%)"},[`& .${pickersSlideTransitionClasses["slideExitActiveLeft-left"]}`]:{willChange:"transform",transform:"translate(-100%)",transition:i,zIndex:0},[`& .${pickersSlideTransitionClasses["slideExitActiveLeft-right"]}`]:{willChange:"transform",transform:"translate(100%)",transition:i,zIndex:0}}});function PickersSlideTransition(a){const i=useThemeProps$6({props:a,name:"MuiPickersSlideTransition"}),{children:o,className:s,reduceAnimations:$,transKey:j}=i,_e=_objectWithoutPropertiesLoose(i,_excluded$e),et=useUtilityClasses$a(i),tt=useTheme$1();if($)return jsxRuntimeExports.jsx("div",{className:clsx(et.root,s),children:o});const nt={exit:et.exit,enterActive:et.enterActive,enter:et.enter,exitActive:et.exitActive};return jsxRuntimeExports.jsx(PickersSlideTransitionRoot,{className:clsx(et.root,s),childFactory:at=>reactExports.cloneElement(at,{classNames:nt}),role:"presentation",children:jsxRuntimeExports.jsx(CSSTransition$1,_extends({mountOnEnter:!0,unmountOnExit:!0,timeout:tt.transitions.duration.complex,classNames:nt},_e,{children:o}),j)})}const useIsDateDisabled=({shouldDisableDate:a,shouldDisableMonth:i,shouldDisableYear:o,minDate:s,maxDate:$,disableFuture:j,disablePast:_e,timezone:et})=>{const tt=useLocalizationContext();return reactExports.useCallback(nt=>validateDate({adapter:tt,value:nt,props:{shouldDisableDate:a,shouldDisableMonth:i,shouldDisableYear:o,minDate:s,maxDate:$,disableFuture:j,disablePast:_e,timezone:et}})!==null,[tt,a,i,o,s,$,j,_e,et])},getDayCalendarUtilityClass=a=>generateUtilityClass$1("MuiDayCalendar",a);generateUtilityClasses$1("MuiDayCalendar",["root","header","weekDayLabel","loadingContainer","slideTransition","monthContainer","weekContainer","weekNumberLabel","weekNumber"]);const _excluded$d=["parentProps","day","focusableDay","selectedDays","isDateDisabled","currentMonthNumber","isViewFocused"],_excluded2$3=["ownerState"],useUtilityClasses$9=a=>{const{classes:i}=a;return composeClasses({root:["root"],header:["header"],weekDayLabel:["weekDayLabel"],loadingContainer:["loadingContainer"],slideTransition:["slideTransition"],monthContainer:["monthContainer"],weekContainer:["weekContainer"],weekNumberLabel:["weekNumberLabel"],weekNumber:["weekNumber"]},getDayCalendarUtilityClass,i)},weeksContainerHeight=(DAY_SIZE+DAY_MARGIN*2)*6,PickersCalendarDayRoot=styled("div",{name:"MuiDayCalendar",slot:"Root",overridesResolver:(a,i)=>i.root})({}),PickersCalendarDayHeader=styled("div",{name:"MuiDayCalendar",slot:"Header",overridesResolver:(a,i)=>i.header})({display:"flex",justifyContent:"center",alignItems:"center"}),PickersCalendarWeekDayLabel=styled(Typography$1,{name:"MuiDayCalendar",slot:"WeekDayLabel",overridesResolver:(a,i)=>i.weekDayLabel})(({theme:a})=>({width:36,height:40,margin:"0 2px",textAlign:"center",display:"flex",justifyContent:"center",alignItems:"center",color:(a.vars||a).palette.text.secondary})),PickersCalendarWeekNumberLabel=styled(Typography$1,{name:"MuiDayCalendar",slot:"WeekNumberLabel",overridesResolver:(a,i)=>i.weekNumberLabel})(({theme:a})=>({width:36,height:40,margin:"0 2px",textAlign:"center",display:"flex",justifyContent:"center",alignItems:"center",color:a.palette.text.disabled})),PickersCalendarWeekNumber=styled(Typography$1,{name:"MuiDayCalendar",slot:"WeekNumber",overridesResolver:(a,i)=>i.weekNumber})(({theme:a})=>_extends({},a.typography.caption,{width:DAY_SIZE,height:DAY_SIZE,padding:0,margin:`0 ${DAY_MARGIN}px`,color:a.palette.text.disabled,fontSize:"0.75rem",alignItems:"center",justifyContent:"center",display:"inline-flex"})),PickersCalendarLoadingContainer=styled("div",{name:"MuiDayCalendar",slot:"LoadingContainer",overridesResolver:(a,i)=>i.loadingContainer})({display:"flex",justifyContent:"center",alignItems:"center",minHeight:weeksContainerHeight}),PickersCalendarSlideTransition=styled(PickersSlideTransition,{name:"MuiDayCalendar",slot:"SlideTransition",overridesResolver:(a,i)=>i.slideTransition})({minHeight:weeksContainerHeight}),PickersCalendarWeekContainer=styled("div",{name:"MuiDayCalendar",slot:"MonthContainer",overridesResolver:(a,i)=>i.monthContainer})({overflow:"hidden"}),PickersCalendarWeek=styled("div",{name:"MuiDayCalendar",slot:"WeekContainer",overridesResolver:(a,i)=>i.weekContainer})({margin:`${DAY_MARGIN}px 0`,display:"flex",justifyContent:"center"});function WrappedDay(a){var i,o,s;let{parentProps:$,day:j,focusableDay:_e,selectedDays:et,isDateDisabled:tt,currentMonthNumber:nt,isViewFocused:at}=a,it=_objectWithoutPropertiesLoose(a,_excluded$d);const{disabled:st,disableHighlightToday:lt,isMonthSwitchingAnimating:ct,showDaysOutsideCurrentMonth:rt,components:ut,componentsProps:ot,slots:dt,slotProps:pt,timezone:mt}=$,ft=useUtils(),ht=useNow(mt),yt=_e!==null&&ft.isSameDay(j,_e),bt=et.some(It=>ft.isSameDay(It,j)),gt=ft.isSameDay(j,ht),xt=(i=(o=dt==null?void 0:dt.day)!=null?o:ut==null?void 0:ut.Day)!=null?i:PickersDay,vt=useSlotProps({elementType:xt,externalSlotProps:(s=pt==null?void 0:pt.day)!=null?s:ot==null?void 0:ot.day,additionalProps:_extends({disableHighlightToday:lt,showDaysOutsideCurrentMonth:rt,role:"gridcell",isAnimating:ct,"data-timestamp":ft.toJsDate(j).valueOf()},it),ownerState:_extends({},$,{day:j,selected:bt})}),Lt=_objectWithoutPropertiesLoose(vt,_excluded2$3),$t=reactExports.useMemo(()=>st||tt(j),[st,tt,j]),Tt=reactExports.useMemo(()=>ft.getMonth(j)!==nt,[ft,j,nt]),Et=reactExports.useMemo(()=>{const It=ft.startOfMonth(ft.setMonth(j,nt));return rt?ft.isSameDay(j,ft.startOfWeek(It)):ft.isSameDay(j,It)},[nt,j,rt,ft]),Dt=reactExports.useMemo(()=>{const It=ft.endOfMonth(ft.setMonth(j,nt));return rt?ft.isSameDay(j,ft.endOfWeek(It)):ft.isSameDay(j,It)},[nt,j,rt,ft]);return jsxRuntimeExports.jsx(xt,_extends({},Lt,{day:j,disabled:$t,autoFocus:at&&yt,today:gt,outsideCurrentMonth:Tt,isFirstVisibleCell:Et,isLastVisibleCell:Dt,selected:bt,tabIndex:yt?0:-1,"aria-selected":bt,"aria-current":gt?"date":void 0}))}function DayCalendar(a){const i=useThemeProps$6({props:a,name:"MuiDayCalendar"}),{onFocusedDayChange:o,className:s,currentMonth:$,selectedDays:j,focusedDay:_e,loading:et,onSelectedDaysChange:tt,onMonthSwitchingAnimationEnd:nt,readOnly:at,reduceAnimations:it,renderLoading:st=()=>jsxRuntimeExports.jsx("span",{children:"..."}),slideDirection:lt,TransitionProps:ct,disablePast:rt,disableFuture:ut,minDate:ot,maxDate:dt,shouldDisableDate:pt,shouldDisableMonth:mt,shouldDisableYear:ft,dayOfWeekFormatter:ht,hasFocus:yt,onFocusedViewChange:bt,gridLabelId:gt,displayWeekNumber:xt,fixedWeekNumber:vt,autoFocus:Lt,timezone:$t}=i,Tt=useNow($t),Et=useUtils(),Dt=useUtilityClasses$9(i),Ct=useTheme$1().direction==="rtl",jt=ht||((yn,Pn)=>Et.format(Pn,"weekdayShort").charAt(0).toUpperCase()),Zt=useIsDateDisabled({shouldDisableDate:pt,shouldDisableMonth:mt,shouldDisableYear:ft,minDate:ot,maxDate:dt,disablePast:rt,disableFuture:ut,timezone:$t}),Xt=useLocaleText(),[sn,Ft]=useControlled({name:"DayCalendar",state:"hasFocus",controlled:yt,default:Lt??!1}),[wt,kt]=reactExports.useState(()=>_e||Tt),At=useEventCallback(yn=>{at||tt(yn)}),Pt=yn=>{Zt(yn)||(o(yn),kt(yn),bt==null||bt(!0),Ft(!0))},Mt=useEventCallback((yn,Pn)=>{switch(yn.key){case"ArrowUp":Pt(Et.addDays(Pn,-7)),yn.preventDefault();break;case"ArrowDown":Pt(Et.addDays(Pn,7)),yn.preventDefault();break;case"ArrowLeft":{const cn=Et.addDays(Pn,Ct?1:-1),xn=Et.addMonths(Pn,Ct?1:-1),hn=findClosestEnabledDate({utils:Et,date:cn,minDate:Ct?cn:Et.startOfMonth(xn),maxDate:Ct?Et.endOfMonth(xn):cn,isDateDisabled:Zt,timezone:$t});Pt(hn||cn),yn.preventDefault();break}case"ArrowRight":{const cn=Et.addDays(Pn,Ct?-1:1),xn=Et.addMonths(Pn,Ct?-1:1),hn=findClosestEnabledDate({utils:Et,date:cn,minDate:Ct?Et.startOfMonth(xn):cn,maxDate:Ct?cn:Et.endOfMonth(xn),isDateDisabled:Zt,timezone:$t});Pt(hn||cn),yn.preventDefault();break}case"Home":Pt(Et.startOfWeek(Pn)),yn.preventDefault();break;case"End":Pt(Et.endOfWeek(Pn)),yn.preventDefault();break;case"PageUp":Pt(Et.addMonths(Pn,1)),yn.preventDefault();break;case"PageDown":Pt(Et.addMonths(Pn,-1)),yn.preventDefault();break}}),Ot=useEventCallback((yn,Pn)=>Pt(Pn)),Bt=useEventCallback((yn,Pn)=>{sn&&Et.isSameDay(wt,Pn)&&(bt==null||bt(!1))}),zt=Et.getMonth($),Gt=reactExports.useMemo(()=>j.filter(yn=>!!yn).map(yn=>Et.startOfDay(yn)),[Et,j]),Wt=zt,qt=reactExports.useMemo(()=>reactExports.createRef(),[Wt]),tn=Et.startOfWeek(Tt),ln=reactExports.useMemo(()=>{const yn=Et.startOfMonth($),Pn=Et.endOfMonth($);return Zt(wt)||Et.isAfterDay(wt,Pn)||Et.isBeforeDay(wt,yn)?findClosestEnabledDate({utils:Et,date:wt,minDate:yn,maxDate:Pn,disablePast:rt,disableFuture:ut,isDateDisabled:Zt,timezone:$t}):wt},[$,ut,rt,wt,Zt,Et,$t]),gn=reactExports.useMemo(()=>{const yn=Et.setTimezone($,$t),Pn=Et.getWeekArray(yn);let cn=Et.addMonths(yn,1);for(;vt&&Pn.length<vt;){const xn=Et.getWeekArray(cn),hn=Et.isSameDay(Pn[Pn.length-1][0],xn[0][0]);xn.slice(hn?1:0).forEach(en=>{Pn.length<vt&&Pn.push(en)}),cn=Et.addMonths(cn,1)}return Pn},[$,vt,Et,$t]);return jsxRuntimeExports.jsxs(PickersCalendarDayRoot,{role:"grid","aria-labelledby":gt,className:Dt.root,children:[jsxRuntimeExports.jsxs(PickersCalendarDayHeader,{role:"row",className:Dt.header,children:[xt&&jsxRuntimeExports.jsx(PickersCalendarWeekNumberLabel,{variant:"caption",role:"columnheader","aria-label":Xt.calendarWeekNumberHeaderLabel,className:Dt.weekNumberLabel,children:Xt.calendarWeekNumberHeaderText}),getWeekdays(Et,Tt).map((yn,Pn)=>{var cn;const xn=Et.format(yn,"weekdayShort");return jsxRuntimeExports.jsx(PickersCalendarWeekDayLabel,{variant:"caption",role:"columnheader","aria-label":Et.format(Et.addDays(tn,Pn),"weekday"),className:Dt.weekDayLabel,children:(cn=jt==null?void 0:jt(xn,yn))!=null?cn:xn},xn+Pn.toString())})]}),et?jsxRuntimeExports.jsx(PickersCalendarLoadingContainer,{className:Dt.loadingContainer,children:st()}):jsxRuntimeExports.jsx(PickersCalendarSlideTransition,_extends({transKey:Wt,onExited:nt,reduceAnimations:it,slideDirection:lt,className:clsx(s,Dt.slideTransition)},ct,{nodeRef:qt,children:jsxRuntimeExports.jsx(PickersCalendarWeekContainer,{ref:qt,role:"rowgroup",className:Dt.monthContainer,children:gn.map((yn,Pn)=>jsxRuntimeExports.jsxs(PickersCalendarWeek,{role:"row",className:Dt.weekContainer,"aria-rowindex":Pn+1,children:[xt&&jsxRuntimeExports.jsx(PickersCalendarWeekNumber,{className:Dt.weekNumber,role:"rowheader","aria-label":Xt.calendarWeekNumberAriaLabelText(Et.getWeekNumber(yn[0])),children:Xt.calendarWeekNumberText(Et.getWeekNumber(yn[0]))}),yn.map((cn,xn)=>jsxRuntimeExports.jsx(WrappedDay,{parentProps:i,day:cn,selectedDays:Gt,focusableDay:ln,onKeyDown:Mt,onFocus:Ot,onBlur:Bt,onDaySelect:At,isDateDisabled:Zt,currentMonthNumber:zt,isViewFocused:sn,"aria-colindex":xn+1},cn.toString()))]},`week-${yn[0]}`))})}))]})}const createCalendarStateReducer=(a,i,o)=>(s,$)=>{switch($.type){case"changeMonth":return _extends({},s,{slideDirection:$.direction,currentMonth:$.newMonth,isMonthSwitchingAnimating:!a});case"finishMonthSwitchingAnimation":return _extends({},s,{isMonthSwitchingAnimating:!1});case"changeFocusedDay":{if(s.focusedDay!=null&&$.focusedDay!=null&&o.isSameDay($.focusedDay,s.focusedDay))return s;const j=$.focusedDay!=null&&!i&&!o.isSameMonth(s.currentMonth,$.focusedDay);return _extends({},s,{focusedDay:$.focusedDay,isMonthSwitchingAnimating:j&&!a&&!$.withoutMonthSwitchingAnimation,currentMonth:j?o.startOfMonth($.focusedDay):s.currentMonth,slideDirection:$.focusedDay!=null&&o.isAfterDay($.focusedDay,s.currentMonth)?"left":"right"})}default:throw new Error("missing support")}},useCalendarState=a=>{const{value:i,referenceDate:o,defaultCalendarMonth:s,disableFuture:$,disablePast:j,disableSwitchToMonthOnDayFocus:_e=!1,maxDate:et,minDate:tt,onMonthChange:nt,reduceAnimations:at,shouldDisableDate:it,timezone:st}=a,lt=useUtils(),ct=reactExports.useRef(createCalendarStateReducer(!!at,_e,lt)).current,rt=reactExports.useMemo(()=>{let yt=null;return o?yt=o:s&&(yt=lt.startOfMonth(s)),singleItemValueManager.getInitialReferenceValue({value:i,utils:lt,timezone:st,props:a,referenceDate:yt,granularity:SECTION_TYPE_GRANULARITY.day})},[]),[ut,ot]=reactExports.useReducer(ct,{isMonthSwitchingAnimating:!1,focusedDay:rt,currentMonth:lt.startOfMonth(rt),slideDirection:"left"}),dt=reactExports.useCallback(yt=>{ot(_extends({type:"changeMonth"},yt)),nt&&nt(yt.newMonth)},[nt]),pt=reactExports.useCallback(yt=>{const bt=yt;lt.isSameMonth(bt,ut.currentMonth)||dt({newMonth:lt.startOfMonth(bt),direction:lt.isAfterDay(bt,ut.currentMonth)?"left":"right"})},[ut.currentMonth,dt,lt]),mt=useIsDateDisabled({shouldDisableDate:it,minDate:tt,maxDate:et,disableFuture:$,disablePast:j,timezone:st}),ft=reactExports.useCallback(()=>{ot({type:"finishMonthSwitchingAnimation"})},[]),ht=useEventCallback((yt,bt)=>{mt(yt)||ot({type:"changeFocusedDay",focusedDay:yt,withoutMonthSwitchingAnimation:bt})});return{referenceDate:rt,calendarState:ut,changeMonth:pt,changeFocusedDay:ht,isDateDisabled:mt,onMonthSwitchingAnimationEnd:ft,handleChangeMonth:dt}},_excluded$c=["ownerState"],useClearableField=({clearable:a,fieldProps:i,InputProps:o,onClear:s,slots:$,slotProps:j,components:_e,componentsProps:et})=>{var tt,nt,at,it,st,lt;const ct=useLocaleText(),rt=(tt=(nt=$==null?void 0:$.clearButton)!=null?nt:_e==null?void 0:_e.ClearButton)!=null?tt:IconButton$1,ut=useSlotProps({elementType:rt,externalSlotProps:(at=j==null?void 0:j.clearButton)!=null?at:et==null?void 0:et.clearButton,ownerState:{},className:"clearButton",additionalProps:{title:ct.fieldClearLabel}}),ot=_objectWithoutPropertiesLoose(ut,_excluded$c),dt=(it=(st=$==null?void 0:$.clearIcon)!=null?st:_e==null?void 0:_e.ClearIcon)!=null?it:ClearIcon,pt=useSlotProps({elementType:dt,externalSlotProps:(lt=j==null?void 0:j.clearIcon)!=null?lt:et==null?void 0:et.clearIcon,ownerState:{}}),mt=_extends({},o,{endAdornment:jsxRuntimeExports.jsxs(reactExports.Fragment,{children:[a&&jsxRuntimeExports.jsx(InputAdornment$1,{position:"end",sx:{marginRight:o!=null&&o.endAdornment?-1:-1.5},children:jsxRuntimeExports.jsx(rt,_extends({},ot,{onClick:s,children:jsxRuntimeExports.jsx(dt,_extends({fontSize:"small"},pt))}))}),o==null?void 0:o.endAdornment]})}),ft=_extends({},i,{sx:[{"& .clearButton":{opacity:1},"@media (pointer: fine)":{"& .clearButton":{opacity:0},"&:hover, &:focus-within":{".clearButton":{opacity:1}}}},...Array.isArray(i.sx)?i.sx:[i.sx]]});return{InputProps:mt,fieldProps:ft}},_excluded$b=["components","componentsProps","slots","slotProps","InputProps","inputProps"],_excluded2$2=["inputRef"],_excluded3$1=["ref","onPaste","onKeyDown","inputMode","readOnly","clearable","onClear"],DateField=reactExports.forwardRef(function a(i,o){var s,$,j;const _e=useThemeProps$6({props:i,name:"MuiDateField"}),{components:et,componentsProps:tt,slots:nt,slotProps:at,InputProps:it,inputProps:st}=_e,lt=_objectWithoutPropertiesLoose(_e,_excluded$b),ct=_e,rt=(s=($=nt==null?void 0:nt.textField)!=null?$:et==null?void 0:et.TextField)!=null?s:TextField$1,ut=useSlotProps({elementType:rt,externalSlotProps:(j=at==null?void 0:at.textField)!=null?j:tt==null?void 0:tt.textField,externalForwardedProps:lt,ownerState:ct}),{inputRef:ot}=ut,dt=_objectWithoutPropertiesLoose(ut,_excluded2$2);dt.inputProps=_extends({},st,dt.inputProps),dt.InputProps=_extends({},it,dt.InputProps);const pt=useDateField({props:dt,inputRef:ot}),{ref:mt,onPaste:ft,onKeyDown:ht,inputMode:yt,readOnly:bt,clearable:gt,onClear:xt}=pt,vt=_objectWithoutPropertiesLoose(pt,_excluded3$1),{InputProps:Lt,fieldProps:$t}=useClearableField({onClear:xt,clearable:gt,fieldProps:vt,InputProps:vt.InputProps,slots:nt,slotProps:at,components:et,componentsProps:tt});return jsxRuntimeExports.jsx(rt,_extends({ref:o},$t,{InputProps:_extends({},Lt,{readOnly:bt}),inputProps:_extends({},vt.inputProps,{inputMode:yt,onPaste:ft,onKeyDown:ht,ref:mt})}))}),getPickersFadeTransitionGroupUtilityClass=a=>generateUtilityClass$1("MuiPickersFadeTransitionGroup",a);generateUtilityClasses$1("MuiPickersFadeTransitionGroup",["root"]);const useUtilityClasses$8=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getPickersFadeTransitionGroupUtilityClass,i)},PickersFadeTransitionGroupRoot=styled(TransitionGroup$1,{name:"MuiPickersFadeTransitionGroup",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"block",position:"relative"});function PickersFadeTransitionGroup(a){const i=useThemeProps$6({props:a,name:"MuiPickersFadeTransitionGroup"}),{children:o,className:s,reduceAnimations:$,transKey:j}=i,_e=useUtilityClasses$8(i),et=useTheme$1();return $?o:jsxRuntimeExports.jsx(PickersFadeTransitionGroupRoot,{className:clsx(_e.root,s),children:jsxRuntimeExports.jsx(Fade$1,{appear:!1,mountOnEnter:!0,unmountOnExit:!0,timeout:{appear:et.transitions.duration.enteringScreen,enter:et.transitions.duration.enteringScreen,exit:0},children:o},j)})}function getPickersMonthUtilityClass(a){return generateUtilityClass$1("MuiPickersMonth",a)}const pickersMonthClasses=generateUtilityClasses$1("MuiPickersMonth",["root","monthButton","disabled","selected"]),_excluded$a=["autoFocus","children","disabled","selected","value","tabIndex","onClick","onKeyDown","onFocus","onBlur","aria-current","aria-label","monthsPerRow"],useUtilityClasses$7=a=>{const{disabled:i,selected:o,classes:s}=a;return composeClasses({root:["root"],monthButton:["monthButton",i&&"disabled",o&&"selected"]},getPickersMonthUtilityClass,s)},PickersMonthRoot=styled("div",{name:"MuiPickersMonth",slot:"Root",overridesResolver:(a,i)=>[i.root]})(({ownerState:a})=>({flexBasis:a.monthsPerRow===3?"33.3%":"25%",display:"flex",alignItems:"center",justifyContent:"center"})),PickersMonthButton=styled("button",{name:"MuiPickersMonth",slot:"MonthButton",overridesResolver:(a,i)=>[i.monthButton,{[`&.${pickersMonthClasses.disabled}`]:i.disabled},{[`&.${pickersMonthClasses.selected}`]:i.selected}]})(({theme:a})=>_extends({color:"unset",backgroundColor:"transparent",border:0,outline:0},a.typography.subtitle1,{margin:"8px 0",height:36,width:72,borderRadius:18,cursor:"pointer","&:focus":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.activeChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha$1(a.palette.action.active,a.palette.action.hoverOpacity)},"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.activeChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha$1(a.palette.action.active,a.palette.action.hoverOpacity)},"&:disabled":{cursor:"auto",pointerEvents:"none"},[`&.${pickersMonthClasses.disabled}`]:{color:(a.vars||a).palette.text.secondary},[`&.${pickersMonthClasses.selected}`]:{color:(a.vars||a).palette.primary.contrastText,backgroundColor:(a.vars||a).palette.primary.main,"&:focus, &:hover":{backgroundColor:(a.vars||a).palette.primary.dark}}})),PickersMonth=reactExports.memo(function a(i){const o=useThemeProps$6({props:i,name:"MuiPickersMonth"}),{autoFocus:s,children:$,disabled:j,selected:_e,value:et,tabIndex:tt,onClick:nt,onKeyDown:at,onFocus:it,onBlur:st,"aria-current":lt,"aria-label":ct}=o,rt=_objectWithoutPropertiesLoose(o,_excluded$a),ut=reactExports.useRef(null),ot=useUtilityClasses$7(o);return useEnhancedEffect(()=>{if(s){var dt;(dt=ut.current)==null||dt.focus()}},[s]),jsxRuntimeExports.jsx(PickersMonthRoot,_extends({className:ot.root,ownerState:o},rt,{children:jsxRuntimeExports.jsx(PickersMonthButton,{ref:ut,disabled:j,type:"button",role:"radio",tabIndex:j?-1:tt,"aria-current":lt,"aria-checked":_e,"aria-label":ct,onClick:dt=>nt(dt,et),onKeyDown:dt=>at(dt,et),onFocus:dt=>it(dt,et),onBlur:dt=>st(dt,et),className:ot.monthButton,ownerState:o,children:$})}))});function getMonthCalendarUtilityClass(a){return generateUtilityClass$1("MuiMonthCalendar",a)}generateUtilityClasses$1("MuiMonthCalendar",["root"]);const _excluded$9=["className","value","defaultValue","referenceDate","disabled","disableFuture","disablePast","maxDate","minDate","onChange","shouldDisableMonth","readOnly","disableHighlightToday","autoFocus","onMonthFocus","hasFocus","onFocusedViewChange","monthsPerRow","timezone","gridLabelId"],useUtilityClasses$6=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getMonthCalendarUtilityClass,i)};function useMonthCalendarDefaultizedProps(a,i){const o=useUtils(),s=useDefaultDates(),$=useThemeProps$6({props:a,name:i});return _extends({disableFuture:!1,disablePast:!1},$,{minDate:applyDefaultDate(o,$.minDate,s.minDate),maxDate:applyDefaultDate(o,$.maxDate,s.maxDate)})}const MonthCalendarRoot=styled("div",{name:"MuiMonthCalendar",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"flex",flexWrap:"wrap",alignContent:"stretch",padding:"0 4px",width:DIALOG_WIDTH,boxSizing:"border-box"}),MonthCalendar=reactExports.forwardRef(function a(i,o){const s=useMonthCalendarDefaultizedProps(i,"MuiMonthCalendar"),{className:$,value:j,defaultValue:_e,referenceDate:et,disabled:tt,disableFuture:nt,disablePast:at,maxDate:it,minDate:st,onChange:lt,shouldDisableMonth:ct,readOnly:rt,disableHighlightToday:ut,autoFocus:ot=!1,onMonthFocus:dt,hasFocus:pt,onFocusedViewChange:mt,monthsPerRow:ft=3,timezone:ht,gridLabelId:yt}=s,bt=_objectWithoutPropertiesLoose(s,_excluded$9),{value:gt,handleValueChange:xt,timezone:vt}=useControlledValueWithTimezone({name:"MonthCalendar",timezone:ht,value:j,defaultValue:_e,onChange:lt,valueManager:singleItemValueManager}),Lt=useNow(vt),$t=useTheme$3(),Tt=useUtils(),Et=reactExports.useMemo(()=>singleItemValueManager.getInitialReferenceValue({value:gt,utils:Tt,props:s,timezone:vt,referenceDate:et,granularity:SECTION_TYPE_GRANULARITY.month}),[]),Dt=s,It=useUtilityClasses$6(Dt),Ct=reactExports.useMemo(()=>Tt.getMonth(Lt),[Tt,Lt]),jt=reactExports.useMemo(()=>gt!=null?Tt.getMonth(gt):ut?null:Tt.getMonth(Et),[gt,Tt,ut,Et]),[Zt,Xt]=reactExports.useState(()=>jt||Ct),[sn,Ft]=useControlled({name:"MonthCalendar",state:"hasFocus",controlled:pt,default:ot??!1}),wt=useEventCallback(zt=>{Ft(zt),mt&&mt(zt)}),kt=reactExports.useCallback(zt=>{const Gt=Tt.startOfMonth(at&&Tt.isAfter(Lt,st)?Lt:st),Wt=Tt.startOfMonth(nt&&Tt.isBefore(Lt,it)?Lt:it),qt=Tt.startOfMonth(zt);return Tt.isBefore(qt,Gt)||Tt.isAfter(qt,Wt)?!0:ct?ct(qt):!1},[nt,at,it,st,Lt,ct,Tt]),At=useEventCallback((zt,Gt)=>{if(rt)return;const Wt=Tt.setMonth(gt??Et,Gt);xt(Wt)}),Pt=useEventCallback(zt=>{kt(Tt.setMonth(gt??Et,zt))||(Xt(zt),wt(!0),dt&&dt(zt))});reactExports.useEffect(()=>{Xt(zt=>jt!==null&&zt!==jt?jt:zt)},[jt]);const Mt=useEventCallback((zt,Gt)=>{switch(zt.key){case"ArrowUp":Pt((12+Gt-3)%12),zt.preventDefault();break;case"ArrowDown":Pt((12+Gt+3)%12),zt.preventDefault();break;case"ArrowLeft":Pt((12+Gt+($t.direction==="ltr"?-1:1))%12),zt.preventDefault();break;case"ArrowRight":Pt((12+Gt+($t.direction==="ltr"?1:-1))%12),zt.preventDefault();break}}),Ot=useEventCallback((zt,Gt)=>{Pt(Gt)}),Bt=useEventCallback((zt,Gt)=>{Zt===Gt&&wt(!1)});return jsxRuntimeExports.jsx(MonthCalendarRoot,_extends({ref:o,className:clsx(It.root,$),ownerState:Dt,role:"radiogroup","aria-labelledby":yt},bt,{children:getMonthsInYear(Tt,gt??Et).map(zt=>{const Gt=Tt.getMonth(zt),Wt=Tt.format(zt,"monthShort"),qt=Tt.format(zt,"month"),tn=Gt===jt,ln=tt||kt(zt);return jsxRuntimeExports.jsx(PickersMonth,{selected:tn,value:Gt,onClick:At,onKeyDown:Mt,autoFocus:sn&&Gt===Zt,disabled:ln,tabIndex:Gt===Zt?0:-1,onFocus:Ot,onBlur:Bt,"aria-current":Ct===Gt?"date":void 0,"aria-label":qt,monthsPerRow:ft,children:Wt},Wt)})}))});function getPickersYearUtilityClass(a){return generateUtilityClass$1("MuiPickersYear",a)}const pickersYearClasses=generateUtilityClasses$1("MuiPickersYear",["root","yearButton","selected","disabled"]),_excluded$8=["autoFocus","className","children","disabled","selected","value","tabIndex","onClick","onKeyDown","onFocus","onBlur","aria-current","yearsPerRow"],useUtilityClasses$5=a=>{const{disabled:i,selected:o,classes:s}=a;return composeClasses({root:["root"],yearButton:["yearButton",i&&"disabled",o&&"selected"]},getPickersYearUtilityClass,s)},PickersYearRoot=styled("div",{name:"MuiPickersYear",slot:"Root",overridesResolver:(a,i)=>[i.root]})(({ownerState:a})=>({flexBasis:a.yearsPerRow===3?"33.3%":"25%",display:"flex",alignItems:"center",justifyContent:"center"})),PickersYearButton=styled("button",{name:"MuiPickersYear",slot:"YearButton",overridesResolver:(a,i)=>[i.yearButton,{[`&.${pickersYearClasses.disabled}`]:i.disabled},{[`&.${pickersYearClasses.selected}`]:i.selected}]})(({theme:a})=>_extends({color:"unset",backgroundColor:"transparent",border:0,outline:0},a.typography.subtitle1,{margin:"6px 0",height:36,width:72,borderRadius:18,cursor:"pointer","&:focus":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.activeChannel} / ${a.vars.palette.action.focusOpacity})`:alpha$1(a.palette.action.active,a.palette.action.focusOpacity)},"&:hover":{backgroundColor:a.vars?`rgba(${a.vars.palette.action.activeChannel} / ${a.vars.palette.action.hoverOpacity})`:alpha$1(a.palette.action.active,a.palette.action.hoverOpacity)},"&:disabled":{cursor:"auto",pointerEvents:"none"},[`&.${pickersYearClasses.disabled}`]:{color:(a.vars||a).palette.text.secondary},[`&.${pickersYearClasses.selected}`]:{color:(a.vars||a).palette.primary.contrastText,backgroundColor:(a.vars||a).palette.primary.main,"&:focus, &:hover":{backgroundColor:(a.vars||a).palette.primary.dark}}})),PickersYear=reactExports.memo(function a(i){const o=useThemeProps$6({props:i,name:"MuiPickersYear"}),{autoFocus:s,className:$,children:j,disabled:_e,selected:et,value:tt,tabIndex:nt,onClick:at,onKeyDown:it,onFocus:st,onBlur:lt,"aria-current":ct}=o,rt=_objectWithoutPropertiesLoose(o,_excluded$8),ut=reactExports.useRef(null),ot=useUtilityClasses$5(o);return reactExports.useEffect(()=>{s&&ut.current.focus()},[s]),jsxRuntimeExports.jsx(PickersYearRoot,_extends({className:clsx(ot.root,$),ownerState:o},rt,{children:jsxRuntimeExports.jsx(PickersYearButton,{ref:ut,disabled:_e,type:"button",role:"radio",tabIndex:_e?-1:nt,"aria-current":ct,"aria-checked":et,onClick:dt=>at(dt,tt),onKeyDown:dt=>it(dt,tt),onFocus:dt=>st(dt,tt),onBlur:dt=>lt(dt,tt),className:ot.yearButton,ownerState:o,children:j})}))});function getYearCalendarUtilityClass(a){return generateUtilityClass$1("MuiYearCalendar",a)}generateUtilityClasses$1("MuiYearCalendar",["root"]);const _excluded$7=["autoFocus","className","value","defaultValue","referenceDate","disabled","disableFuture","disablePast","maxDate","minDate","onChange","readOnly","shouldDisableYear","disableHighlightToday","onYearFocus","hasFocus","onFocusedViewChange","yearsPerRow","timezone","gridLabelId"],useUtilityClasses$4=a=>{const{classes:i}=a;return composeClasses({root:["root"]},getYearCalendarUtilityClass,i)};function useYearCalendarDefaultizedProps(a,i){var o;const s=useUtils(),$=useDefaultDates(),j=useThemeProps$6({props:a,name:i});return _extends({disablePast:!1,disableFuture:!1},j,{yearsPerRow:(o=j.yearsPerRow)!=null?o:3,minDate:applyDefaultDate(s,j.minDate,$.minDate),maxDate:applyDefaultDate(s,j.maxDate,$.maxDate)})}const YearCalendarRoot=styled("div",{name:"MuiYearCalendar",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"flex",flexDirection:"row",flexWrap:"wrap",overflowY:"auto",height:"100%",padding:"0 4px",width:DIALOG_WIDTH,maxHeight:MAX_CALENDAR_HEIGHT,boxSizing:"border-box",position:"relative"}),YearCalendar=reactExports.forwardRef(function a(i,o){const s=useYearCalendarDefaultizedProps(i,"MuiYearCalendar"),{autoFocus:$,className:j,value:_e,defaultValue:et,referenceDate:tt,disabled:nt,disableFuture:at,disablePast:it,maxDate:st,minDate:lt,onChange:ct,readOnly:rt,shouldDisableYear:ut,disableHighlightToday:ot,onYearFocus:dt,hasFocus:pt,onFocusedViewChange:mt,yearsPerRow:ft,timezone:ht,gridLabelId:yt}=s,bt=_objectWithoutPropertiesLoose(s,_excluded$7),{value:gt,handleValueChange:xt,timezone:vt}=useControlledValueWithTimezone({name:"YearCalendar",timezone:ht,value:_e,defaultValue:et,onChange:ct,valueManager:singleItemValueManager}),Lt=useNow(vt),$t=useTheme$3(),Tt=useUtils(),Et=reactExports.useMemo(()=>singleItemValueManager.getInitialReferenceValue({value:gt,utils:Tt,props:s,timezone:vt,referenceDate:tt,granularity:SECTION_TYPE_GRANULARITY.year}),[]),Dt=s,It=useUtilityClasses$4(Dt),Ct=reactExports.useMemo(()=>Tt.getYear(Lt),[Tt,Lt]),jt=reactExports.useMemo(()=>gt!=null?Tt.getYear(gt):ot?null:Tt.getYear(Et),[gt,Tt,ot,Et]),[Zt,Xt]=reactExports.useState(()=>jt||Ct),[sn,Ft]=useControlled({name:"YearCalendar",state:"hasFocus",controlled:pt,default:$??!1}),wt=useEventCallback(Wt=>{Ft(Wt),mt&&mt(Wt)}),kt=reactExports.useCallback(Wt=>{if(it&&Tt.isBeforeYear(Wt,Lt)||at&&Tt.isAfterYear(Wt,Lt)||lt&&Tt.isBeforeYear(Wt,lt)||st&&Tt.isAfterYear(Wt,st))return!0;if(!ut)return!1;const qt=Tt.startOfYear(Wt);return ut(qt)},[at,it,st,lt,Lt,ut,Tt]),At=useEventCallback((Wt,qt)=>{if(rt)return;const tn=Tt.setYear(gt??Et,qt);xt(tn)}),Pt=useEventCallback(Wt=>{kt(Tt.setYear(gt??Et,Wt))||(Xt(Wt),wt(!0),dt==null||dt(Wt))});reactExports.useEffect(()=>{Xt(Wt=>jt!==null&&Wt!==jt?jt:Wt)},[jt]);const Mt=useEventCallback((Wt,qt)=>{switch(Wt.key){case"ArrowUp":Pt(qt-ft),Wt.preventDefault();break;case"ArrowDown":Pt(qt+ft),Wt.preventDefault();break;case"ArrowLeft":Pt(qt+($t.direction==="ltr"?-1:1)),Wt.preventDefault();break;case"ArrowRight":Pt(qt+($t.direction==="ltr"?1:-1)),Wt.preventDefault();break}}),Ot=useEventCallback((Wt,qt)=>{Pt(qt)}),Bt=useEventCallback((Wt,qt)=>{Zt===qt&&wt(!1)}),zt=reactExports.useRef(null),Gt=useForkRef(o,zt);return reactExports.useEffect(()=>{if($||zt.current===null)return;const Wt=zt.current.querySelector('[tabindex="0"]');if(!Wt)return;const qt=Wt.offsetHeight,tn=Wt.offsetTop,ln=zt.current.clientHeight,gn=zt.current.scrollTop,yn=tn+qt;qt>ln||tn<gn||(zt.current.scrollTop=yn-ln/2-qt/2)},[$]),jsxRuntimeExports.jsx(YearCalendarRoot,_extends({ref:Gt,className:clsx(It.root,j),ownerState:Dt,role:"radiogroup","aria-labelledby":yt},bt,{children:Tt.getYearRange(lt,st).map(Wt=>{const qt=Tt.getYear(Wt),tn=qt===jt,ln=nt||kt(Wt);return jsxRuntimeExports.jsx(PickersYear,{selected:tn,value:qt,onClick:At,onKeyDown:Mt,autoFocus:sn&&qt===Zt,disabled:ln,tabIndex:qt===Zt?0:-1,onFocus:Ot,onBlur:Bt,"aria-current":Ct===qt?"date":void 0,yearsPerRow:ft,children:Tt.format(Wt,"year")},Tt.format(Wt,"year"))})}))}),getPickersCalendarHeaderUtilityClass=a=>generateUtilityClass$1("MuiPickersCalendarHeader",a),pickersCalendarHeaderClasses=generateUtilityClasses$1("MuiPickersCalendarHeader",["root","labelContainer","label","switchViewButton","switchViewIcon"]),_excluded$6=["slots","slotProps","components","componentsProps","currentMonth","disabled","disableFuture","disablePast","maxDate","minDate","onMonthChange","onViewChange","view","reduceAnimations","views","labelId","className","timezone"],_excluded2$1=["ownerState"],useUtilityClasses$3=a=>{const{classes:i}=a;return composeClasses({root:["root"],labelContainer:["labelContainer"],label:["label"],switchViewButton:["switchViewButton"],switchViewIcon:["switchViewIcon"]},getPickersCalendarHeaderUtilityClass,i)},PickersCalendarHeaderRoot=styled("div",{name:"MuiPickersCalendarHeader",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"flex",alignItems:"center",marginTop:16,marginBottom:8,paddingLeft:24,paddingRight:12,maxHeight:30,minHeight:30}),PickersCalendarHeaderLabelContainer=styled("div",{name:"MuiPickersCalendarHeader",slot:"LabelContainer",overridesResolver:(a,i)=>i.labelContainer})(({theme:a})=>_extends({display:"flex",overflow:"hidden",alignItems:"center",cursor:"pointer",marginRight:"auto"},a.typography.body1,{fontWeight:a.typography.fontWeightMedium})),PickersCalendarHeaderLabel=styled("div",{name:"MuiPickersCalendarHeader",slot:"Label",overridesResolver:(a,i)=>i.label})({marginRight:6}),PickersCalendarHeaderSwitchViewButton=styled(IconButton$1,{name:"MuiPickersCalendarHeader",slot:"SwitchViewButton",overridesResolver:(a,i)=>i.switchViewButton})(({ownerState:a})=>_extends({marginRight:"auto"},a.view==="year"&&{[`.${pickersCalendarHeaderClasses.switchViewIcon}`]:{transform:"rotate(180deg)"}})),PickersCalendarHeaderSwitchViewIcon=styled(ArrowDropDownIcon,{name:"MuiPickersCalendarHeader",slot:"SwitchViewIcon",overridesResolver:(a,i)=>i.switchViewIcon})(({theme:a})=>({willChange:"transform",transition:a.transitions.create("transform"),transform:"rotate(0deg)"})),PickersCalendarHeader=reactExports.forwardRef(function a(i,o){var s,$,j,_e;const et=useLocaleText(),tt=useUtils(),nt=useThemeProps$6({props:i,name:"MuiPickersCalendarHeader"}),{slots:at,slotProps:it,components:st,currentMonth:lt,disabled:ct,disableFuture:rt,disablePast:ut,maxDate:ot,minDate:dt,onMonthChange:pt,onViewChange:mt,view:ft,reduceAnimations:ht,views:yt,labelId:bt,className:gt,timezone:xt}=nt,vt=_objectWithoutPropertiesLoose(nt,_excluded$6),Lt=nt,$t=useUtilityClasses$3(nt),Tt=(s=($=at==null?void 0:at.switchViewButton)!=null?$:st==null?void 0:st.SwitchViewButton)!=null?s:PickersCalendarHeaderSwitchViewButton,Et=useSlotProps({elementType:Tt,externalSlotProps:it==null?void 0:it.switchViewButton,additionalProps:{size:"small","aria-label":et.calendarViewSwitchingButtonAriaLabel(ft)},ownerState:Lt,className:$t.switchViewButton}),Dt=(j=(_e=at==null?void 0:at.switchViewIcon)!=null?_e:st==null?void 0:st.SwitchViewIcon)!=null?j:PickersCalendarHeaderSwitchViewIcon,It=useSlotProps({elementType:Dt,externalSlotProps:it==null?void 0:it.switchViewIcon,ownerState:void 0,className:$t.switchViewIcon}),Ct=_objectWithoutPropertiesLoose(It,_excluded2$1),jt=()=>pt(tt.addMonths(lt,1),"left"),Zt=()=>pt(tt.addMonths(lt,-1),"right"),Xt=useNextMonthDisabled(lt,{disableFuture:rt,maxDate:ot,timezone:xt}),sn=usePreviousMonthDisabled(lt,{disablePast:ut,minDate:dt,timezone:xt}),Ft=()=>{if(!(yt.length===1||!mt||ct))if(yt.length===2)mt(yt.find(wt=>wt!==ft)||yt[0]);else{const wt=yt.indexOf(ft)!==0?0:1;mt(yt[wt])}};return yt.length===1&&yt[0]==="year"?null:jsxRuntimeExports.jsxs(PickersCalendarHeaderRoot,_extends({},vt,{ownerState:Lt,className:clsx(gt,$t.root),ref:o,children:[jsxRuntimeExports.jsxs(PickersCalendarHeaderLabelContainer,{role:"presentation",onClick:Ft,ownerState:Lt,"aria-live":"polite",className:$t.labelContainer,children:[jsxRuntimeExports.jsx(PickersFadeTransitionGroup,{reduceAnimations:ht,transKey:tt.format(lt,"monthAndYear"),children:jsxRuntimeExports.jsx(PickersCalendarHeaderLabel,{id:bt,ownerState:Lt,className:$t.label,children:tt.format(lt,"monthAndYear")})}),yt.length>1&&!ct&&jsxRuntimeExports.jsx(Tt,_extends({},Et,{children:jsxRuntimeExports.jsx(Dt,_extends({},Ct))}))]}),jsxRuntimeExports.jsx(Fade$1,{in:ft==="day",children:jsxRuntimeExports.jsx(PickersArrowSwitcher,{slots:at,slotProps:it,onGoToPrevious:Zt,isPreviousDisabled:sn,previousLabel:et.previousMonth,onGoToNext:jt,isNextDisabled:Xt,nextLabel:et.nextMonth})})]}))}),getDateCalendarUtilityClass=a=>generateUtilityClass$1("MuiDateCalendar",a);generateUtilityClasses$1("MuiDateCalendar",["root","viewTransitionContainer"]);const _excluded$5=["autoFocus","onViewChange","value","defaultValue","referenceDate","disableFuture","disablePast","defaultCalendarMonth","onChange","onYearChange","onMonthChange","reduceAnimations","shouldDisableDate","shouldDisableMonth","shouldDisableYear","view","views","openTo","className","disabled","readOnly","minDate","maxDate","disableHighlightToday","focusedView","onFocusedViewChange","showDaysOutsideCurrentMonth","fixedWeekNumber","dayOfWeekFormatter","components","componentsProps","slots","slotProps","loading","renderLoading","displayWeekNumber","yearsPerRow","monthsPerRow","timezone"],useUtilityClasses$2=a=>{const{classes:i}=a;return composeClasses({root:["root"],viewTransitionContainer:["viewTransitionContainer"]},getDateCalendarUtilityClass,i)};function useDateCalendarDefaultizedProps(a,i){var o,s,$,j,_e,et,tt;const nt=useUtils(),at=useDefaultDates(),it=useDefaultReduceAnimations(),st=useThemeProps$6({props:a,name:i});return _extends({},st,{loading:(o=st.loading)!=null?o:!1,disablePast:(s=st.disablePast)!=null?s:!1,disableFuture:($=st.disableFuture)!=null?$:!1,openTo:(j=st.openTo)!=null?j:"day",views:(_e=st.views)!=null?_e:["year","day"],reduceAnimations:(et=st.reduceAnimations)!=null?et:it,renderLoading:(tt=st.renderLoading)!=null?tt:()=>jsxRuntimeExports.jsx("span",{children:"..."}),minDate:applyDefaultDate(nt,st.minDate,at.minDate),maxDate:applyDefaultDate(nt,st.maxDate,at.maxDate)})}const DateCalendarRoot=styled(PickerViewRoot,{name:"MuiDateCalendar",slot:"Root",overridesResolver:(a,i)=>i.root})({display:"flex",flexDirection:"column",height:VIEW_HEIGHT}),DateCalendarViewTransitionContainer=styled(PickersFadeTransitionGroup,{name:"MuiDateCalendar",slot:"ViewTransitionContainer",overridesResolver:(a,i)=>i.viewTransitionContainer})({}),DateCalendar=reactExports.forwardRef(function a(i,o){var s,$,j;const _e=useUtils(),et=useId(),tt=useDateCalendarDefaultizedProps(i,"MuiDateCalendar"),{autoFocus:nt,onViewChange:at,value:it,defaultValue:st,referenceDate:lt,disableFuture:ct,disablePast:rt,defaultCalendarMonth:ut,onChange:ot,onYearChange:dt,onMonthChange:pt,reduceAnimations:mt,shouldDisableDate:ft,shouldDisableMonth:ht,shouldDisableYear:yt,view:bt,views:gt,openTo:xt,className:vt,disabled:Lt,readOnly:$t,minDate:Tt,maxDate:Et,disableHighlightToday:Dt,focusedView:It,onFocusedViewChange:Ct,showDaysOutsideCurrentMonth:jt,fixedWeekNumber:Zt,dayOfWeekFormatter:Xt,components:sn,componentsProps:Ft,slots:wt,slotProps:kt,loading:At,renderLoading:Pt,displayWeekNumber:Mt,yearsPerRow:Ot,monthsPerRow:Bt,timezone:zt}=tt,Gt=_objectWithoutPropertiesLoose(tt,_excluded$5),{value:Wt,handleValueChange:qt,timezone:tn}=useControlledValueWithTimezone({name:"DateCalendar",timezone:zt,value:it,defaultValue:st,onChange:ot,valueManager:singleItemValueManager}),{view:ln,setView:gn,focusedView:yn,setFocusedView:Pn,goToNextView:cn,setValueAndGoToNextView:xn}=useViews({view:bt,views:gt,openTo:xt,onChange:qt,onViewChange:at,autoFocus:nt,focusedView:It,onFocusedViewChange:Ct}),{referenceDate:hn,calendarState:en,changeFocusedDay:Jt,changeMonth:vn,handleChangeMonth:$n,isDateDisabled:Mn,onMonthSwitchingAnimationEnd:On}=useCalendarState({value:Wt,defaultCalendarMonth:ut,referenceDate:lt,reduceAnimations:mt,onMonthChange:pt,minDate:Tt,maxDate:Et,shouldDisableDate:ft,disablePast:rt,disableFuture:ct,timezone:tn}),En=Lt&&Wt||Tt,Bn=Lt&&Wt||Et,Hn=`${et}-grid-label`,Wn=yn!==null,_n=(s=($=wt==null?void 0:wt.calendarHeader)!=null?$:sn==null?void 0:sn.CalendarHeader)!=null?s:PickersCalendarHeader,Zn=useSlotProps({elementType:_n,externalSlotProps:(j=kt==null?void 0:kt.calendarHeader)!=null?j:Ft==null?void 0:Ft.calendarHeader,additionalProps:{views:gt,view:ln,currentMonth:en.currentMonth,onViewChange:gn,onMonthChange:(Kt,nn)=>$n({newMonth:Kt,direction:nn}),minDate:En,maxDate:Bn,disabled:Lt,disablePast:rt,disableFuture:ct,reduceAnimations:mt,timezone:tn,labelId:Hn,slots:wt,slotProps:kt},ownerState:tt}),bn=useEventCallback(Kt=>{const nn=_e.startOfMonth(Kt),kn=_e.endOfMonth(Kt),Rn=Mn(Kt)?findClosestEnabledDate({utils:_e,date:Kt,minDate:_e.isBefore(Tt,nn)?nn:Tt,maxDate:_e.isAfter(Et,kn)?kn:Et,disablePast:rt,disableFuture:ct,isDateDisabled:Mn,timezone:tn}):Kt;Rn?(xn(Rn,"finish"),pt==null||pt(nn)):(cn(),vn(nn)),Jt(Rn,!0)}),dn=useEventCallback(Kt=>{const nn=_e.startOfYear(Kt),kn=_e.endOfYear(Kt),Rn=Mn(Kt)?findClosestEnabledDate({utils:_e,date:Kt,minDate:_e.isBefore(Tt,nn)?nn:Tt,maxDate:_e.isAfter(Et,kn)?kn:Et,disablePast:rt,disableFuture:ct,isDateDisabled:Mn,timezone:tn}):Kt;Rn?(xn(Rn,"finish"),dt==null||dt(Rn)):(cn(),vn(nn)),Jt(Rn,!0)}),an=useEventCallback(Kt=>qt(Kt&&mergeDateAndTime(_e,Kt,Wt??hn),"finish",ln));reactExports.useEffect(()=>{Wt!=null&&_e.isValid(Wt)&&vn(Wt)},[Wt]);const In=tt,Dn=useUtilityClasses$2(In),Xn={disablePast:rt,disableFuture:ct,maxDate:Et,minDate:Tt},Yn={disableHighlightToday:Dt,readOnly:$t,disabled:Lt,timezone:tn,gridLabelId:Hn},pn=reactExports.useRef(ln);reactExports.useEffect(()=>{pn.current!==ln&&(yn===pn.current&&Pn(ln,!0),pn.current=ln)},[yn,Pn,ln]);const Ht=reactExports.useMemo(()=>[Wt],[Wt]);return jsxRuntimeExports.jsxs(DateCalendarRoot,_extends({ref:o,className:clsx(Dn.root,vt),ownerState:In},Gt,{children:[jsxRuntimeExports.jsx(_n,_extends({},Zn)),jsxRuntimeExports.jsx(DateCalendarViewTransitionContainer,{reduceAnimations:mt,className:Dn.viewTransitionContainer,transKey:ln,ownerState:In,children:jsxRuntimeExports.jsxs("div",{children:[ln==="year"&&jsxRuntimeExports.jsx(YearCalendar,_extends({},Xn,Yn,{value:Wt,onChange:dn,shouldDisableYear:yt,hasFocus:Wn,onFocusedViewChange:Kt=>Pn("year",Kt),yearsPerRow:Ot,referenceDate:hn})),ln==="month"&&jsxRuntimeExports.jsx(MonthCalendar,_extends({},Xn,Yn,{hasFocus:Wn,className:vt,value:Wt,onChange:bn,shouldDisableMonth:ht,onFocusedViewChange:Kt=>Pn("month",Kt),monthsPerRow:Bt,referenceDate:hn})),ln==="day"&&jsxRuntimeExports.jsx(DayCalendar,_extends({},en,Xn,Yn,{onMonthSwitchingAnimationEnd:On,onFocusedDayChange:Jt,reduceAnimations:mt,selectedDays:Ht,onSelectedDaysChange:an,shouldDisableDate:ft,shouldDisableMonth:ht,shouldDisableYear:yt,hasFocus:Wn,onFocusedViewChange:Kt=>Pn("day",Kt),showDaysOutsideCurrentMonth:jt,fixedWeekNumber:Zt,dayOfWeekFormatter:Xt,displayWeekNumber:Mt,components:sn,componentsProps:Ft,slots:wt,slotProps:kt,loading:At,renderLoading:Pt}))]})})]}))});function getDatePickerToolbarUtilityClass(a){return generateUtilityClass$1("MuiDatePickerToolbar",a)}generateUtilityClasses$1("MuiDatePickerToolbar",["root","title"]);const _excluded$4=["value","isLandscape","onChange","toolbarFormat","toolbarPlaceholder","views","className"],useUtilityClasses$1=a=>{const{classes:i}=a;return composeClasses({root:["root"],title:["title"]},getDatePickerToolbarUtilityClass,i)},DatePickerToolbarRoot=styled(PickersToolbar,{name:"MuiDatePickerToolbar",slot:"Root",overridesResolver:(a,i)=>i.root})({}),DatePickerToolbarTitle=styled(Typography$1,{name:"MuiDatePickerToolbar",slot:"Title",overridesResolver:(a,i)=>i.title})(({ownerState:a})=>_extends({},a.isLandscape&&{margin:"auto 16px auto auto"})),DatePickerToolbar=reactExports.forwardRef(function a(i,o){const s=useThemeProps$6({props:i,name:"MuiDatePickerToolbar"}),{value:$,isLandscape:j,toolbarFormat:_e,toolbarPlaceholder:et="––",views:tt,className:nt}=s,at=_objectWithoutPropertiesLoose(s,_excluded$4),it=useUtils(),st=useLocaleText(),lt=useUtilityClasses$1(s),ct=reactExports.useMemo(()=>{if(!$)return et;const ut=resolveDateFormat(it,{format:_e,views:tt},!0);return it.formatByString($,ut)},[$,_e,et,it,tt]),rt=s;return jsxRuntimeExports.jsx(DatePickerToolbarRoot,_extends({ref:o,toolbarTitle:st.datePickerToolbarTitle,isLandscape:j,className:clsx(lt.root,nt)},at,{children:jsxRuntimeExports.jsx(DatePickerToolbarTitle,{variant:"h4",align:j?"left":"center",ownerState:rt,className:lt.title,children:ct})}))});function useDatePickerDefaultizedProps(a,i){var o,s,$,j;const _e=useUtils(),et=useDefaultDates(),tt=useThemeProps$6({props:a,name:i}),nt=reactExports.useMemo(()=>{var it;return((it=tt.localeText)==null?void 0:it.toolbarTitle)==null?tt.localeText:_extends({},tt.localeText,{datePickerToolbarTitle:tt.localeText.toolbarTitle})},[tt.localeText]),at=(o=tt.slots)!=null?o:uncapitalizeObjectKeys(tt.components);return _extends({},tt,{localeText:nt},applyDefaultViewProps({views:tt.views,openTo:tt.openTo,defaultViews:["year","day"],defaultOpenTo:"day"}),{disableFuture:(s=tt.disableFuture)!=null?s:!1,disablePast:($=tt.disablePast)!=null?$:!1,minDate:applyDefaultDate(_e,tt.minDate,et.minDate),maxDate:applyDefaultDate(_e,tt.maxDate,et.maxDate),slots:_extends({toolbar:DatePickerToolbar},at),slotProps:(j=tt.slotProps)!=null?j:tt.componentsProps})}const _excluded$3=["props","getOpenDialogAriaText"],_excluded2=["ownerState"],_excluded3=["ownerState"],useDesktopPicker=a=>{var i,o,s,$,j;let{props:_e,getOpenDialogAriaText:et}=a,tt=_objectWithoutPropertiesLoose(a,_excluded$3);const{slots:nt,slotProps:at,className:it,sx:st,format:lt,formatDensity:ct,timezone:rt,name:ut,label:ot,inputRef:dt,readOnly:pt,disabled:mt,autoFocus:ft,localeText:ht,reduceAnimations:yt}=_e,bt=useUtils(),gt=reactExports.useRef(null),xt=reactExports.useRef(null),vt=useId(),Lt=(i=at==null||(o=at.toolbar)==null?void 0:o.hidden)!=null?i:!1,{open:$t,actions:Tt,hasUIView:Et,layoutProps:Dt,renderCurrentView:It,shouldRestoreFocus:Ct,fieldProps:jt}=usePicker(_extends({},tt,{props:_e,inputRef:gt,autoFocusView:!0,additionalViewProps:{},wrapperVariant:"desktop"})),Zt=(s=nt.inputAdornment)!=null?s:InputAdornment$1,Xt=useSlotProps({elementType:Zt,externalSlotProps:at==null?void 0:at.inputAdornment,additionalProps:{position:"end"},ownerState:_e}),sn=_objectWithoutPropertiesLoose(Xt,_excluded2),Ft=($=nt.openPickerButton)!=null?$:IconButton$1,wt=useSlotProps({elementType:Ft,externalSlotProps:at==null?void 0:at.openPickerButton,additionalProps:{disabled:mt||pt,onClick:$t?Tt.onClose:Tt.onOpen,"aria-label":et(jt.value,bt),edge:sn.position},ownerState:_e}),kt=_objectWithoutPropertiesLoose(wt,_excluded3),At=nt.openPickerIcon,Pt=nt.field,Mt=useSlotProps({elementType:Pt,externalSlotProps:at==null?void 0:at.field,additionalProps:_extends({},jt,Lt&&{id:vt},{readOnly:pt,disabled:mt,className:it,sx:st,format:lt,formatDensity:ct,timezone:rt,label:ot,name:ut,autoFocus:ft&&!_e.open,focused:$t?!0:void 0}),ownerState:_e});Et&&(Mt.InputProps=_extends({},Mt.InputProps,{ref:xt,[`${sn.position}Adornment`]:jsxRuntimeExports.jsx(Zt,_extends({},sn,{children:jsxRuntimeExports.jsx(Ft,_extends({},kt,{children:jsxRuntimeExports.jsx(At,_extends({},at==null?void 0:at.openPickerIcon))}))}))}));const Ot=_extends({textField:nt.textField,clearIcon:nt.clearIcon,clearButton:nt.clearButton},Mt.slots),Bt=(j=nt.layout)!=null?j:PickersLayout,zt=useForkRef(gt,Mt.inputRef,dt);let Gt=vt;Lt&&(ot?Gt=`${vt}-label`:Gt=void 0);const Wt=_extends({},at,{toolbar:_extends({},at==null?void 0:at.toolbar,{titleId:vt}),popper:_extends({"aria-labelledby":Gt},at==null?void 0:at.popper)});return{renderPicker:()=>jsxRuntimeExports.jsxs(LocalizationProvider,{localeText:ht,children:[jsxRuntimeExports.jsx(Pt,_extends({},Mt,{slots:Ot,slotProps:Wt,inputRef:zt})),jsxRuntimeExports.jsx(PickersPopper,_extends({role:"dialog",placement:"bottom-start",anchorEl:xt.current},Tt,{open:$t,slots:nt,slotProps:Wt,shouldRestoreFocus:Ct,reduceAnimations:yt,children:jsxRuntimeExports.jsx(Bt,_extends({},Dt,Wt==null?void 0:Wt.layout,{slots:nt,slotProps:Wt,children:It()}))}))]})}},renderDateViewCalendar=({view:a,onViewChange:i,views:o,focusedView:s,onFocusedViewChange:$,value:j,defaultValue:_e,referenceDate:et,onChange:tt,className:nt,classes:at,disableFuture:it,disablePast:st,minDate:lt,maxDate:ct,shouldDisableDate:rt,shouldDisableMonth:ut,shouldDisableYear:ot,reduceAnimations:dt,onMonthChange:pt,monthsPerRow:mt,onYearChange:ft,yearsPerRow:ht,defaultCalendarMonth:yt,components:bt,componentsProps:gt,slots:xt,slotProps:vt,loading:Lt,renderLoading:$t,disableHighlightToday:Tt,readOnly:Et,disabled:Dt,showDaysOutsideCurrentMonth:It,dayOfWeekFormatter:Ct,sx:jt,autoFocus:Zt,fixedWeekNumber:Xt,displayWeekNumber:sn,timezone:Ft})=>jsxRuntimeExports.jsx(DateCalendar,{view:a,onViewChange:i,views:o.filter(isDatePickerView),focusedView:s&&isDatePickerView(s)?s:null,onFocusedViewChange:$,value:j,defaultValue:_e,referenceDate:et,onChange:tt,className:nt,classes:at,disableFuture:it,disablePast:st,minDate:lt,maxDate:ct,shouldDisableDate:rt,shouldDisableMonth:ut,shouldDisableYear:ot,reduceAnimations:dt,onMonthChange:pt,monthsPerRow:mt,onYearChange:ft,yearsPerRow:ht,defaultCalendarMonth:yt,components:bt,componentsProps:gt,slots:xt,slotProps:vt,loading:Lt,renderLoading:$t,disableHighlightToday:Tt,readOnly:Et,disabled:Dt,showDaysOutsideCurrentMonth:It,dayOfWeekFormatter:Ct,sx:jt,autoFocus:Zt,fixedWeekNumber:Xt,displayWeekNumber:sn,timezone:Ft}),DesktopDatePicker=reactExports.forwardRef(function a(i,o){var s,$,j,_e;const et=useLocaleText(),tt=useUtils(),nt=useDatePickerDefaultizedProps(i,"MuiDesktopDatePicker"),at=_extends({day:renderDateViewCalendar,month:renderDateViewCalendar,year:renderDateViewCalendar},nt.viewRenderers),it=_extends({},nt,{viewRenderers:at,format:resolveDateFormat(tt,nt,!1),yearsPerRow:(s=nt.yearsPerRow)!=null?s:4,slots:_extends({openPickerIcon:CalendarIcon,field:DateField},nt.slots),slotProps:_extends({},nt.slotProps,{field:lt=>{var ct;return _extends({},resolveComponentProps((ct=nt.slotProps)==null?void 0:ct.field,lt),extractValidationProps(nt),{ref:o})},toolbar:_extends({hidden:!0},($=nt.slotProps)==null?void 0:$.toolbar)})}),{renderPicker:st}=useDesktopPicker({props:it,valueManager:singleItemValueManager,valueType:"date",getOpenDialogAriaText:(j=(_e=it.localeText)==null?void 0:_e.openDatePickerDialogue)!=null?j:et.openDatePickerDialogue,validator:validateDate});return st()});DesktopDatePicker.propTypes={autoFocus:PropTypes.bool,className:PropTypes.string,closeOnSelect:PropTypes.bool,components:PropTypes.object,componentsProps:PropTypes.object,dayOfWeekFormatter:PropTypes.func,defaultCalendarMonth:PropTypes.any,defaultValue:PropTypes.any,disabled:PropTypes.bool,disableFuture:PropTypes.bool,disableHighlightToday:PropTypes.bool,disableOpenPicker:PropTypes.bool,disablePast:PropTypes.bool,displayWeekNumber:PropTypes.bool,fixedWeekNumber:PropTypes.number,format:PropTypes.string,formatDensity:PropTypes.oneOf(["dense","spacious"]),inputRef:refType$1,label:PropTypes.node,loading:PropTypes.bool,localeText:PropTypes.object,maxDate:PropTypes.any,minDate:PropTypes.any,monthsPerRow:PropTypes.oneOf([3,4]),name:PropTypes.string,onAccept:PropTypes.func,onChange:PropTypes.func,onClose:PropTypes.func,onError:PropTypes.func,onMonthChange:PropTypes.func,onOpen:PropTypes.func,onSelectedSectionsChange:PropTypes.func,onViewChange:PropTypes.func,onYearChange:PropTypes.func,open:PropTypes.bool,openTo:PropTypes.oneOf(["day","month","year"]),orientation:PropTypes.oneOf(["landscape","portrait"]),readOnly:PropTypes.bool,reduceAnimations:PropTypes.bool,referenceDate:PropTypes.any,renderLoading:PropTypes.func,selectedSections:PropTypes.oneOfType([PropTypes.oneOf(["all","day","hours","meridiem","minutes","month","seconds","weekDay","year"]),PropTypes.number,PropTypes.shape({endIndex:PropTypes.number.isRequired,startIndex:PropTypes.number.isRequired})]),shouldDisableDate:PropTypes.func,shouldDisableMonth:PropTypes.func,shouldDisableYear:PropTypes.func,showDaysOutsideCurrentMonth:PropTypes.bool,slotProps:PropTypes.object,slots:PropTypes.object,sx:PropTypes.oneOfType([PropTypes.arrayOf(PropTypes.oneOfType([PropTypes.func,PropTypes.object,PropTypes.bool])),PropTypes.func,PropTypes.object]),timezone:PropTypes.string,value:PropTypes.any,view:PropTypes.oneOf(["day","month","year"]),viewRenderers:PropTypes.shape({day:PropTypes.func,month:PropTypes.func,year:PropTypes.func}),views:PropTypes.arrayOf(PropTypes.oneOf(["day","month","year"]).isRequired),yearsPerRow:PropTypes.oneOf([3,4])};const _excluded$2=["props","getOpenDialogAriaText"],useMobilePicker=a=>{var i,o,s;let{props:$,getOpenDialogAriaText:j}=a,_e=_objectWithoutPropertiesLoose(a,_excluded$2);const{slots:et,slotProps:tt,className:nt,sx:at,format:it,formatDensity:st,timezone:lt,name:ct,label:rt,inputRef:ut,readOnly:ot,disabled:dt,localeText:pt}=$,mt=useUtils(),ft=reactExports.useRef(null),ht=useId(),yt=(i=tt==null||(o=tt.toolbar)==null?void 0:o.hidden)!=null?i:!1,{open:bt,actions:gt,layoutProps:xt,renderCurrentView:vt,fieldProps:Lt}=usePicker(_extends({},_e,{props:$,inputRef:ft,autoFocusView:!0,additionalViewProps:{},wrapperVariant:"mobile"})),$t=et.field,Tt=useSlotProps({elementType:$t,externalSlotProps:tt==null?void 0:tt.field,additionalProps:_extends({},Lt,yt&&{id:ht},!(dt||ot)&&{onClick:gt.onOpen,onKeyDown:onSpaceOrEnter(gt.onOpen)},{readOnly:ot??!0,disabled:dt,className:nt,sx:at,format:it,formatDensity:st,timezone:lt,label:rt,name:ct}),ownerState:$});Tt.inputProps=_extends({},Tt.inputProps,{"aria-label":j(Lt.value,mt)});const Et=_extends({textField:et.textField},Tt.slots),Dt=(s=et.layout)!=null?s:PickersLayout,It=useForkRef(ft,Tt.inputRef,ut);let Ct=ht;yt&&(rt?Ct=`${ht}-label`:Ct=void 0);const jt=_extends({},tt,{toolbar:_extends({},tt==null?void 0:tt.toolbar,{titleId:ht}),mobilePaper:_extends({"aria-labelledby":Ct},tt==null?void 0:tt.mobilePaper)});return{renderPicker:()=>jsxRuntimeExports.jsxs(LocalizationProvider,{localeText:pt,children:[jsxRuntimeExports.jsx($t,_extends({},Tt,{slots:Et,slotProps:jt,inputRef:It})),jsxRuntimeExports.jsx(PickersModalDialog,_extends({},gt,{open:bt,slots:et,slotProps:jt,children:jsxRuntimeExports.jsx(Dt,_extends({},xt,jt==null?void 0:jt.layout,{slots:et,slotProps:jt,children:vt()}))}))]})}},MobileDatePicker=reactExports.forwardRef(function a(i,o){var s,$,j;const _e=useLocaleText(),et=useUtils(),tt=useDatePickerDefaultizedProps(i,"MuiMobileDatePicker"),nt=_extends({day:renderDateViewCalendar,month:renderDateViewCalendar,year:renderDateViewCalendar},tt.viewRenderers),at=_extends({},tt,{viewRenderers:nt,format:resolveDateFormat(et,tt,!1),slots:_extends({field:DateField},tt.slots),slotProps:_extends({},tt.slotProps,{field:st=>{var lt;return _extends({},resolveComponentProps((lt=tt.slotProps)==null?void 0:lt.field,st),extractValidationProps(tt),{ref:o})},toolbar:_extends({hidden:!1},(s=tt.slotProps)==null?void 0:s.toolbar)})}),{renderPicker:it}=useMobilePicker({props:at,valueManager:singleItemValueManager,valueType:"date",getOpenDialogAriaText:($=(j=at.localeText)==null?void 0:j.openDatePickerDialogue)!=null?$:_e.openDatePickerDialogue,validator:validateDate});return it()});MobileDatePicker.propTypes={autoFocus:PropTypes.bool,className:PropTypes.string,closeOnSelect:PropTypes.bool,components:PropTypes.object,componentsProps:PropTypes.object,dayOfWeekFormatter:PropTypes.func,defaultCalendarMonth:PropTypes.any,defaultValue:PropTypes.any,disabled:PropTypes.bool,disableFuture:PropTypes.bool,disableHighlightToday:PropTypes.bool,disableOpenPicker:PropTypes.bool,disablePast:PropTypes.bool,displayWeekNumber:PropTypes.bool,fixedWeekNumber:PropTypes.number,format:PropTypes.string,formatDensity:PropTypes.oneOf(["dense","spacious"]),inputRef:refType$1,label:PropTypes.node,loading:PropTypes.bool,localeText:PropTypes.object,maxDate:PropTypes.any,minDate:PropTypes.any,monthsPerRow:PropTypes.oneOf([3,4]),name:PropTypes.string,onAccept:PropTypes.func,onChange:PropTypes.func,onClose:PropTypes.func,onError:PropTypes.func,onMonthChange:PropTypes.func,onOpen:PropTypes.func,onSelectedSectionsChange:PropTypes.func,onViewChange:PropTypes.func,onYearChange:PropTypes.func,open:PropTypes.bool,openTo:PropTypes.oneOf(["day","month","year"]),orientation:PropTypes.oneOf(["landscape","portrait"]),readOnly:PropTypes.bool,reduceAnimations:PropTypes.bool,referenceDate:PropTypes.any,renderLoading:PropTypes.func,selectedSections:PropTypes.oneOfType([PropTypes.oneOf(["all","day","hours","meridiem","minutes","month","seconds","weekDay","year"]),PropTypes.number,PropTypes.shape({endIndex:PropTypes.number.isRequired,startIndex:PropTypes.number.isRequired})]),shouldDisableDate:PropTypes.func,shouldDisableMonth:PropTypes.func,shouldDisableYear:PropTypes.func,showDaysOutsideCurrentMonth:PropTypes.bool,slotProps:PropTypes.object,slots:PropTypes.object,sx:PropTypes.oneOfType([PropTypes.arrayOf(PropTypes.oneOfType([PropTypes.func,PropTypes.object,PropTypes.bool])),PropTypes.func,PropTypes.object]),timezone:PropTypes.string,value:PropTypes.any,view:PropTypes.oneOf(["day","month","year"]),viewRenderers:PropTypes.shape({day:PropTypes.func,month:PropTypes.func,year:PropTypes.func}),views:PropTypes.arrayOf(PropTypes.oneOf(["day","month","year"]).isRequired),yearsPerRow:PropTypes.oneOf([3,4])};const _excluded$1=["desktopModeMediaQuery"],DatePicker=reactExports.forwardRef(function a(i,o){const s=useThemeProps$6({props:i,name:"MuiDatePicker"}),{desktopModeMediaQuery:$=DEFAULT_DESKTOP_MODE_MEDIA_QUERY}=s,j=_objectWithoutPropertiesLoose(s,_excluded$1);return useMediaQuery($,{defaultMatches:!0})?jsxRuntimeExports.jsx(DesktopDatePicker,_extends({ref:o},j)):jsxRuntimeExports.jsx(MobileDatePicker,_extends({ref:o},j))});var weekOfYear$1={exports:{}};(function(a,i){(function(o,s){a.exports=s()})(commonjsGlobal,function(){var o="week",s="year";return function($,j,_e){var et=j.prototype;et.week=function(tt){if(tt===void 0&&(tt=null),tt!==null)return this.add(7*(tt-this.week()),"day");var nt=this.$locale().yearStart||1;if(this.month()===11&&this.date()>25){var at=_e(this).startOf(s).add(1,s).date(nt),it=_e(this).endOf(o);if(at.isBefore(it))return 1}var st=_e(this).startOf(s).date(nt).startOf(o).subtract(1,"millisecond"),lt=this.diff(st,o,!0);return lt<0?_e(this).startOf("week").week():Math.ceil(lt)},et.weeks=function(tt){return tt===void 0&&(tt=null),this.week(tt)}}})})(weekOfYear$1);var weekOfYearExports=weekOfYear$1.exports;const weekOfYear=getDefaultExportFromCjs(weekOfYearExports);var customParseFormat={exports:{}};(function(a,i){(function(o,s){a.exports=s()})(commonjsGlobal,function(){var o={LTS:"h:mm:ss A",LT:"h:mm A",L:"MM/DD/YYYY",LL:"MMMM D, YYYY",LLL:"MMMM D, YYYY h:mm A",LLLL:"dddd, MMMM D, YYYY h:mm A"},s=/(\[[^[]*\])|([-_:/.,()\s]+)|(A|a|YYYY|YY?|MM?M?M?|Do|DD?|hh?|HH?|mm?|ss?|S{1,3}|z|ZZ?)/g,$=/\d\d/,j=/\d\d?/,_e=/\d*[^-_:/,()\s\d]+/,et={},tt=function(rt){return(rt=+rt)+(rt>68?1900:2e3)},nt=function(rt){return function(ut){this[rt]=+ut}},at=[/[+-]\d\d:?(\d\d)?|Z/,function(rt){(this.zone||(this.zone={})).offset=function(ut){if(!ut||ut==="Z")return 0;var ot=ut.match(/([+-]|\d\d)/g),dt=60*ot[1]+(+ot[2]||0);return dt===0?0:ot[0]==="+"?-dt:dt}(rt)}],it=function(rt){var ut=et[rt];return ut&&(ut.indexOf?ut:ut.s.concat(ut.f))},st=function(rt,ut){var ot,dt=et.meridiem;if(dt){for(var pt=1;pt<=24;pt+=1)if(rt.indexOf(dt(pt,0,ut))>-1){ot=pt>12;break}}else ot=rt===(ut?"pm":"PM");return ot},lt={A:[_e,function(rt){this.afternoon=st(rt,!1)}],a:[_e,function(rt){this.afternoon=st(rt,!0)}],S:[/\d/,function(rt){this.milliseconds=100*+rt}],SS:[$,function(rt){this.milliseconds=10*+rt}],SSS:[/\d{3}/,function(rt){this.milliseconds=+rt}],s:[j,nt("seconds")],ss:[j,nt("seconds")],m:[j,nt("minutes")],mm:[j,nt("minutes")],H:[j,nt("hours")],h:[j,nt("hours")],HH:[j,nt("hours")],hh:[j,nt("hours")],D:[j,nt("day")],DD:[$,nt("day")],Do:[_e,function(rt){var ut=et.ordinal,ot=rt.match(/\d+/);if(this.day=ot[0],ut)for(var dt=1;dt<=31;dt+=1)ut(dt).replace(/\[|\]/g,"")===rt&&(this.day=dt)}],M:[j,nt("month")],MM:[$,nt("month")],MMM:[_e,function(rt){var ut=it("months"),ot=(it("monthsShort")||ut.map(function(dt){return dt.slice(0,3)})).indexOf(rt)+1;if(ot<1)throw new Error;this.month=ot%12||ot}],MMMM:[_e,function(rt){var ut=it("months").indexOf(rt)+1;if(ut<1)throw new Error;this.month=ut%12||ut}],Y:[/[+-]?\d+/,nt("year")],YY:[$,function(rt){this.year=tt(rt)}],YYYY:[/\d{4}/,nt("year")],Z:at,ZZ:at};function ct(rt){var ut,ot;ut=rt,ot=et&&et.formats;for(var dt=(rt=ut.replace(/(\[[^\]]+])|(LTS?|l{1,4}|L{1,4})/g,function(gt,xt,vt){var Lt=vt&&vt.toUpperCase();return xt||ot[vt]||o[vt]||ot[Lt].replace(/(\[[^\]]+])|(MMMM|MM|DD|dddd)/g,function($t,Tt,Et){return Tt||Et.slice(1)})})).match(s),pt=dt.length,mt=0;mt<pt;mt+=1){var ft=dt[mt],ht=lt[ft],yt=ht&&ht[0],bt=ht&&ht[1];dt[mt]=bt?{regex:yt,parser:bt}:ft.replace(/^\[|\]$/g,"")}return function(gt){for(var xt={},vt=0,Lt=0;vt<pt;vt+=1){var $t=dt[vt];if(typeof $t=="string")Lt+=$t.length;else{var Tt=$t.regex,Et=$t.parser,Dt=gt.slice(Lt),It=Tt.exec(Dt)[0];Et.call(xt,It),gt=gt.replace(It,"")}}return function(Ct){var jt=Ct.afternoon;if(jt!==void 0){var Zt=Ct.hours;jt?Zt<12&&(Ct.hours+=12):Zt===12&&(Ct.hours=0),delete Ct.afternoon}}(xt),xt}}return function(rt,ut,ot){ot.p.customParseFormat=!0,rt&&rt.parseTwoDigitYear&&(tt=rt.parseTwoDigitYear);var dt=ut.prototype,pt=dt.parse;dt.parse=function(mt){var ft=mt.date,ht=mt.utc,yt=mt.args;this.$u=ht;var bt=yt[1];if(typeof bt=="string"){var gt=yt[2]===!0,xt=yt[3]===!0,vt=gt||xt,Lt=yt[2];xt&&(Lt=yt[2]),et=this.$locale(),!gt&&Lt&&(et=ot.Ls[Lt]),this.$d=function(Dt,It,Ct){try{if(["x","X"].indexOf(It)>-1)return new Date((It==="X"?1e3:1)*Dt);var jt=ct(It)(Dt),Zt=jt.year,Xt=jt.month,sn=jt.day,Ft=jt.hours,wt=jt.minutes,kt=jt.seconds,At=jt.milliseconds,Pt=jt.zone,Mt=new Date,Ot=sn||(Zt||Xt?1:Mt.getDate()),Bt=Zt||Mt.getFullYear(),zt=0;Zt&&!Xt||(zt=Xt>0?Xt-1:Mt.getMonth());var Gt=Ft||0,Wt=wt||0,qt=kt||0,tn=At||0;return Pt?new Date(Date.UTC(Bt,zt,Ot,Gt,Wt,qt,tn+60*Pt.offset*1e3)):Ct?new Date(Date.UTC(Bt,zt,Ot,Gt,Wt,qt,tn)):new Date(Bt,zt,Ot,Gt,Wt,qt,tn)}catch{return new Date("")}}(ft,bt,ht),this.init(),Lt&&Lt!==!0&&(this.$L=this.locale(Lt).$L),vt&&ft!=this.format(bt)&&(this.$d=new Date("")),et={}}else if(bt instanceof Array)for(var $t=bt.length,Tt=1;Tt<=$t;Tt+=1){yt[1]=bt[Tt-1];var Et=ot.apply(this,yt);if(Et.isValid()){this.$d=Et.$d,this.$L=Et.$L,this.init();break}Tt===$t&&(this.$d=new Date(""))}else pt.call(this,mt)}}})})(customParseFormat);var customParseFormatExports=customParseFormat.exports;const customParseFormatPlugin=getDefaultExportFromCjs(customParseFormatExports);var localizedFormat={exports:{}};(function(a,i){(function(o,s){a.exports=s()})(commonjsGlobal,function(){var o={LTS:"h:mm:ss A",LT:"h:mm A",L:"MM/DD/YYYY",LL:"MMMM D, YYYY",LLL:"MMMM D, YYYY h:mm A",LLLL:"dddd, MMMM D, YYYY h:mm A"};return function(s,$,j){var _e=$.prototype,et=_e.format;j.en.formats=o,_e.format=function(tt){tt===void 0&&(tt="YYYY-MM-DDTHH:mm:ssZ");var nt=this.$locale().formats,at=function(it,st){return it.replace(/(\[[^\]]+])|(LTS?|l{1,4}|L{1,4})/g,function(lt,ct,rt){var ut=rt&&rt.toUpperCase();return ct||st[rt]||o[rt]||st[ut].replace(/(\[[^\]]+])|(MMMM|MM|DD|dddd)/g,function(ot,dt,pt){return dt||pt.slice(1)})})}(tt,nt===void 0?{}:nt);return et.call(this,at)}}})})(localizedFormat);var localizedFormatExports=localizedFormat.exports;const localizedFormatPlugin=getDefaultExportFromCjs(localizedFormatExports);var isBetween={exports:{}};(function(a,i){(function(o,s){a.exports=s()})(commonjsGlobal,function(){return function(o,s,$){s.prototype.isBetween=function(j,_e,et,tt){var nt=$(j),at=$(_e),it=(tt=tt||"()")[0]==="(",st=tt[1]===")";return(it?this.isAfter(nt,et):!this.isBefore(nt,et))&&(st?this.isBefore(at,et):!this.isAfter(at,et))||(it?this.isBefore(nt,et):!this.isAfter(nt,et))&&(st?this.isAfter(at,et):!this.isBefore(at,et))}}})})(isBetween);var isBetweenExports=isBetween.exports;const isBetweenPlugin=getDefaultExportFromCjs(isBetweenExports);dayjs.extend(customParseFormatPlugin);dayjs.extend(localizedFormatPlugin);dayjs.extend(isBetweenPlugin);const localeNotFoundWarning=buildWarning(["Your locale has not been found.","Either the locale key is not a supported one. Locales supported by dayjs are available here: https://github.com/iamkun/dayjs/tree/dev/src/locale","Or you forget to import the locale from 'dayjs/locale/{localeUsed}'","fallback on English locale"]),formatTokenMap={YY:"year",YYYY:{sectionType:"year",contentType:"digit",maxLength:4},M:{sectionType:"month",contentType:"digit",maxLength:2},MM:"month",MMM:{sectionType:"month",contentType:"letter"},MMMM:{sectionType:"month",contentType:"letter"},D:{sectionType:"day",contentType:"digit",maxLength:2},DD:"day",Do:{sectionType:"day",contentType:"digit-with-letter"},d:{sectionType:"weekDay",contentType:"digit",maxLength:2},dd:{sectionType:"weekDay",contentType:"letter"},ddd:{sectionType:"weekDay",contentType:"letter"},dddd:{sectionType:"weekDay",contentType:"letter"},A:"meridiem",a:"meridiem",H:{sectionType:"hours",contentType:"digit",maxLength:2},HH:"hours",h:{sectionType:"hours",contentType:"digit",maxLength:2},hh:"hours",m:{sectionType:"minutes",contentType:"digit",maxLength:2},mm:"minutes",s:{sectionType:"seconds",contentType:"digit",maxLength:2},ss:"seconds"},defaultFormats={year:"YYYY",month:"MMMM",monthShort:"MMM",dayOfMonth:"D",weekday:"dddd",weekdayShort:"dd",hours24h:"HH",hours12h:"hh",meridiem:"A",minutes:"mm",seconds:"ss",fullDate:"ll",fullDateWithWeekday:"dddd, LL",keyboardDate:"L",shortDate:"MMM D",normalDate:"D MMMM",normalDateWithWeekday:"ddd, MMM D",monthAndYear:"MMMM YYYY",monthAndDate:"MMMM D",fullTime:"LT",fullTime12h:"hh:mm A",fullTime24h:"HH:mm",fullDateTime:"lll",fullDateTime12h:"ll hh:mm A",fullDateTime24h:"ll HH:mm",keyboardDateTime:"L LT",keyboardDateTime12h:"L hh:mm A",keyboardDateTime24h:"L HH:mm"},MISSING_UTC_PLUGIN=["Missing UTC plugin","To be able to use UTC or timezones, you have to enable the `utc` plugin","Find more information on https://mui.com/x/react-date-pickers/timezone/#day-js-and-utc"].join(`
`),MISSING_TIMEZONE_PLUGIN=["Missing timezone plugin","To be able to use timezones, you have to enable both the `utc` and the `timezone` plugin","Find more information on https://mui.com/x/react-date-pickers/timezone/#day-js-and-timezone"].join(`
`),withLocale=(a,i)=>i?(...o)=>a(...o).locale(i):a;class AdapterDayjs{constructor({locale:i,formats:o,instance:s}={}){var $;this.isMUIAdapter=!0,this.isTimezoneCompatible=!0,this.lib="dayjs",this.rawDayJsInstance=void 0,this.dayjs=void 0,this.locale=void 0,this.formats=void 0,this.escapedCharacters={start:"[",end:"]"},this.formatTokenMap=formatTokenMap,this.setLocaleToValue=j=>{const _e=this.getCurrentLocaleCode();return _e===j.locale()?j:j.locale(_e)},this.hasUTCPlugin=()=>typeof dayjs.utc<"u",this.hasTimezonePlugin=()=>typeof dayjs.tz<"u",this.isSame=(j,_e,et)=>{const tt=this.setTimezone(_e,this.getTimezone(j));return j.format(et)===tt.format(et)},this.cleanTimezone=j=>{switch(j){case"default":return;case"system":return dayjs.tz.guess();default:return j}},this.createSystemDate=j=>{if(this.rawDayJsInstance)return this.rawDayJsInstance(j);if(this.hasUTCPlugin()&&this.hasTimezonePlugin()){const _e=dayjs.tz.guess();return _e!=="UTC"?dayjs.tz(j,_e):dayjs(j)}return dayjs(j)},this.createUTCDate=j=>{if(!this.hasUTCPlugin())throw new Error(MISSING_UTC_PLUGIN);return dayjs.utc(j)},this.createTZDate=(j,_e)=>{if(!this.hasUTCPlugin())throw new Error(MISSING_UTC_PLUGIN);if(!this.hasTimezonePlugin())throw new Error(MISSING_TIMEZONE_PLUGIN);const et=j!==void 0&&!j.endsWith("Z");return dayjs(j).tz(this.cleanTimezone(_e),et)},this.getLocaleFormats=()=>{const j=dayjs.Ls,_e=this.locale||"en";let et=j[_e];return et===void 0&&(localeNotFoundWarning(),et=j.en),et.formats},this.adjustOffset=j=>{if(!this.hasTimezonePlugin())return j;const _e=this.getTimezone(j);if(_e!=="UTC"){var et,tt;const nt=j.tz(this.cleanTimezone(_e),!0);return((et=nt.$offset)!=null?et:0)===((tt=j.$offset)!=null?tt:0)?j:nt}return j},this.date=j=>j===null?null:this.dayjs(j),this.dateWithTimezone=(j,_e)=>{if(j===null)return null;let et;return _e==="UTC"?et=this.createUTCDate(j):_e==="system"||_e==="default"&&!this.hasTimezonePlugin()?et=this.createSystemDate(j):et=this.createTZDate(j,_e),this.locale===void 0?et:et.locale(this.locale)},this.getTimezone=j=>{if(this.hasTimezonePlugin()){var _e;const et=(_e=j.$x)==null?void 0:_e.$timezone;if(et)return et}return this.hasUTCPlugin()&&j.isUTC()?"UTC":"system"},this.setTimezone=(j,_e)=>{if(this.getTimezone(j)===_e)return j;if(_e==="UTC"){if(!this.hasUTCPlugin())throw new Error(MISSING_UTC_PLUGIN);return j.utc()}if(_e==="system")return j.local();if(!this.hasTimezonePlugin()){if(_e==="default")return j;throw new Error(MISSING_TIMEZONE_PLUGIN)}return dayjs.tz(j,this.cleanTimezone(_e))},this.toJsDate=j=>j.toDate(),this.parseISO=j=>this.dayjs(j),this.toISO=j=>j.toISOString(),this.parse=(j,_e)=>j===""?null:this.dayjs(j,_e,this.locale,!0),this.getCurrentLocaleCode=()=>this.locale||"en",this.is12HourCycleInCurrentLocale=()=>/A|a/.test(this.getLocaleFormats().LT||""),this.expandFormat=j=>{const _e=this.getLocaleFormats(),et=tt=>tt.replace(/(\[[^\]]+])|(MMMM|MM|DD|dddd)/g,(nt,at,it)=>at||it.slice(1));return j.replace(/(\[[^\]]+])|(LTS?|l{1,4}|L{1,4})/g,(tt,nt,at)=>{const it=at&&at.toUpperCase();return nt||_e[at]||et(_e[it])})},this.getFormatHelperText=j=>this.expandFormat(j).replace(/a/gi,"(a|p)m").toLocaleLowerCase(),this.isNull=j=>j===null,this.isValid=j=>this.dayjs(j).isValid(),this.format=(j,_e)=>this.formatByString(j,this.formats[_e]),this.formatByString=(j,_e)=>this.dayjs(j).format(_e),this.formatNumber=j=>j,this.getDiff=(j,_e,et)=>j.diff(_e,et),this.isEqual=(j,_e)=>j===null&&_e===null?!0:this.dayjs(j).toDate().getTime()===this.dayjs(_e).toDate().getTime(),this.isSameYear=(j,_e)=>this.isSame(j,_e,"YYYY"),this.isSameMonth=(j,_e)=>this.isSame(j,_e,"YYYY-MM"),this.isSameDay=(j,_e)=>this.isSame(j,_e,"YYYY-MM-DD"),this.isSameHour=(j,_e)=>j.isSame(_e,"hour"),this.isAfter=(j,_e)=>j>_e,this.isAfterYear=(j,_e)=>this.hasUTCPlugin()?!this.isSameYear(j,_e)&&j.utc()>_e.utc():j.isAfter(_e,"year"),this.isAfterDay=(j,_e)=>this.hasUTCPlugin()?!this.isSameDay(j,_e)&&j.utc()>_e.utc():j.isAfter(_e,"day"),this.isBefore=(j,_e)=>j<_e,this.isBeforeYear=(j,_e)=>this.hasUTCPlugin()?!this.isSameYear(j,_e)&&j.utc()<_e.utc():j.isBefore(_e,"year"),this.isBeforeDay=(j,_e)=>this.hasUTCPlugin()?!this.isSameDay(j,_e)&&j.utc()<_e.utc():j.isBefore(_e,"day"),this.isWithinRange=(j,[_e,et])=>j>=_e&&j<=et,this.startOfYear=j=>this.adjustOffset(j.startOf("year")),this.startOfMonth=j=>this.adjustOffset(j.startOf("month")),this.startOfWeek=j=>this.adjustOffset(j.startOf("week")),this.startOfDay=j=>this.adjustOffset(j.startOf("day")),this.endOfYear=j=>this.adjustOffset(j.endOf("year")),this.endOfMonth=j=>this.adjustOffset(j.endOf("month")),this.endOfWeek=j=>this.adjustOffset(j.endOf("week")),this.endOfDay=j=>this.adjustOffset(j.endOf("day")),this.addYears=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"year"):j.add(_e,"year")),this.addMonths=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"month"):j.add(_e,"month")),this.addWeeks=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"week"):j.add(_e,"week")),this.addDays=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"day"):j.add(_e,"day")),this.addHours=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"hour"):j.add(_e,"hour")),this.addMinutes=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"minute"):j.add(_e,"minute")),this.addSeconds=(j,_e)=>this.adjustOffset(_e<0?j.subtract(Math.abs(_e),"second"):j.add(_e,"second")),this.getYear=j=>j.year(),this.getMonth=j=>j.month(),this.getDate=j=>j.date(),this.getHours=j=>j.hour(),this.getMinutes=j=>j.minute(),this.getSeconds=j=>j.second(),this.getMilliseconds=j=>j.millisecond(),this.setYear=(j,_e)=>this.adjustOffset(j.set("year",_e)),this.setMonth=(j,_e)=>this.adjustOffset(j.set("month",_e)),this.setDate=(j,_e)=>this.adjustOffset(j.set("date",_e)),this.setHours=(j,_e)=>this.adjustOffset(j.set("hour",_e)),this.setMinutes=(j,_e)=>this.adjustOffset(j.set("minute",_e)),this.setSeconds=(j,_e)=>this.adjustOffset(j.set("second",_e)),this.setMilliseconds=(j,_e)=>this.adjustOffset(j.set("millisecond",_e)),this.getDaysInMonth=j=>j.daysInMonth(),this.getNextMonth=j=>this.addMonths(j,1),this.getPreviousMonth=j=>this.addMonths(j,-1),this.getMonthArray=j=>{const et=[j.startOf("year")];for(;et.length<12;){const tt=et[et.length-1];et.push(this.addMonths(tt,1))}return et},this.mergeDateAndTime=(j,_e)=>j.hour(_e.hour()).minute(_e.minute()).second(_e.second()),this.getWeekdays=()=>{const j=this.dayjs().startOf("week");return[0,1,2,3,4,5,6].map(_e=>this.formatByString(this.addDays(j,_e),"dd"))},this.getWeekArray=j=>{const _e=this.setLocaleToValue(j),et=_e.startOf("month").startOf("week"),tt=_e.endOf("month").endOf("week");let nt=0,at=et;const it=[];for(;at<tt;){const st=Math.floor(nt/7);it[st]=it[st]||[],it[st].push(at),at=this.addDays(at,1),nt+=1}return it},this.getWeekNumber=j=>j.week(),this.getYearRange=(j,_e)=>{const et=j.startOf("year"),tt=_e.endOf("year"),nt=[];let at=et;for(;at<tt;)nt.push(at),at=this.addYears(at,1);return nt},this.getMeridiemText=j=>j==="am"?"AM":"PM",this.rawDayJsInstance=s,this.dayjs=withLocale(($=this.rawDayJsInstance)!=null?$:dayjs,i),this.locale=i,this.formats=_extends({},defaultFormats,o),dayjs.extend(weekOfYear)}}const DateRangeControl$1=()=>{const a=new URLSearchParams(location.search),i=a.get("startDate"),o=a.get("endDate"),[s,$]=useAtom(dateStartAtom$1),[j,_e]=useAtom(dateEndAtom$1),et=useSetAtom(featureDisabledAlertAtom);return reactExports.useEffect(()=>{i&&o&&($(dayjs(i)),_e(dayjs(o)),et())},[i,o]),jsxRuntimeExports.jsx(FormControl$1,{sx:{display:"flex",flexDirection:"row",justifyContent:"space-between",flexShrink:0},children:jsxRuntimeExports.jsxs(LocalizationProvider,{dateAdapter:AdapterDayjs,children:[jsxRuntimeExports.jsx(DatePicker,{sx:{maxWidth:"12rem"},label:jsxRuntimeExports.jsx("span",{style:{color:"#9e9e9e"},children:"After Date"}),value:s,disableHighlightToday:!0,disableFuture:!0,maxDate:j||null,onChange:tt=>$(tt)}),jsxRuntimeExports.jsx(DatePicker,{label:jsxRuntimeExports.jsx("span",{style:{color:"#9e9e9e"},children:"Before Date"}),sx:{marginLeft:4,maxWidth:"12rem"},value:j,disableFuture:!0,minDate:s||null,onChange:tt=>_e(tt)})]})})},SearchPage=()=>jsxRuntimeExports.jsxs(PageLayout,{padding:3,children:[jsxRuntimeExports.jsx(Box$1,{display:"flex",alignItems:"flex-start",justifyContent:"space-between",flexDirection:"row",gap:2,style:{margin:"0 15em 1em 15em"},children:jsxRuntimeExports.jsx(QueryControl,{})}),jsxRuntimeExports.jsxs(Accordion$1,{defaultExpanded:!0,sx:{my:3},children:[jsxRuntimeExports.jsx(AccordionSummary$1,{expandIcon:jsxRuntimeExports.jsx(default_1$i,{}),style:{backgroundColor:"rgb(30 32 34)"},children:jsxRuntimeExports.jsx(Typography$1,{children:"Additional Fields"})}),jsxRuntimeExports.jsx(AccordionDetails$1,{children:jsxRuntimeExports.jsxs(Box$1,{sx:{display:"flex",justifyContent:"center",padding:"1rem 2rem"},children:[jsxRuntimeExports.jsx("div",{style:{width:"40%"},className:"flex h-full justify-center self-center",children:jsxRuntimeExports.jsx(DateRangeControl$1,{})}),jsxRuntimeExports.jsx("div",{style:{width:"10%",height:"5rem"},className:"flex h-full justify-center",children:jsxRuntimeExports.jsx(Divider$1,{orientation:"vertical",flexItem:!0})}),jsxRuntimeExports.jsx("div",{style:{width:"40%"},className:"flex justify-center ",children:jsxRuntimeExports.jsx(BasicCriteriaControl,{})})]})})]}),jsxRuntimeExports.jsx(RenderByState$1,{})]}),RenderByState$1=()=>{const[a]=useAtom(searchStateAtom);switch(a){case"pending":return jsxRuntimeExports.jsx(PageMessage,{message:"Define search criteria for papers"});case"loading":return jsxRuntimeExports.jsx(Results,{isLoading:!0});case"complete":return jsxRuntimeExports.jsx(Results,{});case"empty":return jsxRuntimeExports.jsx(PageMessage,{message:"No papers found"});case"error":return jsxRuntimeExports.jsx(PageMessage,{message:"Error occurred while fetching papers"})}},Results=({isLoading:a=!1})=>{const[i,o]=useAtom(tabValueAtom),s=(_e,et)=>{o(et)},[$]=useAtom(resultListAtom),j=useSetAtom(updatePaperInListAtom);return reactExports.useEffect(()=>{const _e=et=>{const{id:tt,changes:nt}=et.detail,{field:at,value:it}=nt;j({papersListAtom:resultListAtom,id:tt,field:at,newValue:it})};return window.addEventListener("paperUpdate",_e),()=>{window.removeEventListener("paperUpdate",_e)}},[]),jsxRuntimeExports.jsxs(Box$1,{children:[jsxRuntimeExports.jsx("div",{className:"flex justify-center",style:{marginBottom:-34,marginTop:2},children:jsxRuntimeExports.jsxs(ToggleButtonGroup$1,{color:"secondary",value:i,exclusive:!0,onChange:s,"aria-label":"Data view",sx:{alignSelf:"center"},children:[jsxRuntimeExports.jsx(ToggleButton$1,{value:"table",children:jsxRuntimeExports.jsx(default_1$k,{})}),jsxRuntimeExports.jsx(ToggleButton$1,{value:"grid",children:jsxRuntimeExports.jsx(default_1$j,{})})]})}),jsxRuntimeExports.jsxs(Box$1,{children:[i==="table"&&jsxRuntimeExports.jsx(PapersTable,{papers:$,isLoading:a,placeholderRows:10}),i==="grid"&&jsxRuntimeExports.jsx(Box$1,{sx:{marginTop:6},children:jsxRuntimeExports.jsx(ThumbPapersGrid,{papers:$,isLoading:a,placeholderRows:4})})]})]})};var ArrowBackIos={},_interopRequireDefault$8=interopRequireDefaultExports;Object.defineProperty(ArrowBackIos,"__esModule",{value:!0});var default_1$8=ArrowBackIos.default=void 0,_createSvgIcon$8=_interopRequireDefault$8(requireCreateSvgIcon()),_jsxRuntime$8=jsxRuntimeExports;default_1$8=ArrowBackIos.default=(0,_createSvgIcon$8.default)((0,_jsxRuntime$8.jsx)("path",{d:"M11.67 3.87 9.9 2.1 0 12l9.9 9.9 1.77-1.77L3.54 12z"}),"ArrowBackIos");var ArrowForwardIos={},_interopRequireDefault$7=interopRequireDefaultExports;Object.defineProperty(ArrowForwardIos,"__esModule",{value:!0});var default_1$7=ArrowForwardIos.default=void 0,_createSvgIcon$7=_interopRequireDefault$7(requireCreateSvgIcon()),_jsxRuntime$7=jsxRuntimeExports;default_1$7=ArrowForwardIos.default=(0,_createSvgIcon$7.default)((0,_jsxRuntime$7.jsx)("path",{d:"M6.23 20.23 8 22l10-10L8 2 6.23 3.77 14.46 12z"}),"ArrowForwardIos");function getLoadingButtonUtilityClass(a){return generateUtilityClass$1("MuiLoadingButton",a)}const loadingButtonClasses=generateUtilityClasses$1("MuiLoadingButton",["root","loading","loadingIndicator","loadingIndicatorCenter","loadingIndicatorStart","loadingIndicatorEnd","endIconLoadingEnd","startIconLoadingStart"]),loadingButtonClasses$1=loadingButtonClasses,_excluded=["children","disabled","id","loading","loadingIndicator","loadingPosition","variant"],useUtilityClasses=a=>{const{loading:i,loadingPosition:o,classes:s}=a,$={root:["root",i&&"loading"],startIcon:[i&&`startIconLoading${capitalize$2(o)}`],endIcon:[i&&`endIconLoading${capitalize$2(o)}`],loadingIndicator:["loadingIndicator",i&&`loadingIndicator${capitalize$2(o)}`]},j=composeClasses($,getLoadingButtonUtilityClass,s);return _extends({},s,j)},rootShouldForwardProp=a=>a!=="ownerState"&&a!=="theme"&&a!=="sx"&&a!=="as"&&a!=="classes",LoadingButtonRoot=styled(Button$1,{shouldForwardProp:a=>rootShouldForwardProp(a)||a==="classes",name:"MuiLoadingButton",slot:"Root",overridesResolver:(a,i)=>[i.root,i.startIconLoadingStart&&{[`& .${loadingButtonClasses$1.startIconLoadingStart}`]:i.startIconLoadingStart},i.endIconLoadingEnd&&{[`& .${loadingButtonClasses$1.endIconLoadingEnd}`]:i.endIconLoadingEnd}]})(({ownerState:a,theme:i})=>_extends({[`& .${loadingButtonClasses$1.startIconLoadingStart}, & .${loadingButtonClasses$1.endIconLoadingEnd}`]:{transition:i.transitions.create(["opacity"],{duration:i.transitions.duration.short}),opacity:0}},a.loadingPosition==="center"&&{transition:i.transitions.create(["background-color","box-shadow","border-color"],{duration:i.transitions.duration.short}),[`&.${loadingButtonClasses$1.loading}`]:{color:"transparent"}},a.loadingPosition==="start"&&a.fullWidth&&{[`& .${loadingButtonClasses$1.startIconLoadingStart}, & .${loadingButtonClasses$1.endIconLoadingEnd}`]:{transition:i.transitions.create(["opacity"],{duration:i.transitions.duration.short}),opacity:0,marginRight:-8}},a.loadingPosition==="end"&&a.fullWidth&&{[`& .${loadingButtonClasses$1.startIconLoadingStart}, & .${loadingButtonClasses$1.endIconLoadingEnd}`]:{transition:i.transitions.create(["opacity"],{duration:i.transitions.duration.short}),opacity:0,marginLeft:-8}})),LoadingButtonLoadingIndicator=styled("span",{name:"MuiLoadingButton",slot:"LoadingIndicator",overridesResolver:(a,i)=>{const{ownerState:o}=a;return[i.loadingIndicator,i[`loadingIndicator${capitalize$2(o.loadingPosition)}`]]}})(({theme:a,ownerState:i})=>_extends({position:"absolute",visibility:"visible",display:"flex"},i.loadingPosition==="start"&&(i.variant==="outlined"||i.variant==="contained")&&{left:i.size==="small"?10:14},i.loadingPosition==="start"&&i.variant==="text"&&{left:6},i.loadingPosition==="center"&&{left:"50%",transform:"translate(-50%)",color:(a.vars||a).palette.action.disabled},i.loadingPosition==="end"&&(i.variant==="outlined"||i.variant==="contained")&&{right:i.size==="small"?10:14},i.loadingPosition==="end"&&i.variant==="text"&&{right:6},i.loadingPosition==="start"&&i.fullWidth&&{position:"relative",left:-10},i.loadingPosition==="end"&&i.fullWidth&&{position:"relative",right:-10})),LoadingButton=reactExports.forwardRef(function a(i,o){const s=reactExports.useContext(ButtonGroupContext$1),$=resolveProps(s,i),j=useThemeProps$6({props:$,name:"MuiLoadingButton"}),{children:_e,disabled:et=!1,id:tt,loading:nt=!1,loadingIndicator:at,loadingPosition:it="center",variant:st="text"}=j,lt=_objectWithoutPropertiesLoose(j,_excluded),ct=useId(tt),rt=at??jsxRuntimeExports.jsx(CircularProgress$1,{"aria-labelledby":ct,color:"inherit",size:16}),ut=_extends({},j,{disabled:et,loading:nt,loadingIndicator:rt,loadingPosition:it,variant:st}),ot=useUtilityClasses(ut),dt=nt?jsxRuntimeExports.jsx(LoadingButtonLoadingIndicator,{className:ot.loadingIndicator,ownerState:ut,children:rt}):null;return jsxRuntimeExports.jsxs(LoadingButtonRoot,_extends({disabled:et||nt,id:ct,ref:o},lt,{variant:st,classes:ot,ownerState:ut,children:[ut.loadingPosition==="end"?_e:dt,ut.loadingPosition==="end"?dt:_e]}))}),LoadingButton$1=LoadingButton;var Check={},_interopRequireDefault$6=interopRequireDefaultExports;Object.defineProperty(Check,"__esModule",{value:!0});var default_1$6=Check.default=void 0,_createSvgIcon$6=_interopRequireDefault$6(requireCreateSvgIcon()),_jsxRuntime$6=jsxRuntimeExports;default_1$6=Check.default=(0,_createSvgIcon$6.default)((0,_jsxRuntime$6.jsx)("path",{d:"M9 16.17 4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"}),"Check");var Star={},_interopRequireDefault$5=interopRequireDefaultExports;Object.defineProperty(Star,"__esModule",{value:!0});var default_1$5=Star.default=void 0,_createSvgIcon$5=_interopRequireDefault$5(requireCreateSvgIcon()),_jsxRuntime$5=jsxRuntimeExports;default_1$5=Star.default=(0,_createSvgIcon$5.default)((0,_jsxRuntime$5.jsx)("path",{d:"M12 17.27 18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"}),"Star");const secondaryColor=colors.palette.secondary.main,StepConnectorStyled=styled(StepConnector$1)(({theme:a})=>({[`&.${stepConnectorClasses$1.alternativeLabel}`]:{top:22},[`&.${stepConnectorClasses$1.active}`]:{[`& .${stepConnectorClasses$1.line}`]:{backgroundColor:secondaryColor}},[`&.${stepConnectorClasses$1.completed}`]:{[`& .${stepConnectorClasses$1.line}`]:{backgroundColor:secondaryColor}},[`& .${stepConnectorClasses$1.line}`]:{height:3,border:0,backgroundColor:secondaryColor,borderRadius:1}})),IconStyled=styled("div")(({theme:a,ownerState:i})=>({backgroundColor:colors.palette.background.paper,zIndex:1,color:"#fff",width:50,height:50,display:"flex",borderRadius:"50%",justifyContent:"center",alignItems:"center",...i.active&&{backgroundColor:secondaryColor,border:"2px solid rgba(255,255,255,0.25)",boxShadow:"0 4px 10px 0 rgba(0,0,0,.25)"},...i.completed&&{backgroundColor:secondaryColor}}));function IconWrapper(a){const{active:i,completed:o,className:s}=a,$={1:jsxRuntimeExports.jsx(default_1$5,{}),2:jsxRuntimeExports.jsx(default_1$6,{})};return jsxRuntimeExports.jsx(IconStyled,{ownerState:{completed:o,active:i},className:`${s} mb-3 p-0`,children:$[String(a.icon)]})}function OnboardingStepper({steps:a,activeStep:i,completed:o,handleStep:s}){return jsxRuntimeExports.jsx(Box$1,{sx:{width:"50%"},className:"flex justify-center self-center",children:jsxRuntimeExports.jsx(Stepper$1,{sx:{width:"70%"},nonLinear:!0,activeStep:i,connector:jsxRuntimeExports.jsx(StepConnectorStyled,{}),children:a.map(($,j)=>jsxRuntimeExports.jsx(Step$1,{completed:o[j],children:jsxRuntimeExports.jsx(StepLabel$1,{StepIconComponent:IconWrapper,onClick:s(j),sx:{display:"flex",flexDirection:"column",width:"5rem"},className:"StepLabel",children:$})},$))})})}const config={settings:{isNewUser:!1,autoScrapeNewDates:!0,apiKeyOpenAI:"",scrapeInterval:3},features:["video_generator"],seedReferencesIds:["2308.05481","2307.09909","2307.06159","2307.05082","2307.05300","2305.10601","2307.01933","2307.01577","2307.01548","2307.01403","2307.01204","2307.02276","2307.02046","2307.02295","2307.01928","2307.02477","2307.02485","1801.07243v5","2307.02390","2307.07255","2307.06917","2307.08962","2206.08853","2307.09364","2307.08859","2307.09721","2210.02441","2307.10680","2208.03299","2308.13916","2308.13724","2308.14296","2305.01157","2404.05966","2404.07439"]},onboardingStateAtom=atom("onboarding"),canGoNextAtom=atom(!0),inputIdsAtom=atom([]),autoScrapeDatesAtom=atom(config.settings.autoScrapeNewDates),apiKeyOpenAIAtom=atom(""),recommendButtonDisabledAtom=atom(!1);function ReferencesInput(){const[a,i]=useAtom(inputIdsAtom),[o,s]=reactExports.useState(""),[$,j]=useAtom(recommendButtonDisabledAtom),_e=useSetAtom(canGoNextAtom);reactExports.useEffect(()=>{a.length===0?(_e(!1),$&&j(!1)):_e(!0)},[a]);const et=()=>{const it=[...new Set([...a,...config.seedReferencesIds])];i(it),j(!0)},tt=(it,st,lt)=>{s(st)},nt=it=>{const st=it.split(/[\s,]+/).map(ct=>ct.trim()).filter(ct=>ct.length>0),lt=[...new Set([...a,...st])];i(lt)},at=(it,st,lt,ct)=>{if(lt==="blur"&&(nt(it.target.value),s("")),lt==="createOption"&&nt(ct.option),lt==="removeOption"){const rt=a.filter(ut=>ut!==ct.option);i(rt)}};return jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(Typography$1,{style:{color:"#a1a1a1",marginBottom:"2rem"},variant:"h3",children:"Let's get started"}),jsxRuntimeExports.jsx(Typography$1,{children:"When ranking new papers, we utilize a set of reference papers as benchmarks."}),jsxRuntimeExports.jsx(Typography$1,{children:"Please provide the IDs of arXiv papers that should be used as benchmarks for this ranking process."}),jsxRuntimeExports.jsxs("div",{style:{marginTop:"3rem",display:"flex",flexDirection:"column"},children:[jsxRuntimeExports.jsx(Autocomplete$1,{ChipProps:{sx:{bgcolor:"rgba(0,0,0,.25)"}},multiple:!0,freeSolo:!0,sx:{width:765},id:"seed-references",disableClearable:!0,options:[],limitTags:35,value:a,inputValue:o,onChange:at,onInputChange:tt,renderInput:it=>jsxRuntimeExports.jsx(TextField$1,{...it,InputLabelProps:{style:{color:"#9e9e9e"}},label:"Reference IDs",variant:"outlined",onBlur:st=>at(st,it.InputProps,"blur",{}),InputProps:{...it.InputProps,sx:{borderBottomLeftRadius:0,borderBottomRightRadius:0,minHeight:250},type:"text"}})}),jsxRuntimeExports.jsx(Button$1,{variant:"contained",color:"primary",onClick:et,disabled:$,sx:{borderTopLeftRadius:0,borderTopRightRadius:0},children:"Use recommended"})]})]})}function UserSettings(){const[a,i]=useAtom(autoScrapeDatesAtom),o=useSetAtom(apiKeyOpenAIAtom),[s,$]=reactExports.useState(!1),j=()=>{$(!s)};return jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(Typography$1,{style:{color:"#a1a1a1",marginBottom:"2rem"},variant:"h3",children:"User Settings"}),jsxRuntimeExports.jsx("div",{style:{marginTop:"3rem",display:"flex",flexDirection:"column"},children:jsxRuntimeExports.jsx(Box$1,{sx:{maxWidth:620,m:"auto"},children:jsxRuntimeExports.jsxs(Grid$1,{container:!0,spacing:2,alignItems:"center",children:[jsxRuntimeExports.jsx(Grid$1,{item:!0,xs:3,children:jsxRuntimeExports.jsx(Checkbox$1,{sx:{color:"#9e9e9e !important"},checked:a,onChange:_e=>i(_e.target.checked)})}),jsxRuntimeExports.jsxs(Grid$1,{item:!0,xs:9,children:[jsxRuntimeExports.jsx(Typography$1,{variant:"body1",children:"Automatically scrape new dates"}),jsxRuntimeExports.jsx(Typography$1,{variant:"body2",color:"textSecondary",children:"At noon each day, attempt to scrape and rank papers for that day. If no papers are found, retry every 3 hours until found. New dates will not appear in the calender until papers have been successfully scraped for that day."})]}),jsxRuntimeExports.jsx(Grid$1,{item:!0,xs:3,sx:{mt:4},children:jsxRuntimeExports.jsx(Typography$1,{children:"OpenAI API Key"})}),jsxRuntimeExports.jsx(Grid$1,{item:!0,xs:9,sx:{mt:6},children:jsxRuntimeExports.jsx(TextField$1,{type:s?"text":"password",variant:"outlined",fullWidth:!0,helperText:"Your key is not shared and can be managed in the chat settings",onChange:_e=>o(_e.target.value),InputProps:{endAdornment:jsxRuntimeExports.jsx(InputAdornment$1,{position:"end",children:jsxRuntimeExports.jsx(IconButton$1,{onClick:j,edge:"end",children:s?jsxRuntimeExports.jsx(VisibilityOff,{}):jsxRuntimeExports.jsx(Visibility,{})})})}})})]})})})]})}const steps=["References","Settings"],OnboardPage=()=>jsxRuntimeExports.jsx(PageLayout,{padding:3,children:jsxRuntimeExports.jsx(OnboardFlow,{})});function OnboardFlow(){const[a,i]=reactExports.useState(0),[o,s]=reactExports.useState({}),$=useSetAtom(onboardingStateAtom),j=useAtomValue(inputIdsAtom),_e=useAtomValue(autoScrapeDatesAtom),et=useSetAtom(setSidebarDataAtom),tt=useSetAtom(isNewUserAtom),nt=useAtomValue(apiKeyOpenAIAtom),at=useSetAtom(addAlertAtom),it=useNavigate();async function st(){const ot={inputIds:j,config:{autoScrapeNewDates:_e,apiKeyOpenAI:nt}};$("loading");try{const pt=(await onboard(ot)).data;pt.length&&et(pt),tt(!1),it("/backfill?isNewUser=true")}catch(dt){at({message:"Failed to complete onboarding due to a server error."}),$("onboarding"),console.error("Failed to backfill data",dt)}}const lt=()=>{i(a+1)},ct=()=>{i(ot=>ot-1)},rt=ot=>()=>{i(ot)},ut=()=>{const ot=o;ot[a]=!0,s(ot),a===steps.length-1?st():lt()};return jsxRuntimeExports.jsxs(Box$1,{sx:{width:"100%",display:"flex",flexDirection:"column",justifyContent:"center"},children:[jsxRuntimeExports.jsx(OnboardingStepper,{steps,activeStep:a,completed:o,handleStep:rt}),jsxRuntimeExports.jsx("div",{children:jsxRuntimeExports.jsxs(jsxRuntimeExports.Fragment,{children:[jsxRuntimeExports.jsx(Paper$1,{elevation:2,style:{backgroundColor:colors.palette.background.paper,paddingTop:"2rem",marginTop:"2rem",display:"flex",flexDirection:"column",alignItems:"center",height:"35rem",width:"70rem",overflow:"auto"},className:"px-12 mx-auto",children:jsxRuntimeExports.jsx(RenderByState,{activeStep:a})}),jsxRuntimeExports.jsx(NavigationButtons,{activeStep:a,steps,handleBack:ct,handleSkip:lt,handleNext:ut})]})})]})}const RenderByState=({activeStep:a})=>{switch(a){case 0:return jsxRuntimeExports.jsx(ReferencesInput,{});case 1:return jsxRuntimeExports.jsx(UserSettings,{})}return null};function NavigationButtons({activeStep:a,steps:i,handleBack:o,handleSkip:s,handleNext:$}){const j=useAtomValue(canGoNextAtom),_e=useAtomValue(onboardingStateAtom),et=a===i.length-1,tt=a===0;return jsxRuntimeExports.jsx(Box$1,{sx:{pt:3,display:"flex",justifyContent:"center",width:"100%"},children:jsxRuntimeExports.jsxs("div",{className:"flex justify-between",style:{width:"20rem"},children:[jsxRuntimeExports.jsxs(Button$1,{color:"secondary",variant:"contained",disabled:tt||_e==="loading",onClick:o,children:[jsxRuntimeExports.jsx(default_1$8,{sx:{height:20,width:20}}),"Back"]}),tt&&jsxRuntimeExports.jsx(Button$1,{disabled:j,onClick:s,sx:{mr:1},color:"inherit",children:"Skip"}),et?jsxRuntimeExports.jsxs(LoadingButton$1,{variant:"contained",onClick:$,loading:_e==="loading",sx:{pr:1.5},children:["Done",jsxRuntimeExports.jsx(default_1$6,{sx:{ml:1.1,mt:-.75}})]}):jsxRuntimeExports.jsxs(Button$1,{variant:"contained",disabled:!j,onClick:$,children:["Next",jsxRuntimeExports.jsx(default_1$7,{sx:{ml:1,height:20,width:20}})]})]})})}const batchStateAtom=atom("loading"),buttonsDisabledAtom=atom({left:!1,right:!1,leftEnd:!1,rightEnd:!1}),batchDatesAtom=atom([{value:"2024-05-01",status:"default"},{value:"2024-05-02",status:"default"},{value:"2024-05-03",status:"default"},{value:"2024-05-04",status:"default"},{value:"2024-05-05",status:"default"},{value:"2024-05-06",status:"default"},{value:"2024-05-07",status:"default"},{value:"2024-05-08",status:"default"},{value:"2024-05-09",status:"default"},{value:"2024-05-10",status:"default"},{value:"2024-05-11",status:"default"},{value:"2024-05-12",status:"default"},{value:"2024-05-13",status:"default"},{value:"2024-05-14",status:"default"}]),getDatesAtom=atom(null,async(a,i,o)=>{i(batchStateAtom,"loading");try{const $=a(batchDatesAtom).length>0&&!o.includes("End"),j=o.includes("right")?"left":"right",_e=o==="right"?a(batchDatesAtom).slice(-1)[0]:a(batchDatesAtom)[0],et=_e==null?void 0:_e.value,nt=(await getBatchDates({cursor:$?et:void 0,direction:$?o:j})).data,at=nt.length>0;if((!at||!$)&&(i(buttonsDisabledAtom,it=>({...it,[o]:!0})),o.includes("End")?i(buttonsDisabledAtom,it=>({...it,[o.split("End")[0]]:!0})):i(buttonsDisabledAtom,it=>({...it,[o+"End"]:!0}))),at){const it=o.includes("right")?"left":"right";i(buttonsDisabledAtom,st=>({...st,[it]:!1,[it+"End"]:!1})),i(batchDatesAtom,nt.map(st=>({value:st.value,status:st.status})))}console.log("Loaded dates: ",{records:nt}),i(batchStateAtom,"idle")}catch(s){console.error("Failed to get batch data",s)}}),batchScrapeAtom=atom(null,async(a,i)=>{i(batchStateAtom,"loading");try{const o=a(batchDatesAtom),s=await scrapeBatch(o.map($=>$.value))}catch(o){console.error("Failed to backfill data",o)}});atom(null,async(a,i,{key:o,status:s,count:$})=>{o==="batch"?i(batchStateAtom,"complete"):i(batchDatesAtom,j=>j.map(_e=>_e.value===o?{..._e,status:s,count:$}:_e))});const backfillStateAtom=atom("pending"),dateStartAtom=atom(null),dateEndAtom=atom(null),addDatesAtom=atom(null,async(a,i,o)=>{i(backfillStateAtom,"loading");try{const s=await backfillDates(o),{dateList:$,newCount:j}=s.data;i(setSidebarDataAtom,$),i(backfillStateAtom,"pending"),i(addSnackAtom,{message:`Added ${j} dates`,autoClose:!0}),a(batchStateAtom)==="idle"&&i(getDatesAtom,"rightEnd")}catch(s){console.error("Failed to backfill data",s)}}),DateRangeControl=()=>{const[a,i]=useAtom(dateStartAtom),[o,s]=useAtom(dateEndAtom),$=useAtomValue(backfillStateAtom);useSetAtom(addDatesAtom);const j=useSetAtom(featureDisabledAlertAtom),_e=()=>{a&&o&&j()},et=tt=>{const nt=dayjs(tt.add(20,"days")),at=dayjs(),it=nt.isAfter(at);i(tt),s(it?dayjs().add(-1,"day"):nt)};return jsxRuntimeExports.jsxs(FormControl$1,{required:!0,error:!1,component:"fieldset",variant:"standard",sx:{display:"flex",flexDirection:"column",justifyContent:"center",alignItems:"center",marginBottom:5},children:[jsxRuntimeExports.jsx(LoadingButton$1,{variant:"contained",color:"secondary",disabled:!a||!o,onClick:_e,loading:$==="loading",sx:{marginBottom:4},children:"Load Dates"}),jsxRuntimeExports.jsx(Box$1,{sx:{display:"flex",flexDirection:"row"},children:jsxRuntimeExports.jsxs(LocalizationProvider,{dateAdapter:AdapterDayjs,children:[jsxRuntimeExports.jsx(DatePicker,{label:jsxRuntimeExports.jsx("span",{style:{color:"#9e9e9e"},children:"Start Date"}),value:a,disableHighlightToday:!0,disableFuture:!0,maxDate:dayjs().add(-1,"day"),onChange:et}),jsxRuntimeExports.jsx(DatePicker,{disabled:!0,sx:{marginLeft:4},label:jsxRuntimeExports.jsx("span",{style:{color:"#9e9e9e"},children:"End Date"}),value:o,disableFuture:!0,minDate:a||null})]})})]})};var KeyboardArrowLeft={},_interopRequireDefault$4=interopRequireDefaultExports;Object.defineProperty(KeyboardArrowLeft,"__esModule",{value:!0});var default_1$4=KeyboardArrowLeft.default=void 0,_createSvgIcon$4=_interopRequireDefault$4(requireCreateSvgIcon()),_jsxRuntime$4=jsxRuntimeExports;default_1$4=KeyboardArrowLeft.default=(0,_createSvgIcon$4.default)((0,_jsxRuntime$4.jsx)("path",{d:"M15.41 16.59 10.83 12l4.58-4.59L14 6l-6 6 6 6z"}),"KeyboardArrowLeft");var KeyboardDoubleArrowLeft={},_interopRequireDefault$3=interopRequireDefaultExports;Object.defineProperty(KeyboardDoubleArrowLeft,"__esModule",{value:!0});var default_1$3=KeyboardDoubleArrowLeft.default=void 0,_createSvgIcon$3=_interopRequireDefault$3(requireCreateSvgIcon()),_jsxRuntime$3=jsxRuntimeExports;default_1$3=KeyboardDoubleArrowLeft.default=(0,_createSvgIcon$3.default)([(0,_jsxRuntime$3.jsx)("path",{d:"M17.59 18 19 16.59 14.42 12 19 7.41 17.59 6l-6 6z"},"0"),(0,_jsxRuntime$3.jsx)("path",{d:"m11 18 1.41-1.41L7.83 12l4.58-4.59L11 6l-6 6z"},"1")],"KeyboardDoubleArrowLeft");var KeyboardArrowRight={},_interopRequireDefault$2=interopRequireDefaultExports;Object.defineProperty(KeyboardArrowRight,"__esModule",{value:!0});var default_1$2=KeyboardArrowRight.default=void 0,_createSvgIcon$2=_interopRequireDefault$2(requireCreateSvgIcon()),_jsxRuntime$2=jsxRuntimeExports;default_1$2=KeyboardArrowRight.default=(0,_createSvgIcon$2.default)((0,_jsxRuntime$2.jsx)("path",{d:"M8.59 16.59 13.17 12 8.59 7.41 10 6l6 6-6 6z"}),"KeyboardArrowRight");var KeyboardDoubleArrowRight={},_interopRequireDefault$1=interopRequireDefaultExports;Object.defineProperty(KeyboardDoubleArrowRight,"__esModule",{value:!0});var default_1$1=KeyboardDoubleArrowRight.default=void 0,_createSvgIcon$1=_interopRequireDefault$1(requireCreateSvgIcon()),_jsxRuntime$1=jsxRuntimeExports;default_1$1=KeyboardDoubleArrowRight.default=(0,_createSvgIcon$1.default)([(0,_jsxRuntime$1.jsx)("path",{d:"M6.41 6 5 7.41 9.58 12 5 16.59 6.41 18l6-6z"},"0"),(0,_jsxRuntime$1.jsx)("path",{d:"m13 6-1.41 1.41L16.17 12l-4.58 4.59L13 18l6-6z"},"1")],"KeyboardDoubleArrowRight");var HelpOutline={},_interopRequireDefault=interopRequireDefaultExports;Object.defineProperty(HelpOutline,"__esModule",{value:!0});var default_1=HelpOutline.default=void 0,_createSvgIcon=_interopRequireDefault(requireCreateSvgIcon()),_jsxRuntime=jsxRuntimeExports;default_1=HelpOutline.default=(0,_createSvgIcon.default)((0,_jsxRuntime.jsx)("path",{d:"M11 18h2v-2h-2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2m0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4"}),"HelpOutline");const borderColor="#787878",DualListContainer=newStyled(Box$1)(({theme:a})=>({display:"flex",border:`.0005rem solid ${borderColor}`,borderRadius:"8px",width:"fit-content",midWidth:"20rem"})),StyledList=newStyled(List$2)({padding:0,flex:1,"&:not(:last-child)":{borderRight:".0005rem solid #78787875"}}),StyledListItem=newStyled(ListItem$1)(({status:a})=>({backgroundColor:{default:"inherit",scraping:"#FFA500",ranking:"#125EA8",complete:"#008000",error:"#FF0000"}[a],borderBottom:"1px solid #78787875",padding:"2px 12px","&:last-child":{borderBottom:"none"},".MuiTypography-root":{letterSpacing:"3px"}})),BatchTable=()=>{reactExports.useState(0);const a=5,i=useAtomValue(batchDatesAtom),o=useAtomValue(batchStateAtom),s=Math.ceil(20/a);useSetAtom(getDatesAtom);const $=useAtomValue(buttonsDisabledAtom),j=useSetAtom(featureDisabledAlertAtom),_e=at=>()=>{j()},et=i.reduce((at,it,st)=>{const lt=Math.floor(st/s);return at[lt]||(at[lt]=[]),at[lt].push(it),at},[]),tt=at=>{for(let it=0;it<a-1;it++){const st=at[it]?at[it]:[];if(st.length===s)continue;const lt=s-st.length,ct=new Array(lt).fill({status:"default"});st.length?at[it]=st.concat(ct):at.push(ct)}return at},nt=i.length===0||o==="loading";return jsxRuntimeExports.jsxs(Box$1,{sx:{alignItems:"center",display:"flex",justifyContent:"center",flexDirection:"column",width:"100%"},children:[jsxRuntimeExports.jsx(DualListContainer,{children:jsxRuntimeExports.jsxs("div",{className:"flex flex-col",children:[jsxRuntimeExports.jsx("div",{className:"flex",children:et.length>0?tt(et).map((at,it)=>jsxRuntimeExports.jsx(StyledList,{children:at.map((st,lt)=>jsxRuntimeExports.jsx(StyledListItem,{disablePadding:!0,status:st.status,sx:{height:"3em",maxHeight:"3em",width:"14rem"},children:jsxRuntimeExports.jsx(ListItemText$1,{sx:{font:"inherit",textAlign:"center",letterSpacing:"0px"},primary:st.value?formatDate("MM/DD/YYYY")(st.value):""})},`${st.value}-${lt}`))},`list-${it}`)):jsxRuntimeExports.jsx(Box$1,{sx:{display:"flex",justifyContent:"center",alignItems:"center",width:420,height:120,placeSelf:"center",marginBottom:2,textAlign:"center"},children:"No dates to scrape"})}),jsxRuntimeExports.jsxs(Stack$1,{sx:{borderTop:`.0005rem solid ${borderColor}`,px:2,py:1},direction:"row",justifyContent:"space-between",padding:0,className:"",children:[jsxRuntimeExports.jsx(IconButton$1,{onClick:_e(),disabled:nt||$.leftEnd,children:jsxRuntimeExports.jsx(default_1$3,{})}),jsxRuntimeExports.jsx(IconButton$1,{onClick:_e(),disabled:nt||$.left,children:jsxRuntimeExports.jsx(default_1$4,{})}),jsxRuntimeExports.jsxs("span",{className:"mx-4 text-center self-center mr-2",style:{color:"#ffffff89"},children:["Batch Size",jsxRuntimeExports.jsx("strong",{className:"px-3 py-1 ml-3",style:{color:`${colors.palette.text.primary}`,borderRadius:"8px",backgroundColor:"rgba(0,0,0, 0.1)",border:`.1rem solid ${borderColor}`},children:i.length})]}),jsxRuntimeExports.jsx(IconButton$1,{onClick:_e(),disabled:nt||$.right,children:jsxRuntimeExports.jsx(default_1$2,{})}),jsxRuntimeExports.jsx(IconButton$1,{onClick:_e(),disabled:nt||$.rightEnd,children:jsxRuntimeExports.jsx(default_1$1,{})})]})]})}),jsxRuntimeExports.jsx(BatchScrapeButton,{disabled:nt,dates:i})]})},BatchScrapeButton=({disabled:a,dates:i})=>{const o=useAtomValue(batchStateAtom);useSetAtom(batchScrapeAtom);const s=useSetAtom(featureDisabledAlertAtom),$=o==="complete",j=()=>{s()};return jsxRuntimeExports.jsxs("div",{style:{display:"flex",alignItems:"center"},children:[" ",jsxRuntimeExports.jsxs(LoadingButton$1,{variant:"contained",color:$?"success":"primary",disabled:a,onClick:j,loading:o==="loading",sx:{mt:4,mb:2},children:[jsxRuntimeExports.jsx(Tooltip$1,{title:$?"After scraping a date batch take the opportunity to review the papers, starring the ones you find interesting. Occasionally un-star papers you no longer find interesting.":"Scrape and rank papers for dates in batch. This could take a few minutes. We recommend having less than 75 starred papers as it may reduce the time spent ranking papers.",children:jsxRuntimeExports.jsx(default_1,{sx:{mr:1}})}),$?"View Batch":"Scrape batch"]})]})},formatDate=a=>i=>dayjs(i).format(a),BackfillPage=()=>{new URLSearchParams(location.search).get("isNewUser");const i=({key:o,status:s,data:$})=>{o==="batch"&&s==="complete"||s==="error"&&dayjs(o).format("MM/DD/YYYY")};return jsxRuntimeExports.jsxs(PageLayout,{padding:3,children:[jsxRuntimeExports.jsxs(Paper$1,{sx:{my:4,width:"80rem",mx:"auto"},elevation:2,className:"flex flex-col w-full p-12",children:[jsxRuntimeExports.jsx(DateRangeControl,{}),jsxRuntimeExports.jsx(BatchTable,{})]}),jsxRuntimeExports.jsx(SocketListener,{eventName:"date_status",handleEvent:i,id:"batch-scrape"})]})},ErrorFallback=({error:a,resetError:i})=>jsxRuntimeExports.jsxs("div",{role:"alert",children:[jsxRuntimeExports.jsx("p",{children:"Something went wrong:"}),jsxRuntimeExports.jsx("pre",{children:a.message}),jsxRuntimeExports.jsx("button",{onClick:i,children:"Try again"})]}),useErrorBoundary=()=>{const[a,i]=reactExports.useState(null),o=reactExports.useCallback(()=>{i(null)},[]),s=({children:j})=>a?jsxRuntimeExports.jsx(ErrorFallback,{error:a,resetError:o}):j,$=reactExports.useCallback(j=>{i(j)},[]);return{ErrorBoundary:s,handleError:$}},router=createBrowserRouter([{path:"/",element:jsxRuntimeExports.jsx(Layout,{}),children:[{index:!0,element:jsxRuntimeExports.jsx(Navigate,{to:"/calendar"})},{path:"calendar",element:jsxRuntimeExports.jsx(Calendar,{})},{path:"search",element:jsxRuntimeExports.jsx(SearchPage,{})},{path:"onboard",element:jsxRuntimeExports.jsx(OnboardPage,{})},{path:"backfill",element:jsxRuntimeExports.jsx(BackfillPage,{})},{path:"date/:dateId",element:jsxRuntimeExports.jsx(DateEntryPage,{})},{path:"paper/:paperId",element:jsxRuntimeExports.jsx(PaperEntryPage,{})}]},{path:"/404",element:jsxRuntimeExports.jsx("div",{children:"Not Found"})}]),App=()=>{const{ErrorBoundary:a,handleError:i}=useErrorBoundary();return jsxRuntimeExports.jsx(a,{children:jsxRuntimeExports.jsx(ThemeProvider,{theme:colors,children:jsxRuntimeExports.jsx(Provider,{children:jsxRuntimeExports.jsx(RouterProvider,{router})})})})},container=document.getElementById("root"),root=createRoot(container);root.render(jsxRuntimeExports.jsx(App,{}));
