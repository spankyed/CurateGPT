[
  {
    "id": "2405.18208",
    "date": "2024-05-28",
    "title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with\n  Large Language Models",
    "abstract": "  Recent studies have highlighted their proficiency in some simple tasks like\nwriting and coding through various reasoning strategies. However, LLM agents\nstill struggle with tasks that require comprehensive planning, a process that\nchallenges current models and remains a critical research issue. In this study,\nwe concentrate on travel planning, a Multi-Phases planning problem, that\ninvolves multiple interconnected stages, such as outlining, information\ngathering, and planning, often characterized by the need to manage various\nconstraints and uncertainties. Existing reasoning approaches have struggled to\neffectively address this complex task. Our research aims to address this\nchallenge by developing a human-like planning framework for LLM agents, i.e.,\nguiding the LLM agent to simulate various steps that humans take when solving\nMulti-Phases problems. Specifically, we implement several strategies to enable\nLLM agents to generate a coherent outline for each travel query, mirroring\nhuman planning patterns. Additionally, we integrate Strategy Block and\nKnowledge Block into our framework: Strategy Block facilitates information\ncollection, while Knowledge Block provides essential information for detailed\nplanning. Through our extensive experiments, we demonstrate that our framework\nsignificantly improves the planning capabilities of LLM agents, enabling them\nto tackle the travel planning task with improved efficiency and effectiveness.\nOur experimental results showcase the exceptional performance of the proposed\nframework; when combined with GPT-4-Turbo, it attains $10\\times$ the\nperformance gains in comparison to the baseline framework deployed on\nGPT-4-Turbo.\n",
    "authors": "Chengxing Xie; Difan Zou",
    "status": 0,
    "relevancy": 0.6687598875397698,
    "isStarred": 1,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:52:20.960 +00:00"
  },
  {
    "id": "2405.20309",
    "date": "2024-05-30",
    "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
    "abstract": "  Training models to act as agents that can effectively navigate and perform\nactions in a complex environment, such as a web browser, has typically been\nchallenging due to lack of training data. Large language models (LLMs) have\nrecently demonstrated some capability to navigate novel environments as agents\nin a zero-shot or few-shot fashion, purely guided by natural language\ninstructions as prompts. Recent research has also demonstrated LLMs have the\ncapability to exceed their base performance through self-improvement, i.e.\nfine-tuning on data generated by the model itself. In this work, we explore the\nextent to which LLMs can self-improve their performance as agents in\nlong-horizon tasks in a complex environment using the WebArena benchmark. In\nWebArena, an agent must autonomously navigate and perform actions on web pages\nto achieve a specified objective. We explore fine-tuning on three distinct\nsynthetic training data mixtures and achieve a 31\\% improvement in task\ncompletion rate over the base model on the WebArena benchmark through a\nself-improvement procedure. We additionally contribute novel evaluation metrics\nfor assessing the performance, robustness, capabilities, and quality of\ntrajectories of our fine-tuned agent models to a greater degree than simple,\naggregate-level benchmark scores currently used to measure self-improvement.\n",
    "authors": "Ajay Patel; Markus Hofmarcher; Claudiu Leoveanu-Condrei; Marius-Constantin Dinu; Chris Callison-Burch; Sepp Hochreiter",
    "status": 0,
    "relevancy": 0.6260053599812707,
    "isStarred": 1,
    "keywords": null,
    "createdAt": "2024-05-31 04:58:28.951 +00:00",
    "updatedAt": "2024-05-31 07:09:19.690 +00:00"
  },
  {
    "id": "2405.17974",
    "date": "2024-05-28",
    "title": "Recent Trends in Personalized Dialogue Generation: A Review of Datasets,\n  Methodologies, and Evaluations",
    "abstract": "  Enhancing user engagement through personalization in conversational agents\nhas gained significance, especially with the advent of large language models\nthat generate fluent responses. Personalized dialogue generation, however, is\nmultifaceted and varies in its definition -- ranging from instilling a persona\nin the agent to capturing users' explicit and implicit cues. This paper seeks\nto systemically survey the recent landscape of personalized dialogue\ngeneration, including the datasets employed, methodologies developed, and\nevaluation metrics applied. Covering 22 datasets, we highlight benchmark\ndatasets and newer ones enriched with additional features. We further analyze\n17 seminal works from top conferences between 2021-2023 and identify five\ndistinct types of problems. We also shed light on recent progress by LLMs in\npersonalized dialogue generation. Our evaluation section offers a comprehensive\nsummary of assessment facets and metrics utilized in these works. In\nconclusion, we discuss prevailing challenges and envision prospect directions\nfor future research in personalized dialogue generation.\n",
    "authors": "Yi-Pei Chen; Noriki Nishida; Hideki Nakayama; Yuji Matsumoto",
    "status": 0,
    "relevancy": 0.658808673281338,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:52:09.160 +00:00"
  },
  {
    "id": "2405.18110",
    "date": "2024-05-28",
    "title": "Individual Contributions as Intrinsic Exploration Scaffolds for\n  Multi-agent Reinforcement Learning",
    "abstract": "  In multi-agent reinforcement learning (MARL), effective exploration is\ncritical, especially in sparse reward environments. Although introducing global\nintrinsic rewards can foster exploration in such settings, it often complicates\ncredit assignment among agents. To address this difficulty, we propose\nIndividual Contributions as intrinsic Exploration Scaffolds (ICES), a novel\napproach to motivate exploration by assessing each agent's contribution from a\nglobal view. In particular, ICES constructs exploration scaffolds with Bayesian\nsurprise, leveraging global transition information during centralized training.\nThese scaffolds, used only in training, help to guide individual agents towards\nactions that significantly impact the global latent state transitions.\nAdditionally, ICES separates exploration policies from exploitation policies,\nenabling the former to utilize privileged global information during training.\nExtensive experiments on cooperative benchmark tasks with sparse rewards,\nincluding Google Research Football (GRF) and StarCraft Multi-agent Challenge\n(SMAC), demonstrate that ICES exhibits superior exploration capabilities\ncompared with baselines. The code is publicly available at\nhttps://github.com/LXXXXR/ICES.\n",
    "authors": "Xinran Li; Zifan Liu; Shibo Chen; Jun Zhang",
    "status": 0,
    "relevancy": 0.610816543969512,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.18118",
    "date": "2024-05-28",
    "title": "An approach to improve agent learning via guaranteeing goal reaching in\n  all episodes",
    "abstract": "  Reinforcement learning is commonly concerned with problems of maximizing\naccumulated rewards in Markov decision processes. Oftentimes, a certain goal\nstate or a subset of the state space attain maximal reward. In such a case, the\nenvironment may be considered solved when the goal is reached. Whereas numerous\ntechniques, learning or non-learning based, exist for solving environments,\ndoing so optimally is the biggest challenge. Say, one may choose a reward rate\nwhich penalizes the action effort. Reinforcement learning is currently among\nthe most actively developed frameworks for solving environments optimally by\nvirtue of maximizing accumulated reward, in other words, returns. Yet, tuning\nagents is a notoriously hard task as reported in a series of works. Our aim\nhere is to help the agent learn a near-optimal policy efficiently while\nensuring a goal reaching property of some basis policy that merely solves the\nenvironment. We suggest an algorithm, which is fairly flexible, and can be used\nto augment practically any agent as long as it comprises of a critic. A formal\nproof of a goal reaching property is provided. Simulation experiments on six\nproblems under five agents, including the benchmarked one, provided an\nempirical evidence that the learning can indeed be boosted while ensuring goal\nreaching property.\n",
    "authors": "Pavel Osinenko; Grigory Yaremenko; Georgiy Malaniya; Anton Bolychev",
    "status": 0,
    "relevancy": 0.599505902476585,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.17243",
    "date": "2024-05-27",
    "title": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement\n  Learning",
    "abstract": "  Both entropy-minimizing and entropy-maximizing (curiosity) objectives for\nunsupervised reinforcement learning (RL) have been shown to be effective in\ndifferent environments, depending on the environment's level of natural\nentropy. However, neither method alone results in an agent that will\nconsistently learn intelligent behavior across environments. In an effort to\nfind a single entropy-based method that will encourage emergent behaviors in\nany environment, we propose an agent that can adapt its objective online,\ndepending on the entropy conditions by framing the choice as a multi-armed\nbandit problem. We devise a novel intrinsic feedback signal for the bandit,\nwhich captures the agent's ability to control the entropy in its environment.\nWe demonstrate that such agents can learn to control entropy and exhibit\nemergent behaviors in both high- and low-entropy regimes and can learn skillful\nbehaviors in benchmark tasks. Videos of the trained agents and summarized\nfindings can be found on our project page\nhttps://sites.google.com/view/surprise-adaptive-agents\n",
    "authors": "Adriana Hugessen; Roger Creus Castanyer; Faisal Mohamed; Glen Berseth",
    "status": 0,
    "relevancy": 0.5958386501330759,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.16751",
    "date": "2024-05-27",
    "title": "LLM-Based Cooperative Agents using Information Relevance and Plan\n  Validation",
    "abstract": "  We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by interacting with a 3D scene and cooperating with decentralized\nagents under complex partial observations. This involves managing communication\ncosts and optimizing interaction trajectories in dynamic environments. Our\nresearch focuses on three primary limitations of existing cooperative agent\nsystems. Firstly, current systems demonstrate inefficiency in managing acquired\ninformation through observation, resulting in declining planning performance as\nthe environment becomes more complex with additional objects or goals.\nSecondly, the neglect of false plans in partially observable settings leads to\nsuboptimal cooperative performance, as agents struggle to adapt to\nenvironmental changes influenced by the unseen actions of other agents. Lastly,\nthe failure to incorporate spatial data into decision-making processes\nrestricts the agent's ability to construct optimized trajectories. To overcome\nthese limitations, we propose the RElevance and Validation-Enhanced Cooperative\nLanguage Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.\nREVECA leverages relevance assessment, plan validation, and spatial information\nto enhance the efficiency and robustness of agent cooperation in dynamic and\npartially observable environments while minimizing continuous communication\ncosts and effectively managing irrelevant dummy objects. Our extensive\nexperiments demonstrate the superiority of REVECA over previous approaches,\nincluding those driven by GPT-4.0. Additionally, a user study highlights\nREVECA's potential for achieving trustworthy human-AI cooperation. We expect\nthat REVECA will have significant applications in gaming, XR applications,\neducational tools, and humanoid robots, contributing to substantial economic,\ncommercial, and academic advancements.\n",
    "authors": "SeungWon Seo; Junhyeok Lee; SeongRae Noh; HyeongYeop Kang",
    "status": 0,
    "relevancy": 0.5720936333177186,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.17009",
    "date": "2024-05-27",
    "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
    "abstract": "  Decision making demands intricate interplay between perception, memory, and\nreasoning to discern optimal policies. Conventional approaches to decision\nmaking face challenges related to low sample efficiency and poor\ngeneralization. In contrast, foundation models in language and vision have\nshowcased rapid adaptation to diverse new tasks. Therefore, we advocate for the\nconstruction of foundation agents as a transformative shift in the learning\nparadigm of agents. This proposal is underpinned by the formulation of\nfoundation agents with their fundamental characteristics and challenges\nmotivated by the success of large language models (LLMs). Moreover, we specify\nthe roadmap of foundation agents from large interactive data collection or\ngeneration, to self-supervised pretraining and adaptation, and knowledge and\nvalue alignment with LLMs. Lastly, we pinpoint critical research questions\nderived from the formulation and delineate trends for foundation agents\nsupported by real-world use cases, addressing both technical and theoretical\naspects to propel the field towards a more comprehensive and impactful future.\n",
    "authors": "Xiaoqian Liu; Xingzhou Lou; Jianbin Jiao; Junge Zhang",
    "status": 0,
    "relevancy": 0.5700282258642639,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.20318",
    "date": "2024-05-30",
    "title": "CausalQuest: Collecting Natural Causal Questions for AI Agents",
    "abstract": "  Humans have an innate drive to seek out causality. Whether fuelled by\ncuriosity or specific goals, we constantly question why things happen, how they\nare interconnected, and many other related phenomena. To develop AI agents\ncapable of addressing this natural human quest for causality, we urgently need\na comprehensive dataset of natural causal questions. Unfortunately, existing\ndatasets either contain only artificially-crafted questions that do not reflect\nreal AI usage scenarios or have limited coverage of questions from specific\nsources. To address this gap, we present CausalQuest, a dataset of 13,500\nnaturally occurring questions sourced from social networks, search engines, and\nAI assistants. We formalize the definition of causal questions and establish a\ntaxonomy for finer-grained classification. Through a combined effort of human\nannotators and large language models (LLMs), we carefully label the dataset. We\nfind that 42% of the questions humans ask are indeed causal, with the majority\nseeking to understand the causes behind given effects. Using this dataset, we\ntrain efficient classifiers (up to 2.85B parameters) for the binary task of\nidentifying causal questions, achieving high performance with F1 scores of up\nto 0.877. We conclude with a rich set of future research directions that can\nbuild upon our data and models.\n",
    "authors": "Roberto Ceraolo; Dmitrii Kharlapenko; Amélie Reymond; Rada Mihalcea; Mrinmaya Sachan; Bernhard Schölkopf; Zhijing Jin",
    "status": 0,
    "relevancy": 0.5681685308949704,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:58:28.951 +00:00",
    "updatedAt": "2024-05-31 04:58:28.951 +00:00"
  },
  {
    "id": "2405.20189",
    "date": "2024-05-30",
    "title": "Nadine: An LLM-driven Intelligent Social Robot with Affective\n  Capabilities and Human-like Memory",
    "abstract": "  In this work, we describe our approach to developing an intelligent and\nrobust social robotic system for the Nadine social robot platform. We achieve\nthis by integrating Large Language Models (LLMs) and skilfully leveraging the\npowerful reasoning and instruction-following capabilities of these types of\nmodels to achieve advanced human-like affective and cognitive capabilities.\nThis approach is novel compared to the current state-of-the-art LLM-based\nagents which do not implement human-like long-term memory or sophisticated\nemotional appraisal. The naturalness of social robots, consisting of multiple\nmodules, highly depends on the performance and capabilities of each component\nof the system and the seamless integration of the components. We built a social\nrobot system that enables generating appropriate behaviours through multimodal\ninput processing, bringing episodic memories accordingly to the recognised\nuser, and simulating the emotional states of the robot induced by the\ninteraction with the human partner. In particular, we introduce an LLM-agent\nframe for social robots, SoR-ReAct, serving as a core component for the\ninteraction module in our system. This design has brought forth the advancement\nof social robots and aims to increase the quality of human-robot interaction.\n",
    "authors": "Hangyeol Kang; Maher Ben Moussa; Nadia Magnenat-Thalmann",
    "status": 0,
    "relevancy": 0.5658114955122293,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:58:28.951 +00:00",
    "updatedAt": "2024-05-31 04:58:28.951 +00:00"
  },
  {
    "id": "2405.18123",
    "date": "2024-05-28",
    "title": "PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning",
    "abstract": "  Modern Tabletop Games present various interesting challenges for Multi-agent\nReinforcement Learning. In this paper, we introduce PyTAG, a new framework that\nsupports interacting with a large collection of games implemented in the\nTabletop Games framework. In this work we highlight the challenges tabletop\ngames provide, from a game-playing agent perspective, along with the\nopportunities they provide for future research. Additionally, we highlight the\ntechnical challenges that involve training Reinforcement Learning agents on\nthese games. To explore the Multi-agent setting provided by PyTAG we train the\npopular Proximal Policy Optimisation Reinforcement Learning algorithm using\nself-play on a subset of games and evaluate the trained policies against some\nsimple agents and Monte-Carlo Tree Search implemented in the Tabletop Games\nframework.\n",
    "authors": "Martin Balla; George E. M. Long; James Goodman; Raluca D. Gaina; Diego Perez-Liebana",
    "status": 0,
    "relevancy": 0.5615817257458973,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.17287",
    "date": "2024-05-27",
    "title": "Opinion-Guided Reinforcement Learning",
    "abstract": "  Human guidance is often desired in reinforcement learning to improve the\nperformance of the learning agent. However, human insights are often mere\nopinions and educated guesses rather than well-formulated arguments. While\nopinions are subject to uncertainty, e.g., due to partial informedness or\nignorance about a problem, they also emerge earlier than hard evidence could be\nproduced. Thus, guiding reinforcement learning agents through opinions offers\nthe potential for more performant learning processes, but comes with the\nchallenge of modeling and managing opinions in a formal way. In this article,\nwe present a method to guide reinforcement learning agents through opinions. To\nthis end, we provide an end-to-end method to model and manage advisors'\nopinions. To assess the utility of the approach, we evaluate it with synthetic\nand human advisors, at different levels of uncertainty, and under multiple\nadvise strategies. Our results indicate that opinions, even if uncertain,\nimprove the performance of reinforcement learning agents, resulting in higher\nrewards, more efficient exploration, and a better reinforced policy. Although\nwe demonstrate our approach in a simplified topological running example, our\napproach is applicable to complex problems with higher dimensions as well.\n",
    "authors": "Kyanna Dagenais; Istvan David",
    "status": 0,
    "relevancy": 0.5491430684706172,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.18650",
    "date": "2024-05-28",
    "title": "Approximating Human Models During Argumentation-based Dialogues",
    "abstract": "  Explainable AI Planning (XAIP) aims to develop AI agents that can effectively\nexplain their decisions and actions to human users, fostering trust and\nfacilitating human-AI collaboration. A key challenge in XAIP is model\nreconciliation, which seeks to align the mental models of AI agents and humans.\nWhile existing approaches often assume a known and deterministic human model,\nthis simplification may not capture the complexities and uncertainties of\nreal-world interactions. In this paper, we propose a novel framework that\nenables AI agents to learn and update a probabilistic human model through\nargumentation-based dialogues. Our approach incorporates trust-based and\ncertainty-based update mechanisms, allowing the agent to refine its\nunderstanding of the human's mental state based on the human's expressed trust\nin the agent's arguments and certainty in their own arguments. We employ a\nprobability weighting function inspired by prospect theory to capture the\nrelationship between trust and perceived probability, and use a Bayesian\napproach to update the agent's probability distribution over possible human\nmodels. We conduct a human-subject study to empirically evaluate the\neffectiveness of our approach in an argumentation scenario, demonstrating its\nability to capture the dynamics of human belief formation and adaptation.\n",
    "authors": "Yinxu Tang; Stylianos Loukas Vasileiou; William Yeoh",
    "status": 0,
    "relevancy": 0.5392468235525845,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.18688",
    "date": "2024-05-29",
    "title": "Efficient Preference-based Reinforcement Learning via Aligned Experience\n  Estimation",
    "abstract": "  Preference-based reinforcement learning (PbRL) has shown impressive\ncapabilities in training agents without reward engineering. However, a notable\nlimitation of PbRL is its dependency on substantial human feedback. This\ndependency stems from the learning loop, which entails accurate reward learning\ncompounded with value/policy learning, necessitating a considerable number of\nsamples. To boost the learning loop, we propose SEER, an efficient PbRL method\nthat integrates label smoothing and policy regularization techniques. Label\nsmoothing reduces overfitting of the reward model by smoothing human preference\nlabels. Additionally, we bootstrap a conservative estimate $\\widehat{Q}$ using\nwell-supported state-action pairs from the current replay memory to mitigate\noverestimation bias and utilize it for policy learning regularization. Our\nexperimental results across a variety of complex tasks, both in online and\noffline settings, demonstrate that our approach improves feedback efficiency,\noutperforming state-of-the-art methods by a large margin. Ablation studies\nfurther reveal that SEER achieves a more accurate Q-function compared to prior\nwork.\n",
    "authors": "Fengshuo Bai; Rui Zhao; Hongming Zhang; Sijia Cui; Ying Wen; Yaodong Yang; Bo Xu; Lei Han",
    "status": 0,
    "relevancy": 0.5366568000368498,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 05:21:04.773 +00:00",
    "updatedAt": "2024-05-31 05:21:04.773 +00:00"
  },
  {
    "id": "2405.18092",
    "date": "2024-05-28",
    "title": "LLM experiments with simulation: Large Language Model Multi-Agent System\n  for Process Simulation Parametrization in Digital Twins",
    "abstract": "  This paper presents a novel design of a multi-agent system framework that\napplies a large language model (LLM) to automate the parametrization of process\nsimulations in digital twins. We propose a multi-agent framework that includes\nfour types of agents: observation, reasoning, decision and summarization. By\nenabling dynamic interaction between LLM agents and simulation model, the\ndeveloped system can automatically explore the parametrization of the\nsimulation and use heuristic reasoning to determine a set of parameters to\ncontrol the simulation to achieve an objective. The proposed approach enhances\nthe simulation model by infusing it with heuristics from LLM and enables\nautonomous search for feasible parametrization to solve a user task.\nFurthermore, the system has the potential to increase user-friendliness and\nreduce the cognitive load on human users by assisting in complex\ndecision-making processes. The effectiveness and functionality of the system\nare demonstrated through a case study, and the visualized demos are available\nat a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation\n",
    "authors": "Yuchen Xia; Daniel Dittler; Nasser Jazdi; Haonan Chen; Michael Weyrich",
    "status": 0,
    "relevancy": 0.5365061176244996,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.18721",
    "date": "2024-05-29",
    "title": "Correctable Landmark Discovery via Large Models for Vision-Language\n  Navigation",
    "abstract": "  Vision-Language Navigation (VLN) requires the agent to follow language\ninstructions to reach a target position. A key factor for successful navigation\nis to align the landmarks implied in the instruction with diverse visual\nobservations. However, previous VLN agents fail to perform accurate modality\nalignment especially in unexplored scenes, since they learn from limited\nnavigation data and lack sufficient open-world alignment knowledge. In this\nwork, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via\nLarge ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential\nlandmark discovery problem, by introducing a novel correctable landmark\ndiscovery scheme based on two large models ChatGPT and CLIP. Specifically, we\nuse ChatGPT to provide rich open-world landmark cooccurrence commonsense, and\nconduct CLIP-driven landmark discovery based on these commonsense priors. To\nmitigate the noise in the priors due to the lack of visual constraints, we\nintroduce a learnable cooccurrence scoring module, which corrects the\nimportance of each cooccurrence according to actual observations for accurate\nlandmark discovery. We further design an observation enhancement strategy for\nan elegant combination of our framework with different VLN agents, where we\nutilize the corrected landmark features to obtain enhanced observation features\nfor action decision. Extensive experimental results on multiple popular VLN\nbenchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE\nover strong baselines. Especially, our CONSOLE establishes the new\nstate-of-the-art results on R2R and R4R in unseen scenarios. Code is available\nat https://github.com/expectorlin/CONSOLE.\n",
    "authors": "Bingqian Lin; Yunshuang Nie; Ziming Wei; Yi Zhu; Hang Xu; Shikui Ma; Jianzhuang Liu; Xiaodan Liang",
    "status": 0,
    "relevancy": 0.5362481468861078,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 05:21:04.773 +00:00",
    "updatedAt": "2024-05-31 05:21:04.773 +00:00"
  },
  {
    "id": "2405.19946",
    "date": "2024-05-30",
    "title": "Learning to Discuss Strategically: A Case Study on One Night Ultimate\n  Werewolf",
    "abstract": "  Communication is a fundamental aspect of human society, facilitating the\nexchange of information and beliefs among people. Despite the advancements in\nlarge language models (LLMs), recent agents built with these often neglect the\ncontrol over discussion tactics, which are essential in communication scenarios\nand games. As a variant of the famous communication game Werewolf, One Night\nUltimate Werewolf (ONUW) requires players to develop strategic discussion\npolicies due to the potential role changes that increase the uncertainty and\ncomplexity of the game. In this work, we first present the existence of the\nPerfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with\ndiscussion and one without. The results showcase that the discussion greatly\nchanges players' utilities by affecting their beliefs, emphasizing the\nsignificance of discussion tactics. Based on the insights obtained from the\nanalyses, we propose an RL-instructed language agent framework, where a\ndiscussion policy trained by reinforcement learning (RL) is employed to\ndetermine appropriate discussion tactics to adopt. Our experimental results on\nseveral ONUW game settings demonstrate the effectiveness and generalizability\nof our proposed framework.\n",
    "authors": "Xuanfa Jin; Ziyan Wang; Yali Du; Meng Fang; Haifeng Zhang; Jun Wang",
    "status": 0,
    "relevancy": 0.5235238922992325,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:58:28.951 +00:00",
    "updatedAt": "2024-05-31 04:58:28.951 +00:00"
  },
  {
    "id": "2405.16766",
    "date": "2024-05-27",
    "title": "Reframing the Relationship in Out-of-Distribution Detection",
    "abstract": "  The remarkable achievements of Large Language Models (LLMs) have captivated\nthe attention of both academia and industry, transcending their initial role in\ndialogue generation. The utilization of LLMs as intermediary agents in various\ntasks has yielded promising results, sparking a wave of innovation in\nartificial intelligence. Building on these breakthroughs, we introduce a novel\napproach that integrates the agent paradigm into the Out-of-distribution (OOD)\ndetection task, aiming to enhance its robustness and adaptability. Our proposed\nmethod, Concept Matching with Agent (CMA), employs neutral prompts as agents to\naugment the CLIP-based OOD detection process. These agents function as dynamic\nobservers and communication hubs, interacting with both In-distribution (ID)\nlabels and data inputs to form vector triangle relationships. This triangular\nframework offers a more nuanced approach than the traditional binary\nrelationship, allowing for better separation and identification of ID and OOD\ninputs. Our extensive experimental results showcase the superior performance of\nCMA over both zero-shot and training-required methods in a diverse array of\nreal-world scenarios.\n",
    "authors": "YuXiao Lee; Xiaofeng Cao",
    "status": 0,
    "relevancy": 0.5193883135881757,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.16994",
    "date": "2024-05-27",
    "title": "Vision-and-Language Navigation Generative Pretrained Transformer",
    "abstract": "  In the Vision-and-Language Navigation (VLN) field, agents are tasked with\nnavigating real-world scenes guided by linguistic instructions. Enabling the\nagent to adhere to instructions throughout the process of navigation represents\na significant challenge within the domain of VLN. To address this challenge,\ncommon approaches often rely on encoders to explicitly record past locations\nand actions, increasing model complexity and resource consumption. Our\nproposal, the Vision-and-Language Navigation Generative Pretrained Transformer\n(VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory\nsequence dependencies, bypassing the need for historical encoding modules. This\nmethod allows for direct historical information access through trajectory\nsequence, enhancing efficiency. Furthermore, our model separates the training\nprocess into offline pre-training with imitation learning and online\nfine-tuning with reinforcement learning. This distinction allows for more\nfocused training objectives and improved performance. Performance assessments\non the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art\nencoder-based models.\n",
    "authors": "Wen Hanlin",
    "status": 0,
    "relevancy": 0.5154482457814616,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.19334",
    "date": "2024-05-29",
    "title": "LLMs Meet Multimodal Generation and Editing: A Survey",
    "abstract": "  With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on understanding. This\nsurvey elaborates on multimodal generation across different domains, including\nimage, video, 3D, and audio, where we highlight the notable advancements with\nmilestone works in these fields. Specifically, we exhaustively investigate the\nkey technical components behind methods and multimodal datasets utilized in\nthese studies. Moreover, we dig into tool-augmented multimodal agents that can\nuse existing generative models for human-computer interaction. Lastly, we also\ncomprehensively discuss the advancement in AI safety and investigate emerging\napplications as well as future prospects. Our work provides a systematic and\ninsightful overview of multimodal generation, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n",
    "authors": "Yingqing He; Zhaoyang Liu; Jingye Chen; Zeyue Tian; Hongyu Liu; Xiaowei Chi; Runtao Liu; Ruibin Yuan; Yazhou Xing; Wenhai Wang; Jifeng Dai; Yong Zhang; Wei Xue; Qifeng Liu; Yike Guo; Qifeng Chen",
    "status": 0,
    "relevancy": 0.5151688872910891,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 05:21:04.773 +00:00",
    "updatedAt": "2024-05-31 05:21:04.773 +00:00"
  },
  {
    "id": "2405.17631",
    "date": "2024-05-27",
    "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation\n  Experiments",
    "abstract": "  Agents based on large language models have shown great potential in\naccelerating scientific discovery by leveraging their rich background knowledge\nand reasoning capabilities. Here, we develop BioDiscoveryAgent, an agent that\ndesigns new experiments, reasons about their outcomes, and efficiently\nnavigates the hypothesis space to reach desired solutions. We demonstrate our\nagent on the problem of designing genetic perturbation experiments, where the\naim is to find a small subset out of many possible genes that, when perturbed,\nresult in a specific phenotype (e.g., cell growth). Utilizing its biological\nknowledge, BioDiscoveryAgent can uniquely design new experiments without the\nneed to train a machine learning model or explicitly design an acquisition\nfunction. Moreover, BioDiscoveryAgent achieves an average of 18% improvement in\ndetecting desired phenotypes across five datasets, compared to existing\nBayesian optimization baselines specifically trained for this task. Our\nevaluation includes one dataset that is unpublished, ensuring it is not part of\nthe language model's training data. Additionally, BioDiscoveryAgent predicts\ngene combinations to perturb twice as accurately as a random baseline, a task\nso far not explored in the context of closed-loop experiment design. The agent\nalso has access to tools for searching the biomedical literature, executing\ncode to analyze biological datasets, and prompting another agent to critically\nevaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every\nstage, representing an accessible new paradigm in the computational design of\nbiological experiments with the potential to augment scientists' capabilities.\n",
    "authors": "Yusuf Roohani; Jian Vora; Qian Huang; Zachary Steinhart; Alexander Marson; Percy Liang; Jure Leskovec",
    "status": 0,
    "relevancy": 0.510634061615632,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.17691",
    "date": "2024-05-27",
    "title": "Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and\n  Partially Observable Environments",
    "abstract": "  Agents, whether software or hardware, perceive their environment through\nsensors and act using actuators, often operating in dynamic, partially\nobservable settings. They face challenges like incomplete and noisy data,\nunforeseen situations, and the need to adapt goals in real-time. Traditional\nreasoning and ML methods, including Reinforcement Learning (RL), help but are\nlimited by data needs, predefined goals, and extensive exploration periods.\nOntologies offer a solution by integrating diverse information sources,\nenhancing decision-making in complex environments. This thesis introduces an\nontology-enhanced decision-making model (OntoDeM) for autonomous agents.\nOntoDeM enriches agents' domain knowledge, allowing them to interpret\nunforeseen events, generate or adapt goals, and make better decisions. Key\ncontributions include: 1. An ontology-based method to improve agents' real-time\nobservations using prior knowledge. 2. The OntoDeM model for handling dynamic,\nunforeseen situations by evolving or generating new goals. 3. Implementation\nand evaluation in four real-world applications, demonstrating its\neffectiveness. Compared to traditional and advanced learning algorithms,\nOntoDeM shows superior performance in improving agents' observations and\ndecision-making in dynamic, partially observable environments.\n",
    "authors": "Saeedeh Ghanadbashi; Fatemeh Golpayegani",
    "status": 0,
    "relevancy": 0.5103616698161793,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.18917",
    "date": "2024-05-29",
    "title": "Causal Action Influence Aware Counterfactual Data Augmentation",
    "abstract": "  Offline data are both valuable and practical resources for teaching robots\ncomplex behaviors. Ideally, learning agents should not be constrained by the\nscarcity of available demonstrations, but rather generalize beyond the training\ndistribution. However, the complexity of real-world scenarios typically\nrequires huge amounts of data to prevent neural network policies from picking\nup on spurious correlations and learning non-causal relationships. We propose\nCAIAC, a data augmentation method that can create feasible synthetic\ntransitions from a fixed dataset without having access to online environment\ninteractions. By utilizing principled methods for quantifying causal influence,\nwe are able to perform counterfactual reasoning by swapping\n$\\it{action}$-unaffected parts of the state-space between independent\ntrajectories in the dataset. We empirically show that this leads to a\nsubstantial increase in robustness of offline learning algorithms against\ndistributional shift.\n",
    "authors": "Núria Armengol Urpí; Marco Bagatella; Marin Vlastelica; Georg Martius",
    "status": 0,
    "relevancy": 0.5087991529676509,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 05:21:04.773 +00:00",
    "updatedAt": "2024-05-31 05:21:04.773 +00:00"
  },
  {
    "id": "2405.18180",
    "date": "2024-05-28",
    "title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive\n  Shielding",
    "abstract": "  Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical impediment towards deploying RL agents in many\nreal-world scenarios. Training RL agents in unknown, black-box environments\nposes an even greater safety risk when prior knowledge of the domain/task is\nunavailable. We introduce ADVICE (Adaptive Shielding with a Contrastive\nAutoencoder), a novel post-shielding technique that distinguishes safe and\nunsafe features of state-action pairs during training, thus protecting the RL\nagent from executing actions that yield potentially hazardous outcomes. Our\ncomprehensive experimental evaluation against state-of-the-art safe RL\nexploration techniques demonstrates how ADVICE can significantly reduce safety\nviolations during training while maintaining a competitive outcome reward.\n",
    "authors": "Daniel Bethell; Simos Gerasimou; Radu Calinescu; Calum Imrie",
    "status": 0,
    "relevancy": 0.5066017536340098,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.16899",
    "date": "2024-05-27",
    "title": "Partial Models for Building Adaptive Model-Based Reinforcement Learning\n  Agents",
    "abstract": "  In neuroscience, one of the key behavioral tests for determining whether a\nsubject of study exhibits model-based behavior is to study its adaptiveness to\nlocal changes in the environment. In reinforcement learning, however, recent\nstudies have shown that modern model-based agents display poor adaptivity to\nsuch changes. The main reason for this is that modern agents are typically\ndesigned to improve sample efficiency in single task settings and thus do not\ntake into account the challenges that can arise in other settings. In local\nadaptation settings, one particularly important challenge is in quickly\nbuilding and maintaining a sufficiently accurate model after a local change.\nThis is challenging for deep model-based agents as their models and replay\nbuffers are monolithic structures lacking distribution shift handling\ncapabilities. In this study, we show that the conceptually simple idea of\npartial models can allow deep model-based agents to overcome this challenge and\nthus allow for building locally adaptive model-based agents. By modeling the\ndifferent parts of the state space through different models, the agent can not\nonly maintain a model that is accurate across the state space, but it can also\nquickly adapt it in the presence of a local change in the environment. We\ndemonstrate this by showing that the use of partial models in agents such as\ndeep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the\nlocal changes in their environments.\n",
    "authors": "Safa Alver; Ali Rahimi-Kalahroudi; Doina Precup",
    "status": 0,
    "relevancy": 0.49376953607242435,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.19047",
    "date": "2024-05-29",
    "title": "Statistical Context Detection for Deep Lifelong Reinforcement Learning",
    "abstract": "  Context detection involves labeling segments of an online stream of data as\nbelonging to different tasks. Task labels are used in lifelong learning\nalgorithms to perform consolidation or other procedures that prevent\ncatastrophic forgetting. Inferring task labels from online experiences remains\na challenging problem. Most approaches assume finite and low-dimension\nobservation spaces or a preliminary training phase during which task labels are\nlearned. Moreover, changes in the transition or reward functions can be\ndetected only in combination with a policy, and therefore are more difficult to\ndetect than changes in the input distribution. This paper presents an approach\nto learning both policies and labels in an online deep reinforcement learning\nsetting. The key idea is to use distance metrics, obtained via optimal\ntransport methods, i.e., Wasserstein distance, on suitable latent action-reward\nspaces to measure distances between sets of data points from past and current\nstreams. Such distances can then be used for statistical tests based on an\nadapted Kolmogorov-Smirnov calculation to assign labels to sequences of\nexperiences. A rollback procedure is introduced to learn multiple policies by\nensuring that only the appropriate data is used to train the corresponding\npolicy. The combination of task detection and policy deployment allows for the\noptimization of lifelong reinforcement learning agents without an oracle that\nprovides task labels. The approach is tested using two benchmarks and the\nresults show promising performance when compared with related context detection\nalgorithms. The results suggest that optimal transport statistical methods\nprovide an explainable and justifiable procedure for online context detection\nand reward optimization in lifelong reinforcement learning.\n",
    "authors": "Jeffery Dick; Saptarshi Nath; Christos Peridis; Eseoghene Benjamin; Soheil Kolouri; Andrea Soltoggio",
    "status": 0,
    "relevancy": 0.48755249325037087,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 05:21:04.773 +00:00",
    "updatedAt": "2024-05-31 05:21:04.773 +00:00"
  },
  {
    "id": "2405.16946",
    "date": "2024-05-27",
    "title": "Biological Neurons Compete with Deep Reinforcement Learning in Sample\n  Efficiency in a Simulated Gameworld",
    "abstract": "  How do biological systems and machine learning algorithms compare in the\nnumber of samples required to show significant improvements in completing a\ntask? We compared the learning efficiency of in vitro biological neural\nnetworks to the state-of-the-art deep reinforcement learning (RL) algorithms in\na simplified simulation of the game `Pong'. Using DishBrain, a system that\nembodies in vitro neural networks with in silico computation using a\nhigh-density multi-electrode array, we contrasted the learning rate and the\nperformance of these biological systems against time-matched learning from\nthree state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same\ngame environment. This allowed a meaningful comparison between biological\nneural systems and deep RL. We find that when samples are limited to a\nreal-world time course, even these very simple biological cultures outperformed\ndeep RL algorithms across various game performance characteristics, implying a\nhigher sample efficiency. Ultimately, even when tested across multiple types of\ninformation input to assess the impact of higher dimensional data input,\nbiological neurons showcased faster learning than all deep reinforcement\nlearning agents.\n",
    "authors": "Moein Khajehnejad; Forough Habibollahi; Aswin Paul; Adeel Razi; Brett J. Kagan",
    "status": 0,
    "relevancy": 0.4771512731192644,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.17372",
    "date": "2024-05-27",
    "title": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with\n  Next-Patch Prediction",
    "abstract": "  Simulating realistic interactions among traffic agents is crucial for\nefficiently validating the safety of autonomous driving systems. Existing\nleading simulators primarily use an encoder-decoder structure to encode the\nhistorical trajectories for future simulation. However, such a paradigm\ncomplicates the model architecture, and the manual separation of history and\nfuture trajectories leads to low data utilization. To address these challenges,\nwe propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a\ndecoder-only, autoregressive architecture designed to simulate the sequential\nmotion of multiple agents. Crucially, our approach discards the traditional\nseparation between \"history\" and \"future,\" treating each time step as the\n\"current\" one, resulting in a simpler, more parameter- and data-efficient\ndesign that scales seamlessly with data and computation. Additionally, we\nintroduce the Next-Patch Prediction Paradigm (NP3), which enables models to\nreason at the patch level of trajectories and capture long-range\nspatial-temporal interactions. BehaviorGPT ranks first across several metrics\non the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in\nmulti-agent and agent-map interactions. We outperformed state-of-the-art models\nwith a realism score of 0.741 and improved the minADE metric to 1.540, with an\napproximately 91.6% reduction in model parameters.\n",
    "authors": "Zikang Zhou; Haibo Hu; Xinhong Chen; Jianping Wang; Nan Guan; Kui Wu; Yung-Hui Li; Yu-Kai Huang; Chun Jason Xue",
    "status": 0,
    "relevancy": 0.46652290848841327,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.18044",
    "date": "2024-05-28",
    "title": "Cognitive Insights and Stable Coalition Matching for Fostering\n  Multi-Agent Cooperation",
    "abstract": "  Cognitive abilities, such as Theory of Mind (ToM), play a vital role in\nfacilitating cooperation in human social interactions. However, our study\nreveals that agents with higher ToM abilities may not necessarily exhibit\nbetter cooperative behavior compared to those with lower ToM abilities. To\naddress this challenge, we propose a novel matching coalition mechanism that\nleverages the strengths of agents with different ToM levels by explicitly\nconsidering belief alignment and specialized abilities when forming coalitions.\nOur proposed matching algorithm seeks to find stable coalitions that maximize\nthe potential for cooperative behavior and ensure long-term viability. By\nincorporating cognitive insights into the design of multi-agent systems, our\nwork demonstrates the potential of leveraging ToM to create more sophisticated\nand human-like coordination strategies that foster cooperation and improve\noverall system performance.\n",
    "authors": "Jiaqi Shao; Tianjun Yuan; Tao Lin; Xuanyu Cao; Bing Luo",
    "status": 0,
    "relevancy": 0.46599837101190855,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.16830",
    "date": "2024-05-27",
    "title": "Structured Graph Network for Constrained Robot Crowd Navigation with Low\n  Fidelity Simulation",
    "abstract": "  We investigate the feasibility of deploying reinforcement learning (RL)\npolicies for constrained crowd navigation using a low-fidelity simulator. We\nintroduce a representation of the dynamic environment, separating human and\nobstacle representations. Humans are represented through detected states, while\nobstacles are represented as computed point clouds based on maps and robot\nlocalization. This representation enables RL policies trained in a low-fidelity\nsimulator to deploy in real world with a reduced sim2real gap. Additionally, we\npropose a spatio-temporal graph to model the interactions between agents and\nobstacles. Based on the graph, we use attention mechanisms to capture the\nrobot-human, human-human, and human-obstacle interactions. Our method\nsignificantly improves navigation performance in both simulated and real-world\nenvironments. Video demonstrations can be found at\nhttps://sites.google.com/view/constrained-crowdnav/home.\n",
    "authors": "Shuijing Liu; Kaiwen Hong; Neeloy Chakraborty; Katherine Driggs-Campbell",
    "status": 0,
    "relevancy": 0.45884806849533766,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.19815",
    "date": "2024-05-30",
    "title": "Efficient Stimuli Generation using Reinforcement Learning in Design\n  Verification",
    "abstract": "  The increasing design complexity of System-on-Chips (SoCs) has led to\nsignificant verification challenges, particularly in meeting coverage targets\nwithin a timely manner. At present, coverage closure is heavily dependent on\nconstrained random and coverage driven verification methodologies where the\nrandomized stimuli are bounded to verify certain scenarios and to reach\ncoverage goals. This process is said to be exhaustive and to consume a lot of\nproject time. In this paper, a novel methodology is proposed to generate\nefficient stimuli with the help of Reinforcement Learning (RL) to reach the\nmaximum code coverage of the Design Under Verification (DUV). Additionally, an\nautomated framework is created using metamodeling to generate a SystemVerilog\ntestbench and an RL environment for any given design. The proposed approach is\napplied to various designs and the produced results proves that the RL agent\nprovides effective stimuli to achieve code coverage faster in comparison with\nbaseline random simulations. Furthermore, various RL agents and reward schemes\nare analyzed in our work.\n",
    "authors": "Deepak Narayan Gadde; Thomas Nalapat; Aman Kumar; Djones Lettnin; Wolfgang Kunz; Sebastian Simon",
    "status": 0,
    "relevancy": 0.44928077451128423,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:58:28.951 +00:00",
    "updatedAt": "2024-05-31 04:58:28.951 +00:00"
  },
  {
    "id": "2405.19238",
    "date": "2024-05-29",
    "title": "Explanation-based Belief Revision: Moving Beyond Minimalism to\n  Explanatory Understanding",
    "abstract": "  In belief revision, agents typically modify their beliefs when they receive\nsome new piece of information that is in conflict with them. The guiding\nprinciple behind most belief revision frameworks is that of minimalism, which\nadvocates minimal changes to existing beliefs. However, minimalism may not\nnecessarily capture the nuanced ways in which human agents reevaluate and\nmodify their beliefs. In contrast, the explanatory hypothesis indicates that\npeople are inherently driven to seek explanations for inconsistencies, thereby\nstriving for explanatory coherence rather than minimal changes when revising\nbeliefs. Our contribution in this paper is two-fold. Motivated by the\nexplanatory hypothesis, we first present a novel, yet simple belief revision\noperator that, given a belief base and an explanation for an explanandum, it\nrevises the belief bases in a manner that preserves the explanandum and is not\nnecessarily minimal. We call this operator explanation-based belief revision.\nSecond, we conduct two human-subject studies to empirically validate our\napproach and investigate belief revision behavior in real-world scenarios. Our\nfindings support the explanatory hypothesis and provide insights into the\nstrategies people employ when resolving inconsistencies.\n",
    "authors": "Stylianos Loukas Vasileiou; William Yeoh",
    "status": 0,
    "relevancy": 0.4043755067873307,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 05:21:04.773 +00:00",
    "updatedAt": "2024-05-31 05:21:04.773 +00:00"
  },
  {
    "id": "2405.16887",
    "date": "2024-05-27",
    "title": "A Large Language Model-based multi-agent manufacturing system for\n  intelligent shopfloor",
    "abstract": "  As productivity advances, the demand of customers for multi-variety and\nsmall-batch production is increasing, thereby putting forward higher\nrequirements for manufacturing systems. When production tasks frequent changes\ndue to this demand, traditional manufacturing systems often cannot response\npromptly. The multi-agent manufacturing system is proposed to address this\nproblem. However, because of technical limitations, the negotiation among\nagents in this kind of system is realized through predefined heuristic rules,\nwhich is not intelligent enough to deal with the multi-variety and small batch\nproduction. To this end, a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor is proposed in the present\nstudy. This system delineates the diverse agents and defines their\ncollaborative methods. The roles of the agents encompass Machine Server Agent\n(MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and\nDecision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability\nof analyzing the shopfloor condition and choosing the most suitable machine, as\nopposed to executing a predefined program artificially. The negotiation between\nBAs and BIA is the most crucial step in connecting manufacturing resources.\nWith the support of TA and DA, BIA will finalize the distribution of orders,\nrelying on the information of each machine returned by BA. MSAs bears the\nresponsibility for connecting the agents with the physical shopfloor. This\nsystem aims to distribute and transmit workpieces through the collaboration of\nthe agents with these distinct roles, distinguishing it from other scheduling\napproaches. Comparative experiments were also conducted to validate the\nperformance of this system.\n",
    "authors": "Zhen Zhao; Dunbing Tang; Haihua Zhu; Zequn Zhang; Kai Chen; Changchun Liu; Yuchen Ji",
    "status": 0,
    "relevancy": 0.39624565173825943,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:34.029 +00:00",
    "updatedAt": "2024-05-31 04:51:34.029 +00:00"
  },
  {
    "id": "2405.17746",
    "date": "2024-05-28",
    "title": "Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective",
    "abstract": "  Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks,\nposing concerning threats to their reliable deployment. Recent research reveals\nthat backdoors can be erased from infected DNNs by pruning a specific group of\nneurons, while how to effectively identify and remove these backdoor-associated\nneurons remains an open challenge. Most of the existing defense methods rely on\ndefined rules and focus on neuron's local properties, ignoring the exploration\nand optimization of pruning policies. To address this gap, we propose an\nOptimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN)\nand Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP\nfirst models the target DNN as graphs based on neuron connectivity, and then\nuses GNN-based RL agents to learn graph embeddings and find a suitable pruning\npolicy. To the best of our knowledge, this is the first attempt to employ GNN\nand RL for optimizing pruning policies in the field of backdoor defense.\nExperiments show, with a small amount of clean data, ONP can effectively prune\nthe backdoor neurons implanted by a set of backdoor attacks at the cost of\nnegligible performance degradation, achieving a new state-of-the-art\nperformance for backdoor mitigation.\n",
    "authors": "Nan Li; Haiyang Yu; Ping Yi",
    "status": 0,
    "relevancy": 0.3860347237446391,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.18300",
    "date": "2024-05-28",
    "title": "CompetEvo: Towards Morphological Evolution from Competition",
    "abstract": "  Training an agent to adapt to specific tasks through co-optimization of\nmorphology and control has widely attracted attention. However, whether there\nexists an optimal configuration and tactics for agents in a multiagent\ncompetition scenario is still an issue that is challenging to definitively\nconclude. In this context, we propose competitive evolution (CompetEvo), which\nco-evolves agents' designs and tactics in confrontation. We build arenas\nconsisting of three animals and their evolved derivatives, placing agents with\ndifferent morphologies in direct competition with each other. The results\nreveal that our method enables agents to evolve a more suitable design and\nstrategy for fighting compared to fixed-morph agents, allowing them to obtain\nadvantages in combat scenarios. Moreover, we demonstrate the amazing and\nimpressive behaviors that emerge when confrontations are conducted under\nasymmetrical morphs.\n",
    "authors": "Kangyao Huang; Di Guo; Xinyu Zhang; Xiangyang Ji; Huaping Liu",
    "status": 0,
    "relevancy": 0.3731645532957174,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:51:45.387 +00:00",
    "updatedAt": "2024-05-31 04:51:45.387 +00:00"
  },
  {
    "id": "2405.19743",
    "date": "2024-05-30",
    "title": "May the Dance be with You: Dance Generation Framework for Non-Humanoids",
    "abstract": "  We hypothesize dance as a motion that forms a visual rhythm from music, where\nthe visual rhythm can be perceived from an optical flow. If an agent can\nrecognize the relationship between visual rhythm and music, it will be able to\ndance by generating a motion to create a visual rhythm that matches the music.\nBased on this, we propose a framework for any kind of non-humanoid agents to\nlearn how to dance from human videos. Our framework works in two processes: (1)\ntraining a reward model which perceives the relationship between optical flow\n(visual rhythm) and music from human dance videos, (2) training the\nnon-humanoid dancer based on that reward model, and reinforcement learning. Our\nreward model consists of two feature encoders for optical flow and music. They\nare trained based on contrastive learning which makes the higher similarity\nbetween concurrent optical flow and music features. With this reward model, the\nagent learns dancing by getting a higher reward when its action creates an\noptical flow whose feature has a higher similarity with the given music\nfeature. Experiment results show that generated dance motion can align with the\nmusic beat properly, and user study result indicates that our framework is more\npreferred by humans compared to the baselines. To the best of our knowledge,\nour work of non-humanoid agents which learn dance from human videos is\nunprecedented. An example video can be found at https://youtu.be/dOUPvo-O3QY.\n",
    "authors": "Hyemin Ahn",
    "status": 0,
    "relevancy": 0.35629470656024,
    "isStarred": 0,
    "keywords": null,
    "createdAt": "2024-05-31 04:58:28.951 +00:00",
    "updatedAt": "2024-05-31 04:58:28.951 +00:00"
  }
]